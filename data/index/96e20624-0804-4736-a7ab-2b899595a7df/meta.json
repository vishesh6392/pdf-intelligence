[
  {
    "vector_id": 0,
    "chunk_id": "p1_c0",
    "page_number": 1,
    "text": "MATHEMATICS  FOR \nMACHINE LEAR NING\nMarc Peter Deisenroth\nA. Aldo Faisal\nCheng Soon On g\nMATHEMATICS FOR MACHINE LEARNING DEISENROTH ET AL.\nThe fundamental mathematical tools needed to understand machine \nlearning include linear algebra, analytic geometry, matrix decompositions, \nvector calculus, optimization, probability and statistics. These topics \nare traditionally taught in disparate courses, making it hard for data \nscience or computer science students, or professionals, to ef\ufb01  ciently learn \nthe mathematics. This self-contained textbook bridges the gap between \nmathematical and machine learning texts, introducing the mathematical \nconcepts with a minimum of prerequisites. It uses these concepts to \nderive four central machine learning methods: linear regression, principal \ncomponen"
  },
  {
    "vector_id": 1,
    "chunk_id": "p1_c1",
    "page_number": 1,
    "text": "learning texts, introducing the mathematical \nconcepts with a minimum of prerequisites. It uses these concepts to \nderive four central machine learning methods: linear regression, principal \ncomponent analysis, Gaussian mixture models and support vector machines. \nFor students and others with a mathematical background, these derivations \nprovide a starting point to machine learning texts. For those learning the \nmathematics for the \ufb01  rst time, the methods help build intuition and practical \nexperience with applying mathematical concepts. Every chapter includes \nworked examples and exercises to test understanding. Programming \ntutorials are offered on the book\u2019s web site.\nMARC PETER DEISENROTH  is Senior Lecturer in Statistical Machine \nLearning at the Department of Computing, \u00cemperial Co"
  },
  {
    "vector_id": 2,
    "chunk_id": "p1_c2",
    "page_number": 1,
    "text": "to test understanding. Programming \ntutorials are offered on the book\u2019s web site.\nMARC PETER DEISENROTH  is Senior Lecturer in Statistical Machine \nLearning at the Department of Computing, \u00cemperial College London.\nA. ALDO FAISAL  leads the Brain & Behaviour Lab at Imperial College \nLondon, where he is also Reader in Neurotechnology at the Department of \nBioengineering and the Department of Computing.\nCHENG SOON ONG  is Principal Research Scientist at the Machine Learning \nResearch Group, Data61, CSIRO. He is also Adjunct Associate Professor at \nAustralian National University.\nCover image courtesy of Daniel Bosma / Moment / Getty Images\nCover design by Holly Johnson\nDeisenrith et al. 9781108455145 Cover. C M Y K"
  },
  {
    "vector_id": 3,
    "chunk_id": "p1_c3",
    "page_number": 1,
    "text": "esy of Daniel Bosma / Moment / Getty Images\nCover design by Holly Johnson\nDeisenrith et al. 9781108455145 Cover. C M Y K"
  },
  {
    "vector_id": 4,
    "chunk_id": "p3_c0",
    "page_number": 3,
    "text": "Contents\nForeword 1\nPart I Mathematical Foundations 9\n1 Introduction and Motivation 11\n1.1 Finding Words for Intuitions 12\n1.2 Two Ways to Read This Book 13\n1.3 Exercises and Feedback 16\n2 Linear Algebra 17\n2.1 Systems of Linear Equations 19\n2.2 Matrices 22\n2.3 Solving Systems of Linear Equations 27\n2.4 Vector Spaces 35\n2.5 Linear Independence 40\n2.6 Basis and Rank 44\n2.7 Linear Mappings 48\n2.8 Affine Spaces 61\n2.9 Further Reading 63\nExercises 64\n3 Analytic Geometry 70\n3.1 Norms 71\n3.2 Inner Products 72\n3.3 Lengths and Distances 75\n3.4 Angles and Orthogonality 76\n3.5 Orthonormal Basis 78\n3.6 Orthogonal Complement 79\n3.7 Inner Product of Functions 80\n3.8 Orthogonal Projections 81\n3.9 Rotations 91\n3.10 Further Reading 94\nExercises 96\n4 Matrix Decompositions 98\n4.1 Determinant and Trace 99\ni"
  },
  {
    "vector_id": 5,
    "chunk_id": "p3_c1",
    "page_number": 3,
    "text": "rthogonal Complement 79\n3.7 Inner Product of Functions 80\n3.8 Orthogonal Projections 81\n3.9 Rotations 91\n3.10 Further Reading 94\nExercises 96\n4 Matrix Decompositions 98\n4.1 Determinant and Trace 99\ni\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 6,
    "chunk_id": "p4_c0",
    "page_number": 4,
    "text": "ii Contents\n4.2 Eigenvalues and Eigenvectors 105\n4.3 Cholesky Decomposition 114\n4.4 Eigendecomposition and Diagonalization 115\n4.5 Singular Value Decomposition 119\n4.6 Matrix Approximation 129\n4.7 Matrix Phylogeny 134\n4.8 Further Reading 135\nExercises 137\n5 Vector Calculus 139\n5.1 Differentiation of Univariate Functions 141\n5.2 Partial Differentiation and Gradients 146\n5.3 Gradients of Vector-Valued Functions 149\n5.4 Gradients of Matrices 155\n5.5 Useful Identities for Computing Gradients 158\n5.6 Backpropagation and Automatic Differentiation 159\n5.7 Higher-Order Derivatives 164\n5.8 Linearization and Multivariate Taylor Series 165\n5.9 Further Reading 170\nExercises 170\n6 Probability and Distributions 172\n6.1 Construction of a Probability Space 172\n6.2 Discrete and Continuous Probabilities 178"
  },
  {
    "vector_id": 7,
    "chunk_id": "p4_c1",
    "page_number": 4,
    "text": "n and Multivariate Taylor Series 165\n5.9 Further Reading 170\nExercises 170\n6 Probability and Distributions 172\n6.1 Construction of a Probability Space 172\n6.2 Discrete and Continuous Probabilities 178\n6.3 Sum Rule, Product Rule, and Bayes\u2019 Theorem 183\n6.4 Summary Statistics and Independence 186\n6.5 Gaussian Distribution 197\n6.6 Conjugacy and the Exponential Family 205\n6.7 Change of Variables/Inverse Transform 214\n6.8 Further Reading 221\nExercises 222\n7 Continuous Optimization 225\n7.1 Optimization Using Gradient Descent 227\n7.2 Constrained Optimization and Lagrange Multipliers 233\n7.3 Convex Optimization 236\n7.4 Further Reading 246\nExercises 247\nPart II Central Machine Learning Problems 249\n8 When Models Meet Data 251\n8.1 Data, Models, and Learning 251\n8.2 Empirical Risk Minimization 258\n8."
  },
  {
    "vector_id": 8,
    "chunk_id": "p4_c2",
    "page_number": 4,
    "text": "timization 236\n7.4 Further Reading 246\nExercises 247\nPart II Central Machine Learning Problems 249\n8 When Models Meet Data 251\n8.1 Data, Models, and Learning 251\n8.2 Empirical Risk Minimization 258\n8.3 Parameter Estimation 265\n8.4 Probabilistic Modeling and Inference 272\n8.5 Directed Graphical Models 278\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 9,
    "chunk_id": "p5_c0",
    "page_number": 5,
    "text": "Contents iii\n8.6 Model Selection 283\n9 Linear Regression 289\n9.1 Problem Formulation 291\n9.2 Parameter Estimation 292\n9.3 Bayesian Linear Regression 303\n9.4 Maximum Likelihood as Orthogonal Projection 313\n9.5 Further Reading 315\n10 Dimensionality Reduction with Principal Component Analysis 317\n10.1 Problem Setting 318\n10.2 Maximum Variance Perspective 320\n10.3 Projection Perspective 325\n10.4 Eigenvector Computation and Low-Rank Approximations 333\n10.5 PCA in High Dimensions 335\n10.6 Key Steps of PCA in Practice 336\n10.7 Latent Variable Perspective 339\n10.8 Further Reading 343\n11 Density Estimation with Gaussian Mixture Models 348\n11.1 Gaussian Mixture Model 349\n11.2 Parameter Learning via Maximum Likelihood 350\n11.3 EM Algorithm 360\n11.4 Latent-Variable Perspective 363\n11.5 Further Reading"
  },
  {
    "vector_id": 10,
    "chunk_id": "p5_c1",
    "page_number": 5,
    "text": "tion with Gaussian Mixture Models 348\n11.1 Gaussian Mixture Model 349\n11.2 Parameter Learning via Maximum Likelihood 350\n11.3 EM Algorithm 360\n11.4 Latent-Variable Perspective 363\n11.5 Further Reading 368\n12 Classification with Support Vector Machines 370\n12.1 Separating Hyperplanes 372\n12.2 Primal Support Vector Machine 374\n12.3 Dual Support Vector Machine 383\n12.4 Kernels 388\n12.5 Numerical Solution 390\n12.6 Further Reading 392\nReferences 395\nIndex 407\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 11,
    "chunk_id": "p7_c0",
    "page_number": 7,
    "text": "Foreword\nMachine learning is the latest in a long line of attempts to distill human\nknowledge and reasoning into a form that is suitable for constructing ma-\nchines and engineering automated systems. As machine learning becomes\nmore ubiquitous and its software packages become easier to use, it is nat-\nural and desirable that the low-level technical details are abstracted away\nand hidden from the practitioner. However, this brings with it the danger\nthat a practitioner becomes unaware of the design decisions and, hence,\nthe limits of machine learning algorithms.\nThe enthusiastic practitioner who is interested to learn more about the\nmagic behind successful machine learning algorithms currently faces a\ndaunting set of pre-requisite knowledge:\nProgramming languages and data analysis tools\nLar"
  },
  {
    "vector_id": 12,
    "chunk_id": "p7_c1",
    "page_number": 7,
    "text": "o is interested to learn more about the\nmagic behind successful machine learning algorithms currently faces a\ndaunting set of pre-requisite knowledge:\nProgramming languages and data analysis tools\nLarge-scale computation and the associated frameworks\nMathematics and statistics and how machine learning builds on it\nAt universities, introductory courses on machine learning tend to spend\nearly parts of the course covering some of these pre-requisites. For histori-\ncal reasons, courses in machine learning tend to be taught in the computer\nscience department, where students are often trained in the first two areas\nof knowledge, but not so much in mathematics and statistics.\nCurrent machine learning textbooks primarily focus on machine learn-\ning algorithms and methodologies and assume that the"
  },
  {
    "vector_id": 13,
    "chunk_id": "p7_c2",
    "page_number": 7,
    "text": "first two areas\nof knowledge, but not so much in mathematics and statistics.\nCurrent machine learning textbooks primarily focus on machine learn-\ning algorithms and methodologies and assume that the reader is com-\npetent in mathematics and statistics. Therefore, these books only spend\none or two chapters on background mathematics, either at the beginning\nof the book or as appendices. We have found many people who want to\ndelve into the foundations of basic machine learning methods who strug-\ngle with the mathematical knowledge required to read a machine learning\ntextbook. Having taught undergraduate and graduate courses at universi-\nties, we find that the gap between high school mathematics and the math-\nematics level required to read a standard machine learning textbook is too\nbig for ma"
  },
  {
    "vector_id": 14,
    "chunk_id": "p7_c3",
    "page_number": 7,
    "text": "raduate and graduate courses at universi-\nties, we find that the gap between high school mathematics and the math-\nematics level required to read a standard machine learning textbook is too\nbig for many people.\nThis book brings the mathematical foundations of basic machine learn-\ning concepts to the fore and collects the information in a single place so\nthat this skills gap is narrowed or even closed.\n1\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 15,
    "chunk_id": "p7_c4",
    "page_number": 7,
    "text": "download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 16,
    "chunk_id": "p8_c0",
    "page_number": 8,
    "text": "2 Foreword\nWhy Another Book on Machine Learning?\nMachine learning builds upon the language of mathematics to express\nconcepts that seem intuitively obvious but that are surprisingly difficult\nto formalize. Once formalized properly , we can gain insights into the task\nwe want to solve. One common complaint of students of mathematics\naround the globe is that the topics covered seem to have little relevance\nto practical problems. We believe that machine learning is an obvious and\ndirect motivation for people to learn mathematics.\nThis book is intended to be a guidebook to the vast mathematical lit-\nerature that forms the foundations of modern machine learning. We mo-\u201cMath is linked in\nthe popular mind\nwith phobia and\nanxiety . You\u2019d think\nwe\u2019re discussing\nspiders.\u201d (Strogatz,\n2014, page 281)"
  },
  {
    "vector_id": 17,
    "chunk_id": "p8_c1",
    "page_number": 8,
    "text": "t-\nerature that forms the foundations of modern machine learning. We mo-\u201cMath is linked in\nthe popular mind\nwith phobia and\nanxiety . You\u2019d think\nwe\u2019re discussing\nspiders.\u201d (Strogatz,\n2014, page 281)\ntivate the need for mathematical concepts by directly pointing out their\nusefulness in the context of fundamental machine learning problems. In\nthe interest of keeping the book short, many details and more advanced\nconcepts have been left out. Equipped with the basic concepts presented\nhere, and how they fit into the larger context of machine learning, the\nreader can find numerous resources for further study , which we provide at\nthe end of the respective chapters. For readers with a mathematical back-\nground, this book provides a brief but precisely stated glimpse of machine\nlearning. In cont"
  },
  {
    "vector_id": 18,
    "chunk_id": "p8_c2",
    "page_number": 8,
    "text": "rther study , which we provide at\nthe end of the respective chapters. For readers with a mathematical back-\nground, this book provides a brief but precisely stated glimpse of machine\nlearning. In contrast to other books that focus on methods and models\nof machine learning (MacKay, 2003; Bishop, 2006; Alpaydin, 2010; Bar-\nber, 2012; Murphy, 2012; Shalev-Shwartz and Ben-David, 2014; Rogers\nand Girolami, 2016) or programmatic aspects of machine learning (M\u00a8uller\nand Guido, 2016; Raschka and Mirjalili, 2017; Chollet and Allaire, 2018),\nwe provide only four representative examples of machine learning algo-\nrithms. Instead, we focus on the mathematical concepts behind the models\nthemselves. We hope that readers will be able to gain a deeper understand-\ning of the basic questions in machine learn"
  },
  {
    "vector_id": 19,
    "chunk_id": "p8_c3",
    "page_number": 8,
    "text": "ng algo-\nrithms. Instead, we focus on the mathematical concepts behind the models\nthemselves. We hope that readers will be able to gain a deeper understand-\ning of the basic questions in machine learning and connect practical ques-\ntions arising from the use of machine learning with fundamental choices\nin the mathematical model.\nWe do not aim to write a classical machine learning book. Instead, our\nintention is to provide the mathematical background, applied to four cen-\ntral machine learning problems, to make it easier to read other machine\nlearning textbooks.\nWho Is the Target Audience?\nAs applications of machine learning become widespread in society , we\nbelieve that everybody should have some understanding of its underlying\nprinciples. This book is written in an academic mathematical s"
  },
  {
    "vector_id": 20,
    "chunk_id": "p8_c4",
    "page_number": 8,
    "text": "pplications of machine learning become widespread in society , we\nbelieve that everybody should have some understanding of its underlying\nprinciples. This book is written in an academic mathematical style, which\nenables us to be precise about the concepts behind machine learning. We\nencourage readers unfamiliar with this seemingly terse style to persevere\nand to keep the goals of each topic in mind. We sprinkle comments and\nremarks throughout the text, in the hope that it provides useful guidance\nwith respect to the big picture.\nThe book assumes the reader to have mathematical knowledge commonly\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 21,
    "chunk_id": "p8_c5",
    "page_number": 8,
    "text": "ly\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 22,
    "chunk_id": "p9_c0",
    "page_number": 9,
    "text": "Foreword 3\ncovered in high school mathematics and physics. For example, the reader\nshould have seen derivatives and integrals before, and geometric vectors\nin two or three dimensions. Starting from there, we generalize these con-\ncepts. Therefore, the target audience of the book includes undergraduate\nuniversity students, evening learners and learners participating in online\nmachine learning courses.\nIn analogy to music, there are three types of interaction that people\nhave with machine learning:\nAstute Listener The democratization of machine learning by the pro-\nvision of open-source software, online tutorials and cloud-based tools al-\nlows users to not worry about the specifics of pipelines. Users can focus on\nextracting insights from data using off-the-shelf tools. This enables non-\ntec"
  },
  {
    "vector_id": 23,
    "chunk_id": "p9_c1",
    "page_number": 9,
    "text": ", online tutorials and cloud-based tools al-\nlows users to not worry about the specifics of pipelines. Users can focus on\nextracting insights from data using off-the-shelf tools. This enables non-\ntech-savvy domain experts to benefit from machine learning. This is sim-\nilar to listening to music; the user is able to choose and discern between\ndifferent types of machine learning, and benefits from it. More experi-\nenced users are like music critics, asking important questions about the\napplication of machine learning in society such as ethics, fairness, and pri-\nvacy of the individual. We hope that this book provides a foundation for\nthinking about the certification and risk management of machine learning\nsystems, and allows them to use their domain expertise to build better\nmachine learnin"
  },
  {
    "vector_id": 24,
    "chunk_id": "p9_c2",
    "page_number": 9,
    "text": "that this book provides a foundation for\nthinking about the certification and risk management of machine learning\nsystems, and allows them to use their domain expertise to build better\nmachine learning systems.\nExperienced Artist Skilled practitioners of machine learning can plug\nand play different tools and libraries into an analysis pipeline. The stereo-\ntypical practitioner would be a data scientist or engineer who understands\nmachine learning interfaces and their use cases, and is able to perform\nwonderful feats of prediction from data. This is similar to a virtuoso play-\ning music, where highly skilled practitioners can bring existing instru-\nments to life and bring enjoyment to their audience. Using the mathe-\nmatics presented here as a primer, practitioners would be able to under-\ns"
  },
  {
    "vector_id": 25,
    "chunk_id": "p9_c3",
    "page_number": 9,
    "text": "highly skilled practitioners can bring existing instru-\nments to life and bring enjoyment to their audience. Using the mathe-\nmatics presented here as a primer, practitioners would be able to under-\nstand the benefits and limits of their favorite method, and to extend and\ngeneralize existing machine learning algorithms. We hope that this book\nprovides the impetus for more rigorous and principled development of\nmachine learning methods.\nFledgling Composer As machine learning is applied to new domains,\ndevelopers of machine learning need to develop new methods and extend\nexisting algorithms. They are often researchers who need to understand\nthe mathematical basis of machine learning and uncover relationships be-\ntween different tasks. This is similar to composers of music who, within\nthe rul"
  },
  {
    "vector_id": 26,
    "chunk_id": "p9_c4",
    "page_number": 9,
    "text": "y are often researchers who need to understand\nthe mathematical basis of machine learning and uncover relationships be-\ntween different tasks. This is similar to composers of music who, within\nthe rules and structure of musical theory , create new and amazing pieces.\nWe hope this book provides a high-level overview of other technical books\nfor people who want to become composers of machine learning. There is\na great need in society for new researchers who are able to propose and\nexplore novel approaches for attacking the many challenges of learning\nfrom data.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 27,
    "chunk_id": "p9_c5",
    "page_number": 9,
    "text": "al, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 28,
    "chunk_id": "p10_c0",
    "page_number": 10,
    "text": "4 Foreword\nAcknowledgments\nWe are grateful to many people who looked at early drafts of the book\nand suffered through painful expositions of concepts. We tried to imple-\nment their ideas that we did not vehemently disagree with. We would\nlike to especially acknowledge Christfried Webers for his careful reading\nof many parts of the book, and his detailed suggestions on structure and\npresentation. Many friends and colleagues have also been kind enough\nto provide their time and energy on different versions of each chapter.\nWe have been lucky to benefit from the generosity of the online commu-\nnity , who have suggested improvements viahttps://github.com, which\ngreatly improved the book.\nThe following people have found bugs, proposed clarifications and sug-\ngested relevant literature, either vi"
  },
  {
    "vector_id": 29,
    "chunk_id": "p10_c1",
    "page_number": 10,
    "text": "y , who have suggested improvements viahttps://github.com, which\ngreatly improved the book.\nThe following people have found bugs, proposed clarifications and sug-\ngested relevant literature, either via https://github.com or personal\ncommunication. Their names are sorted alphabetically .\nAbdul-Ganiy Usman\nAdam Gaier\nAdele Jackson\nAditya Menon\nAlasdair Tran\nAleksandar Krnjaic\nAlexander Makrigiorgos\nAlfredo Canziani\nAli Shafti\nAmr Khalifa\nAndrew Tanggara\nAngus Gruen\nAntal A. Buss\nAntoine Toisoul Le Cann\nAreg Sarvazyan\nArtem Artemev\nArtyom Stepanov\nBill Kromydas\nBob Williamson\nBoon Ping Lim\nChao Qu\nCheng Li\nChris Sherlock\nChristopher Gray\nDaniel McNamara\nDaniel Wood\nDarren Siegel\nDavid Johnston\nDawei Chen\nEllen Broad\nFengkuangtian Zhu\nFiona Condon\nGeorgios Theodorou\nHe Xin\nIrene Raissa Kameni"
  },
  {
    "vector_id": 30,
    "chunk_id": "p10_c2",
    "page_number": 10,
    "text": "u\nCheng Li\nChris Sherlock\nChristopher Gray\nDaniel McNamara\nDaniel Wood\nDarren Siegel\nDavid Johnston\nDawei Chen\nEllen Broad\nFengkuangtian Zhu\nFiona Condon\nGeorgios Theodorou\nHe Xin\nIrene Raissa Kameni\nJakub Nabaglo\nJames Hensman\nJamie Liu\nJean Kaddour\nJean-Paul Ebejer\nJerry Qiang\nJitesh Sindhare\nJohn Lloyd\nJonas Ngnawe\nJon Martin\nJustin Hsi\nKai Arulkumaran\nKamil Dreczkowski\nLily Wang\nLionel Tondji Ngoupeyou\nLydia Kn\u00a8ufing\nMahmoud Aslan\nMark Hartenstein\nMark van der Wilk\nMarkus Hegland\nMartin Hewing\nMatthew Alger\nMatthew Lee\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 31,
    "chunk_id": "p10_c3",
    "page_number": 10,
    "text": "s://mml-book.com."
  },
  {
    "vector_id": 32,
    "chunk_id": "p11_c0",
    "page_number": 11,
    "text": "Foreword 5\nMaximus McCann\nMengyan Zhang\nMichael Bennett\nMichael Pedersen\nMinjeong Shin\nMohammad Malekzadeh\nNaveen Kumar\nNico Montali\nOscar Armas\nPatrick Henriksen\nPatrick Wieschollek\nPattarawat Chormai\nPaul Kelly\nPetros Christodoulou\nPiotr Januszewski\nPranav Subramani\nQuyu Kong\nRagib Zaman\nRui Zhang\nRyan-Rhys Griffiths\nSalomon Kabongo\nSamuel Ogunmola\nSandeep Mavadia\nSarvesh Nikumbh\nSebastian Raschka\nSenanayak Sesh Kumar Karri\nSeung-Heon Baek\nShahbaz Chaudhary\nShakir Mohamed\nShawn Berry\nSheikh Abdul Raheem Ali\nSheng Xue\nSridhar Thiagarajan\nSyed Nouman Hasany\nSzymon Brych\nThomas B\u00a8uhler\nTimur Sharapov\nTom Melamed\nVincent Adam\nVincent Dutordoir\nVu Minh\nWasim Aftab\nWen Zhi\nWojciech Stokowiec\nXiaonan Chong\nXiaowei Zhang\nYazhou Hao\nYicheng Luo\nYoung Lee\nYu Lu\nYun Cheng\nYuxiao Huang\nZac Cranko\nZi"
  },
  {
    "vector_id": 33,
    "chunk_id": "p11_c1",
    "page_number": 11,
    "text": "arapov\nTom Melamed\nVincent Adam\nVincent Dutordoir\nVu Minh\nWasim Aftab\nWen Zhi\nWojciech Stokowiec\nXiaonan Chong\nXiaowei Zhang\nYazhou Hao\nYicheng Luo\nYoung Lee\nYu Lu\nYun Cheng\nYuxiao Huang\nZac Cranko\nZijian Cao\nZoe Nolan\nContributors through GitHub, whose real names were not listed on their\nGitHub profile, are:\nSamDataMad\nbumptiousmonkey\nidoamihai\ndeepakiim\ninsad\nHorizonP\ncs-maillist\nkudo23\nempet\nvictorBigand\n17SKYE\njessjing1995\nWe are also very grateful to Parameswaran Raman and the many anony-\nmous reviewers, organized by Cambridge University Press, who read one\nor more chapters of earlier versions of the manuscript, and provided con-\nstructive criticism that led to considerable improvements. A special men-\ntion goes to Dinesh Singh Negi, our LATEX support, for detailed and prompt\nadvice a"
  },
  {
    "vector_id": 34,
    "chunk_id": "p11_c2",
    "page_number": 11,
    "text": "sions of the manuscript, and provided con-\nstructive criticism that led to considerable improvements. A special men-\ntion goes to Dinesh Singh Negi, our LATEX support, for detailed and prompt\nadvice about LATEX-related issues. Last but not least, we are very grateful\nto our editor Lauren Cowles, who has been patiently guiding us through\nthe gestation process of this book.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 35,
    "chunk_id": "p12_c0",
    "page_number": 12,
    "text": "6 Foreword\nTable of Symbols\nSymbol Typical meaning\na, b, c, \u03b1, \u03b2, \u03b3Scalars are lowercase\nx, y, z Vectors are bold lowercase\nA, B, C Matrices are bold uppercase\nx\u22a4, A\u22a4 Transpose of a vector or matrix\nA\u22121 Inverse of a matrix\n\u27e8x, y\u27e9 Inner product of x and y\nx\u22a4y Dot product of x and y\nB = (b1, b2, b3) (Ordered) tuple\nB = [b1, b2, b3] Matrix of column vectors stacked horizontally\nB = {b1, b2, b3} Set of vectors (unordered)\nZ, N Integers and natural numbers, respectively\nR, C Real and complex numbers, respectively\nRn n-dimensional vector space of real numbers\n\u2200x Universal quantifier: for all x\n\u2203x Existential quantifier: there exists x\na := b a is defined as b\na =: b b is defined as a\na \u221d b a is proportional to b, i.e., a = constant \u00b7 b\ng \u25e6 f Function composition: \u201cg after f\u201d\n\u21d0 \u21d2 If and only if\n="
  },
  {
    "vector_id": 36,
    "chunk_id": "p12_c1",
    "page_number": 12,
    "text": "istential quantifier: there exists x\na := b a is defined as b\na =: b b is defined as a\na \u221d b a is proportional to b, i.e., a = constant \u00b7 b\ng \u25e6 f Function composition: \u201cg after f\u201d\n\u21d0 \u21d2 If and only if\n=\u21d2 Implies\nA, C Sets\na \u2208 A a is an element of set A\n\u2205 Empty set\nA\\B A without B: the set of elements in A but not in B\nD Number of dimensions; indexed by d = 1, . . . , D\nN Number of data points; indexed by n = 1, . . . , N\nIm Identity matrix of size m \u00d7 m\n0m,n Matrix of zeros of size m \u00d7 n\n1m,n Matrix of ones of size m \u00d7 n\nei Standard/canonical vector (where i is the component that is 1)\ndim Dimensionality of vector space\nrk(A) Rank of matrix A\nIm(\u03a6) Image of linear mapping \u03a6\nker(\u03a6) Kernel (null space) of a linear mapping \u03a6\nspan[b1] Span (generating set) of b1\ntr(A) Trace of A\ndet(A) Determina"
  },
  {
    "vector_id": 37,
    "chunk_id": "p12_c2",
    "page_number": 12,
    "text": "sionality of vector space\nrk(A) Rank of matrix A\nIm(\u03a6) Image of linear mapping \u03a6\nker(\u03a6) Kernel (null space) of a linear mapping \u03a6\nspan[b1] Span (generating set) of b1\ntr(A) Trace of A\ndet(A) Determinant of A\n| \u00b7 | Absolute value or determinant (depending on context)\n\u2225\u00b7\u2225 Norm; Euclidean, unless specified\n\u03bb Eigenvalue or Lagrange multiplier\nE\u03bb Eigenspace corresponding to eigenvalue \u03bb\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 38,
    "chunk_id": "p13_c0",
    "page_number": 13,
    "text": "Foreword 7\nSymbol Typical meaning\nx \u22a5 y Vectors x and y are orthogonal\nV Vector space\nV \u22a5 Orthogonal complement of vector space VPN\nn=1 xn Sum of the xn: x1 + . . .+ xNQN\nn=1 xn Product of the xn: x1 \u00b7 . . .\u00b7 xN\n\u03b8 Parameter vector\n\u2202f\n\u2202x Partial derivative of f with respect to x\ndf\ndx Total derivative of f with respect to x\n\u2207 Gradient\nf\u2217 = minx f(x) The smallest function value of f\nx\u2217 \u2208 arg minx f(x) The value x\u2217 that minimizes f (note: arg min returns a set of values)\nL Lagrangian\nL Negative log-likelihood\u0000n\nk\n\u0001\nBinomial coefficient, n choose k\nVX[x] Variance of x with respect to the random variable X\nEX[x] Expectation of x with respect to the random variable X\nCovX,Y [x, y] Covariance between x and y.\nX \u22a5 \u22a5Y |Z X is conditionally independent of Y given Z\nX \u223c p Random variable X is distrib"
  },
  {
    "vector_id": 39,
    "chunk_id": "p13_c1",
    "page_number": 13,
    "text": "riable X\nEX[x] Expectation of x with respect to the random variable X\nCovX,Y [x, y] Covariance between x and y.\nX \u22a5 \u22a5Y |Z X is conditionally independent of Y given Z\nX \u223c p Random variable X is distributed according to p\nN\n\u0000\n\u00b5, \u03a3\n\u0001\nGaussian distribution with mean \u00b5 and covariance \u03a3\nBer(\u00b5) Bernoulli distribution with parameter \u00b5\nBin(N, \u00b5) Binomial distribution with parameters N, \u00b5\nBeta(\u03b1, \u03b2) Beta distribution with parameters \u03b1, \u03b2\nTable of Abbreviations and Acronyms\nAcronym Meaning\ne.g. Exempli gratia (Latin: for example)\nGMM Gaussian mixture model\ni.e. Id est (Latin: this means)\ni.i.d. Independent, identically distributed\nMAP Maximum a posteriori\nMLE Maximum likelihood estimation/estimator\nONB Orthonormal basis\nPCA Principal component analysis\nPPCA Probabilistic principal component analysis"
  },
  {
    "vector_id": 40,
    "chunk_id": "p13_c2",
    "page_number": 13,
    "text": "nt, identically distributed\nMAP Maximum a posteriori\nMLE Maximum likelihood estimation/estimator\nONB Orthonormal basis\nPCA Principal component analysis\nPPCA Probabilistic principal component analysis\nREF Row-echelon form\nSPD Symmetric, positive definite\nSVM Support vector machine\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 41,
    "chunk_id": "p15_c0",
    "page_number": 15,
    "text": "Part I\nMathematical Foundations\n9\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 42,
    "chunk_id": "p17_c0",
    "page_number": 17,
    "text": "1\nIntroduction and Motivation\nMachine learning is about designing algorithms that automatically extract\nvaluable information from data. The emphasis here is on \u201cautomatic\u201d, i.e.,\nmachine learning is concerned about general-purpose methodologies that\ncan be applied to many datasets, while producing something that is mean-\ningful. There are three concepts that are at the core of machine learning:\ndata, a model, and learning.\nSince machine learning is inherently data driven, data is at the core data\nof machine learning. The goal of machine learning is to design general-\npurpose methodologies to extract valuable patterns from data, ideally\nwithout much domain-specific expertise. For example, given a large corpus\nof documents (e.g., books in many libraries), machine learning methods\ncan be used"
  },
  {
    "vector_id": 43,
    "chunk_id": "p17_c1",
    "page_number": 17,
    "text": "xtract valuable patterns from data, ideally\nwithout much domain-specific expertise. For example, given a large corpus\nof documents (e.g., books in many libraries), machine learning methods\ncan be used to automatically find relevant topics that are shared across\ndocuments (Hoffman et al., 2010). To achieve this goal, we design mod-\nels that are typically related to the process that generates data, similar to model\nthe dataset we are given. For example, in a regression setting, the model\nwould describe a function that maps inputs to real-valued outputs. To\nparaphrase Mitchell (1997): A model is said to learn from data if its per-\nformance on a given task improves after the data is taken into account.\nThe goal is to find good models that generalize well to yet unseen data,\nwhich we may care a"
  },
  {
    "vector_id": 44,
    "chunk_id": "p17_c2",
    "page_number": 17,
    "text": "said to learn from data if its per-\nformance on a given task improves after the data is taken into account.\nThe goal is to find good models that generalize well to yet unseen data,\nwhich we may care about in the future. Learning can be understood as a learning\nway to automatically find patterns and structure in data by optimizing the\nparameters of the model.\nWhile machine learning has seen many success stories, and software is\nreadily available to design and train rich and flexible machine learning\nsystems, we believe that the mathematical foundations of machine learn-\ning are important in order to understand fundamental principles upon\nwhich more complicated machine learning systems are built. Understand-\ning these principles can facilitate creating new machine learning solutions,\nunderst"
  },
  {
    "vector_id": 45,
    "chunk_id": "p17_c3",
    "page_number": 17,
    "text": "er to understand fundamental principles upon\nwhich more complicated machine learning systems are built. Understand-\ning these principles can facilitate creating new machine learning solutions,\nunderstanding and debugging existing approaches, and learning about the\ninherent assumptions and limitations of the methodologies we are work-\ning with.\n11\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 46,
    "chunk_id": "p17_c4",
    "page_number": 17,
    "text": "re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 47,
    "chunk_id": "p18_c0",
    "page_number": 18,
    "text": "12 Introduction and Motivation\n1.1 Finding Words for Intuitions\nA challenge we face regularly in machine learning is that concepts and\nwords are slippery , and a particular component of the machine learning\nsystem can be abstracted to different mathematical concepts. For example,\nthe word \u201calgorithm\u201d is used in at least two different senses in the con-\ntext of machine learning. In the first sense, we use the phrase \u201cmachine\nlearning algorithm\u201d to mean a system that makes predictions based on in-\nput data. We refer to these algorithms as predictors. In the second sense,predictor\nwe use the exact same phrase \u201cmachine learning algorithm\u201d to mean a\nsystem that adapts some internal parameters of the predictor so that it\nperforms well on future unseen input data. Here we refer to this adapta-\nti"
  },
  {
    "vector_id": 48,
    "chunk_id": "p18_c1",
    "page_number": 18,
    "text": "t same phrase \u201cmachine learning algorithm\u201d to mean a\nsystem that adapts some internal parameters of the predictor so that it\nperforms well on future unseen input data. Here we refer to this adapta-\ntion as training a system.training\nThis book will not resolve the issue of ambiguity , but we want to high-\nlight upfront that, depending on the context, the same expressions can\nmean different things. However, we attempt to make the context suffi-\nciently clear to reduce the level of ambiguity .\nThe first part of this book introduces the mathematical concepts and\nfoundations needed to talk about the three main components of a machine\nlearning system: data, models, and learning. We will briefly outline these\ncomponents here, and we will revisit them again in Chapter 8 once we\nhave discussed the"
  },
  {
    "vector_id": 49,
    "chunk_id": "p18_c2",
    "page_number": 18,
    "text": "e three main components of a machine\nlearning system: data, models, and learning. We will briefly outline these\ncomponents here, and we will revisit them again in Chapter 8 once we\nhave discussed the necessary mathematical concepts.\nWhile not all data is numerical, it is often useful to consider data in\na number format. In this book, we assume that data has already been\nappropriately converted into a numerical representation suitable for read-\ning into a computer program. Therefore, we think of data as vectors. Asdata as vectors\nanother illustration of how subtle words are, there are (at least) three\ndifferent ways to think about vectors: a vector as an array of numbers (a\ncomputer science view), a vector as an arrow with a direction and magni-\ntude (a physics view), and a vector as an obj"
  },
  {
    "vector_id": 50,
    "chunk_id": "p18_c3",
    "page_number": 18,
    "text": ") three\ndifferent ways to think about vectors: a vector as an array of numbers (a\ncomputer science view), a vector as an arrow with a direction and magni-\ntude (a physics view), and a vector as an object that obeys addition and\nscaling (a mathematical view).\nA model is typically used to describe a process for generating data, sim-model\nilar to the dataset at hand. Therefore, good models can also be thought\nof as simplified versions of the real (unknown) data-generating process,\ncapturing aspects that are relevant for modeling the data and extracting\nhidden patterns from it. A good model can then be used to predict what\nwould happen in the real world without performing real-world experi-\nments.\nWe now come to the crux of the matter, the learning component oflearning\nmachine learning. Assume"
  },
  {
    "vector_id": 51,
    "chunk_id": "p18_c4",
    "page_number": 18,
    "text": "en be used to predict what\nwould happen in the real world without performing real-world experi-\nments.\nWe now come to the crux of the matter, the learning component oflearning\nmachine learning. Assume we are given a dataset and a suitable model.\nTraining the model means to use the data available to optimize some pa-\nrameters of the model with respect to a utility function that evaluates how\nwell the model predicts the training data. Most training methods can be\nthought of as an approach analogous to climbing a hill to reach its peak.\nIn this analogy , the peak of the hill corresponds to a maximum of some\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 52,
    "chunk_id": "p18_c5",
    "page_number": 18,
    "text": "mum of some\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 53,
    "chunk_id": "p19_c0",
    "page_number": 19,
    "text": "1.2 Two Ways to Read This Book 13\ndesired performance measure. However, in practice, we are interested in\nthe model to perform well on unseen data. Performing well on data that\nwe have already seen (training data) may only mean that we found a\ngood way to memorize the data. However, this may not generalize well to\nunseen data, and, in practical applications, we often need to expose our\nmachine learning system to situations that it has not encountered before.\nLet us summarize the main concepts of machine learning that we cover\nin this book:\nWe represent data as vectors.\nWe choose an appropriate model, either using the probabilistic or opti-\nmization view.\nWe learn from available data by using numerical optimization methods\nwith the aim that the model performs well on data not used for train"
  },
  {
    "vector_id": 54,
    "chunk_id": "p19_c1",
    "page_number": 19,
    "text": "model, either using the probabilistic or opti-\nmization view.\nWe learn from available data by using numerical optimization methods\nwith the aim that the model performs well on data not used for training.\n1.2 Two Ways to Read This Book\nWe can consider two strategies for understanding the mathematics for\nmachine learning:\nBottom-up: Building up the concepts from foundational to more ad-\nvanced. This is often the preferred approach in more technical fields,\nsuch as mathematics. This strategy has the advantage that the reader\nat all times is able to rely on their previously learned concepts. Unfor-\ntunately , for a practitioner many of the foundational concepts are not\nparticularly interesting by themselves, and the lack of motivation means\nthat most foundational definitions are quickly forgo"
  },
  {
    "vector_id": 55,
    "chunk_id": "p19_c2",
    "page_number": 19,
    "text": "r-\ntunately , for a practitioner many of the foundational concepts are not\nparticularly interesting by themselves, and the lack of motivation means\nthat most foundational definitions are quickly forgotten.\nTop-down: Drilling down from practical needs to more basic require-\nments. This goal-driven approach has the advantage that the readers\nknow at all times why they need to work on a particular concept, and\nthere is a clear path of required knowledge. The downside of this strat-\negy is that the knowledge is built on potentially shaky foundations, and\nthe readers have to remember a set of words that they do not have any\nway of understanding.\nWe decided to write this book in a modular way to separate foundational\n(mathematical) concepts from applications so that this book can be read\nin both"
  },
  {
    "vector_id": 56,
    "chunk_id": "p19_c3",
    "page_number": 19,
    "text": "that they do not have any\nway of understanding.\nWe decided to write this book in a modular way to separate foundational\n(mathematical) concepts from applications so that this book can be read\nin both ways. The book is split into two parts, where Part I lays the math-\nematical foundations and Part II applies the concepts from Part I to a set\nof fundamental machine learning problems, which form four pillars of\nmachine learning as illustrated in Figure 1.1: regression, dimensionality\nreduction, density estimation, and classification. Chapters in Part I mostly\nbuild upon the previous ones, but it is possible to skip a chapter and work\nbackward if necessary . Chapters in Part II are only loosely coupled and\ncan be read in any order. There are many pointers forward and backward\n\u00a92024 M. P. Deis"
  },
  {
    "vector_id": 57,
    "chunk_id": "p19_c4",
    "page_number": 19,
    "text": "is possible to skip a chapter and work\nbackward if necessary . Chapters in Part II are only loosely coupled and\ncan be read in any order. There are many pointers forward and backward\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 58,
    "chunk_id": "p20_c0",
    "page_number": 20,
    "text": "14 Introduction and Motivation\nFigure 1.1 The\nfoundations and\nfour pillars of\nmachine learning.\nClassi\ufb01cation \nDensity \nEstimation \nRegression \nDimensionality \nReduction \nMachine Learning\nVector CalculusProbability & DistributionsOptimization\nAnalytic Geometry Matrix DecompositionLinear Algebra\nbetween the two parts of the book to link mathematical concepts with\nmachine learning algorithms.\nOf course there are more than two ways to read this book. Most readers\nlearn using a combination of top-down and bottom-up approaches, some-\ntimes building up basic mathematical skills before attempting more com-\nplex concepts, but also choosing topics based on applications of machine\nlearning.\nPart I Is about Mathematics\nThe four pillars of machine learning we cover in this book (see Figure 1.1)\nrequir"
  },
  {
    "vector_id": 59,
    "chunk_id": "p20_c1",
    "page_number": 20,
    "text": "e com-\nplex concepts, but also choosing topics based on applications of machine\nlearning.\nPart I Is about Mathematics\nThe four pillars of machine learning we cover in this book (see Figure 1.1)\nrequire a solid mathematical foundation, which is laid out in Part I.\nWe represent numerical data as vectors and represent a table of such\ndata as a matrix. The study of vectors and matrices is calledlinear algebra,\nwhich we introduce in Chapter 2. The collection of vectors as a matrix islinear algebra\nalso described there.\nGiven two vectors representing two objects in the real world, we want\nto make statements about their similarity . The idea is that vectors that\nare similar should be predicted to have similar outputs by our machine\nlearning algorithm (our predictor). To formalize the idea of simi"
  },
  {
    "vector_id": 60,
    "chunk_id": "p20_c2",
    "page_number": 20,
    "text": "atements about their similarity . The idea is that vectors that\nare similar should be predicted to have similar outputs by our machine\nlearning algorithm (our predictor). To formalize the idea of similarity be-\ntween vectors, we need to introduce operations that take two vectors as\ninput and return a numerical value representing their similarity . The con-\nstruction of similarity and distances is central to analytic geometry and isanalytic geometry\ndiscussed in Chapter 3.\nIn Chapter 4, we introduce some fundamental concepts about matri-\nces and matrix decomposition. Some operations on matrices are extremelymatrix\ndecomposition useful in machine learning, and they allow for an intuitive interpretation\nof the data and more efficient learning.\nWe often consider data to be noisy observations o"
  },
  {
    "vector_id": 61,
    "chunk_id": "p20_c3",
    "page_number": 20,
    "text": "are extremelymatrix\ndecomposition useful in machine learning, and they allow for an intuitive interpretation\nof the data and more efficient learning.\nWe often consider data to be noisy observations of some true underly-\ning signal. We hope that by applying machine learning we can identify the\nsignal from the noise. This requires us to have a language for quantify-\ning what \u201cnoise\u201d means. We often would also like to have predictors that\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 62,
    "chunk_id": "p21_c0",
    "page_number": 21,
    "text": "1.2 Two Ways to Read This Book 15\nallow us to express some sort of uncertainty , e.g., to quantify the confi-\ndence we have about the value of the prediction at a particular test data\npoint. Quantification of uncertainty is the realm of probability theory and probability theory\nis covered in Chapter 6.\nTo train machine learning models, we typically find parameters that\nmaximize some performance measure. Many optimization techniques re-\nquire the concept of a gradient, which tells us the direction in which to\nsearch for a solution. Chapter 5 is about vector calculus and details the vector calculus\nconcept of gradients, which we subsequently use in Chapter 7, where we\ntalk about optimization to find maxima/minima of functions. optimization\nPart II Is about Machine Learning\nThe second part of"
  },
  {
    "vector_id": 63,
    "chunk_id": "p21_c1",
    "page_number": 21,
    "text": "lus\nconcept of gradients, which we subsequently use in Chapter 7, where we\ntalk about optimization to find maxima/minima of functions. optimization\nPart II Is about Machine Learning\nThe second part of the book introduces four pillars of machine learning\nas shown in Figure 1.1. We illustrate how the mathematical concepts in-\ntroduced in the first part of the book are the foundation for each pillar.\nBroadly speaking, chapters are ordered by difficulty (in ascending order).\nIn Chapter 8, we restate the three components of machine learning\n(data, models, and parameter estimation) in a mathematical fashion. In\naddition, we provide some guidelines for building experimental set-ups\nthat guard against overly optimistic evaluations of machine learning sys-\ntems. Recall that the goal is to build a p"
  },
  {
    "vector_id": 64,
    "chunk_id": "p21_c2",
    "page_number": 21,
    "text": "fashion. In\naddition, we provide some guidelines for building experimental set-ups\nthat guard against overly optimistic evaluations of machine learning sys-\ntems. Recall that the goal is to build a predictor that performs well on\nunseen data.\nIn Chapter 9, we will have a close look at linear regression, where our linear regression\nobjective is to find functions that map inputsx \u2208 RD to corresponding ob-\nserved function values y \u2208 R, which we can interpret as the labels of their\nrespective inputs. We will discuss classical model fitting (parameter esti-\nmation) via maximum likelihood and maximum a posteriori estimation,\nas well as Bayesian linear regression, where we integrate the parameters\nout instead of optimizing them.\nChapter 10 focuses on dimensionality reduction, the second pillar i"
  },
  {
    "vector_id": 65,
    "chunk_id": "p21_c3",
    "page_number": 21,
    "text": "um a posteriori estimation,\nas well as Bayesian linear regression, where we integrate the parameters\nout instead of optimizing them.\nChapter 10 focuses on dimensionality reduction, the second pillar in Fig- dimensionality\nreductionure 1.1, using principal component analysis. The key objective of dimen-\nsionality reduction is to find a compact, lower-dimensional representation\nof high-dimensional data x \u2208 RD, which is often easier to analyze than\nthe original data. Unlike regression, dimensionality reduction is only con-\ncerned about modeling the data \u2013 there are no labels associated with a\ndata point x.\nIn Chapter 11, we will move to our third pillar: density estimation. The density estimation\nobjective of density estimation is to find a probability distribution that de-\nscribes a given da"
  },
  {
    "vector_id": 66,
    "chunk_id": "p21_c4",
    "page_number": 21,
    "text": "a point x.\nIn Chapter 11, we will move to our third pillar: density estimation. The density estimation\nobjective of density estimation is to find a probability distribution that de-\nscribes a given dataset. We will focus on Gaussian mixture models for this\npurpose, and we will discuss an iterative scheme to find the parameters of\nthis model. As in dimensionality reduction, there are no labels associated\nwith the data points x \u2208 RD. However, we do not seek a low-dimensional\nrepresentation of the data. Instead, we are interested in a density model\nthat describes the data.\nChapter 12 concludes the book with an in-depth discussion of the fourth\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 67,
    "chunk_id": "p21_c5",
    "page_number": 21,
    "text": "e book with an in-depth discussion of the fourth\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 68,
    "chunk_id": "p22_c0",
    "page_number": 22,
    "text": "16 Introduction and Motivation\npillar: classification. We will discuss classification in the context of supportclassification\nvector machines. Similar to regression (Chapter 9), we have inputs x and\ncorresponding labels y. However, unlike regression, where the labels were\nreal-valued, the labels in classification are integers, which requires special\ncare.\n1.3 Exercises and Feedback\nWe provide some exercises in Part I, which can be done mostly by pen and\npaper. For Part II, we provide programming tutorials (jupyter notebooks)\nto explore some properties of the machine learning algorithms we discuss\nin this book.\nWe appreciate that Cambridge University Press strongly supports our\naim to democratize education and learning by making this book freely\navailable for download at\nhttps://mml-book.co"
  },
  {
    "vector_id": 69,
    "chunk_id": "p22_c1",
    "page_number": 22,
    "text": "uss\nin this book.\nWe appreciate that Cambridge University Press strongly supports our\naim to democratize education and learning by making this book freely\navailable for download at\nhttps://mml-book.com\nwhere tutorials, errata, and additional materials can be found. Mistakes\ncan be reported and feedback provided using the preceding URL.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 70,
    "chunk_id": "p23_c0",
    "page_number": 23,
    "text": "2\nLinear Algebra\nWhen formalizing intuitive concepts, a common approach is to construct a\nset of objects (symbols) and a set of rules to manipulate these objects. This\nis known as an algebra. Linear algebra is the study of vectors and certain algebra\nrules to manipulate vectors. The vectors many of us know from school are\ncalled \u201cgeometric vectors\u201d, which are usually denoted by a small arrow\nabove the letter, e.g., \u2212 \u2192x and \u2212 \u2192y . In this book, we discuss more general\nconcepts of vectors and use a bold letter to represent them, e.g., x and y.\nIn general, vectors are special objects that can be added together and\nmultiplied by scalars to produce another object of the same kind. From\nan abstract mathematical viewpoint, any object that satisfies these two\nproperties can be considered a vector"
  },
  {
    "vector_id": 71,
    "chunk_id": "p23_c1",
    "page_number": 23,
    "text": "added together and\nmultiplied by scalars to produce another object of the same kind. From\nan abstract mathematical viewpoint, any object that satisfies these two\nproperties can be considered a vector. Here are some examples of such\nvector objects:\n1. Geometric vectors. This example of a vector may be familiar from high\nschool mathematics and physics. Geometric vectors \u2013 see Figure 2.1(a)\n\u2013 are directed segments, which can be drawn (at least in two dimen-\nsions). Two geometric vectors\n\u2192\nx,\n\u2192\ny can be added, such that\n\u2192\nx+\n\u2192\ny =\n\u2192\nz\nis another geometric vector. Furthermore, multiplication by a scalar\n\u03bb\n\u2192\nx, \u03bb \u2208 R, is also a geometric vector. In fact, it is the original vector\nscaled by \u03bb. Therefore, geometric vectors are instances of the vector\nconcepts introduced previously . Interpreting"
  },
  {
    "vector_id": 72,
    "chunk_id": "p23_c2",
    "page_number": 23,
    "text": "scalar\n\u03bb\n\u2192\nx, \u03bb \u2208 R, is also a geometric vector. In fact, it is the original vector\nscaled by \u03bb. Therefore, geometric vectors are instances of the vector\nconcepts introduced previously . Interpreting vectors as geometric vec-\ntors enables us to use our intuitions about direction and magnitude to\nreason about mathematical operations.\n2. Polynomials are also vectors; see Figure 2.1(b): Two polynomials can\nFigure 2.1\nDifferent types of\nvectors. Vectors can\nbe surprising\nobjects, including\n(a) geometric\nvectors\nand (b) polynomials.\n\u2192\nx \u2192\ny\n\u2192\nx +\n\u2192\ny\n(a) Geometric vectors.\n\u22122 0 2\nx\n\u22126\n\u22124\n\u22122\n0\n2\n4\ny (b) Polynomials.\n17\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is fr"
  },
  {
    "vector_id": 73,
    "chunk_id": "p23_c3",
    "page_number": 23,
    "text": "(b) Polynomials.\n17\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 74,
    "chunk_id": "p24_c0",
    "page_number": 24,
    "text": "18 Linear Algebra\nbe added together, which results in another polynomial; and they can\nbe multiplied by a scalar \u03bb \u2208 R, and the result is a polynomial as\nwell. Therefore, polynomials are (rather unusual) instances of vectors.\nNote that polynomials are very different from geometric vectors. While\ngeometric vectors are concrete \u201cdrawings\u201d, polynomials are abstract\nconcepts. However, they are both vectors in the sense previously de-\nscribed.\n3. Audio signals are vectors. Audio signals are represented as a series of\nnumbers. We can add audio signals together, and their sum is a new\naudio signal. If we scale an audio signal, we also obtain an audio signal.\nTherefore, audio signals are a type of vector, too.\n4. Elements of Rn (tuples of n real numbers) are vectors. Rn is more\nabstract than polyn"
  },
  {
    "vector_id": 75,
    "chunk_id": "p24_c1",
    "page_number": 24,
    "text": "f we scale an audio signal, we also obtain an audio signal.\nTherefore, audio signals are a type of vector, too.\n4. Elements of Rn (tuples of n real numbers) are vectors. Rn is more\nabstract than polynomials, and it is the concept we focus on in this\nbook. For instance,\na =\n\uf8ee\n\uf8f0\n1\n2\n3\n\uf8f9\n\uf8fb \u2208 R3 (2.1)\nis an example of a triplet of numbers. Adding two vectors a, b \u2208 Rn\ncomponent-wise results in another vector: a + b = c \u2208 Rn. Moreover,\nmultiplying a \u2208 Rn by \u03bb \u2208 R results in a scaled vector \u03bba \u2208 Rn.\nConsidering vectors as elements of Rn has an additional benefit thatBe careful to check\nwhether array\noperations actually\nperform vector\noperations when\nimplementing on a\ncomputer.\nit loosely corresponds to arrays of real numbers on a computer. Many\nprogramming languages support array operations, whi"
  },
  {
    "vector_id": 76,
    "chunk_id": "p24_c2",
    "page_number": 24,
    "text": "operations actually\nperform vector\noperations when\nimplementing on a\ncomputer.\nit loosely corresponds to arrays of real numbers on a computer. Many\nprogramming languages support array operations, which allow for con-\nvenient implementation of algorithms that involve vector operations.\nLinear algebra focuses on the similarities between these vector concepts.\nWe can add them together and multiply them by scalars. We will largelyPavel Grinfeld\u2019s\nseries on linear\nalgebra:\nhttp://tinyurl.\ncom/nahclwm\nGilbert Strang\u2019s\ncourse on linear\nalgebra:\nhttp://tinyurl.\ncom/bdfbu8s5\n3Blue1Brown series\non linear algebra:\nhttps://tinyurl.\ncom/h5g4kps\nfocus on vectors in Rn since most algorithms in linear algebra are for-\nmulated in Rn. We will see in Chapter 8 that we often consider data to\nbe represented a"
  },
  {
    "vector_id": 77,
    "chunk_id": "p24_c3",
    "page_number": 24,
    "text": "ar algebra:\nhttps://tinyurl.\ncom/h5g4kps\nfocus on vectors in Rn since most algorithms in linear algebra are for-\nmulated in Rn. We will see in Chapter 8 that we often consider data to\nbe represented as vectors in Rn. In this book, we will focus on finite-\ndimensional vector spaces, in which case there is a 1:1 correspondence\nbetween any kind of vector and Rn. When it is convenient, we will use\nintuitions about geometric vectors and consider array-based algorithms.\nOne major idea in mathematics is the idea of \u201cclosure\u201d. This is the ques-\ntion: What is the set of all things that can result from my proposed oper-\nations? In the case of vectors: What is the set of vectors that can result by\nstarting with a small set of vectors, and adding them to each other and\nscaling them? This results in a"
  },
  {
    "vector_id": 78,
    "chunk_id": "p24_c4",
    "page_number": 24,
    "text": "my proposed oper-\nations? In the case of vectors: What is the set of vectors that can result by\nstarting with a small set of vectors, and adding them to each other and\nscaling them? This results in a vector space (Section 2.4). The concept of\na vector space and its properties underlie much of machine learning. The\nconcepts introduced in this chapter are summarized in Figure 2.2.\nThis chapter is mostly based on the lecture notes and books by Drumm\nand Weil (2001), Strang (2003), Hogben (2013), Liesen and Mehrmann\n(2015), as well as Pavel Grinfeld\u2019s Linear Algebra series. Other excellent\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 79,
    "chunk_id": "p24_c5",
    "page_number": 24,
    "text": "2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 80,
    "chunk_id": "p25_c0",
    "page_number": 25,
    "text": "2.1 Systems of Linear Equations 19\nFigure 2.2 A mind\nmap of the concepts\nintroduced in this\nchapter, along with\nwhere they are used\nin other parts of the\nbook.\nVector\nVector space\nMatrixChapter 5Vector calculus Group\nSystem oflinear equations\nMatrixinverseGaussianelimination\nLinear/affinemapping\nLinearindependence\nBasis\nChapter 10DimensionalityreductionChapter 12ClassificationChapter 3Analytic geometry\ncomposes\nclosure\nAbelianwith+represents\nrepresents\nsolved bysolves\nproperty of\nmaximal set\nresources are Gilbert Strang\u2019s Linear Algebra course at MIT and the Linear\nAlgebra Series by 3Blue1Brown.\nLinear algebra plays an important role in machine learning and gen-\neral mathematics. The concepts introduced in this chapter are further ex-\npanded to include the idea of geometry in Chapter 3. In"
  },
  {
    "vector_id": 81,
    "chunk_id": "p25_c1",
    "page_number": 25,
    "text": "n.\nLinear algebra plays an important role in machine learning and gen-\neral mathematics. The concepts introduced in this chapter are further ex-\npanded to include the idea of geometry in Chapter 3. In Chapter 5, we\nwill discuss vector calculus, where a principled knowledge of matrix op-\nerations is essential. In Chapter 10, we will use projections (to be intro-\nduced in Section 3.8) for dimensionality reduction with principal compo-\nnent analysis (PCA). In Chapter 9, we will discuss linear regression, where\nlinear algebra plays a central role for solving least-squares problems.\n2.1 Systems of Linear Equations\nSystems of linear equations play a central part of linear algebra. Many\nproblems can be formulated as systems of linear equations, and linear\nalgebra gives us the tools for solving th"
  },
  {
    "vector_id": 82,
    "chunk_id": "p25_c2",
    "page_number": 25,
    "text": "Linear Equations\nSystems of linear equations play a central part of linear algebra. Many\nproblems can be formulated as systems of linear equations, and linear\nalgebra gives us the tools for solving them.\nExample 2.1\nA company produces products N1, . . . , Nn for which resources\nR1, . . . , Rm are required. To produce a unit of product Nj, aij units of\nresource Ri are needed, where i = 1, . . . , mand j = 1, . . . , n.\nThe objective is to find an optimal production plan, i.e., a plan of how\nmany units xj of product Nj should be produced if a total of bi units of\nresource Ri are available and (ideally) no resources are left over.\nIf we produce x1, . . . , xn units of the corresponding products, we need\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press ("
  },
  {
    "vector_id": 83,
    "chunk_id": "p25_c3",
    "page_number": 25,
    "text": "deally) no resources are left over.\nIf we produce x1, . . . , xn units of the corresponding products, we need\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 84,
    "chunk_id": "p26_c0",
    "page_number": 26,
    "text": "20 Linear Algebra\na total of\nai1x1 + \u00b7 \u00b7\u00b7+ ainxn (2.2)\nmany units of resourceRi. An optimal production plan(x1, . . . , xn) \u2208 Rn,\ntherefore, has to satisfy the following system of equations:\na11x1 + \u00b7 \u00b7\u00b7+ a1nxn = b1\n...\nam1x1 + \u00b7 \u00b7\u00b7+ amnxn = bm\n, (2.3)\nwhere aij \u2208 R and bi \u2208 R.\nEquation (2.3) is the general form of a system of linear equations , andsystem of linear\nequations x1, . . . , xn are the unknowns of this system. Every n-tuple (x1, . . . , xn) \u2208\nRn that satisfies (2.3) is a solution of the linear equation system.solution\nExample 2.2\nThe system of linear equations\nx1 + x2 + x3 = 3 (1)\nx1 \u2212 x2 + 2 x3 = 2 (2)\n2x1 + 3 x3 = 1 (3)\n(2.4)\nhas no solution: Adding the first two equations yields2x1 +3x3 = 5, which\ncontradicts the third equation (3).\nLet us have a look at the system of linear"
  },
  {
    "vector_id": 85,
    "chunk_id": "p26_c1",
    "page_number": 26,
    "text": "x1 \u2212 x2 + 2 x3 = 2 (2)\n2x1 + 3 x3 = 1 (3)\n(2.4)\nhas no solution: Adding the first two equations yields2x1 +3x3 = 5, which\ncontradicts the third equation (3).\nLet us have a look at the system of linear equations\nx1 + x2 + x3 = 3 (1)\nx1 \u2212 x2 + 2 x3 = 2 (2)\nx2 + x3 = 2 (3)\n. (2.5)\nFrom the first and third equation, it follows that x1 = 1. From (1)+(2),\nwe get 2x1 + 3x3 = 5, i.e., x3 = 1. From (3), we then get that x2 = 1.\nTherefore, (1, 1, 1) is the only possible and unique solution (verify that\n(1, 1, 1) is a solution by plugging in).\nAs a third example, we consider\nx1 + x2 + x3 = 3 (1)\nx1 \u2212 x2 + 2 x3 = 2 (2)\n2x1 + 3 x3 = 5 (3)\n. (2.6)\nSince (1)+(2)=(3), we can omit the third equation (redundancy). From\n(1) and (2), we get2x1 = 5\u22123x3 and 2x2 = 1+x3. We definex3 = a \u2208 R\nas a free variable, su"
  },
  {
    "vector_id": 86,
    "chunk_id": "p26_c2",
    "page_number": 26,
    "text": "+ 2 x3 = 2 (2)\n2x1 + 3 x3 = 5 (3)\n. (2.6)\nSince (1)+(2)=(3), we can omit the third equation (redundancy). From\n(1) and (2), we get2x1 = 5\u22123x3 and 2x2 = 1+x3. We definex3 = a \u2208 R\nas a free variable, such that any triplet\n\u00125\n2 \u2212 3\n2a, 1\n2 + 1\n2a, a\n\u0013\n, a \u2208 R (2.7)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 87,
    "chunk_id": "p27_c0",
    "page_number": 27,
    "text": "2.1 Systems of Linear Equations 21\nFigure 2.3 The\nsolution space of a\nsystem of two linear\nequations with two\nvariables can be\ngeometrically\ninterpreted as the\nintersection of two\nlines. Every linear\nequation represents\na line.\n2x1 \u2212 4x2 = 1\n4x1 + 4x2 = 5\nx1\nx2\nis a solution of the system of linear equations, i.e., we obtain a solution\nset that contains infinitely many solutions.\nIn general, for a real-valued system of linear equations we obtain either\nno, exactly one, or infinitely many solutions. Linear regression (Chapter 9)\nsolves a version of Example 2.1 when we cannot solve the system of linear\nequations.\nRemark (Geometric Interpretation of Systems of Linear Equations) . In a\nsystem of linear equations with two variables x1, x2, each linear equation\ndefines a line on the x1x2-plane."
  },
  {
    "vector_id": 88,
    "chunk_id": "p27_c1",
    "page_number": 27,
    "text": "linear\nequations.\nRemark (Geometric Interpretation of Systems of Linear Equations) . In a\nsystem of linear equations with two variables x1, x2, each linear equation\ndefines a line on the x1x2-plane. Since a solution to a system of linear\nequations must satisfy all equations simultaneously , the solution set is the\nintersection of these lines. This intersection set can be a line (if the linear\nequations describe the same line), a point, or empty (when the lines are\nparallel). An illustration is given in Figure 2.3 for the system\n4x1 + 4x2 = 5\n2x1 \u2212 4x2 = 1 (2.8)\nwhere the solution space is the point (x1, x2) = (1, 1\n4 ). Similarly , for three\nvariables, each linear equation determines a plane in three-dimensional\nspace. When we intersect these planes, i.e., satisfy all linear equations at"
  },
  {
    "vector_id": 89,
    "chunk_id": "p27_c2",
    "page_number": 27,
    "text": "point (x1, x2) = (1, 1\n4 ). Similarly , for three\nvariables, each linear equation determines a plane in three-dimensional\nspace. When we intersect these planes, i.e., satisfy all linear equations at\nthe same time, we can obtain a solution set that is a plane, a line, a point\nor empty (when the planes have no common intersection). \u2662\nFor a systematic approach to solving systems of linear equations, we\nwill introduce a useful compact notation. We collect the coefficients aij\ninto vectors and collect the vectors into matrices. In other words, we write\nthe system from (2.3) in the following form:\n\uf8ee\n\uf8ef\uf8f0\na11\n...\nam1\n\uf8f9\n\uf8fa\uf8fbx1 +\n\uf8ee\n\uf8ef\uf8f0\na12\n...\nam2\n\uf8f9\n\uf8fa\uf8fbx2 + \u00b7 \u00b7\u00b7+\n\uf8ee\n\uf8ef\uf8f0\na1n\n...\namn\n\uf8f9\n\uf8fa\uf8fbxn =\n\uf8ee\n\uf8ef\uf8f0\nb1\n...\nbm\n\uf8f9\n\uf8fa\uf8fb (2.9)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press ("
  },
  {
    "vector_id": 90,
    "chunk_id": "p27_c3",
    "page_number": 27,
    "text": "\uf8ee\n\uf8ef\uf8f0\na11\n...\nam1\n\uf8f9\n\uf8fa\uf8fbx1 +\n\uf8ee\n\uf8ef\uf8f0\na12\n...\nam2\n\uf8f9\n\uf8fa\uf8fbx2 + \u00b7 \u00b7\u00b7+\n\uf8ee\n\uf8ef\uf8f0\na1n\n...\namn\n\uf8f9\n\uf8fa\uf8fbxn =\n\uf8ee\n\uf8ef\uf8f0\nb1\n...\nbm\n\uf8f9\n\uf8fa\uf8fb (2.9)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 91,
    "chunk_id": "p28_c0",
    "page_number": 28,
    "text": "22 Linear Algebra\n\u21d0 \u21d2\n\uf8ee\n\uf8ef\uf8f0\na11 \u00b7 \u00b7\u00b7 a1n\n... ...\nam1 \u00b7 \u00b7\u00b7 amn\n\uf8f9\n\uf8fa\uf8fb\n\uf8ee\n\uf8ef\uf8f0\nx1\n...\nxn\n\uf8f9\n\uf8fa\uf8fb =\n\uf8ee\n\uf8ef\uf8f0\nb1\n...\nbm\n\uf8f9\n\uf8fa\uf8fb. (2.10)\nIn the following, we will have a close look at these matrices and de-\nfine computation rules. We will return to solving linear equations in Sec-\ntion 2.3.\n2.2 Matrices\nMatrices play a central role in linear algebra. They can be used to com-\npactly represent systems of linear equations, but they also represent linear\nfunctions (linear mappings) as we will see later in Section 2.7. Before we\ndiscuss some of these interesting topics, let us first define what a matrix\nis and what kind of operations we can do with matrices. We will see more\nproperties of matrices in Chapter 4.\nDefinition 2.1 (Matrix). With m, n\u2208 N a real-valued (m, n) matrix A ismatrix\nan m\u00b7n-tuple of elementsaij,"
  },
  {
    "vector_id": 92,
    "chunk_id": "p28_c1",
    "page_number": 28,
    "text": "d of operations we can do with matrices. We will see more\nproperties of matrices in Chapter 4.\nDefinition 2.1 (Matrix). With m, n\u2208 N a real-valued (m, n) matrix A ismatrix\nan m\u00b7n-tuple of elementsaij, i = 1, . . . , m, j = 1, . . . , n, which is ordered\naccording to a rectangular scheme consisting of m rows and n columns:\nA =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\na11 a12 \u00b7 \u00b7\u00b7 a1n\na21 a22 \u00b7 \u00b7\u00b7 a2n\n... ... ...\nam1 am2 \u00b7 \u00b7\u00b7 amn\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb, a ij \u2208 R. (2.11)\nBy convention (1, n)-matrices are calledrows and (m, 1)-matrices are calledrow\ncolumns. These special matrices are also called row/column vectors.column\nrow vector\ncolumn vector\nFigure 2.4 By\nstacking its\ncolumns, a matrix A\ncan be represented\nas a long vector a.\nre-shape\nA\u2208R4\u00d72 a\u2208R8\nRm\u00d7n is the set of all real-valued (m, n)-matrices. A \u2208 Rm\u00d7n can be\nequivalently represented"
  },
  {
    "vector_id": 93,
    "chunk_id": "p28_c2",
    "page_number": 28,
    "text": "Figure 2.4 By\nstacking its\ncolumns, a matrix A\ncan be represented\nas a long vector a.\nre-shape\nA\u2208R4\u00d72 a\u2208R8\nRm\u00d7n is the set of all real-valued (m, n)-matrices. A \u2208 Rm\u00d7n can be\nequivalently represented as a \u2208 Rmn by stacking all n columns of the\nmatrix into a long vector; see Figure 2.4.\n2.2.1 Matrix Addition and Multiplication\nThe sum of two matricesA \u2208 Rm\u00d7n, B \u2208 Rm\u00d7n is defined as the element-\nwise sum, i.e.,\nA + B :=\n\uf8ee\n\uf8ef\uf8f0\na11 + b11 \u00b7 \u00b7\u00b7 a1n + b1n\n... ...\nam1 + bm1 \u00b7 \u00b7\u00b7 amn + bmn\n\uf8f9\n\uf8fa\uf8fb \u2208 Rm\u00d7n . (2.12)\nFor matrices A \u2208 Rm\u00d7n, B \u2208 Rn\u00d7k, the elements cij of the productNote the size of the\nmatrices. C = AB \u2208 Rm\u00d7k are computed as\nC =\nnp.einsum(\u2019il,\nlj\u2019, A, B) cij =\nnX\nl=1\nailblj, i = 1, . . . , m, j= 1, . . . , k. (2.13)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://"
  },
  {
    "vector_id": 94,
    "chunk_id": "p28_c3",
    "page_number": 28,
    "text": ". C = AB \u2208 Rm\u00d7k are computed as\nC =\nnp.einsum(\u2019il,\nlj\u2019, A, B) cij =\nnX\nl=1\nailblj, i = 1, . . . , m, j= 1, . . . , k. (2.13)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 95,
    "chunk_id": "p29_c0",
    "page_number": 29,
    "text": "2.2 Matrices 23\nThis means, to compute element cij we multiply the elements of the ith There are n columns\nin A and n rows in\nB so that we can\ncompute ailblj for\nl = 1, . . . , n.\nCommonly , the dot\nproduct between\ntwo vectors a, b is\ndenoted by a\u22a4b or\n\u27e8a, b\u27e9.\nrow of A with the jth column of B and sum them up. Later in Section 3.2,\nwe will call this the dot product of the corresponding row and column. In\ncases, where we need to be explicit that we are performing multiplication,\nwe use the notation A \u00b7 B to denote multiplication (explicitly showing\n\u201c\u00b7\u201d).\nRemark. Matrices can only be multiplied if their \u201cneighboring\u201d dimensions\nmatch. For instance, an n \u00d7 k-matrix A can be multiplied with a k \u00d7 m-\nmatrix B, but only from the left side:\nA|{z}\nn\u00d7k\nB|{z}\nk\u00d7m\n= C|{z}\nn\u00d7m\n(2.14)\nThe product BA is"
  },
  {
    "vector_id": 96,
    "chunk_id": "p29_c1",
    "page_number": 29,
    "text": "if their \u201cneighboring\u201d dimensions\nmatch. For instance, an n \u00d7 k-matrix A can be multiplied with a k \u00d7 m-\nmatrix B, but only from the left side:\nA|{z}\nn\u00d7k\nB|{z}\nk\u00d7m\n= C|{z}\nn\u00d7m\n(2.14)\nThe product BA is not defined ifm \u0338= n since the neighboring dimensions\ndo not match. \u2662\nRemark. Matrix multiplication isnot defined as an element-wise operation\non matrix elements, i.e., cij \u0338= aijbij (even if the size of A, B was cho-\nsen appropriately). This kind of element-wise multiplication often appears\nin programming languages when we multiply (multi-dimensional) arrays\nwith each other, and is called a Hadamard product. \u2662 Hadamard product\nExample 2.3\nFor A =\n\u00141 2 3\n3 2 1\n\u0015\n\u2208 R2\u00d73, B =\n\uf8ee\n\uf8f0\n0 2\n1 \u22121\n0 1\n\uf8f9\n\uf8fb \u2208 R3\u00d72, we obtain\nAB =\n\u00141 2 3\n3 2 1\n\u0015\uf8ee\n\uf8f0\n0 2\n1 \u22121\n0 1\n\uf8f9\n\uf8fb =\n\u00142 3\n2 5\n\u0015\n\u2208 R2\u00d72, (2.15)\nBA =\n\uf8ee\n\uf8f0\n0 2"
  },
  {
    "vector_id": 97,
    "chunk_id": "p29_c2",
    "page_number": 29,
    "text": "mard product. \u2662 Hadamard product\nExample 2.3\nFor A =\n\u00141 2 3\n3 2 1\n\u0015\n\u2208 R2\u00d73, B =\n\uf8ee\n\uf8f0\n0 2\n1 \u22121\n0 1\n\uf8f9\n\uf8fb \u2208 R3\u00d72, we obtain\nAB =\n\u00141 2 3\n3 2 1\n\u0015\uf8ee\n\uf8f0\n0 2\n1 \u22121\n0 1\n\uf8f9\n\uf8fb =\n\u00142 3\n2 5\n\u0015\n\u2208 R2\u00d72, (2.15)\nBA =\n\uf8ee\n\uf8f0\n0 2\n1 \u22121\n0 1\n\uf8f9\n\uf8fb\n\u00141 2 3\n3 2 1\n\u0015\n=\n\uf8ee\n\uf8f0\n6 4 2\n\u22122 0 2\n3 2 1\n\uf8f9\n\uf8fb \u2208 R3\u00d73 . (2.16)\nFigure 2.5 Even if\nboth matrix\nmultiplications AB\nand BA are\ndefined, the\ndimensions of the\nresults can be\ndifferent.\nFrom this example, we can already see that matrix multiplication is not\ncommutative, i.e., AB \u0338= BA; see also Figure 2.5 for an illustration.\nDefinition 2.2 (Identity Matrix). In Rn\u00d7n, we define the identity matrix\nidentity matrix\nIn :=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1 0 \u00b7 \u00b7\u00b7 0 \u00b7 \u00b7\u00b7 0\n0 1 \u00b7 \u00b7\u00b7 0 \u00b7 \u00b7\u00b7 0\n... ... ... ... ... ...\n0 0 \u00b7 \u00b7\u00b7 1 \u00b7 \u00b7\u00b7 0\n... ... ... ... ... ...\n0 0 \u00b7 \u00b7\u00b7 0 \u00b7 \u00b7\u00b7 1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u2208 Rn\u00d7n (2.17)\n\u00a92024 M. P. De"
  },
  {
    "vector_id": 98,
    "chunk_id": "p29_c3",
    "page_number": 29,
    "text": "atrix\nidentity matrix\nIn :=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1 0 \u00b7 \u00b7\u00b7 0 \u00b7 \u00b7\u00b7 0\n0 1 \u00b7 \u00b7\u00b7 0 \u00b7 \u00b7\u00b7 0\n... ... ... ... ... ...\n0 0 \u00b7 \u00b7\u00b7 1 \u00b7 \u00b7\u00b7 0\n... ... ... ... ... ...\n0 0 \u00b7 \u00b7\u00b7 0 \u00b7 \u00b7\u00b7 1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u2208 Rn\u00d7n (2.17)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 99,
    "chunk_id": "p30_c0",
    "page_number": 30,
    "text": "24 Linear Algebra\nas the n \u00d7 n-matrix containing 1 on the diagonal and 0 everywhere else.\nNow that we defined matrix multiplication, matrix addition and the\nidentity matrix, let us have a look at some properties of matrices:\nassociativity\nAssociativity:\n\u2200A \u2208 Rm\u00d7n, B \u2208 Rn\u00d7p, C \u2208 Rp\u00d7q : (AB)C = A(BC) (2.18)\ndistributivity\nDistributivity:\n\u2200A, B \u2208 Rm\u00d7n, C, D \u2208 Rn\u00d7p : (A + B)C = AC + BC (2.19a)\nA(C + D) = AC + AD (2.19b)\nMultiplication with the identity matrix:\n\u2200A \u2208 Rm\u00d7n : ImA = AIn = A (2.20)\nNote that Im \u0338= In for m \u0338= n.\n2.2.2 Inverse and Transpose\nDefinition 2.3 (Inverse). Consider a square matrix A \u2208 Rn\u00d7n. Let matrixA square matrix\npossesses the same\nnumber of columns\nand rows.\nB \u2208 Rn\u00d7n have the property that AB = In = BA. B is called the\ninverse of A and denoted by A\u22121.\ninverse Unfortunat"
  },
  {
    "vector_id": 100,
    "chunk_id": "p30_c1",
    "page_number": 30,
    "text": "rix A \u2208 Rn\u00d7n. Let matrixA square matrix\npossesses the same\nnumber of columns\nand rows.\nB \u2208 Rn\u00d7n have the property that AB = In = BA. B is called the\ninverse of A and denoted by A\u22121.\ninverse Unfortunately , not every matrix A possesses an inverse A\u22121. If this\ninverse does exist, A is called regular/invertible/nonsingular, otherwiseregular\ninvertible\nnonsingular\nsingular/noninvertible. When the matrix inverse exists, it is unique. In Sec-\nsingular\nnoninvertible\ntion 2.3, we will discuss a general way to compute the inverse of a matrix\nby solving a system of linear equations.\nRemark (Existence of the Inverse of a 2 \u00d7 2-matrix). Consider a matrix\nA :=\n\u0014a11 a12\na21 a22\n\u0015\n\u2208 R2\u00d72 . (2.21)\nIf we multiply A with\nA\u2032 :=\n\u0014 a22 \u2212a12\n\u2212a21 a11\n\u0015\n(2.22)\nwe obtain\nAA\u2032 =\n\u0014a11a22 \u2212 a12a21 0\n0 a11a22 \u2212 a12a21"
  },
  {
    "vector_id": 101,
    "chunk_id": "p30_c2",
    "page_number": 30,
    "text": "the Inverse of a 2 \u00d7 2-matrix). Consider a matrix\nA :=\n\u0014a11 a12\na21 a22\n\u0015\n\u2208 R2\u00d72 . (2.21)\nIf we multiply A with\nA\u2032 :=\n\u0014 a22 \u2212a12\n\u2212a21 a11\n\u0015\n(2.22)\nwe obtain\nAA\u2032 =\n\u0014a11a22 \u2212 a12a21 0\n0 a11a22 \u2212 a12a21\n\u0015\n= (a11a22 \u2212 a12a21)I .\n(2.23)\nTherefore,\nA\u22121 = 1\na11a22 \u2212 a12a21\n\u0014 a22 \u2212a12\n\u2212a21 a11\n\u0015\n(2.24)\nif and only if a11a22 \u2212a12a21 \u0338= 0. In Section 4.1, we will see thata11a22 \u2212\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 102,
    "chunk_id": "p31_c0",
    "page_number": 31,
    "text": "2.2 Matrices 25\na12a21 is the determinant of a2\u00d72-matrix. Furthermore, we can generally\nuse the determinant to check whether a matrix is invertible. \u2662\nExample 2.4 (Inverse Matrix)\nThe matrices\nA =\n\uf8ee\n\uf8f0\n1 2 1\n4 4 5\n6 7 7\n\uf8f9\n\uf8fb , B =\n\uf8ee\n\uf8f0\n\u22127 \u22127 6\n2 1 \u22121\n4 5 \u22124\n\uf8f9\n\uf8fb (2.25)\nare inverse to each other since AB = I = BA.\nDefinition 2.4 (Transpose). For A \u2208 Rm\u00d7n the matrix B \u2208 Rn\u00d7m with\nbij = aji is called the transpose of A. We write B = A\u22a4. transpose\nThe main diagonal\n(sometimes called\n\u201cprincipal diagonal\u201d,\n\u201cprimary diagonal\u201d,\n\u201cleading diagonal\u201d,\nor \u201cmajor diagonal\u201d)\nof a matrix A is the\ncollection of entries\nAij where i = j.\nIn general, A\u22a4 can be obtained by writing the columns ofA as the rows\nof A\u22a4. The following are important properties of inverses and transposes:\nThe scalar case of\n(2.28) is\n1\n2+"
  },
  {
    "vector_id": 103,
    "chunk_id": "p31_c1",
    "page_number": 31,
    "text": "tries\nAij where i = j.\nIn general, A\u22a4 can be obtained by writing the columns ofA as the rows\nof A\u22a4. The following are important properties of inverses and transposes:\nThe scalar case of\n(2.28) is\n1\n2+4 = 1\n6 \u0338= 1\n2 + 1\n4 .\nAA\u22121 = I = A\u22121A (2.26)\n(AB)\u22121 = B\u22121A\u22121 (2.27)\n(A + B)\u22121 \u0338= A\u22121 + B\u22121 (2.28)\n(A\u22a4)\u22a4 = A (2.29)\n(AB)\u22a4 = B\u22a4A\u22a4 (2.30)\n(A + B)\u22a4 = A\u22a4 + B\u22a4 (2.31)\nDefinition 2.5 (Symmetric Matrix). A matrix A \u2208 Rn\u00d7n is symmetric if symmetric matrix\nA = A\u22a4.\nNote that only (n, n)-matrices can be symmetric. Generally , we call\n(n, n)-matrices also square matrices because they possess the same num- square matrix\nber of rows and columns. Moreover, if A is invertible, then so is A\u22a4, and\n(A\u22121)\u22a4 = (A\u22a4)\u22121 =: A\u2212\u22a4.\nRemark (Sum and Product of Symmetric Matrices) . The sum of symmet-\nric matrices A, B \u2208 Rn\u00d7"
  },
  {
    "vector_id": 104,
    "chunk_id": "p31_c2",
    "page_number": 31,
    "text": "are matrix\nber of rows and columns. Moreover, if A is invertible, then so is A\u22a4, and\n(A\u22121)\u22a4 = (A\u22a4)\u22121 =: A\u2212\u22a4.\nRemark (Sum and Product of Symmetric Matrices) . The sum of symmet-\nric matrices A, B \u2208 Rn\u00d7n is always symmetric. However, although their\nproduct is always defined, it is generally not symmetric:\n\u00141 0\n0 0\n\u0015\u00141 1\n1 1\n\u0015\n=\n\u00141 1\n0 0\n\u0015\n. (2.32)\n\u2662\n2.2.3 Multiplication by a Scalar\nLet us look at what happens to matrices when they are multiplied by a\nscalar \u03bb \u2208 R. Let A \u2208 Rm\u00d7n and \u03bb \u2208 R. Then \u03bbA = K, Kij = \u03bb aij.\nPractically ,\u03bb scales each element of A. For \u03bb, \u03c8\u2208 R, the following holds:\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 105,
    "chunk_id": "p31_c3",
    "page_number": 31,
    "text": "P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 106,
    "chunk_id": "p32_c0",
    "page_number": 32,
    "text": "26 Linear Algebra\nassociativity\nAssociativity:\n(\u03bb\u03c8)C = \u03bb(\u03c8C), C \u2208 Rm\u00d7n\n\u03bb(BC) = (\u03bbB)C = B(\u03bbC) = (BC)\u03bb, B \u2208 Rm\u00d7n, C \u2208 Rn\u00d7k.\nNote that this allows us to move scalar values around.\n(\u03bbC)\u22a4 = C\u22a4\u03bb\u22a4 = C\u22a4\u03bb = \u03bbC\u22a4 since \u03bb = \u03bb\u22a4 for all \u03bb \u2208 R.distributivity\nDistributivity:\n(\u03bb + \u03c8)C = \u03bbC + \u03c8C, C \u2208 Rm\u00d7n\n\u03bb(B + C) = \u03bbB + \u03bbC, B, C \u2208 Rm\u00d7n\nExample 2.5 (Distributivity)\nIf we define\nC :=\n\u00141 2\n3 4\n\u0015\n, (2.33)\nthen for any \u03bb, \u03c8\u2208 R we obtain\n(\u03bb + \u03c8)C =\n\u0014(\u03bb + \u03c8)1 ( \u03bb + \u03c8)2\n(\u03bb + \u03c8)3 ( \u03bb + \u03c8)4\n\u0015\n=\n\u0014 \u03bb + \u03c8 2\u03bb + 2\u03c8\n3\u03bb + 3\u03c8 4\u03bb + 4\u03c8\n\u0015\n(2.34a)\n=\n\u0014 \u03bb 2\u03bb\n3\u03bb 4\u03bb\n\u0015\n+\n\u0014 \u03c8 2\u03c8\n3\u03c8 4\u03c8\n\u0015\n= \u03bbC + \u03c8C . (2.34b)\n2.2.4 Compact Representations of Systems of Linear Equations\nIf we consider the system of linear equations\n2x1 + 3x2 + 5x3 = 1\n4x1 \u2212 2x2 \u2212 7x3 = 8\n9x1 + 5x2 \u2212 3x3 = 2\n(2.35)\nand use the rules for matrix multiplication, we can write"
  },
  {
    "vector_id": 107,
    "chunk_id": "p32_c1",
    "page_number": 32,
    "text": "f Systems of Linear Equations\nIf we consider the system of linear equations\n2x1 + 3x2 + 5x3 = 1\n4x1 \u2212 2x2 \u2212 7x3 = 8\n9x1 + 5x2 \u2212 3x3 = 2\n(2.35)\nand use the rules for matrix multiplication, we can write this equation\nsystem in a more compact form as\n\uf8ee\n\uf8f0\n2 3 5\n4 \u22122 \u22127\n9 5 \u22123\n\uf8f9\n\uf8fb\n\uf8ee\n\uf8f0\nx1\nx2\nx3\n\uf8f9\n\uf8fb =\n\uf8ee\n\uf8f0\n1\n8\n2\n\uf8f9\n\uf8fb. (2.36)\nNote that x1 scales the first column, x2 the second one, and x3 the third\none.\nGenerally , a system of linear equations can be compactly represented in\ntheir matrix form as Ax = b; see (2.3), and the product Ax is a (linear)\ncombination of the columns of A. We will discuss linear combinations in\nmore detail in Section 2.5.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 108,
    "chunk_id": "p32_c2",
    "page_number": 32,
    "text": "ombinations in\nmore detail in Section 2.5.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 109,
    "chunk_id": "p33_c0",
    "page_number": 33,
    "text": "2.3 Solving Systems of Linear Equations 27\n2.3 Solving Systems of Linear Equations\nIn (2.3), we introduced the general form of an equation system, i.e.,\na11x1 + \u00b7 \u00b7\u00b7+ a1nxn = b1\n...\nam1x1 + \u00b7 \u00b7\u00b7+ amnxn = bm ,\n(2.37)\nwhere aij \u2208 R and bi \u2208 R are known constants and xj are unknowns,\ni = 1, . . . , m, j = 1, . . . , n. Thus far, we saw that matrices can be used as\na compact way of formulating systems of linear equations so that we can\nwrite Ax = b, see (2.10). Moreover, we defined basic matrix operations,\nsuch as addition and multiplication of matrices. In the following, we will\nfocus on solving systems of linear equations and provide an algorithm for\nfinding the inverse of a matrix.\n2.3.1 Particular and General Solution\nBefore discussing how to generally solve systems of linear equations, le"
  },
  {
    "vector_id": 110,
    "chunk_id": "p33_c1",
    "page_number": 33,
    "text": "systems of linear equations and provide an algorithm for\nfinding the inverse of a matrix.\n2.3.1 Particular and General Solution\nBefore discussing how to generally solve systems of linear equations, let\nus have a look at an example. Consider the system of equations\n\u00141 0 8 \u22124\n0 1 2 12\n\u0015\n\uf8ee\n\uf8ef\uf8ef\uf8f0\nx1\nx2\nx3\nx4\n\uf8f9\n\uf8fa\uf8fa\uf8fb =\n\u001442\n8\n\u0015\n. (2.38)\nThe system has two equations and four unknowns. Therefore, in general\nwe would expect infinitely many solutions. This system of equations is\nin a particularly easy form, where the first two columns consist of a 1\nand a 0. Remember that we want to find scalars x1, . . . , x4, such thatP4\ni=1 xici = b, where we define ci to be the ith column of the matrix and\nb the right-hand-side of (2.38). A solution to the problem in (2.38) can\nbe found immediately by taking 42 time"
  },
  {
    "vector_id": 111,
    "chunk_id": "p33_c2",
    "page_number": 33,
    "text": "x4, such thatP4\ni=1 xici = b, where we define ci to be the ith column of the matrix and\nb the right-hand-side of (2.38). A solution to the problem in (2.38) can\nbe found immediately by taking 42 times the first column and 8 times the\nsecond column so that\nb =\n\u001442\n8\n\u0015\n= 42\n\u00141\n0\n\u0015\n+ 8\n\u00140\n1\n\u0015\n. (2.39)\nTherefore, a solution is [42, 8, 0, 0]\u22a4. This solution is called a particular particular solution\nsolution or special solution. However, this is not the only solution of this special solution\nsystem of linear equations. To capture all the other solutions, we need\nto be creative in generating 0 in a non-trivial way using the columns of\nthe matrix: Adding 0 to our special solution does not change the special\nsolution. To do so, we express the third column using the first two columns\n(which are of"
  },
  {
    "vector_id": 112,
    "chunk_id": "p33_c3",
    "page_number": 33,
    "text": "non-trivial way using the columns of\nthe matrix: Adding 0 to our special solution does not change the special\nsolution. To do so, we express the third column using the first two columns\n(which are of this very simple form)\n\u00148\n2\n\u0015\n= 8\n\u00141\n0\n\u0015\n+ 2\n\u00140\n1\n\u0015\n(2.40)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 113,
    "chunk_id": "p34_c0",
    "page_number": 34,
    "text": "28 Linear Algebra\nso that 0 = 8c1 + 2c2 \u2212 1c3 + 0c4 and (x1, x2, x3, x4) = (8, 2, \u22121, 0). In\nfact, any scaling of this solution by \u03bb1 \u2208 R produces the 0 vector, i.e.,\n\u00141 0 8 \u22124\n0 1 2 12\n\u0015\n\uf8eb\n\uf8ec\uf8ec\uf8ed\u03bb1\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n8\n2\n\u22121\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\uf8f6\n\uf8f7\uf8f7\uf8f8 = \u03bb1(8c1 + 2c2 \u2212 c3) = 0 . (2.41)\nFollowing the same line of reasoning, we express the fourth column of the\nmatrix in (2.38) using the first two columns and generate another set of\nnon-trivial versions of 0 as\n\u00141 0 8 \u22124\n0 1 2 12\n\u0015\n\uf8eb\n\uf8ec\uf8ec\uf8ed\u03bb2\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22124\n12\n0\n\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\uf8f6\n\uf8f7\uf8f7\uf8f8 = \u03bb2(\u22124c1 + 12c2 \u2212 c4) = 0 (2.42)\nfor any \u03bb2 \u2208 R. Putting everything together, we obtain all solutions of the\nequation system in (2.38), which is called the general solution, as the setgeneral solution\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\nx \u2208 R4 : x =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n42\n8\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb + \u03bb1\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n8\n2\n\u22121\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb + \u03bb2\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22124\n12\n0\n\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fb , \u03bb1, \u03bb2"
  },
  {
    "vector_id": 114,
    "chunk_id": "p34_c1",
    "page_number": 34,
    "text": "equation system in (2.38), which is called the general solution, as the setgeneral solution\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\nx \u2208 R4 : x =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n42\n8\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb + \u03bb1\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n8\n2\n\u22121\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb + \u03bb2\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22124\n12\n0\n\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fb , \u03bb1, \u03bb2 \u2208 R\n\uf8fc\n\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8fe\n. (2.43)\nRemark. The general approach we followed consisted of the following\nthree steps:\n1. Find a particular solution to Ax = b.\n2. Find all solutions to Ax = 0.\n3. Combine the solutions from steps 1. and 2. to the general solution.\nNeither the general nor the particular solution is unique. \u2662\nThe system of linear equations in the preceding example was easy to\nsolve because the matrix in (2.38) has this particularly convenient form,\nwhich allowed us to find the particular and the general solution by in-\nspection. However, general equation systems are not of this simple form.\nFor"
  },
  {
    "vector_id": 115,
    "chunk_id": "p34_c2",
    "page_number": 34,
    "text": "ix in (2.38) has this particularly convenient form,\nwhich allowed us to find the particular and the general solution by in-\nspection. However, general equation systems are not of this simple form.\nFortunately , there exists a constructive algorithmic way of transforming\nany system of linear equations into this particularly simple form: Gaussian\nelimination. Key to Gaussian elimination are elementary transformations\nof systems of linear equations, which transform the equation system into\na simple form. Then, we can apply the three steps to the simple form that\nwe just discussed in the context of the example in (2.38).\n2.3.2 Elementary Transformations\nKey to solving a system of linear equations areelementary transformationselementary\ntransformations that keep the solution set the same, but t"
  },
  {
    "vector_id": 116,
    "chunk_id": "p34_c3",
    "page_number": 34,
    "text": "f the example in (2.38).\n2.3.2 Elementary Transformations\nKey to solving a system of linear equations areelementary transformationselementary\ntransformations that keep the solution set the same, but that transform the equation system\ninto a simpler form:\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 117,
    "chunk_id": "p35_c0",
    "page_number": 35,
    "text": "2.3 Solving Systems of Linear Equations 29\nExchange of two equations (rows in the matrix representing the system\nof equations)\nMultiplication of an equation (row) with a constant \u03bb \u2208 R\\{0}\nAddition of two equations (rows)\nExample 2.6\nFor a \u2208 R, we seek all solutions of the following system of equations:\n\u22122x1 + 4 x2 \u2212 2x3 \u2212 x4 + 4 x5 = \u22123\n4x1 \u2212 8x2 + 3 x3 \u2212 3x4 + x5 = 2\nx1 \u2212 2x2 + x3 \u2212 x4 + x5 = 0\nx1 \u2212 2x2 \u2212 3x4 + 4 x5 = a\n. (2.44)\nWe start by converting this system of equations into the compact matrix\nnotation Ax = b. We no longer mention the variables x explicitly and\nbuild the augmented matrix (in the form\n\u0002\nA|b\n\u0003\n) augmented matrix\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22122 4 \u22122 \u22121 4 \u22123\n4 \u22128 3 \u22123 1 2\n1 \u22122 1 \u22121 1 0\n1 \u22122 0 \u22123 4 a\n\uf8f9\n\uf8fa\uf8fa\uf8fb\nSwap with R3\nSwap with R1\nwhere we used the vertical line to separate the left-hand sid"
  },
  {
    "vector_id": 118,
    "chunk_id": "p35_c1",
    "page_number": 35,
    "text": "ix (in the form\n\u0002\nA|b\n\u0003\n) augmented matrix\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22122 4 \u22122 \u22121 4 \u22123\n4 \u22128 3 \u22123 1 2\n1 \u22122 1 \u22121 1 0\n1 \u22122 0 \u22123 4 a\n\uf8f9\n\uf8fa\uf8fa\uf8fb\nSwap with R3\nSwap with R1\nwhere we used the vertical line to separate the left-hand side from the\nright-hand side in (2.44). We use \u21dd to indicate a transformation of the\naugmented matrix using elementary transformations. The augmented\nmatrix\n\u0002\nA|b\n\u0003\ncompactly\nrepresents the\nsystem of linear\nequations Ax = b.\nSwapping Rows 1 and 3 leads to\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 \u22122 1 \u22121 1 0\n4 \u22128 3 \u22123 1 2\n\u22122 4 \u22122 \u22121 4 \u22123\n1 \u22122 0 \u22123 4 a\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u22124R1\n+2R1\n\u2212R1\nWhen we now apply the indicated transformations (e.g., subtract Row 1\nfour times from Row 2), we obtain\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 \u22122 1 \u22121 1 0\n0 0 \u22121 1 \u22123 2\n0 0 0 \u22123 6 \u22123\n0 0 \u22121 \u22122 3 a\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u2212R2 \u2212 R3\n\u21dd\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 \u22122 1 \u22121 1 0\n0 0 \u22121 1 \u22123 2\n0 0 0 \u22123 6 \u22123\n0 0 0 0 0 a+1\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u00b7(\u22121)\n\u00b7(\u22121"
  },
  {
    "vector_id": 119,
    "chunk_id": "p35_c2",
    "page_number": 35,
    "text": "t Row 1\nfour times from Row 2), we obtain\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 \u22122 1 \u22121 1 0\n0 0 \u22121 1 \u22123 2\n0 0 0 \u22123 6 \u22123\n0 0 \u22121 \u22122 3 a\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u2212R2 \u2212 R3\n\u21dd\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 \u22122 1 \u22121 1 0\n0 0 \u22121 1 \u22123 2\n0 0 0 \u22123 6 \u22123\n0 0 0 0 0 a+1\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u00b7(\u22121)\n\u00b7(\u22121\n3 )\n\u21dd\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 \u22122 1 \u22121 1 0\n0 0 1 \u22121 3 \u22122\n0 0 0 1 \u22122 1\n0 0 0 0 0 a+1\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 120,
    "chunk_id": "p36_c0",
    "page_number": 36,
    "text": "30 Linear Algebra\nThis (augmented) matrix is in a convenient form, the row-echelon formrow-echelon form\n(REF). Reverting this compact notation back into the explicit notation with\nthe variables we seek, we obtain\nx1 \u2212 2x2 + x3 \u2212 x4 + x5 = 0\nx3 \u2212 x4 + 3 x5 = \u22122\nx4 \u2212 2x5 = 1\n0 = a + 1\n. (2.45)\nOnly for a = \u22121 this system can be solved. A particular solution isparticular solution\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nx1\nx2\nx3\nx4\nx5\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n2\n0\n\u22121\n1\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n. (2.46)\nThe general solution, which captures the set of all possible solutions, isgeneral solution\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nx \u2208 R5 : x =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n2\n0\n\u22121\n1\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n+ \u03bb1\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n2\n1\n0\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n+ \u03bb2\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n2\n0\n\u22121\n2\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, \u03bb 1, \u03bb2 \u2208 R\n\uf8fc\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8fe\n. (2.47)\nIn the following, we will detail a constructive way to obtain a particular\nand general solution of a syste"
  },
  {
    "vector_id": 121,
    "chunk_id": "p36_c1",
    "page_number": 36,
    "text": "\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n2\n1\n0\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n+ \u03bb2\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n2\n0\n\u22121\n2\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, \u03bb 1, \u03bb2 \u2208 R\n\uf8fc\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8fe\n. (2.47)\nIn the following, we will detail a constructive way to obtain a particular\nand general solution of a system of linear equations.\nRemark (Pivots and Staircase Structure). The leading coefficient of a row\n(first nonzero number from the left) is called the pivot and is alwayspivot\nstrictly to the right of the pivot of the row above it. Therefore, any equa-\ntion system in row-echelon form always has a \u201cstaircase\u201d structure. \u2662\nDefinition 2.6 (Row-Echelon Form). A matrix is in row-echelon form ifrow-echelon form\nAll rows that contain only zeros are at the bottom of the matrix; corre-\nspondingly , all rows that contain at least one nonzero element are on\ntop of rows that contain only zeros.\nLooking at no"
  },
  {
    "vector_id": 122,
    "chunk_id": "p36_c2",
    "page_number": 36,
    "text": "form\nAll rows that contain only zeros are at the bottom of the matrix; corre-\nspondingly , all rows that contain at least one nonzero element are on\ntop of rows that contain only zeros.\nLooking at nonzero rows only , the first nonzero number from the left\n(also called the pivot or the leading coefficient) is always strictly to thepivot\nleading coefficient right of the pivot of the row above it.\nIn other texts, it is\nsometimes required\nthat the pivot is 1.\nRemark (Basic and Free Variables) . The variables corresponding to the\npivots in the row-echelon form are called basic variables and the other\nbasic variable variables are free variables . For example, in (2.45), x1, x3, x4 are basic\nfree variable variables, whereas x2, x5 are free variables. \u2662\nRemark (Obtaining a Particular Solution) ."
  },
  {
    "vector_id": 123,
    "chunk_id": "p36_c3",
    "page_number": 36,
    "text": "her\nbasic variable variables are free variables . For example, in (2.45), x1, x3, x4 are basic\nfree variable variables, whereas x2, x5 are free variables. \u2662\nRemark (Obtaining a Particular Solution) . The row-echelon form makes\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 124,
    "chunk_id": "p37_c0",
    "page_number": 37,
    "text": "2.3 Solving Systems of Linear Equations 31\nour lives easier when we need to determine a particular solution. To do\nthis, we express the right-hand side of the equation system using the pivot\ncolumns, such that b = PP\ni=1 \u03bbipi, where pi, i= 1, . . . , P, are the pivot\ncolumns. The \u03bbi are determined easiest if we start with the rightmost pivot\ncolumn and work our way to the left.\nIn the previous example, we would try to find \u03bb1, \u03bb2, \u03bb3 so that\n\u03bb1\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n0\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb + \u03bb2\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n1\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb + \u03bb3\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22121\n\u22121\n1\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n0\n\u22122\n1\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb . (2.48)\nFrom here, we find relatively directly that\u03bb3 = 1, \u03bb2 = \u22121, \u03bb1 = 2. When\nwe put everything together, we must not forget the non-pivot columns\nfor which we set the coefficients implicitly to 0. Therefore, we get the\nparticular solution x = [2, 0, \u2212"
  },
  {
    "vector_id": 125,
    "chunk_id": "p37_c1",
    "page_number": 37,
    "text": "1, \u03bb2 = \u22121, \u03bb1 = 2. When\nwe put everything together, we must not forget the non-pivot columns\nfor which we set the coefficients implicitly to 0. Therefore, we get the\nparticular solution x = [2, 0, \u22121, 1, 0]\u22a4. \u2662\nRemark (Reduced Row Echelon Form). An equation system is in reduced reduced\nrow-echelon formrow-echelon form (also: row-reduced echelon form or row canonical form) if\nIt is in row-echelon form.\nEvery pivot is 1.\nThe pivot is the only nonzero entry in its column.\n\u2662\nThe reduced row-echelon form will play an important role later in Sec-\ntion 2.3.3 because it allows us to determine the general solution of a sys-\ntem of linear equations in a straightforward way . Gaussian\neliminationRemark (Gaussian Elimination). Gaussian elimination is an algorithm that\nperforms elementary transformat"
  },
  {
    "vector_id": 126,
    "chunk_id": "p37_c2",
    "page_number": 37,
    "text": "neral solution of a sys-\ntem of linear equations in a straightforward way . Gaussian\neliminationRemark (Gaussian Elimination). Gaussian elimination is an algorithm that\nperforms elementary transformations to bring a system of linear equations\ninto reduced row-echelon form. \u2662\nExample 2.7 (Reduced Row Echelon Form)\nVerify that the following matrix is in reduced row-echelon form (the pivots\nare in bold):\nA =\n\uf8ee\n\uf8f0\n1 3 0 0 3\n0 0 1 0 9\n0 0 0 1 \u22124\n\uf8f9\n\uf8fb . (2.49)\nThe key idea for finding the solutions of Ax = 0 is to look at the non-\npivot columns, which we will need to express as a (linear) combination of\nthe pivot columns. The reduced row echelon form makes this relatively\nstraightforward, and we express the non-pivot columns in terms of sums\nand multiples of the pivot columns that are on their lef"
  },
  {
    "vector_id": 127,
    "chunk_id": "p37_c3",
    "page_number": 37,
    "text": "of\nthe pivot columns. The reduced row echelon form makes this relatively\nstraightforward, and we express the non-pivot columns in terms of sums\nand multiples of the pivot columns that are on their left: The second col-\numn is 3 times the first column (we can ignore the pivot columns on the\nright of the second column). Therefore, to obtain 0, we need to subtract\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 128,
    "chunk_id": "p38_c0",
    "page_number": 38,
    "text": "32 Linear Algebra\nthe second column from three times the first column. Now, we look at the\nfifth column, which is our second non-pivot column. The fifth column can\nbe expressed as 3 times the first pivot column, 9 times the second pivot\ncolumn, and \u22124 times the third pivot column. We need to keep track of\nthe indices of the pivot columns and translate this into3 times the first col-\numn, 0 times the second column (which is a non-pivot column), 9 times\nthe third column (which is our second pivot column), and \u22124 times the\nfourth column (which is the third pivot column). Then we need to subtract\nthe fifth column to obtain0. In the end, we are still solving a homogeneous\nequation system.\nTo summarize, all solutions of Ax = 0, x \u2208 R5 are given by\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nx \u2208 R5 : x = \u03bb1\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n3\n\u22121\n0\n0\n0"
  },
  {
    "vector_id": 129,
    "chunk_id": "p38_c1",
    "page_number": 38,
    "text": "the fifth column to obtain0. In the end, we are still solving a homogeneous\nequation system.\nTo summarize, all solutions of Ax = 0, x \u2208 R5 are given by\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nx \u2208 R5 : x = \u03bb1\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n3\n\u22121\n0\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n+ \u03bb2\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n3\n0\n9\n\u22124\n\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, \u03bb 1, \u03bb2 \u2208 R\n\uf8fc\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8fe\n. (2.50)\n2.3.3 The Minus-1 Trick\nIn the following, we introduce a practical trick for reading out the solu-\ntions x of a homogeneous system of linear equations Ax = 0, where\nA \u2208 Rk\u00d7n, x \u2208 Rn.\nTo start, we assume thatA is in reduced row-echelon form without any\nrows that just contain zeros, i.e.,\nA =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0 \u00b7 \u00b7\u00b7 0 1 \u2217 \u00b7 \u00b7\u00b7 \u22170 \u2217 \u00b7 \u00b7 \u00b7 \u22170 \u2217 \u00b7 \u00b7 \u00b7 \u2217\n... ... 0 0 \u00b7 \u00b7\u00b7 0 1 \u2217 \u00b7 \u00b7 \u00b7 \u2217... ... ...\n... ... ... ... ... 0 ... ... ... ... ...\n... ... ... ... ... ... ... ... 0 ... ...\n0 \u00b7 \u00b7\u00b7 0 0 0 \u00b7 \u00b7\u00b7 0 0 0 \u00b7 \u00b7\u00b7 0 1 \u2217 \u00b7 \u00b7 \u00b7 \u2217\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa"
  },
  {
    "vector_id": 130,
    "chunk_id": "p38_c2",
    "page_number": 38,
    "text": "\u22170 \u2217 \u00b7 \u00b7 \u00b7 \u22170 \u2217 \u00b7 \u00b7 \u00b7 \u2217\n... ... 0 0 \u00b7 \u00b7\u00b7 0 1 \u2217 \u00b7 \u00b7 \u00b7 \u2217... ... ...\n... ... ... ... ... 0 ... ... ... ... ...\n... ... ... ... ... ... ... ... 0 ... ...\n0 \u00b7 \u00b7\u00b7 0 0 0 \u00b7 \u00b7\u00b7 0 0 0 \u00b7 \u00b7\u00b7 0 1 \u2217 \u00b7 \u00b7 \u00b7 \u2217\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\n(2.51)\nwhere \u2217 can be an arbitrary real number, with the constraints that the first\nnonzero entry per row must be1 and all other entries in the corresponding\ncolumn must be 0. The columns j1, . . . , jk with the pivots (marked in\nbold) are the standard unit vectorse1, . . . ,ek \u2208 Rk. We extend this matrix\nto an n \u00d7 n-matrix \u02dcA by adding n \u2212 k rows of the form\n\u0002\n0 \u00b7 \u00b7\u00b7 0 \u22121 0 \u00b7 \u00b7\u00b7 0\n\u0003\n(2.52)\nso that the diagonal of the augmented matrix \u02dcA contains either 1 or \u22121.\nThen, the columns of \u02dcA that contain the \u22121 as pivots are solutions of\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d"
  },
  {
    "vector_id": 131,
    "chunk_id": "p38_c3",
    "page_number": 38,
    "text": "so that the diagonal of the augmented matrix \u02dcA contains either 1 or \u22121.\nThen, the columns of \u02dcA that contain the \u22121 as pivots are solutions of\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 132,
    "chunk_id": "p39_c0",
    "page_number": 39,
    "text": "2.3 Solving Systems of Linear Equations 33\nthe homogeneous equation system Ax = 0. To be more precise, these\ncolumns form a basis (Section 2.6.1) of the solution space of Ax = 0,\nwhich we will later call the kernel or null space (see Section 2.7.3). kernel\nnull space\nExample 2.8 (Minus-1 Trick)\nLet us revisit the matrix in (2.49), which is already in reduced REF:\nA =\n\uf8ee\n\uf8f0\n1 3 0 0 3\n0 0 1 0 9\n0 0 0 1 \u22124\n\uf8f9\n\uf8fb . (2.53)\nWe now augment this matrix to a 5 \u00d7 5 matrix by adding rows of the\nform (2.52) at the places where the pivots on the diagonal are missing\nand obtain\n\u02dcA =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1 3 0 0 3\n0 \u22121 0 0 0\n0 0 1 0 9\n0 0 0 1 \u22124\n0 0 0 0 \u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n. (2.54)\nFrom this form, we can immediately read out the solutions of Ax = 0 by\ntaking the columns of \u02dcA, which contain \u22121 on the diagonal:\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nx \u2208 R"
  },
  {
    "vector_id": 133,
    "chunk_id": "p39_c1",
    "page_number": 39,
    "text": "0 0 1 0 9\n0 0 0 1 \u22124\n0 0 0 0 \u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n. (2.54)\nFrom this form, we can immediately read out the solutions of Ax = 0 by\ntaking the columns of \u02dcA, which contain \u22121 on the diagonal:\n\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nx \u2208 R5 : x = \u03bb1\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n3\n\u22121\n0\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n+ \u03bb2\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n3\n0\n9\n\u22124\n\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, \u03bb 1, \u03bb2 \u2208 R\n\uf8fc\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8fe\n, (2.55)\nwhich is identical to the solution in (2.50) that we obtained by \u201cinsight\u201d.\nCalculating the Inverse\nTo compute the inverse A\u22121 of A \u2208 Rn\u00d7n, we need to find a matrix X\nthat satisfies AX = In. Then, X = A\u22121. We can write this down as\na set of simultaneous linear equations AX = In, where we solve for\nX = [x1| \u00b7\u00b7 \u00b7 |xn]. We use the augmented matrix notation for a compact\nrepresentation of this set of systems of linear equations and obtain\n\u0002\nA|In\n\u0003\n\u21dd \u00b7 \u00b7\u00b7\u21dd\n\u0002\nIn|A\u22121\u0003\n. (2.56)\nThis means that i"
  },
  {
    "vector_id": 134,
    "chunk_id": "p39_c2",
    "page_number": 39,
    "text": "ve for\nX = [x1| \u00b7\u00b7 \u00b7 |xn]. We use the augmented matrix notation for a compact\nrepresentation of this set of systems of linear equations and obtain\n\u0002\nA|In\n\u0003\n\u21dd \u00b7 \u00b7\u00b7\u21dd\n\u0002\nIn|A\u22121\u0003\n. (2.56)\nThis means that if we bring the augmented equation system into reduced\nrow-echelon form, we can read out the inverse on the right-hand side of\nthe equation system. Hence, determining the inverse of a matrix is equiv-\nalent to solving systems of linear equations.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 135,
    "chunk_id": "p40_c0",
    "page_number": 40,
    "text": "34 Linear Algebra\nExample 2.9 (Calculating an Inverse Matrix by Gaussian Elimination)\nTo determine the inverse of\nA =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 0 2 0\n1 1 0 0\n1 2 0 1\n1 1 1 1\n\uf8f9\n\uf8fa\uf8fa\uf8fb (2.57)\nwe write down the augmented matrix\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 0 2 0 1 0 0 0\n1 1 0 0 0 1 0 0\n1 2 0 1 0 0 1 0\n1 1 1 1 0 0 0 1\n\uf8f9\n\uf8fa\uf8fa\uf8fb\nand use Gaussian elimination to bring it into reduced row-echelon form\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 0 0 0 \u22121 2 \u22122 2\n0 1 0 0 1 \u22121 2 \u22122\n0 0 1 0 1 \u22121 1 \u22121\n0 0 0 1 \u22121 0 \u22121 2\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\nsuch that the desired inverse is given as its right-hand side:\nA\u22121 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22121 2 \u22122 2\n1 \u22121 2 \u22122\n1 \u22121 1 \u22121\n\u22121 0 \u22121 2\n\uf8f9\n\uf8fa\uf8fa\uf8fb . (2.58)\nWe can verify that (2.58) is indeed the inverse by performing the multi-\nplication AA\u22121 and observing that we recover I4.\n2.3.4 Algorithms for Solving a System of Linear Equations\nIn the following, we briefly discuss approaches to sol"
  },
  {
    "vector_id": 136,
    "chunk_id": "p40_c1",
    "page_number": 40,
    "text": "d the inverse by performing the multi-\nplication AA\u22121 and observing that we recover I4.\n2.3.4 Algorithms for Solving a System of Linear Equations\nIn the following, we briefly discuss approaches to solving a system of lin-\near equations of the form Ax = b. We make the assumption that a solu-\ntion exists. Should there be no solution, we need to resort to approximate\nsolutions, which we do not cover in this chapter. One way to solve the ap-\nproximate problem is using the approach of linear regression, which we\ndiscuss in detail in Chapter 9.\nIn special cases, we may be able to determine the inverse A\u22121, such\nthat the solution of Ax = b is given as x = A\u22121b. However, this is\nonly possible if A is a square matrix and invertible, which is often not the\ncase. Otherwise, under mild assumptions (i."
  },
  {
    "vector_id": 137,
    "chunk_id": "p40_c2",
    "page_number": 40,
    "text": "se A\u22121, such\nthat the solution of Ax = b is given as x = A\u22121b. However, this is\nonly possible if A is a square matrix and invertible, which is often not the\ncase. Otherwise, under mild assumptions (i.e., A needs to have linearly\nindependent columns) we can use the transformation\nAx = b \u21d0 \u21d2A\u22a4Ax = A\u22a4b \u21d0 \u21d2x = (A\u22a4A)\u22121A\u22a4b (2.59)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 138,
    "chunk_id": "p41_c0",
    "page_number": 41,
    "text": "2.4 Vector Spaces 35\nand use the Moore-Penrose pseudo-inverse (A\u22a4A)\u22121A\u22a4 to determine the Moore-Penrose\npseudo-inversesolution (2.59) that solves Ax = b, which also corresponds to the mini-\nmum norm least-squares solution. A disadvantage of this approach is that\nit requires many computations for the matrix-matrix product and comput-\ning the inverse of A\u22a4A. Moreover, for reasons of numerical precision it\nis generally not recommended to compute the inverse or pseudo-inverse.\nIn the following, we therefore briefly discuss alternative approaches to\nsolving systems of linear equations.\nGaussian elimination plays an important role when computing deter-\nminants (Section 4.1), checking whether a set of vectors is linearly inde-\npendent (Section 2.5), computing the inverse of a matrix (Section 2.2.2"
  },
  {
    "vector_id": 139,
    "chunk_id": "p41_c1",
    "page_number": 41,
    "text": "ination plays an important role when computing deter-\nminants (Section 4.1), checking whether a set of vectors is linearly inde-\npendent (Section 2.5), computing the inverse of a matrix (Section 2.2.2),\ncomputing the rank of a matrix (Section 2.6.2), and determining a basis\nof a vector space (Section 2.6.1). Gaussian elimination is an intuitive and\nconstructive way to solve a system of linear equations with thousands of\nvariables. However, for systems with millions of variables, it is impracti-\ncal as the required number of arithmetic operations scales cubically in the\nnumber of simultaneous equations.\nIn practice, systems of many linear equations are solved indirectly , by ei-\nther stationary iterative methods, such as the Richardson method, the Ja-\ncobi method, the Gau\u00df-Seidel method, an"
  },
  {
    "vector_id": 140,
    "chunk_id": "p41_c2",
    "page_number": 41,
    "text": "quations.\nIn practice, systems of many linear equations are solved indirectly , by ei-\nther stationary iterative methods, such as the Richardson method, the Ja-\ncobi method, the Gau\u00df-Seidel method, and the successive over-relaxation\nmethod, or Krylov subspace methods, such as conjugate gradients, gener-\nalized minimal residual, or biconjugate gradients. We refer to the books\nby Stoer and Burlirsch (2002), Strang (2003), and Liesen and Mehrmann\n(2015) for further details.\nLet x\u2217 be a solution of Ax = b. The key idea of these iterative methods\nis to set up an iteration of the form\nx(k+1) = Cx(k) + d (2.60)\nfor suitable C and d that reduces the residual error\u2225x(k+1) \u2212x\u2217\u2225 in every\niteration and converges to x\u2217. We will introduce norms \u2225 \u00b7 \u2225, which allow\nus to compute similarities between vecto"
  },
  {
    "vector_id": 141,
    "chunk_id": "p41_c3",
    "page_number": 41,
    "text": "+ d (2.60)\nfor suitable C and d that reduces the residual error\u2225x(k+1) \u2212x\u2217\u2225 in every\niteration and converges to x\u2217. We will introduce norms \u2225 \u00b7 \u2225, which allow\nus to compute similarities between vectors, in Section 3.1.\n2.4 Vector Spaces\nThus far, we have looked at systems of linear equations and how to solve\nthem (Section 2.3). We saw that systems of linear equations can be com-\npactly represented using matrix-vector notation (2.10). In the following,\nwe will have a closer look at vector spaces, i.e., a structured space in which\nvectors live.\nIn the beginning of this chapter, we informally characterized vectors as\nobjects that can be added together and multiplied by a scalar, and they\nremain objects of the same type. Now, we are ready to formalize this,\nand we will start by introducing th"
  },
  {
    "vector_id": 142,
    "chunk_id": "p41_c4",
    "page_number": 41,
    "text": "aracterized vectors as\nobjects that can be added together and multiplied by a scalar, and they\nremain objects of the same type. Now, we are ready to formalize this,\nand we will start by introducing the concept of a group, which is a set\nof elements and an operation defined on these elements that keeps some\nstructure of the set intact.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 143,
    "chunk_id": "p42_c0",
    "page_number": 42,
    "text": "36 Linear Algebra\n2.4.1 Groups\nGroups play an important role in computer science. Besides providing a\nfundamental framework for operations on sets, they are heavily used in\ncryptography , coding theory , and graphics.\nDefinition 2.7 (Group). Consider a set G and an operation \u2297 : G\u00d7G \u2192 G\ndefined on G. Then G := (G, \u2297) is called a group if the following hold:group\nclosure\n1. Closure of G under \u2297: \u2200x, y\u2208 G: x \u2297 y \u2208 Gassociativity\n2. Associativity: \u2200x, y, z\u2208 G: (x \u2297 y) \u2297 z = x \u2297 (y \u2297 z)neutral element\n3. Neutral element: \u2203e \u2208 G \u2200x \u2208 G: x \u2297 e = x and e \u2297 x = xinverse element\n4. Inverse element: \u2200x \u2208 G \u2203y \u2208 G: x \u2297 y = e and y \u2297 x = e, where e is\nthe neutral element. We often write x\u22121 to denote the inverse element\nof x.\nRemark. The inverse element is defined with respect to the operation \u2297\nand d"
  },
  {
    "vector_id": 144,
    "chunk_id": "p42_c1",
    "page_number": 42,
    "text": "\u2208 G \u2203y \u2208 G: x \u2297 y = e and y \u2297 x = e, where e is\nthe neutral element. We often write x\u22121 to denote the inverse element\nof x.\nRemark. The inverse element is defined with respect to the operation \u2297\nand does not necessarily mean 1\nx . \u2662\nIf additionally \u2200x, y\u2208 G: x \u2297 y = y \u2297 x, then G = (G, \u2297) is an AbelianAbelian group\ngroup (commutative).\nExample 2.10 (Groups)\nLet us have a look at some examples of sets with associated operations\nand see whether they are groups:\n(Z, +) is an Abelian group.\n(N0, +) is not a group: Although (N0, +) possesses a neutral elementN0 := N \u222a {0}\n(0), the inverse elements are missing.\n(Z, \u00b7) is not a group: Although(Z, \u00b7) contains a neutral element (1), the\ninverse elements for any z \u2208 Z, z\u0338= \u00b11, are missing.\n(R, \u00b7) is not a group since 0 does not possess an inverse el"
  },
  {
    "vector_id": 145,
    "chunk_id": "p42_c2",
    "page_number": 42,
    "text": "are missing.\n(Z, \u00b7) is not a group: Although(Z, \u00b7) contains a neutral element (1), the\ninverse elements for any z \u2208 Z, z\u0338= \u00b11, are missing.\n(R, \u00b7) is not a group since 0 does not possess an inverse element.\n(R\\{0}, \u00b7) is Abelian.\n(Rn, +), (Zn, +), n\u2208 N are Abelian if+ is defined componentwise, i.e.,\n(x1, \u00b7 \u00b7\u00b7, xn) + (y1, \u00b7 \u00b7\u00b7, yn) = (x1 + y1, \u00b7 \u00b7\u00b7, xn + yn). (2.61)\nThen, (x1, \u00b7 \u00b7\u00b7, xn)\u22121 := ( \u2212x1, \u00b7 \u00b7\u00b7, \u2212xn) is the inverse element and\ne = (0, \u00b7 \u00b7\u00b7, 0) is the neutral element.\n(Rm\u00d7n, +), the set of m \u00d7 n-matrices is Abelian (with componentwise\naddition as defined in (2.61)).\nLet us have a closer look at(Rn\u00d7n, \u00b7), i.e., the set ofn\u00d7n-matrices with\nmatrix multiplication as defined in (2.13).\n\u2013 Closure and associativity follow directly from the definition of matrix\nmultiplication.\n\u2013 Neutral ele"
  },
  {
    "vector_id": 146,
    "chunk_id": "p42_c3",
    "page_number": 42,
    "text": "look at(Rn\u00d7n, \u00b7), i.e., the set ofn\u00d7n-matrices with\nmatrix multiplication as defined in (2.13).\n\u2013 Closure and associativity follow directly from the definition of matrix\nmultiplication.\n\u2013 Neutral element: The identity matrix In is the neutral element with\nrespect to matrix multiplication \u201c\u00b7\u201d in (Rn\u00d7n, \u00b7).\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 147,
    "chunk_id": "p43_c0",
    "page_number": 43,
    "text": "2.4 Vector Spaces 37\n\u2013 Inverse element: If the inverse exists (A is regular), then A\u22121 is the\ninverse element of A \u2208 Rn\u00d7n, and in exactly this case (Rn\u00d7n, \u00b7) is a\ngroup, called the general linear group.\nDefinition 2.8 (General Linear Group) . The set of regular (invertible)\nmatrices A \u2208 Rn\u00d7n is a group with respect to matrix multiplication as\ndefined in (2.13) and is called general linear group GL(n, R). However, general linear group\nsince matrix multiplication is not commutative, the group is not Abelian.\n2.4.2 Vector Spaces\nWhen we discussed groups, we looked at sets G and inner operations on\nG, i.e., mappings G \u00d7 G \u2192 Gthat only operate on elements in G. In the\nfollowing, we will consider sets that in addition to an inner operation +\nalso contain an outer operation \u00b7, the multiplication"
  },
  {
    "vector_id": 148,
    "chunk_id": "p43_c1",
    "page_number": 43,
    "text": "n\nG, i.e., mappings G \u00d7 G \u2192 Gthat only operate on elements in G. In the\nfollowing, we will consider sets that in addition to an inner operation +\nalso contain an outer operation \u00b7, the multiplication of a vector x \u2208 Gby\na scalar \u03bb \u2208 R. We can think of the inner operation as a form of addition,\nand the outer operation as a form of scaling. Note that the inner/outer\noperations have nothing to do with inner/outer products.\nDefinition 2.9 (Vector Space). A real-valued vector space V = (V, +, \u00b7) is vector space\na set V with two operations\n+ : V \u00d7 V \u2192 V (2.62)\n\u00b7 : R \u00d7 V \u2192 V (2.63)\nwhere\n1. (V, +) is an Abelian group\n2. Distributivity:\n1. \u2200\u03bb \u2208 R, x, y \u2208 V: \u03bb \u00b7 (x + y) = \u03bb \u00b7 x + \u03bb \u00b7 y\n2. \u2200\u03bb, \u03c8\u2208 R, x \u2208 V: (\u03bb + \u03c8) \u00b7 x = \u03bb \u00b7 x + \u03c8 \u00b7 x\n3. Associativity (outer operation): \u2200\u03bb, \u03c8\u2208 R, x \u2208 V: \u03bb\u00b7(\u03c8\u00b7x) = (\u03bb\u03c8"
  },
  {
    "vector_id": 149,
    "chunk_id": "p43_c2",
    "page_number": 43,
    "text": "an Abelian group\n2. Distributivity:\n1. \u2200\u03bb \u2208 R, x, y \u2208 V: \u03bb \u00b7 (x + y) = \u03bb \u00b7 x + \u03bb \u00b7 y\n2. \u2200\u03bb, \u03c8\u2208 R, x \u2208 V: (\u03bb + \u03c8) \u00b7 x = \u03bb \u00b7 x + \u03c8 \u00b7 x\n3. Associativity (outer operation): \u2200\u03bb, \u03c8\u2208 R, x \u2208 V: \u03bb\u00b7(\u03c8\u00b7x) = (\u03bb\u03c8)\u00b7x\n4. Neutral element with respect to the outer operation: \u2200x \u2208 V: 1\u00b7x = x\nThe elements x \u2208 V are called vectors. The neutral element of (V, +) is vector\nthe zero vector 0 = [0, . . . ,0]\u22a4, and the inner operation + is called vector vector addition\naddition. The elements \u03bb \u2208 R are called scalars and the outer operation scalar\n\u00b7 is a multiplication by scalars . Note that a scalar product is something multiplication by\nscalarsdifferent, and we will get to this in Section 3.2.\nRemark. A \u201cvector multiplication\u201d ab, a, b \u2208 Rn, is not defined. Theoret-\nically , we could define an element-wise multi"
  },
  {
    "vector_id": 150,
    "chunk_id": "p43_c3",
    "page_number": 43,
    "text": "ing multiplication by\nscalarsdifferent, and we will get to this in Section 3.2.\nRemark. A \u201cvector multiplication\u201d ab, a, b \u2208 Rn, is not defined. Theoret-\nically , we could define an element-wise multiplication, such that c = ab\nwith cj = ajbj. This \u201carray multiplication\u201d is common to many program-\nming languages but makes mathematically limited sense using the stan-\ndard rules for matrix multiplication: By treating vectors as n \u00d7 1 matrices\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 151,
    "chunk_id": "p44_c0",
    "page_number": 44,
    "text": "38 Linear Algebra\n(which we usually do), we can use the matrix multiplication as defined\nin (2.13). However, then the dimensions of the vectors do not match. Only\nthe following multiplications for vectors are defined: ab\u22a4 \u2208 Rn\u00d7n (outerouter product\nproduct), a\u22a4b \u2208 R (inner/scalar/dot product). \u2662\nExample 2.11 (Vector Spaces)\nLet us have a look at some important examples:\nV = Rn, n\u2208 N is a vector space with operations defined as follows:\n\u2013 Addition: x+y = (x1, . . . , xn)+( y1, . . . , yn) = (x1 +y1, . . . , xn +yn)\nfor all x, y \u2208 Rn\n\u2013 Multiplication by scalars: \u03bbx = \u03bb(x1, . . . , xn) = ( \u03bbx1, . . . , \u03bbxn) for\nall \u03bb \u2208 R, x \u2208 Rn\nV = Rm\u00d7n, m, n\u2208 N is a vector space with\n\u2013 Addition: A + B =\n\uf8ee\n\uf8ef\uf8f0\na11 + b11 \u00b7 \u00b7\u00b7 a1n + b1n\n... ...\nam1 + bm1 \u00b7 \u00b7\u00b7 amn + bmn\n\uf8f9\n\uf8fa\uf8fb is defined ele-\nmentwise for all A, B"
  },
  {
    "vector_id": 152,
    "chunk_id": "p44_c1",
    "page_number": 44,
    "text": ". . , \u03bbxn) for\nall \u03bb \u2208 R, x \u2208 Rn\nV = Rm\u00d7n, m, n\u2208 N is a vector space with\n\u2013 Addition: A + B =\n\uf8ee\n\uf8ef\uf8f0\na11 + b11 \u00b7 \u00b7\u00b7 a1n + b1n\n... ...\nam1 + bm1 \u00b7 \u00b7\u00b7 amn + bmn\n\uf8f9\n\uf8fa\uf8fb is defined ele-\nmentwise for all A, B \u2208 V\n\u2013 Multiplication by scalars: \u03bbA =\n\uf8ee\n\uf8ef\uf8f0\n\u03bba11 \u00b7 \u00b7\u00b7 \u03bba1n\n... ...\n\u03bbam1 \u00b7 \u00b7\u00b7 \u03bbamn\n\uf8f9\n\uf8fa\uf8fb as defined in\nSection 2.2. Remember that Rm\u00d7n is equivalent to Rmn.\nV = C, with the standard definition of addition of complex numbers.\nRemark. In the following, we will denote a vector space (V, +, \u00b7) by V\nwhen + and \u00b7 are the standard vector addition and scalar multiplication.\nMoreover, we will use the notation x \u2208 V for vectors in V to simplify\nnotation. \u2662\nRemark. The vector spaces Rn, Rn\u00d71, R1\u00d7n are only different in the way\nwe write vectors. In the following, we will not make a distinction between\nRn an"
  },
  {
    "vector_id": 153,
    "chunk_id": "p44_c2",
    "page_number": 44,
    "text": "n x \u2208 V for vectors in V to simplify\nnotation. \u2662\nRemark. The vector spaces Rn, Rn\u00d71, R1\u00d7n are only different in the way\nwe write vectors. In the following, we will not make a distinction between\nRn and Rn\u00d71, which allows us to write n-tuples as column vectorscolumn vector\nx =\n\uf8ee\n\uf8ef\uf8f0\nx1\n...\nxn\n\uf8f9\n\uf8fa\uf8fb. (2.64)\nThis simplifies the notation regarding vector space operations. However,\nwe do distinguish between Rn\u00d71 and R1\u00d7n (the row vectors) to avoid con-row vector\nfusion with matrix multiplication. By default, we write x to denote a col-\numn vector, and a row vector is denoted by x\u22a4, the transpose of x. \u2662transpose\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 154,
    "chunk_id": "p44_c3",
    "page_number": 44,
    "text": ". \u2662transpose\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 155,
    "chunk_id": "p45_c0",
    "page_number": 45,
    "text": "2.4 Vector Spaces 39\n2.4.3 Vector Subspaces\nIn the following, we will introduce vector subspaces. Intuitively , they are\nsets contained in the original vector space with the property that when\nwe perform vector space operations on elements within this subspace, we\nwill never leave it. In this sense, they are \u201cclosed\u201d. Vector subspaces are a\nkey idea in machine learning. For example, Chapter 10 demonstrates how\nto use vector subspaces for dimensionality reduction.\nDefinition 2.10 (Vector Subspace). Let V = (V, +, \u00b7) be a vector space\nand U \u2286 V, U \u0338= \u2205. Then U = (U, +, \u00b7) is called vector subspace of V (or vector subspace\nlinear subspace) if U is a vector space with the vector space operations + linear subspace\nand \u00b7 restricted to U \u00d7Uand R\u00d7U . We writeU \u2286 V to denote a subspace\nU of V .\nIf"
  },
  {
    "vector_id": 156,
    "chunk_id": "p45_c1",
    "page_number": 45,
    "text": "ce of V (or vector subspace\nlinear subspace) if U is a vector space with the vector space operations + linear subspace\nand \u00b7 restricted to U \u00d7Uand R\u00d7U . We writeU \u2286 V to denote a subspace\nU of V .\nIf U \u2286 Vand V is a vector space, then U naturally inherits many prop-\nerties directly fromV because they hold for allx \u2208 V, and in particular for\nall x \u2208 U \u2286 V. This includes the Abelian group properties, the distribu-\ntivity , the associativity and the neutral element. To determine whether\n(U, +, \u00b7) is a subspace of V we still do need to show\n1. U \u0338= \u2205, in particular: 0 \u2208 U\n2. Closure of U:\na. With respect to the outer operation: \u2200\u03bb \u2208 R\u2200x \u2208 U: \u03bbx \u2208 U.\nb. With respect to the inner operation: \u2200x, y \u2208 U: x + y \u2208 U.\nExample 2.12 (Vector Subspaces)\nLet us have a look at some examples:\nFor every vecto"
  },
  {
    "vector_id": 157,
    "chunk_id": "p45_c2",
    "page_number": 45,
    "text": "respect to the outer operation: \u2200\u03bb \u2208 R\u2200x \u2208 U: \u03bbx \u2208 U.\nb. With respect to the inner operation: \u2200x, y \u2208 U: x + y \u2208 U.\nExample 2.12 (Vector Subspaces)\nLet us have a look at some examples:\nFor every vector space V , the trivial subspaces are V itself and {0}.\nOnly example D in Figure 2.6 is a subspace ofR2 (with the usual inner/\nouter operations). In A and C, the closure property is violated; B does\nnot contain 0.\nThe solution set of a homogeneous system of linear equations Ax = 0\nwith n unknowns x = [x1, . . . , xn]\u22a4 is a subspace of Rn.\nThe solution of an inhomogeneous system of linear equations Ax =\nb, b \u0338= 0 is not a subspace of Rn.\nThe intersection of arbitrarily many subspaces is a subspace itself.\nFigure 2.6 Not all\nsubsets of R2 are\nsubspaces. In A and\nC, the closure\nproperty is violat"
  },
  {
    "vector_id": 158,
    "chunk_id": "p45_c3",
    "page_number": 45,
    "text": "Ax =\nb, b \u0338= 0 is not a subspace of Rn.\nThe intersection of arbitrarily many subspaces is a subspace itself.\nFigure 2.6 Not all\nsubsets of R2 are\nsubspaces. In A and\nC, the closure\nproperty is violated;\nB does not contain\n0. Only D is a\nsubspace.\n0 0 0 0\nA B\nC\nD\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 159,
    "chunk_id": "p46_c0",
    "page_number": 46,
    "text": "40 Linear Algebra\nRemark. Every subspace U \u2286 (Rn, +, \u00b7) is the solution space of a homo-\ngeneous system of linear equations Ax = 0 for x \u2208 Rn. \u2662\n2.5 Linear Independence\nIn the following, we will have a close look at what we can do with vectors\n(elements of the vector space). In particular, we can add vectors together\nand multiply them with scalars. The closure property guarantees that we\nend up with another vector in the same vector space. It is possible to find\na set of vectors with which we can represent every vector in the vector\nspace by adding them together and scaling them. This set of vectors is\na basis, and we will discuss them in Section 2.6.1. Before we get there,\nwe will need to introduce the concepts of linear combinations and linear\nindependence.\nDefinition 2.11 (Linear Combin"
  },
  {
    "vector_id": 160,
    "chunk_id": "p46_c1",
    "page_number": 46,
    "text": "ectors is\na basis, and we will discuss them in Section 2.6.1. Before we get there,\nwe will need to introduce the concepts of linear combinations and linear\nindependence.\nDefinition 2.11 (Linear Combination). Consider a vector space V and a\nfinite number of vectors x1, . . . ,xk \u2208 V . Then, every v \u2208 V of the form\nv = \u03bb1x1 + \u00b7 \u00b7\u00b7+ \u03bbkxk =\nkX\ni=1\n\u03bbixi \u2208 V (2.65)\nwith \u03bb1, . . . , \u03bbk \u2208 R is a linear combination of the vectors x1, . . . ,xk.linear combination\nThe 0-vector can always be written as the linear combination of k vec-\ntors x1, . . . ,xk because 0 = Pk\ni=1 0xi is always true. In the following,\nwe are interested in non-trivial linear combinations of a set of vectors to\nrepresent 0, i.e., linear combinations of vectors x1, . . . ,xk, where not all\ncoefficients \u03bbi in (2.65) are 0.\nDefinit"
  },
  {
    "vector_id": 161,
    "chunk_id": "p46_c2",
    "page_number": 46,
    "text": "ing,\nwe are interested in non-trivial linear combinations of a set of vectors to\nrepresent 0, i.e., linear combinations of vectors x1, . . . ,xk, where not all\ncoefficients \u03bbi in (2.65) are 0.\nDefinition 2.12 (Linear (In)dependence). Let us consider a vector space\nV with k \u2208 N and x1, . . . ,xk \u2208 V . If there is a non-trivial linear com-\nbination, such that 0 = Pk\ni=1 \u03bbixi with at least one \u03bbi \u0338= 0, the vectors\nx1, . . . ,xk are linearly dependent. If only the trivial solution exists, i.e.,linearly dependent\n\u03bb1 = . . .= \u03bbk = 0 the vectors x1, . . . ,xk are linearly independent.linearly\nindependent\nLinear independence is one of the most important concepts in linear\nalgebra. Intuitively , a set of linearly independent vectors consists of vectors\nthat have no redundancy , i.e., if we remove a"
  },
  {
    "vector_id": 162,
    "chunk_id": "p46_c3",
    "page_number": 46,
    "text": "dent\nLinear independence is one of the most important concepts in linear\nalgebra. Intuitively , a set of linearly independent vectors consists of vectors\nthat have no redundancy , i.e., if we remove any of those vectors from\nthe set, we will lose something. Throughout the next sections, we will\nformalize this intuition more.\nExample 2.13 (Linearly Dependent Vectors)\nA geographic example may help to clarify the concept of linear indepen-\ndence. A person in Nairobi (Kenya) describing where Kigali (Rwanda) is\nmight say ,\u201cYou can get to Kigali by first going506 km Northwest to Kam-\npala (Uganda) and then374 kmSouthwest.\u201d. This is sufficient information\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 163,
    "chunk_id": "p46_c4",
    "page_number": 46,
    "text": "nd then374 kmSouthwest.\u201d. This is sufficient information\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 164,
    "chunk_id": "p47_c0",
    "page_number": 47,
    "text": "2.5 Linear Independence 41\nto describe the location of Kigali because the geographic coordinate sys-\ntem may be considered a two-dimensional vector space (ignoring altitude\nand the Earth\u2019s curved surface). The person may add, \u201cIt is about 751 km\nWest of here.\u201d Although this last statement is true, it is not necessary to\nfind Kigali given the previous information (see Figure 2.7 for an illus-\ntration). In this example, the \u201c 506 km Northwest\u201d vector (blue) and the\n\u201c374 kmSouthwest\u201d vector (purple) are linearly independent. This means\nthe Southwest vector cannot be described in terms of the Northwest vec-\ntor, and vice versa. However, the third \u201c 751 km West\u201d vector (black) is a\nlinear combination of the other two vectors, and it makes the set of vec-\ntors linearly dependent. Equivalently ,"
  },
  {
    "vector_id": 165,
    "chunk_id": "p47_c1",
    "page_number": 47,
    "text": "hwest vec-\ntor, and vice versa. However, the third \u201c 751 km West\u201d vector (black) is a\nlinear combination of the other two vectors, and it makes the set of vec-\ntors linearly dependent. Equivalently , given \u201c751 km West\u201d and \u201c374 km\nSouthwest\u201d can be linearly combined to obtain \u201c506 km Northwest\u201d.\nFigure 2.7\nGeographic example\n(with crude\napproximations to\ncardinal directions)\nof linearly\ndependent vectors\nin a\ntwo-dimensional\nspace (plane).\n506 km Northwest\n751 km West\n374 km Southwest\n374 km Southwest\nKampala\nNairobi\nKigali\nRemark. The following properties are useful to find out whether vectors\nare linearly independent:\nk vectors are either linearly dependent or linearly independent. There\nis no third option.\nIf at least one of the vectors x1, . . . ,xk is 0 then they are linearly de-\npen"
  },
  {
    "vector_id": 166,
    "chunk_id": "p47_c2",
    "page_number": 47,
    "text": "rs\nare linearly independent:\nk vectors are either linearly dependent or linearly independent. There\nis no third option.\nIf at least one of the vectors x1, . . . ,xk is 0 then they are linearly de-\npendent. The same holds if two vectors are identical.\nThe vectors {x1, . . . ,xk : xi \u0338= 0, i= 1 , . . . , k}, k \u2a7e 2, are linearly\ndependent if and only if (at least) one of them is a linear combination\nof the others. In particular, if one vector is a multiple of another vector,\ni.e., xi = \u03bbxj, \u03bb\u2208 R then the set {x1, . . . ,xk : xi \u0338= 0, i= 1, . . . , k}\nis linearly dependent.\nA practical way of checking whether vectorsx1, . . . ,xk \u2208 V are linearly\nindependent is to use Gaussian elimination: Write all vectors as columns\nof a matrix A and perform Gaussian elimination until the matrix is in\nrow ec"
  },
  {
    "vector_id": 167,
    "chunk_id": "p47_c3",
    "page_number": 47,
    "text": "king whether vectorsx1, . . . ,xk \u2208 V are linearly\nindependent is to use Gaussian elimination: Write all vectors as columns\nof a matrix A and perform Gaussian elimination until the matrix is in\nrow echelon form (the reduced row-echelon form is unnecessary here):\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 168,
    "chunk_id": "p48_c0",
    "page_number": 48,
    "text": "42 Linear Algebra\n\u2013 The pivot columns indicate the vectors, which are linearly indepen-\ndent of the vectors on the left. Note that there is an ordering of vec-\ntors when the matrix is built.\n\u2013 The non-pivot columns can be expressed as linear combinations of\nthe pivot columns on their left. For instance, the row-echelon form\n\u00141 3 0\n0 0 2\n\u0015\n(2.66)\ntells us that the first and third columns are pivot columns. The sec-\nond column is a non-pivot column because it is three times the first\ncolumn.\nAll column vectors are linearly independent if and only if all columns\nare pivot columns. If there is at least one non-pivot column, the columns\n(and, therefore, the corresponding vectors) are linearly dependent.\n\u2662\nExample 2.14\nConsider R4 with\nx1 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n2\n\u22123\n4\n\uf8f9\n\uf8fa\uf8fa\uf8fb, x2 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n1\n0\n2\n\uf8f9\n\uf8fa\uf8fa\uf8fb, x3 =\n\uf8ee"
  },
  {
    "vector_id": 169,
    "chunk_id": "p48_c1",
    "page_number": 48,
    "text": "least one non-pivot column, the columns\n(and, therefore, the corresponding vectors) are linearly dependent.\n\u2662\nExample 2.14\nConsider R4 with\nx1 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n2\n\u22123\n4\n\uf8f9\n\uf8fa\uf8fa\uf8fb, x2 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n1\n0\n2\n\uf8f9\n\uf8fa\uf8fa\uf8fb, x3 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22121\n\u22122\n1\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fb. (2.67)\nTo check whether they are linearly dependent, we follow the general ap-\nproach and solve\n\u03bb1x1 + \u03bb2x2 + \u03bb3x3 = \u03bb1\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n2\n\u22123\n4\n\uf8f9\n\uf8fa\uf8fa\uf8fb + \u03bb2\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n1\n0\n2\n\uf8f9\n\uf8fa\uf8fa\uf8fb + \u03bb3\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22121\n\u22122\n1\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fb = 0 (2.68)\nfor \u03bb1, . . . , \u03bb3. We write the vectors xi, i = 1 , 2, 3, as the columns of a\nmatrix and apply elementary row operations until we identify the pivot\ncolumns:\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 1 \u22121\n2 1 \u22122\n\u22123 0 1\n4 2 1\n\uf8f9\n\uf8fa\uf8fa\uf8fb \u21dd \u00b7 \u00b7\u00b7\u21dd\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 1 \u22121\n0 1 0\n0 0 1\n0 0 0\n\uf8f9\n\uf8fa\uf8fa\uf8fb . (2.69)\nHere, every column of the matrix is a pivot column. Therefore, there is no\nnon-trivial solution, and we require \u03bb1 = 0, \u03bb2"
  },
  {
    "vector_id": 170,
    "chunk_id": "p48_c2",
    "page_number": 48,
    "text": "\u22122\n\u22123 0 1\n4 2 1\n\uf8f9\n\uf8fa\uf8fa\uf8fb \u21dd \u00b7 \u00b7\u00b7\u21dd\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 1 \u22121\n0 1 0\n0 0 1\n0 0 0\n\uf8f9\n\uf8fa\uf8fa\uf8fb . (2.69)\nHere, every column of the matrix is a pivot column. Therefore, there is no\nnon-trivial solution, and we require \u03bb1 = 0, \u03bb2 = 0, \u03bb3 = 0 to solve the\nequation system. Hence, the vectors x1, x2, x3 are linearly independent.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 171,
    "chunk_id": "p49_c0",
    "page_number": 49,
    "text": "2.5 Linear Independence 43\nRemark. Consider a vector space V with k linearly independent vectors\nb1, . . . ,bk and m linear combinations\nx1 =\nkX\ni=1\n\u03bbi1bi ,\n...\nxm =\nkX\ni=1\n\u03bbimbi .\n(2.70)\nDefining B = [b1, . . . ,bk] as the matrix whose columns are the linearly\nindependent vectors b1, . . . ,bk, we can write\nxj = B\u03bbj , \u03bbj =\n\uf8ee\n\uf8ef\uf8f0\n\u03bb1j\n...\n\u03bbkj\n\uf8f9\n\uf8fa\uf8fb , j = 1, . . . , m , (2.71)\nin a more compact form.\nWe want to test whether x1, . . . ,xm are linearly independent. For this\npurpose, we follow the general approach of testing whenPm\nj=1 \u03c8jxj = 0.\nWith (2.71), we obtain\nmX\nj=1\n\u03c8jxj =\nmX\nj=1\n\u03c8jB\u03bbj = B\nmX\nj=1\n\u03c8j\u03bbj . (2.72)\nThis means that {x1, . . . ,xm} are linearly independent if and only if the\ncolumn vectors {\u03bb1, . . . ,\u03bbm} are linearly independent.\n\u2662\nRemark. In a vector spaceV , m linear combina"
  },
  {
    "vector_id": 172,
    "chunk_id": "p49_c1",
    "page_number": 49,
    "text": "X\nj=1\n\u03c8j\u03bbj . (2.72)\nThis means that {x1, . . . ,xm} are linearly independent if and only if the\ncolumn vectors {\u03bb1, . . . ,\u03bbm} are linearly independent.\n\u2662\nRemark. In a vector spaceV , m linear combinations ofk vectors x1, . . . ,xk\nare linearly dependent if m > k. \u2662\nExample 2.15\nConsider a set of linearly independent vectors b1, b2, b3, b4 \u2208 Rn and\nx1 = b1 \u2212 2b2 + b3 \u2212 b4\nx2 = \u22124b1 \u2212 2b2 + 4 b4\nx3 = 2 b1 + 3 b2 \u2212 b3 \u2212 3b4\nx4 = 17 b1 \u2212 10b2 + 11 b3 + b4\n. (2.73)\nAre the vectors x1, . . . ,x4 \u2208 Rn linearly independent? To answer this\nquestion, we investigate whether the column vectors\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n\u22122\n1\n\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22124\n\u22122\n0\n4\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n2\n3\n\u22121\n\u22123\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n17\n\u221210\n11\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\uf8fc\n\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8fe\n(2.74)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press ("
  },
  {
    "vector_id": 173,
    "chunk_id": "p49_c2",
    "page_number": 49,
    "text": "\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n\u22122\n1\n\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22124\n\u22122\n0\n4\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n2\n3\n\u22121\n\u22123\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n17\n\u221210\n11\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\uf8fc\n\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8fe\n(2.74)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 174,
    "chunk_id": "p50_c0",
    "page_number": 50,
    "text": "44 Linear Algebra\nare linearly independent. The reduced row-echelon form of the corre-\nsponding linear equation system with coefficient matrix\nA =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 \u22124 2 17\n\u22122 \u22122 3 \u221210\n1 0 \u22121 11\n\u22121 4 \u22123 1\n\uf8f9\n\uf8fa\uf8fa\uf8fb (2.75)\nis given as\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 0 0 \u22127\n0 1 0 \u221215\n0 0 1 \u221218\n0 0 0 0\n\uf8f9\n\uf8fa\uf8fa\uf8fb . (2.76)\nWe see that the corresponding linear equation system is non-trivially solv-\nable: The last column is not a pivot column, andx4 = \u22127x1\u221215x2\u221218x3.\nTherefore, x1, . . . ,x4 are linearly dependent as x4 can be expressed as a\nlinear combination of x1, . . . ,x3.\n2.6 Basis and Rank\nIn a vector space V , we are particularly interested in sets of vectorsA that\npossess the property that any vector v \u2208 V can be obtained by a linear\ncombination of vectors in A. These vectors are special vectors, and in the\nfollowing, we will ch"
  },
  {
    "vector_id": 175,
    "chunk_id": "p50_c1",
    "page_number": 50,
    "text": "erested in sets of vectorsA that\npossess the property that any vector v \u2208 V can be obtained by a linear\ncombination of vectors in A. These vectors are special vectors, and in the\nfollowing, we will characterize them.\n2.6.1 Generating Set and Basis\nDefinition 2.13 (Generating Set and Span). Consider a vector space V =\n(V, +, \u00b7) and set of vectors A = {x1, . . . ,xk} \u2286 V. If every vector v \u2208\nV can be expressed as a linear combination of x1, . . . ,xk, A is called a\ngenerating set of V . The set of all linear combinations of vectors in A isgenerating set\ncalled the span of A. If A spans the vector spaceV , we write V = span[A]span\nor V = span[x1, . . . ,xk].\nGenerating sets are sets of vectors that span vector (sub)spaces, i.e.,\nevery vector can be represented as a linear combination of the v"
  },
  {
    "vector_id": 176,
    "chunk_id": "p50_c2",
    "page_number": 50,
    "text": "r spaceV , we write V = span[A]span\nor V = span[x1, . . . ,xk].\nGenerating sets are sets of vectors that span vector (sub)spaces, i.e.,\nevery vector can be represented as a linear combination of the vectors\nin the generating set. Now, we will be more specific and characterize the\nsmallest generating set that spans a vector (sub)space.\nDefinition 2.14 (Basis). Consider a vector space V = (V, +, \u00b7) and A \u2286\nV. A generating set A of V is called minimal if there exists no smaller setminimal\n\u02dcA \u228a A \u2286 Vthat spans V . Every linearly independent generating set of V\nis minimal and is called a basis of V .basis\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 177,
    "chunk_id": "p50_c3",
    "page_number": 50,
    "text": ".basis\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 178,
    "chunk_id": "p51_c0",
    "page_number": 51,
    "text": "2.6 Basis and Rank 45\nLet V = ( V, +, \u00b7) be a vector space and B \u2286 V, B \u0338= \u2205. Then, the\nfollowing statements are equivalent: A basis is a minimal\ngenerating set and a\nmaximal linearly\nindependent set of\nvectors.\nB is a basis of V .\nB is a minimal generating set.\nB is a maximal linearly independent set of vectors inV , i.e., adding any\nother vector to this set will make it linearly dependent.\nEvery vector x \u2208 V is a linear combination of vectors fromB, and every\nlinear combination is unique, i.e., with\nx =\nkX\ni=1\n\u03bbibi =\nkX\ni=1\n\u03c8ibi (2.77)\nand \u03bbi, \u03c8i \u2208 R, bi \u2208 Bit follows that \u03bbi = \u03c8i, i= 1, . . . , k.\nExample 2.16\nIn R3, the canonical/standard basis is canonical basis\nB =\n\uf8f1\n\uf8f2\n\uf8f3\n\uf8ee\n\uf8f0\n1\n0\n0\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n0\n1\n0\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n0\n0\n1\n\uf8f9\n\uf8fb\n\uf8fc\n\uf8fd\n\uf8fe. (2.78)\nDifferent bases in R3 are\nB1 =\n\uf8f1\n\uf8f2\n\uf8f3\n\uf8ee\n\uf8f0\n1\n0\n0\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0"
  },
  {
    "vector_id": 179,
    "chunk_id": "p51_c1",
    "page_number": 51,
    "text": ". , k.\nExample 2.16\nIn R3, the canonical/standard basis is canonical basis\nB =\n\uf8f1\n\uf8f2\n\uf8f3\n\uf8ee\n\uf8f0\n1\n0\n0\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n0\n1\n0\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n0\n0\n1\n\uf8f9\n\uf8fb\n\uf8fc\n\uf8fd\n\uf8fe. (2.78)\nDifferent bases in R3 are\nB1 =\n\uf8f1\n\uf8f2\n\uf8f3\n\uf8ee\n\uf8f0\n1\n0\n0\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n1\n1\n0\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n1\n1\n1\n\uf8f9\n\uf8fb\n\uf8fc\n\uf8fd\n\uf8fe, B2 =\n\uf8f1\n\uf8f2\n\uf8f3\n\uf8ee\n\uf8f0\n0.5\n0.8\n0.4\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n1.8\n0.3\n0.3\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n\u22122.2\n\u22121.3\n3.5\n\uf8f9\n\uf8fb\n\uf8fc\n\uf8fd\n\uf8fe. (2.79)\nThe set\nA =\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n2\n3\n4\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n2\n\u22121\n0\n2\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n1\n0\n\u22124\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\uf8fc\n\uf8f4\uf8f4\uf8fd\n\uf8f4\uf8f4\uf8fe\n(2.80)\nis linearly independent, but not a generating set (and no basis) of R4:\nFor instance, the vector [1, 0, 0, 0]\u22a4 cannot be obtained by a linear com-\nbination of elements in A.\nRemark. Every vector space V possesses a basis B. The preceding exam-\nples show that there can be many bases of a vector space V , i.e., there is\nno unique basis. However, all bases possess the same num"
  },
  {
    "vector_id": 180,
    "chunk_id": "p51_c2",
    "page_number": 51,
    "text": "mark. Every vector space V possesses a basis B. The preceding exam-\nples show that there can be many bases of a vector space V , i.e., there is\nno unique basis. However, all bases possess the same number of elements,\nthe basis vectors. \u2662 basis vector\nWe only consider finite-dimensional vector spaces V . In this case, the\ndimension of V is the number of basis vectors of V , and we write dim(V ). dimension\nIf U \u2286 V is a subspace of V , then dim(U) \u2a7d dim(V ) and dim(U) =\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 181,
    "chunk_id": "p52_c0",
    "page_number": 52,
    "text": "46 Linear Algebra\ndim(V ) if and only if U = V . Intuitively , the dimension of a vector space\ncan be thought of as the number of independent directions in this vector\nspace.The dimension of a\nvector space\ncorresponds to the\nnumber of its basis\nvectors.\nRemark. The dimension of a vector space is not necessarily the number\nof elements in a vector. For instance, the vector space V = span[\n\u00140\n1\n\u0015\n] is\none-dimensional, although the basis vector possesses two elements. \u2662\nRemark. A basis of a subspace U = span[x1, . . . ,xm] \u2286 Rn can be found\nby executing the following steps:\n1. Write the spanning vectors as columns of a matrix A\n2. Determine the row-echelon form of A.\n3. The spanning vectors associated with the pivot columns are a basis of\nU.\n\u2662\nExample 2.17 (Determining a Basis)\nFor a vector su"
  },
  {
    "vector_id": 182,
    "chunk_id": "p52_c1",
    "page_number": 52,
    "text": "ectors as columns of a matrix A\n2. Determine the row-echelon form of A.\n3. The spanning vectors associated with the pivot columns are a basis of\nU.\n\u2662\nExample 2.17 (Determining a Basis)\nFor a vector subspace U \u2286 R5, spanned by the vectors\nx1 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n2\n\u22121\n\u22121\n\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, x2 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n2\n\u22121\n1\n2\n\u22122\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, x3 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n3\n\u22124\n3\n5\n\u22123\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, x4 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u22121\n8\n\u22125\n\u22126\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u2208 R5, (2.81)\nwe are interested in finding out which vectorsx1, . . . ,x4 are a basis forU.\nFor this, we need to check whether x1, . . . ,x4 are linearly independent.\nTherefore, we need to solve\n4X\ni=1\n\u03bbixi = 0 , (2.82)\nwhich leads to a homogeneous system of equations with matrix\n\u0002\nx1, x2, x3, x4\n\u0003\n=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1 2 3 \u22121\n2 \u22121 \u22124 8\n\u22121 1 3 \u22125\n\u22121 2 5 \u22126\n\u22121 \u22122 \u22123 1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n. (2.83)\nWith the basic transformation rules for system"
  },
  {
    "vector_id": 183,
    "chunk_id": "p52_c2",
    "page_number": 52,
    "text": "ch leads to a homogeneous system of equations with matrix\n\u0002\nx1, x2, x3, x4\n\u0003\n=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1 2 3 \u22121\n2 \u22121 \u22124 8\n\u22121 1 3 \u22125\n\u22121 2 5 \u22126\n\u22121 \u22122 \u22123 1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n. (2.83)\nWith the basic transformation rules for systems of linear equations, we\nobtain the row-echelon form\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1 2 3 \u22121\n2 \u22121 \u22124 8\n\u22121 1 3 \u22125\n\u22121 2 5 \u22126\n\u22121 \u22122 \u22123 1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u21dd \u00b7 \u00b7\u00b7\u21dd\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1 2 3 \u22121\n0 1 2 \u22122\n0 0 0 1\n0 0 0 0\n0 0 0 0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 184,
    "chunk_id": "p53_c0",
    "page_number": 53,
    "text": "2.6 Basis and Rank 47\nSince the pivot columns indicate which set of vectors is linearly indepen-\ndent, we see from the row-echelon form that x1, x2, x4 are linearly inde-\npendent (because the system of linear equations \u03bb1x1 + \u03bb2x2 + \u03bb4x4 = 0\ncan only be solved with \u03bb1 = \u03bb2 = \u03bb4 = 0). Therefore, {x1, x2, x4} is a\nbasis of U.\n2.6.2 Rank\nThe number of linearly independent columns of a matrix A \u2208 Rm\u00d7n\nequals the number of linearly independent rows and is called the rank rank\nof A and is denoted by rk(A).\nRemark. The rank of a matrix has some important properties:\nrk(A) = rk(A\u22a4), i.e., the column rank equals the row rank.\nThe columns of A \u2208 Rm\u00d7n span a subspace U \u2286 Rm with dim(U) =\nrk(A). Later we will call this subspace the image or range. A basis of\nU can be found by applying Gaussian elimina"
  },
  {
    "vector_id": 185,
    "chunk_id": "p53_c1",
    "page_number": 53,
    "text": "ank equals the row rank.\nThe columns of A \u2208 Rm\u00d7n span a subspace U \u2286 Rm with dim(U) =\nrk(A). Later we will call this subspace the image or range. A basis of\nU can be found by applying Gaussian elimination to A to identify the\npivot columns.\nThe rows of A \u2208 Rm\u00d7n span a subspace W \u2286 Rn with dim(W) =\nrk(A). A basis of W can be found by applying Gaussian elimination to\nA\u22a4.\nFor all A \u2208 Rn\u00d7n it holds that A is regular (invertible) if and only if\nrk(A) = n.\nFor all A \u2208 Rm\u00d7n and all b \u2208 Rm it holds that the linear equation\nsystem Ax = b can be solved if and only if rk(A) = rk( A|b), where\nA|b denotes the augmented system.\nFor A \u2208 Rm\u00d7n the subspace of solutions for Ax = 0 possesses dimen-\nsion n \u2212 rk(A). Later, we will call this subspace the kernel or the null kernel\nnull spacespace.\nA matrix A \u2208 R"
  },
  {
    "vector_id": 186,
    "chunk_id": "p53_c2",
    "page_number": 53,
    "text": "the augmented system.\nFor A \u2208 Rm\u00d7n the subspace of solutions for Ax = 0 possesses dimen-\nsion n \u2212 rk(A). Later, we will call this subspace the kernel or the null kernel\nnull spacespace.\nA matrix A \u2208 Rm\u00d7n has full rank if its rank equals the largest possible full rank\nrank for a matrix of the same dimensions. This means that the rank of\na full-rank matrix is the lesser of the number of rows and columns, i.e.,\nrk(A) = min( m, n). A matrix is said to be rank deficient if it does not rank deficient\nhave full rank.\n\u2662\nExample 2.18 (Rank)\nA =\n\uf8ee\n\uf8f0\n1 0 1\n0 1 1\n0 0 0\n\uf8f9\n\uf8fb.\nA has two linearly independent rows/columns so that rk(A) = 2.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 187,
    "chunk_id": "p53_c3",
    "page_number": 53,
    "text": "rows/columns so that rk(A) = 2.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 188,
    "chunk_id": "p54_c0",
    "page_number": 54,
    "text": "48 Linear Algebra\nA =\n\uf8ee\n\uf8f0\n1 2 1\n\u22122 \u22123 1\n3 5 0\n\uf8f9\n\uf8fb.\nWe use Gaussian elimination to determine the rank:\n\uf8ee\n\uf8f0\n1 2 1\n\u22122 \u22123 1\n3 5 0\n\uf8f9\n\uf8fb \u21dd \u00b7 \u00b7\u00b7\u21dd\n\uf8ee\n\uf8f0\n1 2 1\n0 1 3\n0 0 0\n\uf8f9\n\uf8fb . (2.84)\nHere, we see that the number of linearly independent rows and columns\nis 2, such that rk(A) = 2.\n2.7 Linear Mappings\nIn the following, we will study mappings on vector spaces that preserve\ntheir structure, which will allow us to define the concept of a coordinate.\nIn the beginning of the chapter, we said that vectors are objects that can be\nadded together and multiplied by a scalar, and the resulting object is still\na vector. We wish to preserve this property when applying the mapping:\nConsider two real vector spaces V, W. A mapping \u03a6 : V \u2192 W preserves\nthe structure of the vector space if\n\u03a6(x + y) = \u03a6(x) + \u03a6(y) (2.85)\n\u03a6"
  },
  {
    "vector_id": 189,
    "chunk_id": "p54_c1",
    "page_number": 54,
    "text": "r. We wish to preserve this property when applying the mapping:\nConsider two real vector spaces V, W. A mapping \u03a6 : V \u2192 W preserves\nthe structure of the vector space if\n\u03a6(x + y) = \u03a6(x) + \u03a6(y) (2.85)\n\u03a6(\u03bbx) = \u03bb\u03a6(x) (2.86)\nfor all x, y \u2208 V and \u03bb \u2208 R. We can summarize this in the following\ndefinition:\nDefinition 2.15 (Linear Mapping). For vector spaces V, W, a mapping\n\u03a6 : V \u2192 W is called a linear mapping (or vector space homomorphism /linear mapping\nvector space\nhomomorphism\nlinear transformation) if\nlinear\ntransformation\n\u2200x, y \u2208 V \u2200\u03bb, \u03c8\u2208 R : \u03a6(\u03bbx + \u03c8y) = \u03bb\u03a6(x) + \u03c8\u03a6(y) . (2.87)\nIt turns out that we can represent linear mappings as matrices (Sec-\ntion 2.7.1). Recall that we can also collect a set of vectors as columns of a\nmatrix. When working with matrices, we have to keep in mind what the\nmat"
  },
  {
    "vector_id": 190,
    "chunk_id": "p54_c2",
    "page_number": 54,
    "text": "e can represent linear mappings as matrices (Sec-\ntion 2.7.1). Recall that we can also collect a set of vectors as columns of a\nmatrix. When working with matrices, we have to keep in mind what the\nmatrix represents: a linear mapping or a collection of vectors. We will see\nmore about linear mappings in Chapter 4. Before we continue, we will\nbriefly introduce special mappings.\nDefinition 2.16 (Injective, Surjective, Bijective). Consider a mapping \u03a6 :\nV \u2192 W, where V, W can be arbitrary sets. Then \u03a6 is called\ninjective\nInjective if \u2200x, y \u2208 V: \u03a6(x) = \u03a6(y) =\u21d2 x = y.surjective\nSurjective if \u03a6(V) = W.bijective\nBijective if it is injective and surjective.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 191,
    "chunk_id": "p54_c3",
    "page_number": 54,
    "text": "bijective\nBijective if it is injective and surjective.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 192,
    "chunk_id": "p55_c0",
    "page_number": 55,
    "text": "2.7 Linear Mappings 49\nIf \u03a6 is surjective, then every element in W can be \u201creached\u201d from V\nusing \u03a6. A bijective \u03a6 can be \u201cundone\u201d, i.e., there exists a mapping \u03a8 :\nW \u2192 Vso that \u03a8 \u25e6 \u03a6(x) = x. This mapping \u03a8 is then called the inverse\nof \u03a6 and normally denoted by \u03a6\u22121.\nWith these definitions, we introduce the following special cases of linear\nmappings between vector spaces V and W:\nisomorphism\nIsomorphism: \u03a6 : V \u2192 W linear and bijective endomorphism\nEndomorphism: \u03a6 : V \u2192 V linear automorphism\nAutomorphism: \u03a6 : V \u2192 V linear and bijective\nWe define idV : V \u2192 V , x 7\u2192 x as the identity mapping or identity identity mapping\nidentity\nautomorphism\nautomorphism in V .\nExample 2.19 (Homomorphism)\nThe mapping \u03a6 : R2 \u2192 C, \u03a6(x) = x1 + ix2, is a homomorphism:\n\u03a6\n\u0012\u0014x1\nx2\n\u0015\n+\n\u0014y1\ny2\n\u0015\u0013\n= (x1 + y1) + i(x2 + y"
  },
  {
    "vector_id": 193,
    "chunk_id": "p55_c1",
    "page_number": 55,
    "text": "entity identity mapping\nidentity\nautomorphism\nautomorphism in V .\nExample 2.19 (Homomorphism)\nThe mapping \u03a6 : R2 \u2192 C, \u03a6(x) = x1 + ix2, is a homomorphism:\n\u03a6\n\u0012\u0014x1\nx2\n\u0015\n+\n\u0014y1\ny2\n\u0015\u0013\n= (x1 + y1) + i(x2 + y2) = x1 + ix2 + y1 + iy2\n= \u03a6\n\u0012\u0014x1\nx2\n\u0015\u0013\n+ \u03a6\n\u0012\u0014y1\ny2\n\u0015\u0013\n\u03a6\n\u0012\n\u03bb\n\u0014x1\nx2\n\u0015\u0013\n= \u03bbx1 + \u03bbix2 = \u03bb(x1 + ix2) = \u03bb\u03a6\n\u0012\u0014x1\nx2\n\u0015\u0013\n.\n(2.88)\nThis also justifies why complex numbers can be represented as tuples in\nR2: There is a bijective linear mapping that converts the elementwise addi-\ntion of tuples in R2 into the set of complex numbers with the correspond-\ning addition. Note that we only showed linearity , but not the bijection.\nTheorem 2.17 (Theorem 3.59 in Axler (2015)). Finite-dimensional vector\nspaces V and W are isomorphic if and only if dim(V ) = dim(W).\nTheorem 2.17 states that there exists a linear,"
  },
  {
    "vector_id": 194,
    "chunk_id": "p55_c2",
    "page_number": 55,
    "text": "not the bijection.\nTheorem 2.17 (Theorem 3.59 in Axler (2015)). Finite-dimensional vector\nspaces V and W are isomorphic if and only if dim(V ) = dim(W).\nTheorem 2.17 states that there exists a linear, bijective mapping be-\ntween two vector spaces of the same dimension. Intuitively , this means\nthat vector spaces of the same dimension are kind of the same thing, as\nthey can be transformed into each other without incurring any loss.\nTheorem 2.17 also gives us the justification to treat Rm\u00d7n (the vector\nspace of m \u00d7 n-matrices) and Rmn (the vector space of vectors of length\nmn) the same, as their dimensions are mn, and there exists a linear, bi-\njective mapping that transforms one into the other.\nRemark. Consider vector spaces V, W, X. Then:\nFor linear mappings \u03a6 : V \u2192 W and \u03a8 : W \u2192 X, the ma"
  },
  {
    "vector_id": 195,
    "chunk_id": "p55_c3",
    "page_number": 55,
    "text": "dimensions are mn, and there exists a linear, bi-\njective mapping that transforms one into the other.\nRemark. Consider vector spaces V, W, X. Then:\nFor linear mappings \u03a6 : V \u2192 W and \u03a8 : W \u2192 X, the mapping\n\u03a8 \u25e6 \u03a6 : V \u2192 X is also linear.\nIf \u03a6 : V \u2192 W is an isomorphism, then \u03a6\u22121 : W \u2192 V is an isomor-\nphism, too.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 196,
    "chunk_id": "p56_c0",
    "page_number": 56,
    "text": "50 Linear Algebra\nFigure 2.8 Two\ndifferent coordinate\nsystems defined by\ntwo sets of basis\nvectors. A vector x\nhas different\ncoordinate\nrepresentations\ndepending on which\ncoordinate system is\nchosen.\nx x\ne1\ne2\nb1\nb2\nIf \u03a6 : V \u2192 W, \u03a8 : V \u2192 W are linear, then \u03a6 + \u03a8 and \u03bb\u03a6, \u03bb\u2208 R, are\nlinear, too.\n\u2662\n2.7.1 Matrix Representation of Linear Mappings\nAny n-dimensional vector space is isomorphic to Rn (Theorem 2.17). We\nconsider a basis {b1, . . . ,bn} of an n-dimensional vector space V . In the\nfollowing, the order of the basis vectors will be important. Therefore, we\nwrite\nB = (b1, . . . ,bn) (2.89)\nand call this n-tuple an ordered basis of V .ordered basis\nRemark (Notation). We are at the point where notation gets a bit tricky .\nTherefore, we summarize some parts here.B = (b1, . . . ,bn) is an ord"
  },
  {
    "vector_id": 197,
    "chunk_id": "p56_c1",
    "page_number": 56,
    "text": "d call this n-tuple an ordered basis of V .ordered basis\nRemark (Notation). We are at the point where notation gets a bit tricky .\nTherefore, we summarize some parts here.B = (b1, . . . ,bn) is an ordered\nbasis, B = {b1, . . . ,bn} is an (unordered) basis, and B = [b1, . . . ,bn] is a\nmatrix whose columns are the vectors b1, . . . ,bn. \u2662\nDefinition 2.18 (Coordinates). Consider a vector spaceV and an ordered\nbasis B = (b1, . . . ,bn) of V . For any x \u2208 V we obtain a unique represen-\ntation (linear combination)\nx = \u03b11b1 + . . .+ \u03b1nbn (2.90)\nof x with respect to B. Then \u03b11, . . . , \u03b1n are the coordinates of x withcoordinate\nrespect to B, and the vector\n\u03b1 =\n\uf8ee\n\uf8ef\uf8f0\n\u03b11\n...\n\u03b1n\n\uf8f9\n\uf8fa\uf8fb \u2208 Rn (2.91)\nis the coordinate vector/coordinate representation of x with respect to thecoordinate vector\ncoordinate\nre"
  },
  {
    "vector_id": 198,
    "chunk_id": "p56_c2",
    "page_number": 56,
    "text": "rdinates of x withcoordinate\nrespect to B, and the vector\n\u03b1 =\n\uf8ee\n\uf8ef\uf8f0\n\u03b11\n...\n\u03b1n\n\uf8f9\n\uf8fa\uf8fb \u2208 Rn (2.91)\nis the coordinate vector/coordinate representation of x with respect to thecoordinate vector\ncoordinate\nrepresentation\nordered basis B.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 199,
    "chunk_id": "p57_c0",
    "page_number": 57,
    "text": "2.7 Linear Mappings 51\nA basis effectively defines a coordinate system. We are familiar with the\nCartesian coordinate system in two dimensions, which is spanned by the\ncanonical basis vectors e1, e2. In this coordinate system, a vector x \u2208 R2\nhas a representation that tells us how to linearly combine e1 and e2 to\nobtain x. However, any basis of R2 defines a valid coordinate system,\nand the same vector x from before may have a different coordinate rep-\nresentation in the (b1, b2) basis. In Figure 2.8, the coordinates of x with\nrespect to the standard basis (e1, e2) is [2, 2]\u22a4. However, with respect to\nthe basis (b1, b2) the same vector x is represented as [1.09, 0.72]\u22a4, i.e.,\nx = 1.09b1 + 0.72b2. In the following sections, we will discover how to\nobtain this representation.\nExample 2.20\nLet"
  },
  {
    "vector_id": 200,
    "chunk_id": "p57_c1",
    "page_number": 57,
    "text": "pect to\nthe basis (b1, b2) the same vector x is represented as [1.09, 0.72]\u22a4, i.e.,\nx = 1.09b1 + 0.72b2. In the following sections, we will discover how to\nobtain this representation.\nExample 2.20\nLet us have a look at a geometric vector x \u2208 R2 with coordinates [2, 3]\u22a4 Figure 2.9\nDifferent coordinate\nrepresentations of a\nvector x, depending\non the choice of\nbasis.\ne1\ne2 b2\nb1\nx=\u221212b1+52b2\nx= 2e1+ 3e2\nwith respect to the standard basis(e1, e2) of R2. This means, we can write\nx = 2e1 + 3e2. However, we do not have to choose the standard basis to\nrepresent this vector. If we use the basis vectorsb1 = [1, \u22121]\u22a4, b2 = [1, 1]\u22a4\nwe will obtain the coordinates 1\n2 [\u22121, 5]\u22a4 to represent the same vector with\nrespect to (b1, b2) (see Figure 2.9).\nRemark. For an n-dimensional vector space V and an order"
  },
  {
    "vector_id": 201,
    "chunk_id": "p57_c2",
    "page_number": 57,
    "text": "b1 = [1, \u22121]\u22a4, b2 = [1, 1]\u22a4\nwe will obtain the coordinates 1\n2 [\u22121, 5]\u22a4 to represent the same vector with\nrespect to (b1, b2) (see Figure 2.9).\nRemark. For an n-dimensional vector space V and an ordered basis B\nof V , the mapping \u03a6 : Rn \u2192 V , \u03a6(ei) = bi, i = 1 , . . . , n,is linear\n(and because of Theorem 2.17 an isomorphism), where (e1, . . . ,en) is\nthe standard basis of Rn.\n\u2662\nNow we are ready to make an explicit connection between matrices and\nlinear mappings between finite-dimensional vector spaces.\nDefinition 2.19 (Transformation Matrix). Consider vector spaces V, W\nwith corresponding (ordered) basesB = (b1, . . . ,bn) and C = (c1, . . . ,cm).\nMoreover, we consider a linear mapping \u03a6 : V \u2192 W. For j \u2208 {1, . . . , n},\n\u03a6(bj) = \u03b11jc1 + \u00b7 \u00b7\u00b7+ \u03b1mjcm =\nmX\ni=1\n\u03b1ijci (2.92)\nis the unique repre"
  },
  {
    "vector_id": 202,
    "chunk_id": "p57_c3",
    "page_number": 57,
    "text": "dered) basesB = (b1, . . . ,bn) and C = (c1, . . . ,cm).\nMoreover, we consider a linear mapping \u03a6 : V \u2192 W. For j \u2208 {1, . . . , n},\n\u03a6(bj) = \u03b11jc1 + \u00b7 \u00b7\u00b7+ \u03b1mjcm =\nmX\ni=1\n\u03b1ijci (2.92)\nis the unique representation of \u03a6(bj) with respect to C. Then, we call the\nm \u00d7 n-matrix A\u03a6, whose elements are given by\nA\u03a6(i, j) = \u03b1ij , (2.93)\nthe transformation matrix of \u03a6 (with respect to the ordered bases B of V transformation\nmatrixand C of W).\nThe coordinates of \u03a6(bj) with respect to the ordered basis C of W\nare the j-th column of A\u03a6. Consider (finite-dimensional) vector spaces\nV, Wwith ordered bases B, Cand a linear mapping \u03a6 : V \u2192 W with\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 203,
    "chunk_id": "p57_c4",
    "page_number": 57,
    "text": "a linear mapping \u03a6 : V \u2192 W with\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 204,
    "chunk_id": "p58_c0",
    "page_number": 58,
    "text": "52 Linear Algebra\ntransformation matrix A\u03a6. If \u02c6x is the coordinate vector of x \u2208 V with\nrespect to B and \u02c6y the coordinate vector of y = \u03a6(x) \u2208 W with respect\nto C, then\n\u02c6y = A\u03a6 \u02c6x. (2.94)\nThis means that the transformation matrix can be used to map coordinates\nwith respect to an ordered basis in V to coordinates with respect to an\nordered basis in W.\nExample 2.21 (Transformation Matrix)\nConsider a homomorphism \u03a6 : V \u2192 W and ordered bases B =\n(b1, . . . ,b3) of V and C = (c1, . . . ,c4) of W. With\n\u03a6(b1) = c1 \u2212 c2 + 3c3 \u2212 c4\n\u03a6(b2) = 2c1 + c2 + 7c3 + 2c4\n\u03a6(b3) = 3c2 + c3 + 4c4\n(2.95)\nthe transformation matrix A\u03a6 with respect to B and C satisfies \u03a6(bk) =P4\ni=1 \u03b1ikci for k = 1, . . . ,3 and is given as\nA\u03a6 = [\u03b11, \u03b12, \u03b13] =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 2 0\n\u22121 1 3\n3 7 1\n\u22121 2 4\n\uf8f9\n\uf8fa\uf8fa\uf8fb , (2.96)\nwhere the \u03b1j, j= 1, 2, 3"
  },
  {
    "vector_id": 205,
    "chunk_id": "p58_c1",
    "page_number": 58,
    "text": "ormation matrix A\u03a6 with respect to B and C satisfies \u03a6(bk) =P4\ni=1 \u03b1ikci for k = 1, . . . ,3 and is given as\nA\u03a6 = [\u03b11, \u03b12, \u03b13] =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 2 0\n\u22121 1 3\n3 7 1\n\u22121 2 4\n\uf8f9\n\uf8fa\uf8fa\uf8fb , (2.96)\nwhere the \u03b1j, j= 1, 2, 3, are the coordinate vectors of\u03a6(bj) with respect\nto C.\nExample 2.22 (Linear Transformations of Vectors)\nFigure 2.10 Three\nexamples of linear\ntransformations of\nthe vectors shown\nas dots in (a);\n(b) Rotation by 45\u25e6;\n(c) Stretching of the\nhorizontal\ncoordinates by 2;\n(d) Combination of\nreflection, rotation\nand stretching.\n(a) Original data.\n (b) Rotation by 45\u25e6.\n (c) Stretch along the\nhorizontal axis.\n(d) General linear\nmapping.\nWe consider three linear transformations of a set of vectors in R2 with\nthe transformation matrices\nA1 =\n\u0014cos(\u03c0\n4 ) \u2212sin(\u03c0\n4 )\nsin(\u03c0\n4 ) cos( \u03c0\n4 )\n\u0015\n, A2 =\n\u00142 0\n0 1\n\u0015"
  },
  {
    "vector_id": 206,
    "chunk_id": "p58_c2",
    "page_number": 58,
    "text": "is.\n(d) General linear\nmapping.\nWe consider three linear transformations of a set of vectors in R2 with\nthe transformation matrices\nA1 =\n\u0014cos(\u03c0\n4 ) \u2212sin(\u03c0\n4 )\nsin(\u03c0\n4 ) cos( \u03c0\n4 )\n\u0015\n, A2 =\n\u00142 0\n0 1\n\u0015\n, A3 = 1\n2\n\u00143 \u22121\n1 \u22121\n\u0015\n. (2.97)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 207,
    "chunk_id": "p59_c0",
    "page_number": 59,
    "text": "2.7 Linear Mappings 53\nFigure 2.10 gives three examples of linear transformations of a set of vec-\ntors. Figure 2.10(a) shows 400 vectors in R2, each of which is represented\nby a dot at the corresponding (x1, x2)-coordinates. The vectors are ar-\nranged in a square. When we use matrixA1 in (2.97) to linearly transform\neach of these vectors, we obtain the rotated square in Figure 2.10(b). If we\napply the linear mapping represented by A2, we obtain the rectangle in\nFigure 2.10(c) where each x1-coordinate is stretched by 2. Figure 2.10(d)\nshows the original square from Figure 2.10(a) when linearly transformed\nusing A3, which is a combination of a reflection, a rotation, and a stretch.\n2.7.2 Basis Change\nIn the following, we will have a closer look at how transformation matrices\nof a linear map"
  },
  {
    "vector_id": 208,
    "chunk_id": "p59_c1",
    "page_number": 59,
    "text": "transformed\nusing A3, which is a combination of a reflection, a rotation, and a stretch.\n2.7.2 Basis Change\nIn the following, we will have a closer look at how transformation matrices\nof a linear mapping \u03a6 : V \u2192 W change if we change the bases in V and\nW. Consider two ordered bases\nB = (b1, . . . ,bn), \u02dcB = (\u02dcb1, . . . ,\u02dcbn) (2.98)\nof V and two ordered bases\nC = (c1, . . . ,cm), \u02dcC = (\u02dcc1, . . . ,\u02dccm) (2.99)\nof W. Moreover, A\u03a6 \u2208 Rm\u00d7n is the transformation matrix of the linear\nmapping \u03a6 : V \u2192 W with respect to the basesB and C, and \u02dcA\u03a6 \u2208 Rm\u00d7n\nis the corresponding transformation mapping with respect to \u02dcB and \u02dcC.\nIn the following, we will investigate how A and \u02dcA are related, i.e., how/\nwhether we can transform A\u03a6 into \u02dcA\u03a6 if we choose to perform a basis\nchange from B, Cto \u02dcB, \u02dcC.\nRemark. W"
  },
  {
    "vector_id": 209,
    "chunk_id": "p59_c2",
    "page_number": 59,
    "text": "spect to \u02dcB and \u02dcC.\nIn the following, we will investigate how A and \u02dcA are related, i.e., how/\nwhether we can transform A\u03a6 into \u02dcA\u03a6 if we choose to perform a basis\nchange from B, Cto \u02dcB, \u02dcC.\nRemark. We effectively get different coordinate representations of the\nidentity mapping idV . In the context of Figure 2.9, this would mean to\nmap coordinates with respect to (e1, e2) onto coordinates with respect to\n(b1, b2) without changing the vector x. By changing the basis and corre-\nspondingly the representation of vectors, the transformation matrix with\nrespect to this new basis can have a particularly simple form that allows\nfor straightforward computation. \u2662\nExample 2.23 (Basis Change)\nConsider a transformation matrix\nA =\n\u00142 1\n1 2\n\u0015\n(2.100)\nwith respect to the canonical basis in R2. If we defi"
  },
  {
    "vector_id": 210,
    "chunk_id": "p59_c3",
    "page_number": 59,
    "text": "rly simple form that allows\nfor straightforward computation. \u2662\nExample 2.23 (Basis Change)\nConsider a transformation matrix\nA =\n\u00142 1\n1 2\n\u0015\n(2.100)\nwith respect to the canonical basis in R2. If we define a new basis\nB = (\n\u00141\n1\n\u0015\n,\n\u0014 1\n\u22121\n\u0015\n) (2.101)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 211,
    "chunk_id": "p60_c0",
    "page_number": 60,
    "text": "54 Linear Algebra\nwe obtain a diagonal transformation matrix\n\u02dcA =\n\u00143 0\n0 1\n\u0015\n(2.102)\nwith respect to B, which is easier to work with than A.\nIn the following, we will look at mappings that transform coordinate\nvectors with respect to one basis into coordinate vectors with respect to\na different basis. We will state our main result first and then provide an\nexplanation.\nTheorem 2.20 (Basis Change). For a linear mapping\u03a6 : V \u2192 W, ordered\nbases\nB = (b1, . . . ,bn), \u02dcB = (\u02dcb1, . . . ,\u02dcbn) (2.103)\nof V and\nC = (c1, . . . ,cm), \u02dcC = (\u02dcc1, . . . ,\u02dccm) (2.104)\nof W, and a transformation matrix A\u03a6 of \u03a6 with respect to B and C, the\ncorresponding transformation matrix \u02dcA\u03a6 with respect to the bases \u02dcB and \u02dcC\nis given as\n\u02dcA\u03a6 = T\u22121A\u03a6S . (2.105)\nHere, S \u2208 Rn\u00d7n is the transformation matrix of idV that map"
  },
  {
    "vector_id": 212,
    "chunk_id": "p60_c1",
    "page_number": 60,
    "text": "with respect to B and C, the\ncorresponding transformation matrix \u02dcA\u03a6 with respect to the bases \u02dcB and \u02dcC\nis given as\n\u02dcA\u03a6 = T\u22121A\u03a6S . (2.105)\nHere, S \u2208 Rn\u00d7n is the transformation matrix of idV that maps coordinates\nwith respect to \u02dcB onto coordinates with respect to B, and T \u2208 Rm\u00d7m is the\ntransformation matrix of idW that maps coordinates with respect to \u02dcC onto\ncoordinates with respect to C.\nProof Following Drumm and Weil (2001), we can write the vectors of\nthe new basis \u02dcB of V as a linear combination of the basis vectors of B,\nsuch that\n\u02dcbj = s1jb1 + \u00b7 \u00b7\u00b7+ snjbn =\nnX\ni=1\nsijbi , j = 1, . . . , n .(2.106)\nSimilarly , we write the new basis vectors \u02dcC of W as a linear combination\nof the basis vectors of C, which yields\n\u02dcck = t1kc1 + \u00b7 \u00b7\u00b7+ tmkcm =\nmX\nl=1\ntlkcl , k = 1, . . . , m .(2.107)\nWe"
  },
  {
    "vector_id": 213,
    "chunk_id": "p60_c2",
    "page_number": 60,
    "text": ", n .(2.106)\nSimilarly , we write the new basis vectors \u02dcC of W as a linear combination\nof the basis vectors of C, which yields\n\u02dcck = t1kc1 + \u00b7 \u00b7\u00b7+ tmkcm =\nmX\nl=1\ntlkcl , k = 1, . . . , m .(2.107)\nWe define S = (( sij)) \u2208 Rn\u00d7n as the transformation matrix that maps\ncoordinates with respect to \u02dcB onto coordinates with respect to B and\nT = ((tlk)) \u2208 Rm\u00d7m as the transformation matrix that maps coordinates\nwith respect to \u02dcC onto coordinates with respect toC. In particular, thejth\ncolumn of S is the coordinate representation of \u02dcbj with respect to B and\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 214,
    "chunk_id": "p60_c3",
    "page_number": 60,
    "text": "ine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 215,
    "chunk_id": "p61_c0",
    "page_number": 61,
    "text": "2.7 Linear Mappings 55\nthe kth column of T is the coordinate representation of \u02dcck with respect to\nC. Note that both S and T are regular.\nWe are going to look at\u03a6(\u02dcbj) from two perspectives. First, applying the\nmapping \u03a6, we get that for all j = 1, . . . , n\n\u03a6(\u02dcbj) =\nmX\nk=1\n\u02dcakj\u02dcck\n| {z }\n\u2208W\n(2.107)\n=\nmX\nk=1\n\u02dcakj\nmX\nl=1\ntlkcl =\nmX\nl=1\n mX\nk=1\ntlk\u02dcakj\n!\ncl , (2.108)\nwhere we first expressed the new basis vectors \u02dcck \u2208 W as linear com-\nbinations of the basis vectors cl \u2208 W and then swapped the order of\nsummation.\nAlternatively , when we express the \u02dcbj \u2208 V as linear combinations of\nbj \u2208 V , we arrive at\n\u03a6(\u02dcbj)\n(2.106)\n= \u03a6\n nX\ni=1\nsijbi\n!\n=\nnX\ni=1\nsij\u03a6(bi) =\nnX\ni=1\nsij\nmX\nl=1\nalicl (2.109a)\n=\nmX\nl=1\n nX\ni=1\nalisij\n!\ncl , j = 1, . . . , n , (2.109b)\nwhere we exploited the linearity of \u03a6. Compa"
  },
  {
    "vector_id": 216,
    "chunk_id": "p61_c1",
    "page_number": 61,
    "text": "rrive at\n\u03a6(\u02dcbj)\n(2.106)\n= \u03a6\n nX\ni=1\nsijbi\n!\n=\nnX\ni=1\nsij\u03a6(bi) =\nnX\ni=1\nsij\nmX\nl=1\nalicl (2.109a)\n=\nmX\nl=1\n nX\ni=1\nalisij\n!\ncl , j = 1, . . . , n , (2.109b)\nwhere we exploited the linearity of \u03a6. Comparing (2.108) and (2.109b),\nit follows for all j = 1, . . . , nand l = 1, . . . , mthat\nmX\nk=1\ntlk\u02dcakj =\nnX\ni=1\nalisij (2.110)\nand, therefore,\nT \u02dcA\u03a6 = A\u03a6S \u2208 Rm\u00d7n , (2.111)\nsuch that\n\u02dcA\u03a6 = T\u22121A\u03a6S , (2.112)\nwhich proves Theorem 2.20.\nTheorem 2.20 tells us that with a basis change inV (B is replaced with\n\u02dcB) and W (C is replaced with \u02dcC), the transformation matrix A\u03a6 of a\nlinear mapping \u03a6 : V \u2192 W is replaced by an equivalent matrix \u02dcA\u03a6 with\n\u02dcA\u03a6 = T\u22121A\u03a6S. (2.113)\nFigure 2.11 illustrates this relation: Consider a homomorphism \u03a6 : V \u2192\nW and ordered bases B, \u02dcB of V and C, \u02dcC of W. The mapping \u03a6CB is"
  },
  {
    "vector_id": 217,
    "chunk_id": "p61_c2",
    "page_number": 61,
    "text": "eplaced by an equivalent matrix \u02dcA\u03a6 with\n\u02dcA\u03a6 = T\u22121A\u03a6S. (2.113)\nFigure 2.11 illustrates this relation: Consider a homomorphism \u03a6 : V \u2192\nW and ordered bases B, \u02dcB of V and C, \u02dcC of W. The mapping \u03a6CB is an\ninstantiation of \u03a6 and maps basis vectors of B onto linear combinations\nof basis vectors of C. Assume that we know the transformation matrixA\u03a6\nof \u03a6CB with respect to the ordered bases B, C. When we perform a basis\nchange from B to \u02dcB in V and from C to \u02dcC in W, we can determine the\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 218,
    "chunk_id": "p62_c0",
    "page_number": 62,
    "text": "56 Linear Algebra\nFigure 2.11 For a\nhomomorphism\n\u03a6 : V \u2192 W and\nordered bases B, \u02dcB\nof V and C, \u02dcC of W\n(marked in blue),\nwe can express the\nmapping \u03a6 \u02dcC \u02dcB with\nrespect to the bases\n\u02dcB, \u02dcC equivalently as\na composition of the\nhomomorphisms\n\u03a6 \u02dcC \u02dcB =\n\u039e \u02dcCC \u25e6 \u03a6CB \u25e6 \u03a8B \u02dcB\nwith respect to the\nbases in the\nsubscripts. The\ncorresponding\ntransformation\nmatrices are in red.\nV W\nB\n\u02dcB \u02dcC\nC\n\u03a6\n\u03a6CB\n\u03a6\u02dcC\u02dcB\n\u03a8B\u02dcB \u039eC\u02dcCS T\n\u02dcA\u03a6\nA\u03a6\nV W\nB\n\u02dcB \u02dcC\nC\n\u03a6\n\u03a6CB\n\u03a6\u02dcC\u02dcB\n\u03a8B\u02dcB \u039e\u02dcCC= \u039e\u22121\nC\u02dcCS T\u22121\n\u02dcA\u03a6\nA\u03a6\nVector spaces\nOrdered bases\ncorresponding transformation matrix \u02dcA\u03a6 as follows: First, we find the ma-\ntrix representation of the linear mapping\u03a8B \u02dcB : V \u2192 V that maps coordi-\nnates with respect to the new basis \u02dcB onto the (unique) coordinates with\nrespect to the \u201cold\u201d basis B (in V ). Then, we use the transformation ma-\ntrix"
  },
  {
    "vector_id": 219,
    "chunk_id": "p62_c1",
    "page_number": 62,
    "text": "e linear mapping\u03a8B \u02dcB : V \u2192 V that maps coordi-\nnates with respect to the new basis \u02dcB onto the (unique) coordinates with\nrespect to the \u201cold\u201d basis B (in V ). Then, we use the transformation ma-\ntrix A\u03a6 of \u03a6CB : V \u2192 W to map these coordinates onto the coordinates\nwith respect to C in W. Finally , we use a linear mapping\u039e \u02dcCC : W \u2192 W\nto map the coordinates with respect to C onto coordinates with respect to\n\u02dcC. Therefore, we can express the linear mapping\u03a6 \u02dcC \u02dcB as a composition of\nlinear mappings that involve the \u201cold\u201d basis:\n\u03a6 \u02dcC \u02dcB = \u039e \u02dcCC \u25e6 \u03a6CB \u25e6 \u03a8B \u02dcB = \u039e\u22121\nC \u02dcC \u25e6 \u03a6CB \u25e6 \u03a8B \u02dcB . (2.114)\nConcretely , we use\u03a8B \u02dcB = idV and \u039eC \u02dcC = idW , i.e., the identity mappings\nthat map vectors onto themselves, but with respect to a different basis.\nDefinition 2.21 (Equivalence). Two matricesA, \u02dcA \u2208 Rm"
  },
  {
    "vector_id": 220,
    "chunk_id": "p62_c2",
    "page_number": 62,
    "text": "cretely , we use\u03a8B \u02dcB = idV and \u039eC \u02dcC = idW , i.e., the identity mappings\nthat map vectors onto themselves, but with respect to a different basis.\nDefinition 2.21 (Equivalence). Two matricesA, \u02dcA \u2208 Rm\u00d7n are equivalentequivalent\nif there exist regular matrices S \u2208 Rn\u00d7n and T \u2208 Rm\u00d7m, such that\n\u02dcA = T\u22121AS.\nDefinition 2.22 (Similarity). Two matrices A, \u02dcA \u2208 Rn\u00d7n are similar ifsimilar\nthere exists a regular matrix S \u2208 Rn\u00d7n with \u02dcA = S\u22121AS\nRemark. Similar matrices are always equivalent. However, equivalent ma-\ntrices are not necessarily similar. \u2662\nRemark. Consider vector spaces V, W, X. From the remark that follows\nTheorem 2.17, we already know that for linear mappings \u03a6 : V \u2192 W\nand \u03a8 : W \u2192 X the mapping \u03a8 \u25e6 \u03a6 : V \u2192 X is also linear. With\ntransformation matrices A\u03a6 and A\u03a8 of the corresponding ma"
  },
  {
    "vector_id": 221,
    "chunk_id": "p62_c3",
    "page_number": 62,
    "text": "ark that follows\nTheorem 2.17, we already know that for linear mappings \u03a6 : V \u2192 W\nand \u03a8 : W \u2192 X the mapping \u03a8 \u25e6 \u03a6 : V \u2192 X is also linear. With\ntransformation matrices A\u03a6 and A\u03a8 of the corresponding mappings, the\noverall transformation matrix is A\u03a8\u25e6\u03a6 = A\u03a8A\u03a6. \u2662\nIn light of this remark, we can look at basis changes from the perspec-\ntive of composing linear mappings:\nA\u03a6 is the transformation matrix of a linear mapping \u03a6CB : V \u2192 W\nwith respect to the bases B, C.\n\u02dcA\u03a6 is the transformation matrix of the linear mapping \u03a6 \u02dcC \u02dcB : V \u2192 W\nwith respect to the bases \u02dcB, \u02dcC.\nS is the transformation matrix of a linear mapping \u03a8B \u02dcB : V \u2192 V\n(automorphism) that represents \u02dcB in terms of B. Normally ,\u03a8 = idV is\nthe identity mapping in V .\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:ht"
  },
  {
    "vector_id": 222,
    "chunk_id": "p62_c4",
    "page_number": 62,
    "text": "f a linear mapping \u03a8B \u02dcB : V \u2192 V\n(automorphism) that represents \u02dcB in terms of B. Normally ,\u03a8 = idV is\nthe identity mapping in V .\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 223,
    "chunk_id": "p63_c0",
    "page_number": 63,
    "text": "2.7 Linear Mappings 57\nT is the transformation matrix of a linear mapping \u039eC \u02dcC : W \u2192 W\n(automorphism) that represents \u02dcC in terms of C. Normally ,\u039e = idW is\nthe identity mapping in W.\nIf we (informally) write down the transformations just in terms of bases,\nthen A\u03a6 : B \u2192 C, \u02dcA\u03a6 : \u02dcB \u2192 \u02dcC, S : \u02dcB \u2192 B, T : \u02dcC \u2192 C and\nT\u22121 : C \u2192 \u02dcC, and\n\u02dcB \u2192 \u02dcC = \u02dcB \u2192 B\u2192 C \u2192 \u02dcC (2.115)\n\u02dcA\u03a6 = T\u22121A\u03a6S . (2.116)\nNote that the execution order in (2.116) is from right to left because vec-\ntors are multiplied at the right-hand side so that x 7\u2192 Sx 7\u2192 A\u03a6(Sx) 7\u2192\nT\u22121\u0000\nA\u03a6(Sx)\n\u0001\n= \u02dcA\u03a6x.\nExample 2.24 (Basis Change)\nConsider a linear mapping \u03a6 : R3 \u2192 R4 whose transformation matrix is\nA\u03a6 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 2 0\n\u22121 1 3\n3 7 1\n\u22121 2 4\n\uf8f9\n\uf8fa\uf8fa\uf8fb (2.117)\nwith respect to the standard bases\nB = (\n\uf8ee\n\uf8f0\n1\n0\n0\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n0\n1\n0\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n0\n0\n1\n\uf8f9\n\uf8fb) , C"
  },
  {
    "vector_id": 224,
    "chunk_id": "p63_c1",
    "page_number": 63,
    "text": "linear mapping \u03a6 : R3 \u2192 R4 whose transformation matrix is\nA\u03a6 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 2 0\n\u22121 1 3\n3 7 1\n\u22121 2 4\n\uf8f9\n\uf8fa\uf8fa\uf8fb (2.117)\nwith respect to the standard bases\nB = (\n\uf8ee\n\uf8f0\n1\n0\n0\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n0\n1\n0\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n0\n0\n1\n\uf8f9\n\uf8fb) , C = (\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n0\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n0\n1\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n0\n0\n1\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n0\n0\n0\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fb). (2.118)\nWe seek the transformation matrix \u02dcA\u03a6 of \u03a6 with respect to the new bases\n\u02dcB = (\n\uf8ee\n\uf8f0\n1\n1\n0\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n0\n1\n1\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n1\n0\n1\n\uf8f9\n\uf8fb) \u2208 R3, \u02dcC = (\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n1\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n0\n1\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n0\n1\n1\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n0\n0\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fb) . (2.119)\nThen,\nS =\n\uf8ee\n\uf8f0\n1 0 1\n1 1 0\n0 1 1\n\uf8f9\n\uf8fb, T =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 1 0 1\n1 0 1 0\n0 1 1 0\n0 0 0 1\n\uf8f9\n\uf8fa\uf8fa\uf8fb , (2.120)\nwhere the ith column of S is the coordinate representation of \u02dcbi in\nterms of the basis vectors of B. Since B is the standard basis, the co-\nordinate representation is straight"
  },
  {
    "vector_id": 225,
    "chunk_id": "p63_c2",
    "page_number": 63,
    "text": "0 0 0 1\n\uf8f9\n\uf8fa\uf8fa\uf8fb , (2.120)\nwhere the ith column of S is the coordinate representation of \u02dcbi in\nterms of the basis vectors of B. Since B is the standard basis, the co-\nordinate representation is straightforward to find. For a general basis B,\nwe would need to solve a linear equation system to find the \u03bbi such that\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 226,
    "chunk_id": "p64_c0",
    "page_number": 64,
    "text": "58 Linear Algebra\nP3\ni=1 \u03bbibi = \u02dcbj, j = 1, . . . ,3. Similarly , thejth column of T is the coordi-\nnate representation of \u02dccj in terms of the basis vectors of C.\nTherefore, we obtain\n\u02dcA\u03a6 = T\u22121A\u03a6S = 1\n2\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 1 \u22121 \u22121\n1 \u22121 1 \u22121\n\u22121 1 1 1\n0 0 0 2\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n3 2 1\n0 4 2\n10 8 4\n1 6 3\n\uf8f9\n\uf8fa\uf8fa\uf8fb (2.121a)\n=\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22124 \u22124 \u22122\n6 0 0\n4 8 4\n1 6 3\n\uf8f9\n\uf8fa\uf8fa\uf8fb. (2.121b)\nIn Chapter 4, we will be able to exploit the concept of a basis change\nto find a basis with respect to which the transformation matrix of an en-\ndomorphism has a particularly simple (diagonal) form. In Chapter 10, we\nwill look at a data compression problem and find a convenient basis onto\nwhich we can project the data while minimizing the compression loss.\n2.7.3 Image and Kernel\nThe image and kernel of a linear mapping are vector subspaces with ce"
  },
  {
    "vector_id": 227,
    "chunk_id": "p64_c1",
    "page_number": 64,
    "text": "oblem and find a convenient basis onto\nwhich we can project the data while minimizing the compression loss.\n2.7.3 Image and Kernel\nThe image and kernel of a linear mapping are vector subspaces with cer-\ntain important properties. In the following, we will characterize them\nmore carefully .\nDefinition 2.23 (Image and Kernel).\nFor \u03a6 : V \u2192 W, we define the kernel/null spacekernel\nnull space\nker(\u03a6) := \u03a6\u22121(0W ) = {v \u2208 V : \u03a6(v) = 0W } (2.122)\nand the image/rangeimage\nrange\nIm(\u03a6) := \u03a6(V ) = {w \u2208 W|\u2203v \u2208 V : \u03a6(v) = w}. (2.123)\nWe also call V and W also the domain and codomain of \u03a6, respectively .domain\ncodomain\nIntuitively , the kernel is the set of vectorsv \u2208 V that \u03a6 maps onto the\nneutral element 0W \u2208 W. The image is the set of vectors w \u2208 W that\ncan be \u201creached\u201d by \u03a6 from any vector in V . An il"
  },
  {
    "vector_id": 228,
    "chunk_id": "p64_c2",
    "page_number": 64,
    "text": "codomain\nIntuitively , the kernel is the set of vectorsv \u2208 V that \u03a6 maps onto the\nneutral element 0W \u2208 W. The image is the set of vectors w \u2208 W that\ncan be \u201creached\u201d by \u03a6 from any vector in V . An illustration is given in\nFigure 2.12.\nRemark. Consider a linear mapping \u03a6 : V \u2192 W, where V, Ware vector\nspaces.\nIt always holds that \u03a6(0V ) = 0W and, therefore, 0V \u2208 ker(\u03a6). In\nparticular, the null space is never empty .\nIm(\u03a6) \u2286 W is a subspace of W, and ker(\u03a6) \u2286 V is a subspace of V .\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 229,
    "chunk_id": "p65_c0",
    "page_number": 65,
    "text": "2.7 Linear Mappings 59\nFigure 2.12 Kernel\nand image of a\nlinear mapping\n\u03a6 : V \u2192 W.\nIm(\u03a6)\n0W\nker(\u03a6)\n0V\n\u03a6 :V \u2192WV W\n\u03a6 is injective (one-to-one) if and only if ker(\u03a6) = {0}.\n\u2662\nRemark (Null Space and Column Space). Let us consider A \u2208 Rm\u00d7n and\na linear mapping \u03a6 : Rn \u2192 Rm, x 7\u2192 Ax.\nFor A = [a1, . . . ,an], where ai are the columns of A, we obtain\nIm(\u03a6) = {Ax : x \u2208 Rn} =\n( nX\ni=1\nxiai : x1, . . . , xn \u2208 R\n)\n(2.124a)\n= span[a1, . . . ,an] \u2286 Rm , (2.124b)\ni.e., the image is the span of the columns of A, also called the column column space\nspace. Therefore, the column space (image) is a subspace of Rm, where\nm is the \u201cheight\u201d of the matrix.\nrk(A) = dim(Im(\u03a6)).\nThe kernel/null space ker(\u03a6) is the general solution to the homoge-\nneous system of linear equations Ax = 0 and captures all possible\nlinear"
  },
  {
    "vector_id": 230,
    "chunk_id": "p65_c1",
    "page_number": 65,
    "text": "where\nm is the \u201cheight\u201d of the matrix.\nrk(A) = dim(Im(\u03a6)).\nThe kernel/null space ker(\u03a6) is the general solution to the homoge-\nneous system of linear equations Ax = 0 and captures all possible\nlinear combinations of the elements in Rn that produce 0 \u2208 Rm.\nThe kernel is a subspace of Rn, where n is the \u201cwidth\u201d of the matrix.\nThe kernel focuses on the relationship among the columns, and we can\nuse it to determine whether/how we can express a column as a linear\ncombination of other columns.\n\u2662\nExample 2.25 (Image and Kernel of a Linear Mapping)\nThe mapping\n\u03a6 : R4 \u2192 R2,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\nx1\nx2\nx3\nx4\n\uf8f9\n\uf8fa\uf8fa\uf8fb 7\u2192\n\u00141 2 \u22121 0\n1 0 0 1\n\u0015\n\uf8ee\n\uf8ef\uf8ef\uf8f0\nx1\nx2\nx3\nx4\n\uf8f9\n\uf8fa\uf8fa\uf8fb =\n\u0014x1 + 2x2 \u2212 x3\nx1 + x4\n\u0015\n(2.125a)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 231,
    "chunk_id": "p65_c2",
    "page_number": 65,
    "text": "\u00141 2 \u22121 0\n1 0 0 1\n\u0015\n\uf8ee\n\uf8ef\uf8ef\uf8f0\nx1\nx2\nx3\nx4\n\uf8f9\n\uf8fa\uf8fa\uf8fb =\n\u0014x1 + 2x2 \u2212 x3\nx1 + x4\n\u0015\n(2.125a)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 232,
    "chunk_id": "p66_c0",
    "page_number": 66,
    "text": "60 Linear Algebra\n= x1\n\u00141\n1\n\u0015\n+ x2\n\u00142\n0\n\u0015\n+ x3\n\u0014\u22121\n0\n\u0015\n+ x4\n\u00140\n1\n\u0015\n(2.125b)\nis linear. To determineIm(\u03a6), we can take the span of the columns of the\ntransformation matrix and obtain\nIm(\u03a6) = span[\n\u00141\n1\n\u0015\n,\n\u00142\n0\n\u0015\n,\n\u0014\u22121\n0\n\u0015\n,\n\u00140\n1\n\u0015\n] . (2.126)\nTo compute the kernel (null space) of \u03a6, we need to solve Ax = 0, i.e.,\nwe need to solve a homogeneous equation system. To do this, we use\nGaussian elimination to transform A into reduced row-echelon form:\n\u00141 2 \u22121 0\n1 0 0 1\n\u0015\n\u21dd \u00b7 \u00b7\u00b7\u21dd\n\u00141 0 0 1\n0 1 \u22121\n2 \u22121\n2\n\u0015\n. (2.127)\nThis matrix is in reduced row-echelon form, and we can use the Minus-\n1 Trick to compute a basis of the kernel (see Section 2.3.3). Alternatively ,\nwe can express the non-pivot columns (columns 3 and 4) as linear com-\nbinations of the pivot columns (columns 1 and 2). The third column a3"
  },
  {
    "vector_id": 233,
    "chunk_id": "p66_c1",
    "page_number": 66,
    "text": "a basis of the kernel (see Section 2.3.3). Alternatively ,\nwe can express the non-pivot columns (columns 3 and 4) as linear com-\nbinations of the pivot columns (columns 1 and 2). The third column a3 is\nequivalent to \u22121\n2 times the second columna2. Therefore, 0 = a3 + 1\n2 a2. In\nthe same way , we see thata4 = a1\u22121\n2 a2 and, therefore, 0 = a1\u22121\n2 a2\u2212a4.\nOverall, this gives us the kernel (null space) as\nker(\u03a6) = span[\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n0\n1\n2\n1\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22121\n1\n2\n0\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fb] . (2.128)\nrank-nullity\ntheorem Theorem 2.24 (Rank-Nullity Theorem). For vector spaces V, Wand a lin-\near mapping \u03a6 : V \u2192 W it holds that\ndim(ker(\u03a6)) + dim(Im(\u03a6)) = dim(V ) . (2.129)\nThe rank-nullity theorem is also referred to as the fundamental theoremfundamental\ntheorem of linear\nmappings\nof linear mappings (Axler, 2015, theorem"
  },
  {
    "vector_id": 234,
    "chunk_id": "p66_c2",
    "page_number": 66,
    "text": "hat\ndim(ker(\u03a6)) + dim(Im(\u03a6)) = dim(V ) . (2.129)\nThe rank-nullity theorem is also referred to as the fundamental theoremfundamental\ntheorem of linear\nmappings\nof linear mappings (Axler, 2015, theorem 3.22). The following are direct\nconsequences of Theorem 2.24:\nIf dim(Im(\u03a6)) < dim(V ), then ker(\u03a6) is non-trivial, i.e., the kernel\ncontains more than 0V and dim(ker(\u03a6)) \u2a7e 1.\nIf A\u03a6 is the transformation matrix of\u03a6 with respect to an ordered basis\nand dim(Im(\u03a6)) < dim(V ), then the system of linear equationsA\u03a6x =\n0 has infinitely many solutions.\nIf dim(V ) = dim(W), then the three-way equivalence\n\u03a6 is injective \u21d0 \u21d2\u03a6 is surjective \u21d0 \u21d2\u03a6 is bijective\nholds since Im(\u03a6) \u2286 W.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 235,
    "chunk_id": "p66_c3",
    "page_number": 66,
    "text": "is injective \u21d0 \u21d2\u03a6 is surjective \u21d0 \u21d2\u03a6 is bijective\nholds since Im(\u03a6) \u2286 W.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 236,
    "chunk_id": "p67_c0",
    "page_number": 67,
    "text": "2.8 Affine Spaces 61\n2.8 Affine Spaces\nIn the following, we will take a closer look at spaces that are offset from\nthe origin, i.e., spaces that are no longer vector subspaces. Moreover, we\nwill briefly discuss properties of mappings between these affine spaces,\nwhich resemble linear mappings.\nRemark. In the machine learning literature, the distinction between linear\nand affine is sometimes not clear so that we can find references to affine\nspaces/mappings as linear spaces/mappings. \u2662\n2.8.1 Affine Subspaces\nDefinition 2.25 (Affine Subspace). Let V be a vector space, x0 \u2208 V and\nU \u2286 V a subspace. Then the subset\nL = x0 + U := {x0 + u : u \u2208 U} (2.130a)\n= {v \u2208 V |\u2203u \u2208 U : v = x0 + u} \u2286V (2.130b)\nis called affine subspace or linear manifold of V . U is called direction or affine subspace\nlinear"
  },
  {
    "vector_id": 237,
    "chunk_id": "p67_c1",
    "page_number": 67,
    "text": ". Then the subset\nL = x0 + U := {x0 + u : u \u2208 U} (2.130a)\n= {v \u2208 V |\u2203u \u2208 U : v = x0 + u} \u2286V (2.130b)\nis called affine subspace or linear manifold of V . U is called direction or affine subspace\nlinear manifold\ndirection\ndirection space, and x0 is called support point. In Chapter 12, we refer to\ndirection space\nsupport point\nsuch a subspace as a hyperplane.\nhyperplane\nNote that the definition of an affine subspace excludes 0 if x0 /\u2208 U.\nTherefore, an affine subspace is not a (linear) subspace (vector subspace)\nof V for x0 /\u2208 U.\nExamples of affine subspaces are points, lines, and planes in R3, which\ndo not (necessarily) go through the origin.\nRemark. Consider two affine subspaces L = x0 + U and \u02dcL = \u02dcx0 + \u02dcU of a\nvector space V . Then, L \u2286 \u02dcL if and only if U \u2286 \u02dcU and x0 \u2212 \u02dcx0 \u2208 \u02dcU.\nAffine s"
  },
  {
    "vector_id": 238,
    "chunk_id": "p67_c2",
    "page_number": 67,
    "text": "hich\ndo not (necessarily) go through the origin.\nRemark. Consider two affine subspaces L = x0 + U and \u02dcL = \u02dcx0 + \u02dcU of a\nvector space V . Then, L \u2286 \u02dcL if and only if U \u2286 \u02dcU and x0 \u2212 \u02dcx0 \u2208 \u02dcU.\nAffine subspaces are often described byparameters: Consider ak-dimen-\nsional affine space L = x0 + U of V . If (b1, . . . ,bk) is an ordered basis of\nU, then every element x \u2208 L can be uniquely described as\nx = x0 + \u03bb1b1 + . . .+ \u03bbkbk , (2.131)\nwhere \u03bb1, . . . , \u03bbk \u2208 R. This representation is called parametric equation parametric equation\nof L with directional vectors b1, . . . ,bk and parameters \u03bb1, . . . , \u03bbk. \u2662 parameters\nExample 2.26 (Affine Subspaces)\nOne-dimensional affine subspaces are called lines and can be written line\nas y = x0 + \u03bbb1, where \u03bb \u2208 R and U = span[ b1] \u2286 Rn is a one-\ndimensional"
  },
  {
    "vector_id": 239,
    "chunk_id": "p67_c3",
    "page_number": 67,
    "text": ". , \u03bbk. \u2662 parameters\nExample 2.26 (Affine Subspaces)\nOne-dimensional affine subspaces are called lines and can be written line\nas y = x0 + \u03bbb1, where \u03bb \u2208 R and U = span[ b1] \u2286 Rn is a one-\ndimensional subspace of Rn. This means that a line is defined by a sup-\nport point x0 and a vector b1 that defines the direction. See Figure 2.13\nfor an illustration.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 240,
    "chunk_id": "p68_c0",
    "page_number": 68,
    "text": "62 Linear Algebra\nTwo-dimensional affine subspaces of Rn are called planes. The para-plane\nmetric equation for planes is y = x0 + \u03bb1b1 + \u03bb2b2, where \u03bb1, \u03bb2 \u2208 R\nand U = span[ b1, b2] \u2286 Rn. This means that a plane is defined by a\nsupport point x0 and two linearly independent vectors b1, b2 that span\nthe direction space.\nIn Rn, the (n \u2212 1)-dimensional affine subspaces are called hyperplanes,hyperplane\nand the corresponding parametric equation is y = x0 + Pn\u22121\ni=1 \u03bbibi,\nwhere b1, . . . ,bn\u22121 form a basis of an (n \u2212 1)-dimensional subspace\nU of Rn. This means that a hyperplane is defined by a support point\nx0 and (n \u2212 1) linearly independent vectors b1, . . . ,bn\u22121 that span the\ndirection space. In R2, a line is also a hyperplane. In R3, a plane is also\na hyperplane.\nFigure 2.13 Lines\nare affin"
  },
  {
    "vector_id": 241,
    "chunk_id": "p68_c1",
    "page_number": 68,
    "text": "rt point\nx0 and (n \u2212 1) linearly independent vectors b1, . . . ,bn\u22121 that span the\ndirection space. In R2, a line is also a hyperplane. In R3, a plane is also\na hyperplane.\nFigure 2.13 Lines\nare affine subspaces.\nVectors y on a line\nx0 + \u03bbb1 lie in an\naffine subspace L\nwith support point\nx0 and direction b1.\n0\nx0\nb1\ny\nL = x0 + \u03bbb1\nRemark (Inhomogeneous systems of linear equations and affine subspaces).\nFor A \u2208 Rm\u00d7n and x \u2208 Rm, the solution of the system of linear equa-\ntions A\u03bb = x is either the empty set or an affine subspace of Rn of\ndimension n \u2212 rk(A). In particular, the solution of the linear equation\n\u03bb1b1 + . . .+ \u03bbnbn = x, where (\u03bb1, . . . , \u03bbn) \u0338= (0, . . . ,0), is a hyperplane\nin Rn.\nIn Rn, every k-dimensional affine subspace is the solution of an inho-\nmogeneous system of linear"
  },
  {
    "vector_id": 242,
    "chunk_id": "p68_c2",
    "page_number": 68,
    "text": "near equation\n\u03bb1b1 + . . .+ \u03bbnbn = x, where (\u03bb1, . . . , \u03bbn) \u0338= (0, . . . ,0), is a hyperplane\nin Rn.\nIn Rn, every k-dimensional affine subspace is the solution of an inho-\nmogeneous system of linear equations Ax = b, where A \u2208 Rm\u00d7n, b \u2208\nRm and rk(A) = n \u2212 k. Recall that for homogeneous equation systems\nAx = 0 the solution was a vector subspace, which we can also think of\nas a special affine space with support point x0 = 0. \u2662\n2.8.2 Affine Mappings\nSimilar to linear mappings between vector spaces, which we discussed\nin Section 2.7, we can define affine mappings between two affine spaces.\nLinear and affine mappings are closely related. Therefore, many properties\nthat we already know from linear mappings, e.g., that the composition of\nlinear mappings is a linear mapping, also hold for affine"
  },
  {
    "vector_id": 243,
    "chunk_id": "p68_c3",
    "page_number": 68,
    "text": "and affine mappings are closely related. Therefore, many properties\nthat we already know from linear mappings, e.g., that the composition of\nlinear mappings is a linear mapping, also hold for affine mappings.\nDefinition 2.26 (Affine Mapping). For two vector spaces V, W, a linear\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 244,
    "chunk_id": "p69_c0",
    "page_number": 69,
    "text": "2.9 Further Reading 63\nmapping \u03a6 : V \u2192 W, and a \u2208 W, the mapping\n\u03d5 : V \u2192 W (2.132)\nx 7\u2192 a + \u03a6(x) (2.133)\nis an affine mapping from V to W. The vector a is called the translation affine mapping\ntranslation vectorvector of \u03d5.\nEvery affine mapping \u03d5 : V \u2192 W is also the composition of a linear\nmapping \u03a6 : V \u2192 W and a translation \u03c4 : W \u2192 W in W, such that\n\u03d5 = \u03c4 \u25e6 \u03a6. The mappings \u03a6 and \u03c4 are uniquely determined.\nThe composition \u03d5\u2032 \u25e6 \u03d5 of affine mappings \u03d5 : V \u2192 W, \u03d5\u2032 : W \u2192 X is\naffine.\nIf \u03d5 is bijective, affine mappings keep the geometric structure invariant.\nThey then also preserve the dimension and parallelism.\n2.9 Further Reading\nThere are many resources for learning linear algebra, including the text-\nbooks by Strang (2003), Golan (2007), Axler (2015), and Liesen and\nMehrmann (2015). There a"
  },
  {
    "vector_id": 245,
    "chunk_id": "p69_c1",
    "page_number": 69,
    "text": "d parallelism.\n2.9 Further Reading\nThere are many resources for learning linear algebra, including the text-\nbooks by Strang (2003), Golan (2007), Axler (2015), and Liesen and\nMehrmann (2015). There are also several online resources that we men-\ntioned in the introduction to this chapter. We only covered Gaussian elim-\nination here, but there are many other approaches for solving systems of\nlinear equations, and we refer to numerical linear algebra textbooks by\nStoer and Burlirsch (2002), Golub and Van Loan (2012), and Horn and\nJohnson (2013) for an in-depth discussion.\nIn this book, we distinguish between the topics of linear algebra (e.g.,\nvectors, matrices, linear independence, basis) and topics related to the\ngeometry of a vector space. In Chapter 3, we will introduce the inner\nproduct"
  },
  {
    "vector_id": 246,
    "chunk_id": "p69_c2",
    "page_number": 69,
    "text": "guish between the topics of linear algebra (e.g.,\nvectors, matrices, linear independence, basis) and topics related to the\ngeometry of a vector space. In Chapter 3, we will introduce the inner\nproduct, which induces a norm. These concepts allow us to define angles,\nlengths and distances, which we will use for orthogonal projections. Pro-\njections turn out to be key in many machine learning algorithms, such as\nlinear regression and principal component analysis, both of which we will\ncover in Chapters 9 and 10, respectively .\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 247,
    "chunk_id": "p69_c3",
    "page_number": 69,
    "text": "e University Press (2020)."
  },
  {
    "vector_id": 248,
    "chunk_id": "p70_c0",
    "page_number": 70,
    "text": "64 Linear Algebra\nExercises\n2.1 We consider (R\\{\u22121}, \u22c6), where\na \u22c6 b:= ab + a + b, a, b \u2208 R\\{\u22121} (2.134)\na. Show that (R\\{\u22121}, \u22c6) is an Abelian group.\nb. Solve\n3 \u22c6 x \u22c6 x= 15\nin the Abelian group (R\\{\u22121}, \u22c6), where \u22c6 is defined in (2.134).\n2.2 Let n be in N\\{0}. Let k, xbe in Z. We define the congruence class \u00afk of the\ninteger k as the set\nk = {x \u2208 Z | x \u2212 k = 0 (mod n)}\n= {x \u2208 Z | \u2203a \u2208 Z: (x \u2212 k = n \u00b7 a)}.\nWe now define Z/nZ (sometimes written Zn) as the set of all congruence\nclasses modulo n. Euclidean division implies that this set is a finite set con-\ntaining n elements:\nZn = {0, 1, . . . ,n \u2212 1}\nFor all a, b \u2208 Zn, we define\na \u2295 b := a + b\na. Show that (Zn, \u2295) is a group. Is it Abelian?\nb. We now define another operation \u2297 for all a and b in Zn as\na \u2297 b = a \u00d7 b , (2.135)\nwhere a \u00d7 b rep"
  },
  {
    "vector_id": 249,
    "chunk_id": "p70_c1",
    "page_number": 70,
    "text": "n \u2212 1}\nFor all a, b \u2208 Zn, we define\na \u2295 b := a + b\na. Show that (Zn, \u2295) is a group. Is it Abelian?\nb. We now define another operation \u2297 for all a and b in Zn as\na \u2297 b = a \u00d7 b , (2.135)\nwhere a \u00d7 b represents the usual multiplication in Z.\nLet n = 5. Draw the times table of the elements of Z5\\{0} under \u2297, i.e.,\ncalculate the products a \u2297 b for all a and b in Z5\\{0}.\nHence, show that Z5\\{0} is closed under \u2297 and possesses a neutral\nelement for \u2297. Display the inverse of all elements in Z5\\{0} under \u2297.\nConclude that (Z5\\{0}, \u2297) is an Abelian group.\nc. Show that (Z8\\{0}, \u2297) is not a group.\nd. We recall that the B \u00b4ezout theorem states that two integers a and b are\nrelatively prime (i.e., gcd(a, b) = 1) if and only if there exist two integers\nu and v such that au + bv = 1. Show that (Zn\\{0}, \u2297)"
  },
  {
    "vector_id": 250,
    "chunk_id": "p70_c2",
    "page_number": 70,
    "text": "call that the B \u00b4ezout theorem states that two integers a and b are\nrelatively prime (i.e., gcd(a, b) = 1) if and only if there exist two integers\nu and v such that au + bv = 1. Show that (Zn\\{0}, \u2297) is a group if and\nonly if n \u2208 N\\{0} is prime.\n2.3 Consider the set G of 3 \u00d7 3 matrices defined as follows:\nG =\n\uf8f1\n\uf8f2\n\uf8f3\n\uf8ee\n\uf8f0\n1 x z\n0 1 y\n0 0 1\n\uf8f9\n\uf8fb \u2208 R3\u00d73\n\f\f\f\f\f\f\nx, y, z\u2208 R\n\uf8fc\n\uf8fd\n\uf8fe\nWe define \u00b7 as the standard matrix multiplication.\nIs (G, \u00b7) a group? If yes, is it Abelian? Justify your answer.\n2.4 Compute the following matrix products, if possible:\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 251,
    "chunk_id": "p70_c3",
    "page_number": 70,
    "text": ". Feedback:https://mml-book.com."
  },
  {
    "vector_id": 252,
    "chunk_id": "p71_c0",
    "page_number": 71,
    "text": "Exercises 65\na.\n\uf8ee\n\uf8f0\n1 2\n4 5\n7 8\n\uf8f9\n\uf8fb\n\uf8ee\n\uf8f0\n1 1 0\n0 1 1\n1 0 1\n\uf8f9\n\uf8fb\nb.\n\uf8ee\n\uf8f0\n1 2 3\n4 5 6\n7 8 9\n\uf8f9\n\uf8fb\n\uf8ee\n\uf8f0\n1 1 0\n0 1 1\n1 0 1\n\uf8f9\n\uf8fb\nc.\n\uf8ee\n\uf8f0\n1 1 0\n0 1 1\n1 0 1\n\uf8f9\n\uf8fb\n\uf8ee\n\uf8f0\n1 2 3\n4 5 6\n7 8 9\n\uf8f9\n\uf8fb\nd.\n\u0014\n1 2 1 2\n4 1 \u22121 \u22124\n\u0015\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n0 3\n1 \u22121\n2 1\n5 2\n\uf8f9\n\uf8fa\uf8fa\uf8fb\ne.\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n0 3\n1 \u22121\n2 1\n5 2\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u0014\n1 2 1 2\n4 1 \u22121 \u22124\n\u0015\n2.5 Find the set S of all solutions in x of the following inhomogeneous linear\nsystems Ax = b, where A and b are defined as follows:\na.\nA =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 1 \u22121 \u22121\n2 5 \u22127 \u22125\n2 \u22121 1 3\n5 2 \u22124 2\n\uf8f9\n\uf8fa\uf8fa\uf8fb , b =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n\u22122\n4\n6\n\uf8f9\n\uf8fa\uf8fa\uf8fb\nb.\nA =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 \u22121 0 0 1\n1 1 0 \u22123 0\n2 \u22121 0 1 \u22121\n\u22121 2 0 \u22122 \u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fb , b =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n3\n6\n5\n\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n2.6 Using Gaussian elimination, find all solutions of the inhomogeneous equa-\ntion system Ax = b with\nA =\n\uf8ee\n\uf8f0\n0 1 0 0 1 0\n0 0 0 1 1 0\n0 1 0 0 0 1\n\uf8f9\n\uf8fb , b =\n\uf8ee\n\uf8f0\n2\n\u22121\n1\n\uf8f9\n\uf8fb .\n\u00a92024 M. P. Deisenroth, A. A. Faisal,"
  },
  {
    "vector_id": 253,
    "chunk_id": "p71_c1",
    "page_number": 71,
    "text": "Gaussian elimination, find all solutions of the inhomogeneous equa-\ntion system Ax = b with\nA =\n\uf8ee\n\uf8f0\n0 1 0 0 1 0\n0 0 0 1 1 0\n0 1 0 0 0 1\n\uf8f9\n\uf8fb , b =\n\uf8ee\n\uf8f0\n2\n\u22121\n1\n\uf8f9\n\uf8fb .\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 254,
    "chunk_id": "p72_c0",
    "page_number": 72,
    "text": "66 Linear Algebra\n2.7 Find all solutions in x =\n\uf8ee\n\uf8f0\nx1\nx2\nx3\n\uf8f9\n\uf8fb \u2208 R3 of the equation system Ax = 12 x,\nwhere\nA =\n\uf8ee\n\uf8f0\n6 4 3\n6 0 9\n0 8 0\n\uf8f9\n\uf8fb\nand P3\ni=1 xi = 1.\n2.8 Determine the inverses of the following matrices if possible:\na.\nA =\n\uf8ee\n\uf8f0\n2 3 4\n3 4 5\n4 5 6\n\uf8f9\n\uf8fb\nb.\nA =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 0 1 0\n0 1 1 0\n1 1 0 1\n1 1 1 0\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n2.9 Which of the following sets are subspaces of R3?\na. A = {(\u03bb, \u03bb+ \u00b53, \u03bb\u2212 \u00b53) | \u03bb, \u00b5\u2208 R}\nb. B = {(\u03bb2, \u2212\u03bb2, 0) | \u03bb \u2208 R}\nc. Let \u03b3 be in R.\nC = {(\u03be1, \u03be2, \u03be3) \u2208 R3 | \u03be1 \u2212 2\u03be2 + 3\u03be3 = \u03b3}\nd. D = {(\u03be1, \u03be2, \u03be3) \u2208 R3 | \u03be2 \u2208 Z}\n2.10 Are the following sets of vectors linearly independent?\na.\nx1 =\n\uf8ee\n\uf8f0\n2\n\u22121\n3\n\uf8f9\n\uf8fb , x2 =\n\uf8ee\n\uf8f0\n1\n1\n\u22122\n\uf8f9\n\uf8fb , x3 =\n\uf8ee\n\uf8f0\n3\n\u22123\n8\n\uf8f9\n\uf8fb\nb.\nx1 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n2\n1\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, x2 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n1\n0\n1\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, x3 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n0\n0\n1\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n2.11 Write\ny =\n\uf8ee\n\uf8f0\n1\n\u22122\n5\n\uf8f9\n\uf8fb\nas linear comb"
  },
  {
    "vector_id": 255,
    "chunk_id": "p72_c1",
    "page_number": 72,
    "text": "2\n\u22121\n3\n\uf8f9\n\uf8fb , x2 =\n\uf8ee\n\uf8f0\n1\n1\n\u22122\n\uf8f9\n\uf8fb , x3 =\n\uf8ee\n\uf8f0\n3\n\u22123\n8\n\uf8f9\n\uf8fb\nb.\nx1 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n2\n1\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, x2 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n1\n0\n1\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, x3 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n0\n0\n1\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n2.11 Write\ny =\n\uf8ee\n\uf8f0\n1\n\u22122\n5\n\uf8f9\n\uf8fb\nas linear combination of\nx1 =\n\uf8ee\n\uf8f0\n1\n1\n1\n\uf8f9\n\uf8fb , x2 =\n\uf8ee\n\uf8f0\n1\n2\n3\n\uf8f9\n\uf8fb , x3 =\n\uf8ee\n\uf8f0\n2\n\u22121\n1\n\uf8f9\n\uf8fb\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 256,
    "chunk_id": "p73_c0",
    "page_number": 73,
    "text": "Exercises 67\n2.12 Consider two subspaces of R4:\nU1 = span[\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n1\n\u22123\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fb ,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n2\n\u22121\n0\n\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fb ,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22121\n1\n\u22121\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fb] , U 2 = span[\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22121\n\u22122\n2\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fb ,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n2\n\u22122\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fb ,\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22123\n6\n\u22122\n\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fb] .\nDetermine a basis of U1 \u2229 U2.\n2.13 Consider two subspaces U1 and U2, where U1 is the solution space of the\nhomogeneous equation system A1x = 0 and U2 is the solution space of the\nhomogeneous equation system A2x = 0 with\nA1 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 0 1\n1 \u22122 \u22121\n2 1 3\n1 0 1\n\uf8f9\n\uf8fa\uf8fa\uf8fb , A2 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n3 \u22123 0\n1 2 3\n7 \u22125 2\n3 \u22121 2\n\uf8f9\n\uf8fa\uf8fa\uf8fb .\na. Determine the dimension of U1, U2.\nb. Determine bases of U1 and U2.\nc. Determine a basis of U1 \u2229 U2.\n2.14 Consider two subspaces U1 and U2, where U1 is spanned by the columns of\nA1 and U2 is spanned by the columns of A2 with\nA1 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 0 1\n1 \u22122 \u22121\n2 1 3\n1 0 1\n\uf8f9\n\uf8fa\uf8fa\uf8fb ,"
  },
  {
    "vector_id": 257,
    "chunk_id": "p73_c1",
    "page_number": 73,
    "text": "Determine a basis of U1 \u2229 U2.\n2.14 Consider two subspaces U1 and U2, where U1 is spanned by the columns of\nA1 and U2 is spanned by the columns of A2 with\nA1 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 0 1\n1 \u22122 \u22121\n2 1 3\n1 0 1\n\uf8f9\n\uf8fa\uf8fa\uf8fb , A2 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n3 \u22123 0\n1 2 3\n7 \u22125 2\n3 \u22121 2\n\uf8f9\n\uf8fa\uf8fa\uf8fb .\na. Determine the dimension of U1, U2\nb. Determine bases of U1 and U2\nc. Determine a basis of U1 \u2229 U2\n2.15 Let F = {(x, y, z) \u2208 R3 | x+y\u2212z = 0} and G = {(a\u2212b, a+b, a\u22123b) | a, b\u2208 R}.\na. Show that F and G are subspaces of R3.\nb. Calculate F \u2229 G without resorting to any basis vector.\nc. Find one basis for F and one forG, calculate F\u2229G using the basis vectors\npreviously found and check your result with the previous question.\n2.16 Are the following mappings linear?\na. Let a, b\u2208 R.\n\u03a6 : L1([a, b]) \u2192 R\nf 7\u2192 \u03a6(f) =\nZ b\na\nf(x)dx ,\nwhere L1([a, b]) denotes the"
  },
  {
    "vector_id": 258,
    "chunk_id": "p73_c2",
    "page_number": 73,
    "text": "rs\npreviously found and check your result with the previous question.\n2.16 Are the following mappings linear?\na. Let a, b\u2208 R.\n\u03a6 : L1([a, b]) \u2192 R\nf 7\u2192 \u03a6(f) =\nZ b\na\nf(x)dx ,\nwhere L1([a, b]) denotes the set of integrable functions on [a, b].\nb.\n\u03a6 : C1 \u2192 C0\nf 7\u2192 \u03a6(f) = f\u2032 ,\nwhere for k \u2a7e 1, Ck denotes the set of k times continuously differen-\ntiable functions, and C0 denotes the set of continuous functions.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 259,
    "chunk_id": "p74_c0",
    "page_number": 74,
    "text": "68 Linear Algebra\nc.\n\u03a6 : R \u2192 R\nx 7\u2192 \u03a6(x) = cos(x)\nd.\n\u03a6 : R3 \u2192 R2\nx 7\u2192\n\u0014\n1 2 3\n1 4 3\n\u0015\nx\ne. Let \u03b8 be in [0, 2\u03c0[ and\n\u03a6 : R2 \u2192 R2\nx 7\u2192\n\u0014\ncos(\u03b8) sin( \u03b8)\n\u2212sin(\u03b8) cos( \u03b8)\n\u0015\nx\n2.17 Consider the linear mapping\n\u03a6 : R3 \u2192 R4\n\u03a6\n\uf8eb\n\uf8ed\n\uf8ee\n\uf8f0\nx1\nx2\nx3\n\uf8f9\n\uf8fb\n\uf8f6\n\uf8f8 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n3x1 + 2x2 + x3\nx1 + x2 + x3\nx1 \u2212 3x2\n2x1 + 3x2 + x3\n\uf8f9\n\uf8fa\uf8fa\uf8fb\nFind the transformation matrix A\u03a6.\nDetermine rk(A\u03a6).\nCompute the kernel and image of\u03a6. What aredim(ker(\u03a6)) and dim(Im(\u03a6))?\n2.18 Let E be a vector space. Let f and g be two automorphisms on E such that\nf \u25e6 g = id E (i.e., f \u25e6 g is the identity mapping idE). Show that ker(f) =\nker(g \u25e6 f), Im(g) = Im(g \u25e6 f) and that ker(f) \u2229 Im(g) = {0E}.\n2.19 Consider an endomorphism \u03a6 : R3 \u2192 R3 whose transformation matrix\n(with respect to the standard basis in R3) is\nA\u03a6 =\n\uf8ee\n\uf8f0\n1 1 0\n1 \u22121 0\n1 1 1\n\uf8f9\n\uf8fb .\na. De"
  },
  {
    "vector_id": 260,
    "chunk_id": "p74_c1",
    "page_number": 74,
    "text": "= Im(g \u25e6 f) and that ker(f) \u2229 Im(g) = {0E}.\n2.19 Consider an endomorphism \u03a6 : R3 \u2192 R3 whose transformation matrix\n(with respect to the standard basis in R3) is\nA\u03a6 =\n\uf8ee\n\uf8f0\n1 1 0\n1 \u22121 0\n1 1 1\n\uf8f9\n\uf8fb .\na. Determine ker(\u03a6) and Im(\u03a6).\nb. Determine the transformation matrix \u02dcA\u03a6 with respect to the basis\nB = (\n\uf8ee\n\uf8f0\n1\n1\n1\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n1\n2\n1\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n1\n0\n0\n\uf8f9\n\uf8fb) ,\ni.e., perform a basis change toward the new basis B.\n2.20 Let us consider b1, b2, b\u2032\n1, b\u2032\n2, 4 vectors of R2 expressed in the standard basis\nof R2 as\nb1 =\n\u0014\n2\n1\n\u0015\n, b2 =\n\u0014\n\u22121\n\u22121\n\u0015\n, b\u2032\n1 =\n\u0014\n2\n\u22122\n\u0015\n, b\u2032\n2 =\n\u0014\n1\n1\n\u0015\nand let us define two ordered bases B = (b1, b2) and B\u2032 = (b\u2032\n1, b\u2032\n2) of R2.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 261,
    "chunk_id": "p74_c2",
    "page_number": 74,
    "text": "B = (b1, b2) and B\u2032 = (b\u2032\n1, b\u2032\n2) of R2.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 262,
    "chunk_id": "p75_c0",
    "page_number": 75,
    "text": "Exercises 69\na. Show that B and B\u2032 are two bases of R2 and draw those basis vectors.\nb. Compute the matrix P1 that performs a basis change from B\u2032 to B.\nc. We consider c1, c2, c3, three vectors of R3 defined in the standard basis\nof R3 as\nc1 =\n\uf8ee\n\uf8f0\n1\n2\n\u22121\n\uf8f9\n\uf8fb, c2 =\n\uf8ee\n\uf8f0\n0\n\u22121\n2\n\uf8f9\n\uf8fb, c3 =\n\uf8ee\n\uf8f0\n1\n0\n\u22121\n\uf8f9\n\uf8fb\nand we define C = (c1, c2, c3).\n(i) Show that C is a basis of R3, e.g., by using determinants (see\nSection 4.1).\n(ii) Let us call C\u2032 = (c\u2032\n1, c\u2032\n2, c\u2032\n3) the standard basis of R3. Determine\nthe matrix P2 that performs the basis change from C to C\u2032.\nd. We consider a homomorphism \u03a6 : R2 \u2212\u2192R3, such that\n\u03a6(b1 + b2) = c2 + c3\n\u03a6(b1 \u2212 b2) = 2 c1 \u2212 c2 + 3c3\nwhere B = (b1, b2) and C = (c1, c2, c3) are ordered bases of R2 and R3,\nrespectively .\nDetermine the transformation matrix A\u03a6 of \u03a6 with respect to"
  },
  {
    "vector_id": 263,
    "chunk_id": "p75_c1",
    "page_number": 75,
    "text": "at\n\u03a6(b1 + b2) = c2 + c3\n\u03a6(b1 \u2212 b2) = 2 c1 \u2212 c2 + 3c3\nwhere B = (b1, b2) and C = (c1, c2, c3) are ordered bases of R2 and R3,\nrespectively .\nDetermine the transformation matrix A\u03a6 of \u03a6 with respect to the or-\ndered bases B and C.\ne. Determine A\u2032, the transformation matrix of \u03a6 with respect to the bases\nB\u2032 and C\u2032.\nf. Let us consider the vector x \u2208 R2 whose coordinates in B\u2032 are [2, 3]\u22a4.\nIn other words, x = 2b\u2032\n1 + 3b\u2032\n2.\n(i) Calculate the coordinates of x in B.\n(ii) Based on that, compute the coordinates of \u03a6(x) expressed in C.\n(iii) Then, write \u03a6(x) in terms of c\u2032\n1, c\u2032\n2, c\u2032\n3.\n(iv) Use the representation of x in B\u2032 and the matrix A\u2032 to find this\nresult directly .\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 264,
    "chunk_id": "p75_c2",
    "page_number": 75,
    "text": "presentation of x in B\u2032 and the matrix A\u2032 to find this\nresult directly .\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 265,
    "chunk_id": "p76_c0",
    "page_number": 76,
    "text": "3\nAnalytic Geometry\nIn Chapter 2, we studied vectors, vector spaces, and linear mappings at\na general but abstract level. In this chapter, we will add some geomet-\nric interpretation and intuition to all of these concepts. In particular, we\nwill look at geometric vectors and compute their lengths and distances\nor angles between two vectors. To be able to do this, we equip the vec-\ntor space with an inner product that induces the geometry of the vector\nspace. Inner products and their corresponding norms and metrics capture\nthe intuitive notions of similarity and distances, which we use to develop\nthe support vector machine in Chapter 12. We will then use the concepts\nof lengths and angles between vectors to discuss orthogonal projections,\nwhich will play a central role when we discuss princ"
  },
  {
    "vector_id": 266,
    "chunk_id": "p76_c1",
    "page_number": 76,
    "text": "op\nthe support vector machine in Chapter 12. We will then use the concepts\nof lengths and angles between vectors to discuss orthogonal projections,\nwhich will play a central role when we discuss principal component anal-\nysis in Chapter 10 and regression via maximum likelihood estimation in\nChapter 9. Figure 3.1 gives an overview of how concepts in this chapter\nare related and how they are connected to other chapters of the book.\nFigure 3.1 A mind\nmap of the concepts\nintroduced in this\nchapter, along with\nwhen they are used\nin other parts of the\nbook.\nInner product\nNorm\nLengths Orthogonalprojection Angles Rotations\nChapter 4Matrixdecomposition\nChapter 10Dimensionalityreduction\nChapter 9Regression\nChapter 12Classification\ninduces\n70\nThis material is published by Cambridge University Press a"
  },
  {
    "vector_id": 267,
    "chunk_id": "p76_c2",
    "page_number": 76,
    "text": "ction Angles Rotations\nChapter 4Matrixdecomposition\nChapter 10Dimensionalityreduction\nChapter 9Regression\nChapter 12Classification\ninduces\n70\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 268,
    "chunk_id": "p77_c0",
    "page_number": 77,
    "text": "3.1 Norms 71\nFigure 3.3 For\ndifferent norms, the\nred lines indicate\nthe set of vectors\nwith norm 1. Left:\nManhattan norm;\nRight: Euclidean\ndistance.\n1\n1\n1\n1\n\u2225x\u22251 = 1\n\u2225x\u22252 = 1\n3.1 Norms\nWhen we think of geometric vectors, i.e., directed line segments that start\nat the origin, then intuitively the length of a vector is the distance of the\n\u201cend\u201d of this directed line segment from the origin. In the following, we\nwill discuss the notion of the length of vectors using the concept of a norm.\nDefinition 3.1 (Norm). A norm on a vector space V is a function norm\n\u2225 \u00b7 \u2225: V \u2192 R, (3.1)\nx 7\u2192 \u2225x\u2225, (3.2)\nwhich assigns each vector x its length \u2225x\u2225 \u2208R, such that for all \u03bb \u2208 R length\nand x, y \u2208 V the following hold:\nabsolutely\nhomogeneousAbsolutely homogeneous: \u2225\u03bbx\u2225 = |\u03bb|\u2225x\u2225\ntriangle inequalityTriangle inequ"
  },
  {
    "vector_id": 269,
    "chunk_id": "p77_c1",
    "page_number": 77,
    "text": "h assigns each vector x its length \u2225x\u2225 \u2208R, such that for all \u03bb \u2208 R length\nand x, y \u2208 V the following hold:\nabsolutely\nhomogeneousAbsolutely homogeneous: \u2225\u03bbx\u2225 = |\u03bb|\u2225x\u2225\ntriangle inequalityTriangle inequality: \u2225x + y\u2225 \u2a7d \u2225x\u2225 + \u2225y\u2225\npositive definitePositive definite: \u2225x\u2225 \u2a7e 0 and \u2225x\u2225 = 0 \u21d0 \u21d2x = 0\nFigure 3.2 Triangle\ninequality .\na\nb\nc \u2264 a + bIn geometric terms, the triangle inequality states that for any triangle,\nthe sum of the lengths of any two sides must be greater than or equal\nto the length of the remaining side; see Figure 3.2 for an illustration.\nDefinition 3.1 is in terms of a general vector space V (Section 2.4), but\nin this book we will only consider a finite-dimensional vector space Rn.\nRecall that for a vectorx \u2208 Rn we denote the elements of the vector using\na subscript, that is, xi"
  },
  {
    "vector_id": 270,
    "chunk_id": "p77_c2",
    "page_number": 77,
    "text": "r space V (Section 2.4), but\nin this book we will only consider a finite-dimensional vector space Rn.\nRecall that for a vectorx \u2208 Rn we denote the elements of the vector using\na subscript, that is, xi is the ith element of the vector x.\nExample 3.1 (Manhattan Norm)\nThe Manhattan norm on Rn is defined for x \u2208 Rn as Manhattan norm\n\u2225x\u22251 :=\nnX\ni=1\n|xi|, (3.3)\nwhere | \u00b7 |is the absolute value. The left panel of Figure 3.3 shows all\nvectors x \u2208 R2 with \u2225x\u22251 = 1 . The Manhattan norm is also called \u21131 \u21131 norm\nnorm.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 271,
    "chunk_id": "p77_c3",
    "page_number": 77,
    "text": "s (2020)."
  },
  {
    "vector_id": 272,
    "chunk_id": "p78_c0",
    "page_number": 78,
    "text": "72 Analytic Geometry\nExample 3.2 (Euclidean Norm)\nThe Euclidean norm of x \u2208 Rn is defined asEuclidean norm\n\u2225x\u22252 :=\nvuut\nnX\ni=1\nx2\ni =\n\u221a\nx\u22a4x (3.4)\nand computes the Euclidean distance of x from the origin. The right panelEuclidean distance\nof Figure 3.3 shows all vectors x \u2208 R2 with \u2225x\u22252 = 1 . The Euclidean\nnorm is also called \u21132 norm.\u21132 norm\nRemark. Throughout this book, we will use the Euclidean norm (3.4) by\ndefault if not stated otherwise. \u2662\n3.2 Inner Products\nInner products allow for the introduction of intuitive geometrical con-\ncepts, such as the length of a vector and the angle or distance between\ntwo vectors. A major purpose of inner products is to determine whether\nvectors are orthogonal to each other.\n3.2.1 Dot Product\nWe may already be familiar with a particular type of inner pro"
  },
  {
    "vector_id": 273,
    "chunk_id": "p78_c1",
    "page_number": 78,
    "text": "ce between\ntwo vectors. A major purpose of inner products is to determine whether\nvectors are orthogonal to each other.\n3.2.1 Dot Product\nWe may already be familiar with a particular type of inner product, the\nscalar product/dot product in Rn, which is given byscalar product\ndot product\nx\u22a4y =\nnX\ni=1\nxiyi . (3.5)\nWe will refer to this particular inner product as the dot product in this\nbook. However, inner products are more general concepts with specific\nproperties, which we will now introduce.\n3.2.2 General Inner Products\nRecall the linear mapping from Section 2.7, where we can rearrange the\nmapping with respect to addition and multiplication with a scalar. A bi-bilinear mapping\nlinear mapping \u2126 is a mapping with two arguments, and it is linear in\neach argument, i.e., when we look at a vec"
  },
  {
    "vector_id": 274,
    "chunk_id": "p78_c2",
    "page_number": 78,
    "text": "apping with respect to addition and multiplication with a scalar. A bi-bilinear mapping\nlinear mapping \u2126 is a mapping with two arguments, and it is linear in\neach argument, i.e., when we look at a vector space V then it holds that\nfor all x, y, z \u2208 V, \u03bb, \u03c8\u2208 R that\n\u2126(\u03bbx + \u03c8y, z) = \u03bb\u2126(x, z) + \u03c8\u2126(y, z) (3.6)\n\u2126(x, \u03bby + \u03c8z) = \u03bb\u2126(x, y) + \u03c8\u2126(x, z) . (3.7)\nHere, (3.6) asserts that \u2126 is linear in the first argument, and (3.7) asserts\nthat \u2126 is linear in the second argument (see also (2.87)).\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 275,
    "chunk_id": "p79_c0",
    "page_number": 79,
    "text": "3.2 Inner Products 73\nDefinition 3.2. Let V be a vector space and \u2126 : V \u00d7 V \u2192 R be a bilinear\nmapping that takes two vectors and maps them onto a real number. Then\n\u2126 is called symmetric if \u2126(x, y) = \u2126( y, x) for all x, y \u2208 V , i.e., the symmetric\norder of the arguments does not matter.\n\u2126 is called positive definite if positive definite\n\u2200x \u2208 V \\{0} : \u2126(x, x) > 0 , \u2126(0, 0) = 0 . (3.8)\nDefinition 3.3. Let V be a vector space and \u2126 : V \u00d7 V \u2192 R be a bilinear\nmapping that takes two vectors and maps them onto a real number. Then\nA positive definite, symmetric bilinear mapping\u2126 : V \u00d7V \u2192 R is called\nan inner product on V . We typically write \u27e8x, y\u27e9 instead of \u2126(x, y). inner product\nThe pair (V, \u27e8\u00b7, \u00b7\u27e9) is called an inner product space or (real) vector space inner product space\nvector space with\ninn"
  },
  {
    "vector_id": 276,
    "chunk_id": "p79_c1",
    "page_number": 79,
    "text": "inner product on V . We typically write \u27e8x, y\u27e9 instead of \u2126(x, y). inner product\nThe pair (V, \u27e8\u00b7, \u00b7\u27e9) is called an inner product space or (real) vector space inner product space\nvector space with\ninner product\nwith inner product. If we use the dot product defined in (3.5), we call\n(V, \u27e8\u00b7, \u00b7\u27e9) a Euclidean vector space.\nEuclidean vector\nspaceWe will refer to these spaces as inner product spaces in this book.\nExample 3.3 (Inner Product That Is Not the Dot Product)\nConsider V = R2. If we define\n\u27e8x, y\u27e9 := x1y1 \u2212 (x1y2 + x2y1) + 2x2y2 (3.9)\nthen \u27e8\u00b7, \u00b7\u27e9 is an inner product but different from the dot product. The proof\nwill be an exercise.\n3.2.3 Symmetric, Positive Definite Matrices\nSymmetric, positive definite matrices play an important role in machine\nlearning, and they are defined via the inne"
  },
  {
    "vector_id": 277,
    "chunk_id": "p79_c2",
    "page_number": 79,
    "text": "product. The proof\nwill be an exercise.\n3.2.3 Symmetric, Positive Definite Matrices\nSymmetric, positive definite matrices play an important role in machine\nlearning, and they are defined via the inner product. In Section 4.3, we\nwill return to symmetric, positive definite matrices in the context of matrix\ndecompositions. The idea of symmetric positive semidefinite matrices is\nkey in the definition of kernels (Section 12.4).\nConsider an n-dimensional vector space V with an inner product \u27e8\u00b7, \u00b7\u27e9 :\nV \u00d7V \u2192 R (see Definition 3.3) and an ordered basis B = (b1, . . . ,bn) of\nV . Recall from Section 2.6.1 that any vectors x, y \u2208 V can be written as\nlinear combinations of the basis vectors so that x = Pn\ni=1 \u03c8ibi \u2208 V and\ny = Pn\nj=1 \u03bbjbj \u2208 V for suitable \u03c8i, \u03bbj \u2208 R. Due to the bilinearity of the\ninn"
  },
  {
    "vector_id": 278,
    "chunk_id": "p79_c3",
    "page_number": 79,
    "text": ".6.1 that any vectors x, y \u2208 V can be written as\nlinear combinations of the basis vectors so that x = Pn\ni=1 \u03c8ibi \u2208 V and\ny = Pn\nj=1 \u03bbjbj \u2208 V for suitable \u03c8i, \u03bbj \u2208 R. Due to the bilinearity of the\ninner product, it holds for all x, y \u2208 V that\n\u27e8x, y\u27e9 =\n* nX\ni=1\n\u03c8ibi,\nnX\nj=1\n\u03bbjbj\n+\n=\nnX\ni=1\nnX\nj=1\n\u03c8i \u27e8bi, bj\u27e9\u03bbj = \u02c6x\u22a4A\u02c6y , (3.10)\nwhere Aij := \u27e8bi, bj\u27e9 and \u02c6x, \u02c6y are the coordinates of x and y with respect\nto the basis B. This implies that the inner product \u27e8\u00b7, \u00b7\u27e9 is uniquely deter-\nmined through A. The symmetry of the inner product also means that A\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 279,
    "chunk_id": "p79_c4",
    "page_number": 79,
    "text": ". Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 280,
    "chunk_id": "p80_c0",
    "page_number": 80,
    "text": "74 Analytic Geometry\nis symmetric. Furthermore, the positive definiteness of the inner product\nimplies that\n\u2200x \u2208 V \\{0} : x\u22a4Ax > 0 . (3.11)\nDefinition 3.4 (Symmetric, Positive Definite Matrix). A symmetric matrix\nA \u2208 Rn\u00d7n that satisfies (3.11) is called symmetric, positive definite , orsymmetric, positive\ndefinite just positive definite. If only \u2a7e holds in (3.11), then A is called symmetric,\npositive definite\nsymmetric, positive\nsemidefinite\npositive semidefinite.\nExample 3.4 (Symmetric, Positive Definite Matrices)\nConsider the matrices\nA1 =\n\u00149 6\n6 5\n\u0015\n, A2 =\n\u00149 6\n6 3\n\u0015\n. (3.12)\nA1 is positive definite because it is symmetric and\nx\u22a4A1x =\n\u0002\nx1 x2\n\u0003\u00149 6\n6 5\n\u0015\u0014x1\nx2\n\u0015\n(3.13a)\n= 9x2\n1 + 12x1x2 + 5x2\n2 = (3x1 + 2x2)2 + x2\n2 > 0 (3.13b)\nfor all x \u2208 V \\{0}. In contrast, A2 is symmetric but not po"
  },
  {
    "vector_id": 281,
    "chunk_id": "p80_c1",
    "page_number": 80,
    "text": "definite because it is symmetric and\nx\u22a4A1x =\n\u0002\nx1 x2\n\u0003\u00149 6\n6 5\n\u0015\u0014x1\nx2\n\u0015\n(3.13a)\n= 9x2\n1 + 12x1x2 + 5x2\n2 = (3x1 + 2x2)2 + x2\n2 > 0 (3.13b)\nfor all x \u2208 V \\{0}. In contrast, A2 is symmetric but not positive definite\nbecause x\u22a4A2x = 9x2\n1 + 12x1x2 + 3x2\n2 = (3x1 + 2x2)2 \u2212 x2\n2 can be less\nthan 0, e.g., for x = [2, \u22123]\u22a4.\nIf A \u2208 Rn\u00d7n is symmetric, positive definite, then\n\u27e8x, y\u27e9 = \u02c6x\u22a4A\u02c6y (3.14)\ndefines an inner product with respect to an ordered basis B, where \u02c6x and\n\u02c6y are the coordinate representations of x, y \u2208 V with respect to B.\nTheorem 3.5. For a real-valued, finite-dimensional vector space V and an\nordered basis B of V , it holds that \u27e8\u00b7, \u00b7\u27e9 : V \u00d7 V \u2192 R is an inner product if\nand only if there exists a symmetric, positive definite matrix A \u2208 Rn\u00d7n with\n\u27e8x, y\u27e9 = \u02c6x\u22a4A\u02c6y . (3.15)\nThe follo"
  },
  {
    "vector_id": 282,
    "chunk_id": "p80_c2",
    "page_number": 80,
    "text": "V and an\nordered basis B of V , it holds that \u27e8\u00b7, \u00b7\u27e9 : V \u00d7 V \u2192 R is an inner product if\nand only if there exists a symmetric, positive definite matrix A \u2208 Rn\u00d7n with\n\u27e8x, y\u27e9 = \u02c6x\u22a4A\u02c6y . (3.15)\nThe following properties hold if A \u2208 Rn\u00d7n is symmetric and positive\ndefinite:\nThe null space (kernel) of A consists only of 0 because x\u22a4Ax > 0 for\nall x \u0338= 0. This implies that Ax \u0338= 0 if x \u0338= 0.\nThe diagonal elements aii of A are positive because aii = e\u22a4\ni Aei > 0,\nwhere ei is the ith vector of the standard basis in Rn.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 283,
    "chunk_id": "p80_c3",
    "page_number": 80,
    "text": "om."
  },
  {
    "vector_id": 284,
    "chunk_id": "p81_c0",
    "page_number": 81,
    "text": "3.3 Lengths and Distances 75\n3.3 Lengths and Distances\nIn Section 3.1, we already discussed norms that we can use to compute\nthe length of a vector. Inner products and norms are closely related in the\nsense that any inner product induces a norm Inner products\ninduce norms.\n\u2225x\u2225 :=\nq\n\u27e8x, x\u27e9 (3.16)\nin a natural way , such that we can compute lengths of vectors using the in-\nner product. However, not every norm is induced by an inner product. The\nManhattan norm (3.3) is an example of a norm without a corresponding\ninner product. In the following, we will focus on norms that are induced\nby inner products and introduce geometric concepts, such as lengths, dis-\ntances, and angles.\nRemark (Cauchy-Schwarz Inequality). For an inner product vector space\n(V, \u27e8\u00b7, \u00b7\u27e9) the induced norm \u2225 \u00b7 \u2225satisfies the"
  },
  {
    "vector_id": 285,
    "chunk_id": "p81_c1",
    "page_number": 81,
    "text": "oducts and introduce geometric concepts, such as lengths, dis-\ntances, and angles.\nRemark (Cauchy-Schwarz Inequality). For an inner product vector space\n(V, \u27e8\u00b7, \u00b7\u27e9) the induced norm \u2225 \u00b7 \u2225satisfies the Cauchy-Schwarz inequality Cauchy-Schwarz\ninequality\n| \u27e8x, y\u27e9 |\u2a7d \u2225x\u2225\u2225y\u2225. (3.17)\n\u2662\nExample 3.5 (Lengths of Vectors Using Inner Products)\nIn geometry , we are often interested in lengths of vectors. We can now use\nan inner product to compute them using (3.16). Let us take x = [1, 1]\u22a4 \u2208\nR2. If we use the dot product as the inner product, with (3.16) we obtain\n\u2225x\u2225 =\n\u221a\nx\u22a4x =\n\u221a\n12 + 12 =\n\u221a\n2 (3.18)\nas the length of x. Let us now choose a different inner product:\n\u27e8x, y\u27e9 := x\u22a4\n\u0014 1 \u22121\n2\n\u22121\n2 1\n\u0015\ny = x1y1 \u2212 1\n2(x1y2 + x2y1) + x2y2 . (3.19)\nIf we compute the norm of a vector, then this inner product retu"
  },
  {
    "vector_id": 286,
    "chunk_id": "p81_c2",
    "page_number": 81,
    "text": "he length of x. Let us now choose a different inner product:\n\u27e8x, y\u27e9 := x\u22a4\n\u0014 1 \u22121\n2\n\u22121\n2 1\n\u0015\ny = x1y1 \u2212 1\n2(x1y2 + x2y1) + x2y2 . (3.19)\nIf we compute the norm of a vector, then this inner product returns smaller\nvalues than the dot product if x1 and x2 have the same sign (and x1x2 >\n0); otherwise, it returns greater values than the dot product. With this\ninner product, we obtain\n\u27e8x, x\u27e9 = x2\n1 \u2212 x1x2 + x2\n2 = 1 \u2212 1 + 1 = 1 =\u21d2 \u2225x\u2225 =\n\u221a\n1 = 1 , (3.20)\nsuch that x is \u201cshorter\u201d with this inner product than with the dot product.\nDefinition 3.6 (Distance and Metric) . Consider an inner product space\n(V, \u27e8\u00b7, \u00b7\u27e9). Then\nd(x, y) := \u2225x \u2212 y\u2225 =\nq\n\u27e8x \u2212 y, x \u2212 y\u27e9 (3.21)\nis called the distance between x and y for x, y \u2208 V . If we use the dot distance\nproduct as the inner product, then the distance is called"
  },
  {
    "vector_id": 287,
    "chunk_id": "p81_c3",
    "page_number": 81,
    "text": "V, \u27e8\u00b7, \u00b7\u27e9). Then\nd(x, y) := \u2225x \u2212 y\u2225 =\nq\n\u27e8x \u2212 y, x \u2212 y\u27e9 (3.21)\nis called the distance between x and y for x, y \u2208 V . If we use the dot distance\nproduct as the inner product, then the distance is calledEuclidean distance. Euclidean distance\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 288,
    "chunk_id": "p82_c0",
    "page_number": 82,
    "text": "76 Analytic Geometry\nThe mapping\nd : V \u00d7 V \u2192 R (3.22)\n(x, y) 7\u2192 d(x, y) (3.23)\nis called a metric.metric\nRemark. Similar to the length of a vector, the distance between vectors\ndoes not require an inner product: a norm is sufficient. If we have a norm\ninduced by an inner product, the distance may vary depending on the\nchoice of the inner product. \u2662\nA metric d satisfies the following:\n1. d is positive definite, i.e., d(x, y) \u2a7e 0 for all x, y \u2208 V and d(x, y) =positive definite\n0 \u21d0 \u21d2x = y .\n2. d is symmetric, i.e., d(x, y) = d(y, x) for all x, y \u2208 V .symmetric\ntriangle inequality 3. Triangle inequality: d(x, z) \u2a7d d(x, y) + d(y, z) for all x, y, z \u2208 V .\nRemark. At first glance, the lists of properties of inner products and met-\nrics look very similar. However, by comparing Definition 3.3 with"
  },
  {
    "vector_id": 289,
    "chunk_id": "p82_c1",
    "page_number": 82,
    "text": "uality: d(x, z) \u2a7d d(x, y) + d(y, z) for all x, y, z \u2208 V .\nRemark. At first glance, the lists of properties of inner products and met-\nrics look very similar. However, by comparing Definition 3.3 with Defini-\ntion 3.6 we observe that \u27e8x, y\u27e9 and d(x, y) behave in opposite directions.\nVery similar x and y will result in a large value for the inner product and\na small value for the metric. \u2662\n3.4 Angles and Orthogonality\nFigure 3.4 When\nrestricted to [0, \u03c0]\nthen f(\u03c9) = cos(\u03c9)\nreturns a unique\nnumber in the\ninterval [\u22121, 1].\n0 \u03c0/2 \u03c0\n\u03c9\n\u22121\n0\n1\ncos(\u03c9)\nIn addition to enabling the definition of lengths of vectors, as well as the\ndistance between two vectors, inner products also capture the geometry\nof a vector space by defining the angle \u03c9 between two vectors. We use\nthe Cauchy-Schwarz inequality (3."
  },
  {
    "vector_id": 290,
    "chunk_id": "p82_c2",
    "page_number": 82,
    "text": "f vectors, as well as the\ndistance between two vectors, inner products also capture the geometry\nof a vector space by defining the angle \u03c9 between two vectors. We use\nthe Cauchy-Schwarz inequality (3.17) to define angles \u03c9 in inner prod-\nuct spaces between two vectors x, y, and this notion coincides with our\nintuition in R2 and R3. Assume that x \u0338= 0, y \u0338= 0. Then\n\u22121 \u2a7d \u27e8x, y\u27e9\n\u2225x\u2225 \u2225y\u2225 \u2a7d 1 . (3.24)\nTherefore, there exists a unique \u03c9 \u2208 [0, \u03c0], illustrated in Figure 3.4, with\ncos \u03c9 = \u27e8x, y\u27e9\n\u2225x\u2225 \u2225y\u2225 . (3.25)\nThe number \u03c9 is the angle between the vectors x and y. Intuitively , theangle\nangle between two vectors tells us how similar their orientations are. For\nexample, using the dot product, the angle between x and y = 4x, i.e., y\nis a scaled version of x, is 0: Their orientation is the same.\nDra"
  },
  {
    "vector_id": 291,
    "chunk_id": "p82_c3",
    "page_number": 82,
    "text": "two vectors tells us how similar their orientations are. For\nexample, using the dot product, the angle between x and y = 4x, i.e., y\nis a scaled version of x, is 0: Their orientation is the same.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 292,
    "chunk_id": "p83_c0",
    "page_number": 83,
    "text": "3.4 Angles and Orthogonality 77\nExample 3.6 (Angle between Vectors)\nLet us compute the angle betweenx = [1, 1]\u22a4 \u2208 R2 and y = [1, 2]\u22a4 \u2208 R2; Figure 3.5 The\nangle \u03c9 between\ntwo vectors x, y is\ncomputed using the\ninner product.\ny\nx\n10\n1\n\u03c9\nsee Figure 3.5, where we use the dot product as the inner product. Then\nwe get\ncos \u03c9 = \u27e8x, y\u27e9p\n\u27e8x, x\u27e9 \u27e8y, y\u27e9 = x\u22a4yp\nx\u22a4xy\u22a4y\n= 3\u221a\n10 , (3.26)\nand the angle between the two vectors is arccos( 3\u221a\n10 ) \u2248 0.32 rad, which\ncorresponds to about 18\u25e6.\nA key feature of the inner product is that it also allows us to characterize\nvectors that are orthogonal.\nDefinition 3.7 (Orthogonality). Two vectorsx and y are orthogonal if and orthogonal\nonly if \u27e8x, y\u27e9 = 0, and we write x \u22a5 y. If additionally \u2225x\u2225 = 1 = \u2225y\u2225,\ni.e., the vectors are unit vectors, then x and y are orthonorma"
  },
  {
    "vector_id": 293,
    "chunk_id": "p83_c1",
    "page_number": 83,
    "text": "thogonality). Two vectorsx and y are orthogonal if and orthogonal\nonly if \u27e8x, y\u27e9 = 0, and we write x \u22a5 y. If additionally \u2225x\u2225 = 1 = \u2225y\u2225,\ni.e., the vectors are unit vectors, then x and y are orthonormal. orthonormal\nAn implication of this definition is that the 0-vector is orthogonal to\nevery vector in the vector space.\nRemark. Orthogonality is the generalization of the concept of perpendic-\nularity to bilinear forms that do not have to be the dot product. In our\ncontext, geometrically , we can think of orthogonal vectors as having a\nright angle with respect to a specific inner product. \u2662\nExample 3.7 (Orthogonal Vectors)\nFigure 3.6 The\nangle \u03c9 between\ntwo vectors x, y can\nchange depending\non the inner\nproduct.\ny x\n\u22121 10\n1\n\u03c9\nConsider two vectors x = [1, 1]\u22a4, y = [\u22121, 1]\u22a4 \u2208 R2; see Figure 3.6"
  },
  {
    "vector_id": 294,
    "chunk_id": "p83_c2",
    "page_number": 83,
    "text": "le 3.7 (Orthogonal Vectors)\nFigure 3.6 The\nangle \u03c9 between\ntwo vectors x, y can\nchange depending\non the inner\nproduct.\ny x\n\u22121 10\n1\n\u03c9\nConsider two vectors x = [1, 1]\u22a4, y = [\u22121, 1]\u22a4 \u2208 R2; see Figure 3.6.\nWe are interested in determining the angle \u03c9 between them using two\ndifferent inner products. Using the dot product as the inner product yields\nan angle \u03c9 between x and y of 90\u25e6, such that x \u22a5 y. However, if we\nchoose the inner product\n\u27e8x, y\u27e9 = x\u22a4\n\u00142 0\n0 1\n\u0015\ny , (3.27)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 295,
    "chunk_id": "p84_c0",
    "page_number": 84,
    "text": "78 Analytic Geometry\nwe get that the angle \u03c9 between x and y is given by\ncos \u03c9 = \u27e8x, y\u27e9\n\u2225x\u2225\u2225y\u2225 = \u22121\n3 =\u21d2 \u03c9 \u2248 1.91 rad \u2248 109.5\u25e6 , (3.28)\nand x and y are not orthogonal. Therefore, vectors that are orthogonal\nwith respect to one inner product do not have to be orthogonal with re-\nspect to a different inner product.\nDefinition 3.8 (Orthogonal Matrix). A square matrix A \u2208 Rn\u00d7n is an\northogonal matrix if and only if its columns are orthonormal so thatorthogonal matrix\nAA\u22a4 = I = A\u22a4A, (3.29)\nwhich implies that\nA\u22121 = A\u22a4 , (3.30)\ni.e., the inverse is obtained by simply transposing the matrix.It is convention to\ncall these matrices\n\u201corthogonal\u201d but a\nmore precise\ndescription would\nbe \u201corthonormal\u201d.\nTransformations by orthogonal matrices are special because the length\nof a vector x is not changed whe"
  },
  {
    "vector_id": 296,
    "chunk_id": "p84_c1",
    "page_number": 84,
    "text": "ention to\ncall these matrices\n\u201corthogonal\u201d but a\nmore precise\ndescription would\nbe \u201corthonormal\u201d.\nTransformations by orthogonal matrices are special because the length\nof a vector x is not changed when transforming it using an orthogonal\nmatrix A. For the dot product, we obtain\nTransformations\nwith orthogonal\nmatrices preserve\ndistances and\nangles.\n\u2225Ax\u2225\n2\n= (Ax)\u22a4(Ax) = x\u22a4A\u22a4Ax = x\u22a4Ix = x\u22a4x = \u2225x\u2225\n2\n. (3.31)\nMoreover, the angle between any two vectors x, y, as measured by their\ninner product, is also unchanged when transforming both of them using\nan orthogonal matrix A. Assuming the dot product as the inner product,\nthe angle of the images Ax and Ay is given as\ncos \u03c9 = (Ax)\u22a4(Ay)\n\u2225Ax\u2225 \u2225Ay\u2225 = x\u22a4A\u22a4Ayq\nx\u22a4A\u22a4Axy\u22a4A\u22a4Ay\n= x\u22a4y\n\u2225x\u2225 \u2225y\u2225 , (3.32)\nwhich gives exactly the angle between x and y. This means t"
  },
  {
    "vector_id": 297,
    "chunk_id": "p84_c2",
    "page_number": 84,
    "text": "s the inner product,\nthe angle of the images Ax and Ay is given as\ncos \u03c9 = (Ax)\u22a4(Ay)\n\u2225Ax\u2225 \u2225Ay\u2225 = x\u22a4A\u22a4Ayq\nx\u22a4A\u22a4Axy\u22a4A\u22a4Ay\n= x\u22a4y\n\u2225x\u2225 \u2225y\u2225 , (3.32)\nwhich gives exactly the angle between x and y. This means that orthog-\nonal matrices A with A\u22a4 = A\u22121 preserve both angles and distances. It\nturns out that orthogonal matrices define transformations that are rota-\ntions (with the possibility of flips). In Section 3.9, we will discuss more\ndetails about rotations.\n3.5 Orthonormal Basis\nIn Section 2.6.1, we characterized properties of basis vectors and found\nthat in an n-dimensional vector space, we need n basis vectors, i.e., n\nvectors that are linearly independent. In Sections 3.3 and 3.4, we used\ninner products to compute the length of vectors and the angle between\nvectors. In the following, we will d"
  },
  {
    "vector_id": 298,
    "chunk_id": "p84_c3",
    "page_number": 84,
    "text": "asis vectors, i.e., n\nvectors that are linearly independent. In Sections 3.3 and 3.4, we used\ninner products to compute the length of vectors and the angle between\nvectors. In the following, we will discuss the special case where the basis\nvectors are orthogonal to each other and where the length of each basis\nvector is 1. We will call this basis then an orthonormal basis.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 299,
    "chunk_id": "p85_c0",
    "page_number": 85,
    "text": "3.6 Orthogonal Complement 79\nLet us introduce this more formally .\nDefinition 3.9 (Orthonormal Basis). Consider an n-dimensional vector\nspace V and a basis {b1, . . . ,bn} of V . If\n\u27e8bi, bj\u27e9 = 0 for i \u0338= j (3.33)\n\u27e8bi, bi\u27e9 = 1 (3.34)\nfor all i, j= 1, . . . , nthen the basis is called an orthonormal basis (ONB). orthonormal basis\nONBIf only (3.33) is satisfied, then the basis is called anorthogonal basis. Note\northogonal basisthat (3.34) implies that every basis vector has length/norm 1.\nRecall from Section 2.6.1 that we can use Gaussian elimination to find a\nbasis for a vector space spanned by a set of vectors. Assume we are given\na set {\u02dcb1, . . . ,\u02dcbn} of non-orthogonal and unnormalized basis vectors. We\nconcatenate them into a matrix \u02dcB = [\u02dcb1, . . . ,\u02dcbn] and apply Gaussian elim-\ninatio"
  },
  {
    "vector_id": 300,
    "chunk_id": "p85_c1",
    "page_number": 85,
    "text": "a set of vectors. Assume we are given\na set {\u02dcb1, . . . ,\u02dcbn} of non-orthogonal and unnormalized basis vectors. We\nconcatenate them into a matrix \u02dcB = [\u02dcb1, . . . ,\u02dcbn] and apply Gaussian elim-\nination to the augmented matrix (Section 2.3.2) [ \u02dcB \u02dcB\n\u22a4\n| \u02dcB] to obtain an\northonormal basis. This constructive way to iteratively build an orthonor-\nmal basis {b1, . . . ,bn} is called the Gram-Schmidt process (Strang, 2003).\nExample 3.8 (Orthonormal Basis)\nThe canonical/standard basis for a Euclidean vector space Rn is an or-\nthonormal basis, where the inner product is the dot product of vectors.\nIn R2, the vectors\nb1 = 1\u221a\n2\n\u00141\n1\n\u0015\n, b2 = 1\u221a\n2\n\u0014 1\n\u22121\n\u0015\n(3.35)\nform an orthonormal basis since b\u22a4\n1 b2 = 0 and \u2225b1\u2225 = 1 = \u2225b2\u2225.\nWe will exploit the concept of an orthonormal basis in Chapter 12 and\nCha"
  },
  {
    "vector_id": 301,
    "chunk_id": "p85_c2",
    "page_number": 85,
    "text": "R2, the vectors\nb1 = 1\u221a\n2\n\u00141\n1\n\u0015\n, b2 = 1\u221a\n2\n\u0014 1\n\u22121\n\u0015\n(3.35)\nform an orthonormal basis since b\u22a4\n1 b2 = 0 and \u2225b1\u2225 = 1 = \u2225b2\u2225.\nWe will exploit the concept of an orthonormal basis in Chapter 12 and\nChapter 10 when we discuss support vector machines and principal com-\nponent analysis.\n3.6 Orthogonal Complement\nHaving defined orthogonality , we will now look at vector spaces that are\northogonal to each other. This will play an important role in Chapter 10,\nwhen we discuss linear dimensionality reduction from a geometric per-\nspective.\nConsider a D-dimensional vector space V and an M-dimensional sub-\nspace U \u2286 V . Then itsorthogonal complement U\u22a5 is a (D\u2212M)-dimensional orthogonal\ncomplementsubspace of V and contains all vectors in V that are orthogonal to every\nvector in U. Furthermore, U \u2229U\u22a5"
  },
  {
    "vector_id": 302,
    "chunk_id": "p85_c3",
    "page_number": 85,
    "text": "ub-\nspace U \u2286 V . Then itsorthogonal complement U\u22a5 is a (D\u2212M)-dimensional orthogonal\ncomplementsubspace of V and contains all vectors in V that are orthogonal to every\nvector in U. Furthermore, U \u2229U\u22a5 = {0} so that any vector x \u2208 V can be\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 303,
    "chunk_id": "p86_c0",
    "page_number": 86,
    "text": "80 Analytic Geometry\nFigure 3.7 A plane\nU in a\nthree-dimensional\nvector space can be\ndescribed by its\nnormal vector,\nwhich spans its\northogonal\ncomplement U\u22a5.\ne3\ne1\ne2\nw\nU\nuniquely decomposed into\nx =\nMX\nm=1\n\u03bbmbm +\nD\u2212MX\nj=1\n\u03c8jb\u22a5\nj , \u03bb m, \u03c8j \u2208 R, (3.36)\nwhere (b1, . . . ,bM ) is a basis of U and (b\u22a5\n1 , . . . ,b\u22a5\nD\u2212M ) is a basis of U\u22a5.\nTherefore, the orthogonal complement can also be used to describe a\nplane U (two-dimensional subspace) in a three-dimensional vector space.\nMore specifically , the vectorw with \u2225w\u2225 = 1, which is orthogonal to the\nplane U, is the basis vector of U\u22a5. Figure 3.7 illustrates this setting. All\nvectors that are orthogonal to w must (by construction) lie in the plane\nU. The vector w is called the normal vector of U.normal vector\nGenerally , orthogonal complements c"
  },
  {
    "vector_id": 304,
    "chunk_id": "p86_c1",
    "page_number": 86,
    "text": "lustrates this setting. All\nvectors that are orthogonal to w must (by construction) lie in the plane\nU. The vector w is called the normal vector of U.normal vector\nGenerally , orthogonal complements can be used to describe hyperplanes\nin n-dimensional vector and affine spaces.\n3.7 Inner Product of Functions\nThus far, we looked at properties of inner products to compute lengths,\nangles and distances. We focused on inner products of finite-dimensional\nvectors. In the following, we will look at an example of inner products of\na different type of vectors: inner products of functions.\nThe inner products we discussed so far were defined for vectors with a\nfinite number of entries. We can think of a vector x \u2208 Rn as a function\nwith n function values. The concept of an inner product can be general"
  },
  {
    "vector_id": 305,
    "chunk_id": "p86_c2",
    "page_number": 86,
    "text": "ducts we discussed so far were defined for vectors with a\nfinite number of entries. We can think of a vector x \u2208 Rn as a function\nwith n function values. The concept of an inner product can be generalized\nto vectors with an infinite number of entries (countably infinite) and also\ncontinuous-valued functions (uncountably infinite). Then the sum over\nindividual components of vectors (see Equation (3.5) for example) turns\ninto an integral.\nAn inner product of two functions u : R \u2192 R and v : R \u2192 R can be\ndefined as the definite integral\n\u27e8u, v\u27e9 :=\nZ b\na\nu(x)v(x)dx (3.37)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 306,
    "chunk_id": "p86_c3",
    "page_number": 86,
    "text": "ematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 307,
    "chunk_id": "p87_c0",
    "page_number": 87,
    "text": "3.8 Orthogonal Projections 81\nfor lower and upper limits a, b <\u221e, respectively . As with our usual inner\nproduct, we can define norms and orthogonality by looking at the inner\nproduct. If (3.37) evaluates to 0, the functions u and v are orthogonal. To\nmake the preceding inner product mathematically precise, we need to take\ncare of measures and the definition of integrals, leading to the definition of\na Hilbert space. Furthermore, unlike inner products on finite-dimensional\nvectors, inner products on functions may diverge (have infinite value). All\nthis requires diving into some more intricate details of real and functional\nanalysis, which we do not cover in this book.\nExample 3.9 (Inner Product of Functions)\nIf we choose u = sin(x) and v = cos(x), the integrand f(x) = u(x)v(x) Figure 3.8 f"
  },
  {
    "vector_id": 308,
    "chunk_id": "p87_c1",
    "page_number": 87,
    "text": "details of real and functional\nanalysis, which we do not cover in this book.\nExample 3.9 (Inner Product of Functions)\nIf we choose u = sin(x) and v = cos(x), the integrand f(x) = u(x)v(x) Figure 3.8 f(x) =\nsin(x) cos(x).\n\u22122.5 0.0 2.5\nx\n\u22120.5\n0.0\n0.5\nsin(x) cos(x)of (3.37), is shown in Figure 3.8. We see that this function is odd, i.e.,\nf(\u2212x) = \u2212f(x). Therefore, the integral with limits a = \u2212\u03c0, b= \u03c0 of this\nproduct evaluates to 0. Therefore, sin and cos are orthogonal functions.\nRemark. It also holds that the collection of functions\n{1, cos(x), cos(2x), cos(3x), . . .} (3.38)\nis orthogonal if we integrate from \u2212\u03c0 to \u03c0, i.e., any pair of functions are\northogonal to each other. The collection of functions in (3.38) spans a\nlarge subspace of the functions that are even and periodic on[\u2212\u03c0, \u03c0), a"
  },
  {
    "vector_id": 309,
    "chunk_id": "p87_c2",
    "page_number": 87,
    "text": "integrate from \u2212\u03c0 to \u03c0, i.e., any pair of functions are\northogonal to each other. The collection of functions in (3.38) spans a\nlarge subspace of the functions that are even and periodic on[\u2212\u03c0, \u03c0), and\nprojecting functions onto this subspace is the fundamental idea behind\nFourier series. \u2662\nIn Section 6.4.6, we will have a look at a second type of unconventional\ninner products: the inner product of random variables.\n3.8 Orthogonal Projections\nProjections are an important class of linear transformations (besides rota-\ntions and reflections) and play an important role in graphics, coding the-\nory , statistics and machine learning. In machine learning, we often deal\nwith data that is high-dimensional. High-dimensional data is often hard\nto analyze or visualize. However, high-dimensional data"
  },
  {
    "vector_id": 310,
    "chunk_id": "p87_c3",
    "page_number": 87,
    "text": "y , statistics and machine learning. In machine learning, we often deal\nwith data that is high-dimensional. High-dimensional data is often hard\nto analyze or visualize. However, high-dimensional data quite often pos-\nsesses the property that only a few dimensions contain most information,\nand most other dimensions are not essential to describe key properties\nof the data. When we compress or visualize high-dimensional data, we\nwill lose information. To minimize this compression loss, we ideally find\nthe most informative dimensions in the data. As discussed in Chapter 1, \u201cFeature\u201d is a\ncommon expression\nfor data\nrepresentation.\ndata can be represented as vectors, and in this chapter, we will discuss\nsome of the fundamental tools for data compression. More specifically , we\ncan project the or"
  },
  {
    "vector_id": 311,
    "chunk_id": "p87_c4",
    "page_number": 87,
    "text": "pression\nfor data\nrepresentation.\ndata can be represented as vectors, and in this chapter, we will discuss\nsome of the fundamental tools for data compression. More specifically , we\ncan project the original high-dimensional data onto a lower-dimensional\nfeature space and work in this lower-dimensional space to learn more\nabout the dataset and extract relevant patterns. For example, machine\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 312,
    "chunk_id": "p88_c0",
    "page_number": 88,
    "text": "82 Analytic Geometry\nFigure 3.9\nOrthogonal\nprojection (orange\ndots) of a\ntwo-dimensional\ndataset (blue dots)\nonto a\none-dimensional\nsubspace (straight\nline).\n\u22124 \u22122 0 2 4\nx1\n\u22122\n\u22121\n0\n1\n2\nx2\nlearning algorithms, such as principal component analysis (PCA) by Pear-\nson (1901) and Hotelling (1933) and deep neural networks (e.g., deep\nauto-encoders (Deng et al., 2010)), heavily exploit the idea of dimension-\nality reduction. In the following, we will focus on orthogonal projections,\nwhich we will use in Chapter 10 for linear dimensionality reduction and\nin Chapter 12 for classification. Even linear regression, which we discuss\nin Chapter 9, can be interpreted using orthogonal projections. For a given\nlower-dimensional subspace, orthogonal projections of high-dimensional\ndata retain as much inform"
  },
  {
    "vector_id": 313,
    "chunk_id": "p88_c1",
    "page_number": 88,
    "text": "egression, which we discuss\nin Chapter 9, can be interpreted using orthogonal projections. For a given\nlower-dimensional subspace, orthogonal projections of high-dimensional\ndata retain as much information as possible and minimize the difference/\nerror between the original data and the corresponding projection. An il-\nlustration of such an orthogonal projection is given in Figure 3.9. Before\nwe detail how to obtain these projections, let us define what a projection\nactually is.\nDefinition 3.10 (Projection). Let V be a vector space and U \u2286 V a\nsubspace of V . A linear mapping \u03c0 : V \u2192 U is called a projection ifprojection\n\u03c02 = \u03c0 \u25e6 \u03c0 = \u03c0.\nSince linear mappings can be expressed by transformation matrices (see\nSection 2.7), the preceding definition applies equally to a special kind\nof transform"
  },
  {
    "vector_id": 314,
    "chunk_id": "p88_c2",
    "page_number": 88,
    "text": "d a projection ifprojection\n\u03c02 = \u03c0 \u25e6 \u03c0 = \u03c0.\nSince linear mappings can be expressed by transformation matrices (see\nSection 2.7), the preceding definition applies equally to a special kind\nof transformation matrices, the projection matrices P\u03c0, which exhibit theprojection matrix\nproperty that P2\n\u03c0 = P\u03c0.\nIn the following, we will derive orthogonal projections of vectors in the\ninner product space (Rn, \u27e8\u00b7, \u00b7\u27e9) onto subspaces. We will start with one-\ndimensional subspaces, which are also called lines. If not mentioned oth-line\nerwise, we assume the dot product \u27e8x, y\u27e9 = x\u22a4y as the inner product.\n3.8.1 Projection onto One-Dimensional Subspaces (Lines)\nAssume we are given a line (one-dimensional subspace) through the ori-\ngin with basis vector b \u2208 Rn. The line is a one-dimensional subspace\nU \u2286 Rn"
  },
  {
    "vector_id": 315,
    "chunk_id": "p88_c3",
    "page_number": 88,
    "text": "8.1 Projection onto One-Dimensional Subspaces (Lines)\nAssume we are given a line (one-dimensional subspace) through the ori-\ngin with basis vector b \u2208 Rn. The line is a one-dimensional subspace\nU \u2286 Rn spanned by b. When we project x \u2208 Rn onto U, we seek the\nvector \u03c0U (x) \u2208 U that is closest to x. Using geometric arguments, let\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 316,
    "chunk_id": "p89_c0",
    "page_number": 89,
    "text": "3.8 Orthogonal Projections 83\nFigure 3.10\nExamples of\nprojections onto\none-dimensional\nsubspaces.\nb\nx\n\u03c0U(x)\n\u03c9\n(a) Projection of x \u2208 R2 onto a subspace U\nwith basis vector b.\ncos\u03c9\u03c9\nsin\u03c9\nb\nx\n(b) Projection of a two-dimensional vector\nx with \u2225x\u2225 = 1 onto a one-dimensional\nsubspace spanned by b.\nus characterize some properties of the projection \u03c0U (x) (Figure 3.10(a)\nserves as an illustration):\nThe projection \u03c0U (x) is closest to x, where \u201cclosest\u201d implies that the\ndistance \u2225x\u2212\u03c0U (x)\u2225 is minimal. It follows that the segment\u03c0U (x)\u2212x\nfrom \u03c0U (x) to x is orthogonal to U, and therefore the basis vector b of\nU. The orthogonality condition yields \u27e8\u03c0U (x) \u2212 x, b\u27e9 = 0 since angles\nbetween vectors are defined via the inner product. \u03bb is then the\ncoordinate of \u03c0U (x)\nwith respect to b.\nThe projection \u03c0U"
  },
  {
    "vector_id": 317,
    "chunk_id": "p89_c1",
    "page_number": 89,
    "text": "r b of\nU. The orthogonality condition yields \u27e8\u03c0U (x) \u2212 x, b\u27e9 = 0 since angles\nbetween vectors are defined via the inner product. \u03bb is then the\ncoordinate of \u03c0U (x)\nwith respect to b.\nThe projection \u03c0U (x) of x onto U must be an element of U and, there-\nfore, a multiple of the basis vector b that spans U. Hence, \u03c0U (x) = \u03bbb,\nfor some \u03bb \u2208 R.\nIn the following three steps, we determine the coordinate\u03bb, the projection\n\u03c0U (x) \u2208 U, and the projection matrix P\u03c0 that maps any x \u2208 Rn onto U:\n1. Finding the coordinate \u03bb. The orthogonality condition yields\n\u27e8x \u2212 \u03c0U (x), b\u27e9 = 0\n\u03c0U(x)=\u03bbb\n\u21d0 \u21d2 \u27e8x \u2212 \u03bbb, b\u27e9 = 0 . (3.39)\nWe can now exploit the bilinearity of the inner product and arrive at With a general inner\nproduct, we get\n\u03bb = \u27e8x, b\u27e9 if\n\u2225b\u2225 = 1.\u27e8x, b\u27e9 \u2212\u03bb \u27e8b, b\u27e9 = 0 \u21d0 \u21d2\u03bb = \u27e8x, b\u27e9\n\u27e8b, b\u27e9 = \u27e8b, x\u27e9\n\u2225b\u22252 . (3.4"
  },
  {
    "vector_id": 318,
    "chunk_id": "p89_c2",
    "page_number": 89,
    "text": ". (3.39)\nWe can now exploit the bilinearity of the inner product and arrive at With a general inner\nproduct, we get\n\u03bb = \u27e8x, b\u27e9 if\n\u2225b\u2225 = 1.\u27e8x, b\u27e9 \u2212\u03bb \u27e8b, b\u27e9 = 0 \u21d0 \u21d2\u03bb = \u27e8x, b\u27e9\n\u27e8b, b\u27e9 = \u27e8b, x\u27e9\n\u2225b\u22252 . (3.40)\nIn the last step, we exploited the fact that inner products are symmet-\nric. If we choose \u27e8\u00b7, \u00b7\u27e9 to be the dot product, we obtain\n\u03bb = b\u22a4x\nb\u22a4b = b\u22a4x\n\u2225b\u22252 . (3.41)\nIf \u2225b\u2225 = 1, then the coordinate \u03bb of the projection is given by b\u22a4x.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 319,
    "chunk_id": "p90_c0",
    "page_number": 90,
    "text": "84 Analytic Geometry\n2. Finding the projection point \u03c0U (x) \u2208 U. Since \u03c0U (x) = \u03bbb, we imme-\ndiately obtain with (3.40) that\n\u03c0U (x) = \u03bbb = \u27e8x, b\u27e9\n\u2225b\u22252 b = b\u22a4x\n\u2225b\u22252 b, (3.42)\nwhere the last equality holds for the dot product only . We can also\ncompute the length of \u03c0U (x) by means of Definition 3.1 as\n\u2225\u03c0U (x)\u2225 = \u2225\u03bbb\u2225 = |\u03bb| \u2225b\u2225. (3.43)\nHence, our projection is of length |\u03bb| times the length of b. This also\nadds the intuition that \u03bb is the coordinate of \u03c0U (x) with respect to the\nbasis vector b that spans our one-dimensional subspace U.\nIf we use the dot product as an inner product, we get\n\u2225\u03c0U (x)\u2225\n(3.42)\n= |b\u22a4x|\n\u2225b\u22252 \u2225b\u2225\n(3.25)\n= |cos \u03c9| \u2225x\u2225 \u2225b\u2225 \u2225b\u2225\n\u2225b\u22252 = |cos \u03c9| \u2225x\u2225.\n(3.44)\nHere, \u03c9 is the angle between x and b. This equation should be familiar\nfrom trigonometry: If \u2225x\u2225 = 1, then x lies on"
  },
  {
    "vector_id": 320,
    "chunk_id": "p90_c1",
    "page_number": 90,
    "text": ")\u2225\n(3.42)\n= |b\u22a4x|\n\u2225b\u22252 \u2225b\u2225\n(3.25)\n= |cos \u03c9| \u2225x\u2225 \u2225b\u2225 \u2225b\u2225\n\u2225b\u22252 = |cos \u03c9| \u2225x\u2225.\n(3.44)\nHere, \u03c9 is the angle between x and b. This equation should be familiar\nfrom trigonometry: If \u2225x\u2225 = 1, then x lies on the unit circle. It follows\nthat the projection onto the horizontal axis spanned by b is exactlyThe horizontal axis\nis a one-dimensional\nsubspace.\ncos \u03c9, and the length of the corresponding vector \u03c0U (x) = |cos \u03c9|. An\nillustration is given in Figure 3.10(b).\n3. Finding the projection matrix P\u03c0. We know that a projection is a lin-\near mapping (see Definition 3.10). Therefore, there exists a projection\nmatrix P\u03c0, such that \u03c0U (x) = P\u03c0x. With the dot product as inner\nproduct and\n\u03c0U (x) = \u03bbb = b\u03bb = b b\u22a4x\n\u2225b\u22252 = bb\u22a4\n\u2225b\u22252 x, (3.45)\nwe immediately see that\nP\u03c0 = bb\u22a4\n\u2225b\u22252 . (3.46)\nNote that bb\u22a4 (and, c"
  },
  {
    "vector_id": 321,
    "chunk_id": "p90_c2",
    "page_number": 90,
    "text": "ion\nmatrix P\u03c0, such that \u03c0U (x) = P\u03c0x. With the dot product as inner\nproduct and\n\u03c0U (x) = \u03bbb = b\u03bb = b b\u22a4x\n\u2225b\u22252 = bb\u22a4\n\u2225b\u22252 x, (3.45)\nwe immediately see that\nP\u03c0 = bb\u22a4\n\u2225b\u22252 . (3.46)\nNote that bb\u22a4 (and, consequently ,P\u03c0) is a symmetric matrix (of rankProjection matrices\nare always\nsymmetric.\n1), and \u2225b\u22252 = \u27e8b, b\u27e9 is a scalar.\nThe projection matrixP\u03c0 projects any vectorx \u2208 Rn onto the line through\nthe origin with direction b (equivalently , the subspaceU spanned by b).\nRemark. The projection \u03c0U (x) \u2208 Rn is still an n-dimensional vector and\nnot a scalar. However, we no longer requiren coordinates to represent the\nprojection, but only a single one if we want to express it with respect to\nthe basis vector b that spans the subspace U: \u03bb. \u2662\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. F"
  },
  {
    "vector_id": 322,
    "chunk_id": "p90_c3",
    "page_number": 90,
    "text": "represent the\nprojection, but only a single one if we want to express it with respect to\nthe basis vector b that spans the subspace U: \u03bb. \u2662\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 323,
    "chunk_id": "p91_c0",
    "page_number": 91,
    "text": "3.8 Orthogonal Projections 85\nFigure 3.11\nProjection onto a\ntwo-dimensional\nsubspace U with\nbasis b1, b2. The\nprojection \u03c0U (x) of\nx \u2208 R3 onto U can\nbe expressed as a\nlinear combination\nof b1, b2 and the\ndisplacement vector\nx \u2212 \u03c0U (x) is\northogonal to both\nb1 and b2.\n0\nx\nb1\nb2\nU\n\u03c0U (x)\nx \u2212 \u03c0U (x)\nExample 3.10 (Projection onto a Line)\nFind the projection matrix P\u03c0 onto the line through the origin spanned\nby b =\n\u0002\n1 2 2\n\u0003\u22a4\n. b is a direction and a basis of the one-dimensional\nsubspace (line through origin).\nWith (3.46), we obtain\nP\u03c0 = bb\u22a4\nb\u22a4b = 1\n9\n\uf8ee\n\uf8f0\n1\n2\n2\n\uf8f9\n\uf8fb\u0002\n1 2 2\n\u0003\n= 1\n9\n\uf8ee\n\uf8f0\n1 2 2\n2 4 4\n2 4 4\n\uf8f9\n\uf8fb . (3.47)\nLet us now choose a particular x and see whether it lies in the subspace\nspanned by b. For x =\n\u0002\n1 1 1\n\u0003\u22a4\n, the projection is\n\u03c0U (x) = P\u03c0x = 1\n9\n\uf8ee\n\uf8f0\n1 2 2\n2 4 4\n2 4 4\n\uf8f9\n\uf8fb\n\uf8ee\n\uf8f0\n1\n1\n1\n\uf8f9"
  },
  {
    "vector_id": 324,
    "chunk_id": "p91_c1",
    "page_number": 91,
    "text": "4 4\n\uf8f9\n\uf8fb . (3.47)\nLet us now choose a particular x and see whether it lies in the subspace\nspanned by b. For x =\n\u0002\n1 1 1\n\u0003\u22a4\n, the projection is\n\u03c0U (x) = P\u03c0x = 1\n9\n\uf8ee\n\uf8f0\n1 2 2\n2 4 4\n2 4 4\n\uf8f9\n\uf8fb\n\uf8ee\n\uf8f0\n1\n1\n1\n\uf8f9\n\uf8fb = 1\n9\n\uf8ee\n\uf8f0\n5\n10\n10\n\uf8f9\n\uf8fb \u2208 span[\n\uf8ee\n\uf8f0\n1\n2\n2\n\uf8f9\n\uf8fb] . (3.48)\nNote that the application of P\u03c0 to \u03c0U (x) does not change anything, i.e.,\nP\u03c0\u03c0U (x) = \u03c0U (x). This is expected because according to Definition 3.10,\nwe know that a projection matrix P\u03c0 satisfies P2\n\u03c0x = P\u03c0x for all x.\nRemark. With the results from Chapter 4, we can show that \u03c0U (x) is an\neigenvector of P\u03c0, and the corresponding eigenvalue is 1. \u2662\n3.8.2 Projection onto General Subspaces\nIf U is given by a set\nof spanning vectors,\nwhich are not a\nbasis, make sure\nyou determine a\nbasis b1, . . . ,bm\nbefore proceeding.\nIn the following, we loo"
  },
  {
    "vector_id": 325,
    "chunk_id": "p91_c2",
    "page_number": 91,
    "text": ". \u2662\n3.8.2 Projection onto General Subspaces\nIf U is given by a set\nof spanning vectors,\nwhich are not a\nbasis, make sure\nyou determine a\nbasis b1, . . . ,bm\nbefore proceeding.\nIn the following, we look at orthogonal projections of vectors x \u2208 Rn\nonto lower-dimensional subspaces U \u2286 Rn with dim(U) = m \u2a7e 1. An\nillustration is given in Figure 3.11.\nAssume that (b1, . . . ,bm) is an ordered basis ofU. Any projection\u03c0U (x)\nonto U is necessarily an element of U. Therefore, they can be represented\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 326,
    "chunk_id": "p92_c0",
    "page_number": 92,
    "text": "86 Analytic Geometry\nas linear combinations of the basis vectors b1, . . . ,bm of U, such that\n\u03c0U (x) = Pm\ni=1 \u03bbibi.The basis vectors\nform the columns of\nB \u2208 Rn\u00d7m, where\nB = [b1, . . . ,bm].\nAs in the 1D case, we follow a three-step procedure to find the projec-\ntion \u03c0U (x) and the projection matrix P\u03c0:\n1. Find the coordinates \u03bb1, . . . , \u03bbm of the projection (with respect to the\nbasis of U), such that the linear combination\n\u03c0U (x) =\nmX\ni=1\n\u03bbibi = B\u03bb , (3.49)\nB = [b1, . . . ,bm] \u2208 Rn\u00d7m, \u03bb = [\u03bb1, . . . , \u03bbm]\u22a4 \u2208 Rm , (3.50)\nis closest to x \u2208 Rn. As in the 1D case, \u201cclosest\u201d means \u201cminimum\ndistance\u201d, which implies that the vector connecting \u03c0U (x) \u2208 U and\nx \u2208 Rn must be orthogonal to all basis vectors of U. Therefore, we\nobtain m simultaneous conditions (assuming the dot product as the\ninner"
  },
  {
    "vector_id": 327,
    "chunk_id": "p92_c1",
    "page_number": 92,
    "text": "ce\u201d, which implies that the vector connecting \u03c0U (x) \u2208 U and\nx \u2208 Rn must be orthogonal to all basis vectors of U. Therefore, we\nobtain m simultaneous conditions (assuming the dot product as the\ninner product)\n\u27e8b1, x \u2212 \u03c0U (x)\u27e9 = b\u22a4\n1 (x \u2212 \u03c0U (x)) = 0 (3.51)\n...\n\u27e8bm, x \u2212 \u03c0U (x)\u27e9 = b\u22a4\nm(x \u2212 \u03c0U (x)) = 0 (3.52)\nwhich, with \u03c0U (x) = B\u03bb, can be written as\nb\u22a4\n1 (x \u2212 B\u03bb) = 0 (3.53)\n...\nb\u22a4\nm(x \u2212 B\u03bb) = 0 (3.54)\nsuch that we obtain a homogeneous linear equation system\n\uf8ee\n\uf8ef\uf8f0\nb\u22a4\n1\n...\nb\u22a4\nm\n\uf8f9\n\uf8fa\uf8fb\n\uf8ee\n\uf8f0x \u2212 B\u03bb\n\uf8f9\n\uf8fb = 0 \u21d0 \u21d2B\u22a4(x \u2212 B\u03bb) = 0 (3.55)\n\u21d0 \u21d2B\u22a4B\u03bb = B\u22a4x. (3.56)\nThe last expression is called normal equation. Since b1, . . . ,bm are anormal equation\nbasis of U and, therefore, linearly independent, B\u22a4B \u2208 Rm\u00d7m is reg-\nular and can be inverted. This allows us to solve for the coefficients/\ncoordinates\n\u03bb = (B\u22a4B)\u2212"
  },
  {
    "vector_id": 328,
    "chunk_id": "p92_c2",
    "page_number": 92,
    "text": "ce b1, . . . ,bm are anormal equation\nbasis of U and, therefore, linearly independent, B\u22a4B \u2208 Rm\u00d7m is reg-\nular and can be inverted. This allows us to solve for the coefficients/\ncoordinates\n\u03bb = (B\u22a4B)\u22121B\u22a4x. (3.57)\nThe matrix (B\u22a4B)\u22121B\u22a4 is also called the pseudo-inverse of B, whichpseudo-inverse\ncan be computed for non-square matricesB. It only requires thatB\u22a4B\nis positive definite, which is the case if B is full rank. In practical ap-\nplications (e.g., linear regression), we often add a \u201cjitter term\u201d \u03f5I to\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 329,
    "chunk_id": "p93_c0",
    "page_number": 93,
    "text": "3.8 Orthogonal Projections 87\nB\u22a4B to guarantee increased numerical stability and positive definite-\nness. This \u201cridge\u201d can be rigorously derived using Bayesian inference.\nSee Chapter 9 for details.\n2. Find the projection \u03c0U (x) \u2208 U. We already established that \u03c0U (x) =\nB\u03bb. Therefore, with (3.57)\n\u03c0U (x) = B(B\u22a4B)\u22121B\u22a4x. (3.58)\n3. Find the projection matrix P\u03c0. From (3.58), we can immediately see\nthat the projection matrix that solves P\u03c0x = \u03c0U (x) must be\nP\u03c0 = B(B\u22a4B)\u22121B\u22a4 . (3.59)\nRemark. The solution for projecting onto general subspaces includes the\n1D case as a special case: If dim(U) = 1, then B\u22a4B \u2208 R is a scalar and\nwe can rewrite the projection matrix in (3.59) P\u03c0 = B(B\u22a4B)\u22121B\u22a4 as\nP\u03c0 = BB\u22a4\nB\u22a4B , which is exactly the projection matrix in (3.46). \u2662\nExample 3.11 (Projection onto a Two-dimensi"
  },
  {
    "vector_id": 330,
    "chunk_id": "p93_c1",
    "page_number": 93,
    "text": "B\u22a4B \u2208 R is a scalar and\nwe can rewrite the projection matrix in (3.59) P\u03c0 = B(B\u22a4B)\u22121B\u22a4 as\nP\u03c0 = BB\u22a4\nB\u22a4B , which is exactly the projection matrix in (3.46). \u2662\nExample 3.11 (Projection onto a Two-dimensional Subspace)\nFor a subspace U = span[\n\uf8ee\n\uf8f0\n1\n1\n1\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n0\n1\n2\n\uf8f9\n\uf8fb] \u2286 R3 and x =\n\uf8ee\n\uf8f0\n6\n0\n0\n\uf8f9\n\uf8fb \u2208 R3 find the\ncoordinates \u03bb of x in terms of the subspace U, the projection point \u03c0U (x)\nand the projection matrix P\u03c0.\nFirst, we see that the generating set of U is a basis (linear indepen-\ndence) and write the basis vectors of U into a matrix B =\n\uf8ee\n\uf8f0\n1 0\n1 1\n1 2\n\uf8f9\n\uf8fb.\nSecond, we compute the matrix B\u22a4B and the vector B\u22a4x as\nB\u22a4B =\n\u00141 1 1\n0 1 2\n\u0015\uf8ee\n\uf8f0\n1 0\n1 1\n1 2\n\uf8f9\n\uf8fb =\n\u00143 3\n3 5\n\u0015\n, B\u22a4x =\n\u00141 1 1\n0 1 2\n\u0015\uf8ee\n\uf8f0\n6\n0\n0\n\uf8f9\n\uf8fb =\n\u00146\n0\n\u0015\n.\n(3.60)\nThird, we solve the normal equation B\u22a4B\u03bb = B\u22a4x to find \u03bb:\n\u00143 3\n3 5\n\u0015\u0014\u03bb1"
  },
  {
    "vector_id": 331,
    "chunk_id": "p93_c2",
    "page_number": 93,
    "text": "and the vector B\u22a4x as\nB\u22a4B =\n\u00141 1 1\n0 1 2\n\u0015\uf8ee\n\uf8f0\n1 0\n1 1\n1 2\n\uf8f9\n\uf8fb =\n\u00143 3\n3 5\n\u0015\n, B\u22a4x =\n\u00141 1 1\n0 1 2\n\u0015\uf8ee\n\uf8f0\n6\n0\n0\n\uf8f9\n\uf8fb =\n\u00146\n0\n\u0015\n.\n(3.60)\nThird, we solve the normal equation B\u22a4B\u03bb = B\u22a4x to find \u03bb:\n\u00143 3\n3 5\n\u0015\u0014\u03bb1\n\u03bb2\n\u0015\n=\n\u00146\n0\n\u0015\n\u21d0 \u21d2\u03bb =\n\u0014 5\n\u22123\n\u0015\n. (3.61)\nFourth, the projection \u03c0U (x) of x onto U, i.e., into the column space of\nB, can be directly computed via\n\u03c0U (x) = B\u03bb =\n\uf8ee\n\uf8f0\n5\n2\n\u22121\n\uf8f9\n\uf8fb . (3.62)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 332,
    "chunk_id": "p94_c0",
    "page_number": 94,
    "text": "88 Analytic Geometry\nThe corresponding projection error is the norm of the difference vectorprojection error\nbetween the original vector and its projection onto U, i.e.,The projection error\nis also called the\nreconstruction error. \u2225x \u2212 \u03c0U (x)\u2225 =\n\r\r\r\n\u0002\n1 \u22122 1\n\u0003\u22a4\r\r\r =\n\u221a\n6 . (3.63)\nFifth, the projection matrix (for any x \u2208 R3) is given by\nP\u03c0 = B(B\u22a4B)\u22121B\u22a4 = 1\n6\n\uf8ee\n\uf8f0\n5 2 \u22121\n2 2 2\n\u22121 2 5\n\uf8f9\n\uf8fb . (3.64)\nTo verify the results, we can (a) check whether the displacement vector\n\u03c0U (x) \u2212 x is orthogonal to all basis vectors of U, and (b) verify that\nP\u03c0 = P2\n\u03c0 (see Definition 3.10).\nRemark. The projections \u03c0U (x) are still vectors in Rn although they lie in\nan m-dimensional subspace U \u2286 Rn. However, to represent a projected\nvector we only need the m coordinates \u03bb1, . . . , \u03bbm with respect to the\nbasis vec"
  },
  {
    "vector_id": 333,
    "chunk_id": "p94_c1",
    "page_number": 94,
    "text": "(x) are still vectors in Rn although they lie in\nan m-dimensional subspace U \u2286 Rn. However, to represent a projected\nvector we only need the m coordinates \u03bb1, . . . , \u03bbm with respect to the\nbasis vectors b1, . . . ,bm of U. \u2662\nRemark. In vector spaces with general inner products, we have to pay\nattention when computing angles and distances, which are defined by\nmeans of the inner product. \u2662We can find\napproximate\nsolutions to\nunsolvable linear\nequation systems\nusing projections.\nProjections allow us to look at situations where we have a linear system\nAx = b without a solution. Recall that this means that b does not lie in\nthe span of A, i.e., the vector b does not lie in the subspace spanned by\nthe columns of A. Given that the linear equation cannot be solved exactly ,\nwe can find an appro"
  },
  {
    "vector_id": 334,
    "chunk_id": "p94_c2",
    "page_number": 94,
    "text": "means that b does not lie in\nthe span of A, i.e., the vector b does not lie in the subspace spanned by\nthe columns of A. Given that the linear equation cannot be solved exactly ,\nwe can find an approximate solution. The idea is to find the vector in the\nsubspace spanned by the columns ofA that is closest tob, i.e., we compute\nthe orthogonal projection of b onto the subspace spanned by the columns\nof A. This problem arises often in practice, and the solution is called the\nleast-squares solution (assuming the dot product as the inner product) ofleast-squares\nsolution an overdetermined system. This is discussed further in Section 9.4. Using\nreconstruction errors (3.63) is one possible approach to derive principal\ncomponent analysis (Section 10.3).\nRemark. We just looked at projections of vec"
  },
  {
    "vector_id": 335,
    "chunk_id": "p94_c3",
    "page_number": 94,
    "text": "his is discussed further in Section 9.4. Using\nreconstruction errors (3.63) is one possible approach to derive principal\ncomponent analysis (Section 10.3).\nRemark. We just looked at projections of vectorsx onto a subspaceU with\nbasis vectors {b1, . . . ,bk}. If this basis is an ONB, i.e., (3.33) and (3.34)\nare satisfied, the projection equation (3.58) simplifies greatly to\n\u03c0U (x) = BB\u22a4x (3.65)\nsince B\u22a4B = I with coordinates\n\u03bb = B\u22a4x. (3.66)\nThis means that we no longer have to compute the inverse from (3.58),\nwhich saves computation time. \u2662\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 336,
    "chunk_id": "p94_c4",
    "page_number": 94,
    "text": "g\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 337,
    "chunk_id": "p95_c0",
    "page_number": 95,
    "text": "3.8 Orthogonal Projections 89\n3.8.3 Gram-Schmidt Orthogonalization\nProjections are at the core of the Gram-Schmidt method that allows us to\nconstructively transform any basis(b1, . . . ,bn) of an n-dimensional vector\nspace V into an orthogonal/orthonormal basis (u1, . . . ,un) of V . This\nbasis always exists (Liesen and Mehrmann, 2015) and span[b1, . . . ,bn] =\nspan[u1, . . . ,un]. The Gram-Schmidt orthogonalization method iteratively Gram-Schmidt\northogonalizationconstructs an orthogonal basis (u1, . . . ,un) from any basis (b1, . . . ,bn) of\nV as follows:\nu1 := b1 (3.67)\nuk := bk \u2212 \u03c0span[u1,...,uk\u22121](bk) , k = 2, . . . , n . (3.68)\nIn (3.68), the kth basis vector bk is projected onto the subspace spanned\nby the first k \u2212 1 constructed orthogonal vectors u1, . . . ,uk\u22121; see Sec-\ntion 3.8"
  },
  {
    "vector_id": 338,
    "chunk_id": "p95_c1",
    "page_number": 95,
    "text": "...,uk\u22121](bk) , k = 2, . . . , n . (3.68)\nIn (3.68), the kth basis vector bk is projected onto the subspace spanned\nby the first k \u2212 1 constructed orthogonal vectors u1, . . . ,uk\u22121; see Sec-\ntion 3.8.2. This projection is then subtracted from bk and yields a vector\nuk that is orthogonal to the (k \u2212 1)-dimensional subspace spanned by\nu1, . . . ,uk\u22121. Repeating this procedure for all n basis vectors b1, . . . ,bn\nyields an orthogonal basis (u1, . . . ,un) of V . If we normalize the uk, we\nobtain an ONB where \u2225uk\u2225 = 1 for k = 1, . . . , n.\nExample 3.12 (Gram-Schmidt Orthogonalization)\nFigure 3.12\nGram-Schmidt\northogonalization.\n(a) non-orthogonal\nbasis (b1, b2) of R2;\n(b) first constructed\nbasis vector u1 and\northogonal\nprojection of b2\nonto span[u1];\n(c) orthogonal basis\n(u1, u2) of R2.\nb1"
  },
  {
    "vector_id": 339,
    "chunk_id": "p95_c2",
    "page_number": 95,
    "text": "2\nGram-Schmidt\northogonalization.\n(a) non-orthogonal\nbasis (b1, b2) of R2;\n(b) first constructed\nbasis vector u1 and\northogonal\nprojection of b2\nonto span[u1];\n(c) orthogonal basis\n(u1, u2) of R2.\nb1\nb2\n0\n(a) Original non-orthogonal\nbasis vectors b1, b2.\nu1\nb2\n0 \u03c0span[u1](b2)\n(b) First new basis vector\nu1 = b1 and projection of b2\nonto the subspace spanned by\nu1.\nu1\nb2\n0 \u03c0span[u1](b2)\nu2\n(c) Orthogonal basis vectors u1\nand u2 = b2 \u2212 \u03c0span[u1](b2).\nConsider a basis (b1, b2) of R2, where\nb1 =\n\u00142\n0\n\u0015\n, b2 =\n\u00141\n1\n\u0015\n; (3.69)\nsee also Figure 3.12(a). Using the Gram-Schmidt method, we construct an\northogonal basis (u1, u2) of R2 as follows (assuming the dot product as\nthe inner product):\nu1 := b1 =\n\u00142\n0\n\u0015\n, (3.70)\nu2 := b2 \u2212 \u03c0span[u1](b2)\n(3.45)\n= b2 \u2212 u1u\u22a4\n1\n\u2225u1\u2225\n2 b2 =\n\u00141\n1\n\u0015\n\u2212\n\u00141 0\n0 0\n\u0015\u00141\n1\n\u0015"
  },
  {
    "vector_id": 340,
    "chunk_id": "p95_c3",
    "page_number": 95,
    "text": "thogonal basis (u1, u2) of R2 as follows (assuming the dot product as\nthe inner product):\nu1 := b1 =\n\u00142\n0\n\u0015\n, (3.70)\nu2 := b2 \u2212 \u03c0span[u1](b2)\n(3.45)\n= b2 \u2212 u1u\u22a4\n1\n\u2225u1\u2225\n2 b2 =\n\u00141\n1\n\u0015\n\u2212\n\u00141 0\n0 0\n\u0015\u00141\n1\n\u0015\n=\n\u00140\n1\n\u0015\n.\n(3.71)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 341,
    "chunk_id": "p96_c0",
    "page_number": 96,
    "text": "90 Analytic Geometry\nFigure 3.13\nProjection onto an\naffine space.\n(a) original setting;\n(b) setting shifted\nby \u2212x0 so that\nx \u2212 x0 can be\nprojected onto the\ndirection space U;\n(c) projection is\ntranslated back to\nx0 + \u03c0U (x \u2212 x0),\nwhich gives the final\northogonal\nprojection \u03c0L(x).\nL\nx0\nx\nb2\nb10\n(a) Setting.\nb10\nx\u2212x0\nU=L\u2212x0\n\u03c0U(x\u2212x0)b2\n(b) Reduce problem to pro-\njection \u03c0U onto vector sub-\nspace.\nL\nx0\nx\nb2\nb10\n\u03c0L(x)\n(c) Add support point back in\nto get affine projection \u03c0L.\nThese steps are illustrated in Figures 3.12(b) and (c). We immediately see\nthat u1 and u2 are orthogonal, i.e., u\u22a4\n1 u2 = 0.\n3.8.4 Projection onto Affine Subspaces\nThus far, we discussed how to project a vector onto a lower-dimensional\nsubspace U. In the following, we provide a solution to projecting a vector\nonto an affin"
  },
  {
    "vector_id": 342,
    "chunk_id": "p96_c1",
    "page_number": 96,
    "text": "3.8.4 Projection onto Affine Subspaces\nThus far, we discussed how to project a vector onto a lower-dimensional\nsubspace U. In the following, we provide a solution to projecting a vector\nonto an affine subspace.\nConsider the setting in Figure 3.13(a). We are given an affine spaceL =\nx0 + U, where b1, b2 are basis vectors of U. To determine the orthogonal\nprojection \u03c0L(x) of x onto L, we transform the problem into a problem\nthat we know how to solve: the projection onto a vector subspace. In\norder to get there, we subtract the support point x0 from x and from L,\nso that L \u2212 x0 = U is exactly the vector subspace U. We can now use the\northogonal projections onto a subspace we discussed in Section 3.8.2 and\nobtain the projection \u03c0U (x \u2212 x0), which is illustrated in Figure 3.13(b).\nThis project"
  },
  {
    "vector_id": 343,
    "chunk_id": "p96_c2",
    "page_number": 96,
    "text": "e vector subspace U. We can now use the\northogonal projections onto a subspace we discussed in Section 3.8.2 and\nobtain the projection \u03c0U (x \u2212 x0), which is illustrated in Figure 3.13(b).\nThis projection can now be translated back intoL by adding x0, such that\nwe obtain the orthogonal projection onto an affine space L as\n\u03c0L(x) = x0 + \u03c0U (x \u2212 x0) , (3.72)\nwhere \u03c0U (\u00b7) is the orthogonal projection onto the subspace U, i.e., the\ndirection space of L; see Figure 3.13(c).\nFrom Figure 3.13, it is also evident that the distance ofx from the affine\nspace L is identical to the distance of x \u2212 x0 from U, i.e.,\nd(x, L) = \u2225x \u2212 \u03c0L(x)\u2225 = \u2225x \u2212 (x0 + \u03c0U (x \u2212 x0))\u2225 (3.73a)\n= d(x \u2212 x0, \u03c0U (x \u2212 x0)) = d(x \u2212 x0, U) . (3.73b)\nWe will use projections onto an affine subspace to derive the concept of\na separating"
  },
  {
    "vector_id": 344,
    "chunk_id": "p96_c3",
    "page_number": 96,
    "text": ", i.e.,\nd(x, L) = \u2225x \u2212 \u03c0L(x)\u2225 = \u2225x \u2212 (x0 + \u03c0U (x \u2212 x0))\u2225 (3.73a)\n= d(x \u2212 x0, \u03c0U (x \u2212 x0)) = d(x \u2212 x0, U) . (3.73b)\nWe will use projections onto an affine subspace to derive the concept of\na separating hyperplane in Section 12.1.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 345,
    "chunk_id": "p97_c0",
    "page_number": 97,
    "text": "3.9 Rotations 91\nFigure 3.14 A\nrotation rotates\nobjects in a plane\nabout the origin. If\nthe rotation angle is\npositive, we rotate\ncounterclockwise.\nOriginal\nRotated by 112.5\u25e6\nFigure 3.15 The\nrobotic arm needs to\nrotate its joints in\norder to pick up\nobjects or to place\nthem correctly .\nFigure taken\nfrom (Deisenroth\net al., 2015).\n3.9 Rotations\nLength and angle preservation, as discussed in Section 3.4, are the two\ncharacteristics of linear mappings with orthogonal transformation matri-\nces. In the following, we will have a closer look at specific orthogonal\ntransformation matrices, which describe rotations.\nA rotation is a linear mapping (more specifically , an automorphism of rotation\na Euclidean vector space) that rotates a plane by an angle \u03b8 about the\norigin, i.e., the origin is a fixe"
  },
  {
    "vector_id": 346,
    "chunk_id": "p97_c1",
    "page_number": 97,
    "text": "ibe rotations.\nA rotation is a linear mapping (more specifically , an automorphism of rotation\na Euclidean vector space) that rotates a plane by an angle \u03b8 about the\norigin, i.e., the origin is a fixed point. For a positive angle \u03b8 >0, by com-\nmon convention, we rotate in a counterclockwise direction. An example is\nshown in Figure 3.14, where the transformation matrix is\nR =\n\u0014\u22120.38 \u22120.92\n0.92 \u22120.38\n\u0015\n. (3.74)\nImportant application areas of rotations include computer graphics and\nrobotics. For example, in robotics, it is often important to know how to\nrotate the joints of a robotic arm in order to pick up or place an object,\nsee Figure 3.15.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 347,
    "chunk_id": "p97_c2",
    "page_number": 97,
    "text": "to pick up or place an object,\nsee Figure 3.15.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 348,
    "chunk_id": "p98_c0",
    "page_number": 98,
    "text": "92 Analytic Geometry\nFigure 3.16\nRotation of the\nstandard basis in R2\nby an angle \u03b8.\ne1\ne2\n\u03b8\n\u03b8\n\u03a6(e2) = [\u2212sin\u03b8,cos\u03b8]\u22a4\n\u03a6(e1) = [cos\u03b8,sin\u03b8]\u22a4\ncos\u03b8\nsin\u03b8\n\u2212sin\u03b8\ncos\u03b8\n3.9.1 Rotations in R2\nConsider the standard basis\n\u001a\ne1 =\n\u00141\n0\n\u0015\n, e2 =\n\u00140\n1\n\u0015\u001b\nof R2, which defines\nthe standard coordinate system in R2. We aim to rotate this coordinate\nsystem by an angle \u03b8 as illustrated in Figure 3.16. Note that the rotated\nvectors are still linearly independent and, therefore, are a basis ofR2. This\nmeans that the rotation performs a basis change.\nRotations \u03a6 are linear mappings so that we can express them by a\nrotation matrix R(\u03b8). Trigonometry (see Figure 3.16) allows us to de-rotation matrix\ntermine the coordinates of the rotated axes (the image of \u03a6) with respect\nto the standard basis in R2. We obtain\n\u03a6(e1)"
  },
  {
    "vector_id": 349,
    "chunk_id": "p98_c1",
    "page_number": 98,
    "text": "tion matrix R(\u03b8). Trigonometry (see Figure 3.16) allows us to de-rotation matrix\ntermine the coordinates of the rotated axes (the image of \u03a6) with respect\nto the standard basis in R2. We obtain\n\u03a6(e1) =\n\u0014cos \u03b8\nsin \u03b8\n\u0015\n, \u03a6(e2) =\n\u0014\u2212sin \u03b8\ncos \u03b8\n\u0015\n. (3.75)\nTherefore, the rotation matrix that performs the basis change into the\nrotated coordinates R(\u03b8) is given as\nR(\u03b8) =\n\u0002\n\u03a6(e1) \u03a6( e2)\n\u0003\n=\n\u0014cos \u03b8 \u2212sin \u03b8\nsin \u03b8 cos \u03b8\n\u0015\n. (3.76)\n3.9.2 Rotations in R3\nIn contrast to the R2 case, in R3 we can rotate any two-dimensional plane\nabout a one-dimensional axis. The easiest way to specify the general rota-\ntion matrix is to specify how the images of the standard basise1, e2, e3 are\nsupposed to be rotated, and making sure these imagesRe1, Re2, Re3 are\northonormal to each other. We can then obtain a general rot"
  },
  {
    "vector_id": 350,
    "chunk_id": "p98_c2",
    "page_number": 98,
    "text": "atrix is to specify how the images of the standard basise1, e2, e3 are\nsupposed to be rotated, and making sure these imagesRe1, Re2, Re3 are\northonormal to each other. We can then obtain a general rotation matrix\nR by combining the images of the standard basis.\nTo have a meaningful rotation angle, we have to define what \u201ccoun-\nterclockwise\u201d means when we operate in more than two dimensions. We\nuse the convention that a \u201ccounterclockwise\u201d (planar) rotation about an\naxis refers to a rotation about an axis when we look at the axis \u201chead on,\nfrom the end toward the origin\u201d. In R3, there are therefore three (planar)\nrotations about the three standard basis vectors (see Figure 3.17):\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 351,
    "chunk_id": "p98_c3",
    "page_number": 98,
    "text": "ore three (planar)\nrotations about the three standard basis vectors (see Figure 3.17):\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 352,
    "chunk_id": "p99_c0",
    "page_number": 99,
    "text": "3.9 Rotations 93\nFigure 3.17\nRotation of a vector\n(gray) in R3 by an\nangle \u03b8 about the\ne3-axis. The rotated\nvector is shown in\nblue.\ne1\ne2\ne3\n\u03b8\nRotation about the e1-axis\nR1(\u03b8) =\n\u0002\n\u03a6(e1) \u03a6( e2) \u03a6( e3)\n\u0003\n=\n\uf8ee\n\uf8f0\n1 0 0\n0 cos \u03b8 \u2212sin \u03b8\n0 sin \u03b8 cos \u03b8\n\uf8f9\n\uf8fb . (3.77)\nHere, the e1 coordinate is fixed, and the counterclockwise rotation is\nperformed in the e2e3 plane.\nRotation about the e2-axis\nR2(\u03b8) =\n\uf8ee\n\uf8f0\ncos \u03b8 0 sin \u03b8\n0 1 0\n\u2212sin \u03b8 0 cos \u03b8\n\uf8f9\n\uf8fb . (3.78)\nIf we rotate the e1e3 plane about the e2 axis, we need to look at the e2\naxis from its \u201ctip\u201d toward the origin.\nRotation about the e3-axis\nR3(\u03b8) =\n\uf8ee\n\uf8f0\ncos \u03b8 \u2212sin \u03b8 0\nsin \u03b8 cos \u03b8 0\n0 0 1\n\uf8f9\n\uf8fb . (3.79)\nFigure 3.17 illustrates this.\n3.9.3 Rotations in n Dimensions\nThe generalization of rotations from 2D and 3D to n-dimensional Eu-\nclidean vector spaces can b"
  },
  {
    "vector_id": 353,
    "chunk_id": "p99_c1",
    "page_number": 99,
    "text": "\u2212sin \u03b8 0\nsin \u03b8 cos \u03b8 0\n0 0 1\n\uf8f9\n\uf8fb . (3.79)\nFigure 3.17 illustrates this.\n3.9.3 Rotations in n Dimensions\nThe generalization of rotations from 2D and 3D to n-dimensional Eu-\nclidean vector spaces can be intuitively described as fixing n \u2212 2 dimen-\nsions and restrict the rotation to a two-dimensional plane in the n-dimen-\nsional space. As in the three-dimensional case, we can rotate any plane\n(two-dimensional subspace of Rn).\nDefinition 3.11 (Givens Rotation). Let V be an n-dimensional Euclidean\nvector space and \u03a6 : V \u2192 V an automorphism with transformation ma-\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 354,
    "chunk_id": "p99_c2",
    "page_number": 99,
    "text": "al, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 355,
    "chunk_id": "p100_c0",
    "page_number": 100,
    "text": "94 Analytic Geometry\ntrix\nRij(\u03b8) :=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nIi\u22121 0 \u00b7 \u00b7\u00b7 \u00b7\u00b7 \u00b7 0\n0 cos \u03b8 0 \u2212sin \u03b8 0\n0 0 Ij\u2212i\u22121 0 0\n0 sin \u03b8 0 cos \u03b8 0\n0 \u00b7 \u00b7\u00b7 \u00b7 \u00b7\u00b7 0 In\u2212j\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u2208 Rn\u00d7n , (3.80)\nfor 1 \u2a7d i < j\u2a7d n and \u03b8 \u2208 R. Then Rij(\u03b8) is called a Givens rotation.Givens rotation\nEssentially ,Rij(\u03b8) is the identity matrix In with\nrii = cos \u03b8 , rij = \u2212sin \u03b8 , rji = sin \u03b8 , rjj = cos \u03b8 . (3.81)\nIn two dimensions (i.e., n = 2), we obtain (3.76) as a special case.\n3.9.4 Properties of Rotations\nRotations exhibit a number of useful properties, which can be derived by\nconsidering them as orthogonal matrices (Definition 3.8):\nRotations preserve distances, i.e.,\u2225x\u2212y\u2225 = \u2225R\u03b8(x)\u2212R\u03b8(y)\u2225. In other\nwords, rotations leave the distance between any two points unchanged\nafter the transformation.\nRotations preserve angles, i.e., the angle betweenR"
  },
  {
    "vector_id": 356,
    "chunk_id": "p100_c1",
    "page_number": 100,
    "text": "serve distances, i.e.,\u2225x\u2212y\u2225 = \u2225R\u03b8(x)\u2212R\u03b8(y)\u2225. In other\nwords, rotations leave the distance between any two points unchanged\nafter the transformation.\nRotations preserve angles, i.e., the angle betweenR\u03b8x and R\u03b8y equals\nthe angle between x and y.\nRotations in three (or more) dimensions are generally not commuta-\ntive. Therefore, the order in which rotations are applied is important,\neven if they rotate about the same point. Only in two dimensions vector\nrotations are commutative, such that R(\u03d5)R(\u03b8) = R(\u03b8)R(\u03d5) for all\n\u03d5, \u03b8\u2208 [0, 2\u03c0). They form an Abelian group (with multiplication) only if\nthey rotate about the same point (e.g., the origin).\n3.10 Further Reading\nIn this chapter, we gave a brief overview of some of the important concepts\nof analytic geometry , which we will use in later chapter"
  },
  {
    "vector_id": 357,
    "chunk_id": "p100_c2",
    "page_number": 100,
    "text": "tate about the same point (e.g., the origin).\n3.10 Further Reading\nIn this chapter, we gave a brief overview of some of the important concepts\nof analytic geometry , which we will use in later chapters of the book.\nFor a broader and more in-depth overview of some of the concepts we\npresented, we refer to the following excellent books: Axler (2015) and\nBoyd and Vandenberghe (2018).\nInner products allow us to determine specific bases of vector (sub)spaces,\nwhere each vector is orthogonal to all others (orthogonal bases) using the\nGram-Schmidt method. These bases are important in optimization and\nnumerical algorithms for solving linear equation systems. For instance,\nKrylov subspace methods, such as conjugate gradients or the generalized\nminimal residual method (GMRES), minimize residual erro"
  },
  {
    "vector_id": 358,
    "chunk_id": "p100_c3",
    "page_number": 100,
    "text": "numerical algorithms for solving linear equation systems. For instance,\nKrylov subspace methods, such as conjugate gradients or the generalized\nminimal residual method (GMRES), minimize residual errors that are or-\nthogonal to each other (Stoer and Burlirsch, 2002).\nIn machine learning, inner products are important in the context of\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 359,
    "chunk_id": "p101_c0",
    "page_number": 101,
    "text": "3.10 Further Reading 95\nkernel methods (Sch\u00a8olkopf and Smola, 2002). Kernel methods exploit the\nfact that many linear algorithms can be expressed purely by inner prod-\nuct computations. Then, the \u201ckernel trick\u201d allows us to compute these\ninner products implicitly in a (potentially infinite-dimensional) feature\nspace, without even knowing this feature space explicitly . This allowed the\n\u201cnon-linearization\u201d of many algorithms used in machine learning, such as\nkernel-PCA (Sch\u00a8olkopf et al., 1997) for dimensionality reduction. Gaus-\nsian processes (Rasmussen and Williams, 2006) also fall into the category\nof kernel methods and are the current state of the art in probabilistic re-\ngression (fitting curves to data points). The idea of kernels is explored\nfurther in Chapter 12.\nProjections are of"
  },
  {
    "vector_id": 360,
    "chunk_id": "p101_c1",
    "page_number": 101,
    "text": "category\nof kernel methods and are the current state of the art in probabilistic re-\ngression (fitting curves to data points). The idea of kernels is explored\nfurther in Chapter 12.\nProjections are often used in computer graphics, e.g., to generate shad-\nows. In optimization, orthogonal projections are often used to (iteratively)\nminimize residual errors. This also has applications in machine learning,\ne.g., in linear regression where we want to find a (linear) function that\nminimizes the residual errors, i.e., the lengths of the orthogonal projec-\ntions of the data onto the linear function (Bishop, 2006). We will investi-\ngate this further in Chapter 9. PCA (Pearson, 1901; Hotelling, 1933) also\nuses projections to reduce the dimensionality of high-dimensional data.\nWe will discuss this in"
  },
  {
    "vector_id": 361,
    "chunk_id": "p101_c2",
    "page_number": 101,
    "text": "ishop, 2006). We will investi-\ngate this further in Chapter 9. PCA (Pearson, 1901; Hotelling, 1933) also\nuses projections to reduce the dimensionality of high-dimensional data.\nWe will discuss this in more detail in Chapter 10.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 362,
    "chunk_id": "p102_c0",
    "page_number": 102,
    "text": "96 Analytic Geometry\nExercises\n3.1 Show that \u27e8\u00b7, \u00b7\u27e9 defined for all x = [x1, x2]\u22a4 \u2208 R2 and y = [y1, y2]\u22a4 \u2208 R2 by\n\u27e8x, y\u27e9 := x1y1 \u2212 (x1y2 + x2y1) + 2(x2y2)\nis an inner product.\n3.2 Consider R2 with \u27e8\u00b7, \u00b7\u27e9 defined for all x and y in R2 as\n\u27e8x, y\u27e9 := x\u22a4\n\u0014\n2 0\n1 2\n\u0015\n| {z }\n=:A\ny .\nIs \u27e8\u00b7, \u00b7\u27e9 an inner product?\n3.3 Compute the distance between\nx =\n\uf8ee\n\uf8f0\n1\n2\n3\n\uf8f9\n\uf8fb , y =\n\uf8ee\n\uf8f0\n\u22121\n\u22121\n0\n\uf8f9\n\uf8fb\nusing\na. \u27e8x, y\u27e9 := x\u22a4y\nb. \u27e8x, y\u27e9 := x\u22a4Ay , A :=\n\uf8ee\n\uf8f0\n2 1 0\n1 3 \u22121\n0 \u22121 2\n\uf8f9\n\uf8fb\n3.4 Compute the angle between\nx =\n\u0014\n1\n2\n\u0015\n, y =\n\u0014\n\u22121\n\u22121\n\u0015\nusing\na. \u27e8x, y\u27e9 := x\u22a4y\nb. \u27e8x, y\u27e9 := x\u22a4By , B :=\n\u0014\n2 1\n1 3\n\u0015\n3.5 Consider the Euclidean vector space R5 with the dot product. A subspace\nU \u2286 R5 and x \u2208 R5 are given by\nU = span[\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0\n\u22121\n2\n0\n2\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n\u22123\n1\n\u22121\n2\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u22123\n4\n1\n2\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u22121\n\u22123\n5\n0\n7\n\uf8f9\n\uf8fa\uf8fa"
  },
  {
    "vector_id": 363,
    "chunk_id": "p102_c1",
    "page_number": 102,
    "text": "ctor space R5 with the dot product. A subspace\nU \u2286 R5 and x \u2208 R5 are given by\nU = span[\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0\n\u22121\n2\n0\n2\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n\u22123\n1\n\u22121\n2\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u22123\n4\n1\n2\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u22121\n\u22123\n5\n0\n7\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n] , x =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u22121\n\u22129\n\u22121\n4\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\na. Determine the orthogonal projection \u03c0U (x) of x onto U\nb. Determine the distance d(x, U)\n3.6 Consider R3 with the inner product\n\u27e8x, y\u27e9 := x\u22a4\n\uf8ee\n\uf8f0\n2 1 0\n1 2 \u22121\n0 \u22121 2\n\uf8f9\n\uf8fby .\nFurthermore, we define e1, e2, e3 as the standard/canonical basis in R3.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 364,
    "chunk_id": "p103_c0",
    "page_number": 103,
    "text": "Exercises 97\na. Determine the orthogonal projection \u03c0U (e2) of e2 onto\nU = span[e1, e3] .\nHint: Orthogonality is defined through the inner product.\nb. Compute the distance d(e2, U).\nc. Draw the scenario: standard basis vectors and \u03c0U (e2)\n3.7 Let V be a vector space and \u03c0 an endomorphism of V .\na. Prove that \u03c0 is a projection if and only if idV \u2212 \u03c0 is a projection, where\nidV is the identity endomorphism on V .\nb. Assume now that \u03c0 is a projection. CalculateIm(idV \u2212\u03c0) and ker(idV \u2212\u03c0)\nas a function of Im(\u03c0) and ker(\u03c0).\n3.8 Using the Gram-Schmidt method, turn the basis B = ( b1, b2) of a two-\ndimensional subspace U \u2286 R3 into an ONB C = (c1, c2) of U, where\nb1 :=\n\uf8ee\n\uf8f0\n1\n1\n1\n\uf8f9\n\uf8fb, b2 :=\n\uf8ee\n\uf8f0\n\u22121\n2\n0\n\uf8f9\n\uf8fb .\n3.9 Let n \u2208 N and let x1, . . . , xn > 0 be n positive real numbers so that x1 +\n. . .+ xn = 1"
  },
  {
    "vector_id": 365,
    "chunk_id": "p103_c1",
    "page_number": 103,
    "text": "ensional subspace U \u2286 R3 into an ONB C = (c1, c2) of U, where\nb1 :=\n\uf8ee\n\uf8f0\n1\n1\n1\n\uf8f9\n\uf8fb, b2 :=\n\uf8ee\n\uf8f0\n\u22121\n2\n0\n\uf8f9\n\uf8fb .\n3.9 Let n \u2208 N and let x1, . . . , xn > 0 be n positive real numbers so that x1 +\n. . .+ xn = 1. Use the Cauchy-Schwarz inequality and show that\na. Pn\ni=1 x2\ni \u2a7e 1\nn\nb. Pn\ni=1\n1\nxi\n\u2a7e n2\nHint: Think about the dot product on Rn. Then, choose specific vectors\nx, y \u2208 Rn and apply the Cauchy-Schwarz inequality .\n3.10 Rotate the vectors\nx1 :=\n\u0014\n2\n3\n\u0015\n, x2 :=\n\u0014\n0\n\u22121\n\u0015\nby 30\u25e6.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 366,
    "chunk_id": "p104_c0",
    "page_number": 104,
    "text": "4\nMatrix Decompositions\nIn Chapters 2 and 3, we studied ways to manipulate and measure vectors,\nprojections of vectors, and linear mappings. Mappings and transforma-\ntions of vectors can be conveniently described as operations performed by\nmatrices. Moreover, data is often represented in matrix form as well, e.g.,\nwhere the rows of the matrix represent different people and the columns\ndescribe different features of the people, such as weight, height, and socio-\neconomic status. In this chapter, we present three aspects of matrices: how\nto summarize matrices, how matrices can be decomposed, and how these\ndecompositions can be used for matrix approximations.\nWe first consider methods that allow us to describe matrices with just\na few numbers that characterize the overall properties of matric"
  },
  {
    "vector_id": 367,
    "chunk_id": "p104_c1",
    "page_number": 104,
    "text": "how these\ndecompositions can be used for matrix approximations.\nWe first consider methods that allow us to describe matrices with just\na few numbers that characterize the overall properties of matrices. We\nwill do this in the sections on determinants (Section 4.1) and eigenval-\nues (Section 4.2) for the important special case of square matrices. These\ncharacteristic numbers have important mathematical consequences and\nallow us to quickly grasp what useful properties a matrix has. From here\nwe will proceed to matrix decomposition methods: An analogy for ma-\ntrix decomposition is the factoring of numbers, such as the factoring of\n21 into prime numbers 7 \u00b7 3. For this reason matrix decomposition is also\noften referred to as matrix factorization. Matrix decompositions are usedmatrix factoriza"
  },
  {
    "vector_id": 368,
    "chunk_id": "p104_c2",
    "page_number": 104,
    "text": "of numbers, such as the factoring of\n21 into prime numbers 7 \u00b7 3. For this reason matrix decomposition is also\noften referred to as matrix factorization. Matrix decompositions are usedmatrix factorization\nto describe a matrix by means of a different representation using factors\nof interpretable matrices.\nWe will first cover a square-root-like operation for symmetric, positive\ndefinite matrices, the Cholesky decomposition (Section 4.3). From here\nwe will look at two related methods for factorizing matrices into canoni-\ncal forms. The first one is known as matrix diagonalization (Section 4.4),\nwhich allows us to represent the linear mapping using a diagonal trans-\nformation matrix if we choose an appropriate basis. The second method,\nsingular value decomposition (Section 4.5), extends this f"
  },
  {
    "vector_id": 369,
    "chunk_id": "p104_c3",
    "page_number": 104,
    "text": "hich allows us to represent the linear mapping using a diagonal trans-\nformation matrix if we choose an appropriate basis. The second method,\nsingular value decomposition (Section 4.5), extends this factorization to\nnon-square matrices, and it is considered one of the fundamental concepts\nin linear algebra. These decompositions are helpful, as matrices represent-\ning numerical data are often very large and hard to analyze. We conclude\nthe chapter with a systematic overview of the types of matrices and the\ncharacteristic properties that distinguish them in the form of a matrix tax-\nonomy (Section 4.7).\nThe methods that we cover in this chapter will become important in\n98\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A."
  },
  {
    "vector_id": 370,
    "chunk_id": "p104_c4",
    "page_number": 104,
    "text": "on 4.7).\nThe methods that we cover in this chapter will become important in\n98\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 371,
    "chunk_id": "p105_c0",
    "page_number": 105,
    "text": "4.1 Determinant and Trace 99\nFigure 4.1 A mind\nmap of the concepts\nintroduced in this\nchapter, along with\nwhere they are used\nin other parts of the\nbook.\nDeterminant Invertibility Cholesky\nEigenvalues\nEigenvectors Orthogonal matrix Diagonalization\nSVD\nChapter 6Probability& distributions\nChapter 10Dimensionalityreduction\ntests used in\nused in\nused indetermines\nused in\nused in\nused in\nconstructs used in\nused in\nused in\nboth subsequent mathematical chapters, such as Chapter 6, but also in\napplied chapters, such as dimensionality reduction in Chapters 10 or den-\nsity estimation in Chapter 11. This chapter\u2019s overall structure is depicted\nin the mind map of Figure 4.1.\n4.1 Determinant and Trace The determinant\nnotation |A| must\nnot be confused\nwith the absolute\nvalue.\nDeterminants are important"
  },
  {
    "vector_id": 372,
    "chunk_id": "p105_c1",
    "page_number": 105,
    "text": "chapter\u2019s overall structure is depicted\nin the mind map of Figure 4.1.\n4.1 Determinant and Trace The determinant\nnotation |A| must\nnot be confused\nwith the absolute\nvalue.\nDeterminants are important concepts in linear algebra. A determinant is\na mathematical object in the analysis and solution of systems of linear\nequations. Determinants are only defined for square matrices A \u2208 Rn\u00d7n,\ni.e., matrices with the same number of rows and columns. In this book,\nwe write the determinant as det(A) or sometimes as |A| so that\ndet(A) =\n\f\f\f\f\f\f\f\f\f\na11 a12 . . . a1n\na21 a22 . . . a2n\n... ... ...\nan1 an2 . . . ann\n\f\f\f\f\f\f\f\f\f\n. (4.1)\nThe determinant of a square matrix A \u2208 Rn\u00d7n is a function that maps A determinant\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (202"
  },
  {
    "vector_id": 373,
    "chunk_id": "p105_c2",
    "page_number": 105,
    "text": ". ann\n\f\f\f\f\f\f\f\f\f\n. (4.1)\nThe determinant of a square matrix A \u2208 Rn\u00d7n is a function that maps A determinant\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 374,
    "chunk_id": "p106_c0",
    "page_number": 106,
    "text": "100 Matrix Decompositions\nonto a real number. Before providing a definition of the determinant for\ngeneral n \u00d7 n matrices, let us have a look at some motivating examples,\nand define determinants for some special matrices.\nExample 4.1 (Testing for Matrix Invertibility)\nLet us begin with exploring if a square matrix A is invertible (see Sec-\ntion 2.2.2). For the smallest cases, we already know when a matrix\nis invertible. If A is a 1 \u00d7 1 matrix, i.e., it is a scalar number, then\nA = a =\u21d2 A\u22121 = 1\na . Thus a 1\na = 1 holds, if and only if a \u0338= 0.\nFor 2 \u00d7 2 matrices, by the definition of the inverse (Definition 2.3), we\nknow that AA\u22121 = I. Then, with (2.24), the inverse of A is\nA\u22121 = 1\na11a22 \u2212 a12a21\n\u0014 a22 \u2212a12\n\u2212a21 a11\n\u0015\n. (4.2)\nHence, A is invertible if and only if\na11a22 \u2212 a12a21 \u0338= 0 . (4.3"
  },
  {
    "vector_id": 375,
    "chunk_id": "p106_c1",
    "page_number": 106,
    "text": "(Definition 2.3), we\nknow that AA\u22121 = I. Then, with (2.24), the inverse of A is\nA\u22121 = 1\na11a22 \u2212 a12a21\n\u0014 a22 \u2212a12\n\u2212a21 a11\n\u0015\n. (4.2)\nHence, A is invertible if and only if\na11a22 \u2212 a12a21 \u0338= 0 . (4.3)\nThis quantity is the determinant of A \u2208 R2\u00d72, i.e.,\ndet(A) =\n\f\f\f\f\f\na11 a12\na21 a22\n\f\f\f\f\f = a11a22 \u2212 a12a21 . (4.4)\nExample 4.1 points already at the relationship between determinants\nand the existence of inverse matrices. The next theorem states the same\nresult for n \u00d7 n matrices.\nTheorem 4.1. For any square matrixA \u2208 Rn\u00d7n it holds thatA is invertible\nif and only if det(A) \u0338= 0.\nWe have explicit (closed-form) expressions for determinants of small\nmatrices in terms of the elements of the matrix. For n = 1,\ndet(A) = det(a11) = a11 . (4.5)\nFor n = 2,\ndet(A) =\n\f\f\f\f\na11 a12\na21 a22\n\f\f\f\f = a11a22"
  },
  {
    "vector_id": 376,
    "chunk_id": "p106_c2",
    "page_number": 106,
    "text": "(closed-form) expressions for determinants of small\nmatrices in terms of the elements of the matrix. For n = 1,\ndet(A) = det(a11) = a11 . (4.5)\nFor n = 2,\ndet(A) =\n\f\f\f\f\na11 a12\na21 a22\n\f\f\f\f = a11a22 \u2212 a12a21 , (4.6)\nwhich we have observed in the preceding example.\nFor n = 3 (known as Sarrus\u2019 rule),\n\f\f\f\f\f\f\na11 a12 a13\na21 a22 a23\na31 a32 a33\n\f\f\f\f\f\f\n= a11a22a33 + a21a32a13 + a31a12a23 (4.7)\n\u2212 a31a22a13 \u2212 a11a32a23 \u2212 a21a12a33 .\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 377,
    "chunk_id": "p107_c0",
    "page_number": 107,
    "text": "4.1 Determinant and Trace 101\nFor a memory aid of the product terms in Sarrus\u2019 rule, try tracing the\nelements of the triple products in the matrix.\nWe call a square matrix T an upper-triangular matrix if Tij = 0 for upper-triangular\nmatrixi > j, i.e., the matrix is zero below its diagonal. Analogously , we define a\nlower-triangular matrix as a matrix with zeros above its diagonal. For a tri- lower-triangular\nmatrixangular matrix T \u2208 Rn\u00d7n, the determinant is the product of the diagonal\nelements, i.e.,\ndet(T) =\nnY\ni=1\nTii . (4.8)\nThe determinant is\nthe signed volume\nof the parallelepiped\nformed by the\ncolumns of the\nmatrix.\nFigure 4.2 The area\nof the parallelogram\n(shaded region)\nspanned by the\nvectors b and g is\n|det([b, g])|.\nb\ng\nFigure 4.3 The\nvolume of the\nparallelepiped\n(shaded volume)"
  },
  {
    "vector_id": 378,
    "chunk_id": "p107_c1",
    "page_number": 107,
    "text": "by the\ncolumns of the\nmatrix.\nFigure 4.2 The area\nof the parallelogram\n(shaded region)\nspanned by the\nvectors b and g is\n|det([b, g])|.\nb\ng\nFigure 4.3 The\nvolume of the\nparallelepiped\n(shaded volume)\nspanned by vectors\nr, b, g is\n|det([r, b, g])|.\nb\ng r\nExample 4.2 (Determinants as Measures of Volume)\nThe notion of a determinant is natural when we consider it as a mapping\nfrom a set of n vectors spanning an object in Rn. It turns out that the de-\nterminant det(A) is the signed volume of ann-dimensional parallelepiped\nformed by columns of the matrix A.\nFor n = 2 , the columns of the matrix form a parallelogram; see Fig-\nure 4.2. As the angle between vectors gets smaller, the area of a parallel-\nogram shrinks, too. Consider two vectors b, g that form the columns of a\nmatrix A = [b, g]. Then,"
  },
  {
    "vector_id": 379,
    "chunk_id": "p107_c2",
    "page_number": 107,
    "text": "a parallelogram; see Fig-\nure 4.2. As the angle between vectors gets smaller, the area of a parallel-\nogram shrinks, too. Consider two vectors b, g that form the columns of a\nmatrix A = [b, g]. Then, the absolute value of the determinant ofA is the\narea of the parallelogram with vertices 0, b, g, b + g. In particular, if b, g\nare linearly dependent so that b = \u03bbg for some \u03bb \u2208 R, they no longer\nform a two-dimensional parallelogram. Therefore, the corresponding area\nis 0. On the contrary , ifb, g are linearly independent and are multiples of\nthe canonical basis vectors e1, e2 then they can be written as b =\n\u0014b\n0\n\u0015\nand\ng =\n\u00140\ng\n\u0015\n, and the determinant is\n\f\f\f\f\nb 0\n0 g\n\f\f\f\f = bg \u2212 0 = bg.\nThe sign of the determinant indicates the orientation of the spanning\nvectors b, g with respect to the sta"
  },
  {
    "vector_id": 380,
    "chunk_id": "p107_c3",
    "page_number": 107,
    "text": "ritten as b =\n\u0014b\n0\n\u0015\nand\ng =\n\u00140\ng\n\u0015\n, and the determinant is\n\f\f\f\f\nb 0\n0 g\n\f\f\f\f = bg \u2212 0 = bg.\nThe sign of the determinant indicates the orientation of the spanning\nvectors b, g with respect to the standard basis (e1, e2). In our figure, flip-\nping the order tog, b swaps the columns ofA and reverses the orientation\nof the shaded area. This becomes the familiar formula: area = height \u00d7\nlength. This intuition extends to higher dimensions. In R3, we consider\nthree vectors r, b, g \u2208 R3 spanning the edges of a parallelepiped, i.e., a\nsolid with faces that are parallel parallelograms (see Figure 4.3). The ab- The sign of the\ndeterminant\nindicates the\norientation of the\nspanning vectors.\nsolute value of the determinant of the 3 \u00d73 matrix [r, b, g] is the volume\nof the solid. Thus, the determinant"
  },
  {
    "vector_id": 381,
    "chunk_id": "p107_c4",
    "page_number": 107,
    "text": ". The ab- The sign of the\ndeterminant\nindicates the\norientation of the\nspanning vectors.\nsolute value of the determinant of the 3 \u00d73 matrix [r, b, g] is the volume\nof the solid. Thus, the determinant acts as a function that measures the\nsigned volume formed by column vectors composed in a matrix.\nConsider the three linearly independent vectors r, g, b \u2208 R3 given as\nr =\n\uf8ee\n\uf8f0\n2\n0\n\u22128\n\uf8f9\n\uf8fb, g =\n\uf8ee\n\uf8f0\n6\n1\n0\n\uf8f9\n\uf8fb, b =\n\uf8ee\n\uf8f0\n1\n4\n\u22121\n\uf8f9\n\uf8fb . (4.9)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 382,
    "chunk_id": "p108_c0",
    "page_number": 108,
    "text": "102 Matrix Decompositions\nWriting these vectors as the columns of a matrix\nA = [r, g, b] =\n\uf8ee\n\uf8f0\n2 6 1\n0 1 4\n\u22128 0 \u22121\n\uf8f9\n\uf8fb (4.10)\nallows us to compute the desired volume as\nV = |det(A)| = 186 . (4.11)\nComputing the determinant of an n \u00d7 n matrix requires a general algo-\nrithm to solve the cases forn >3, which we are going to explore in the fol-\nlowing. Theorem 4.2 below reduces the problem of computing the deter-\nminant of ann\u00d7n matrix to computing the determinant of(n\u22121)\u00d7(n\u22121)\nmatrices. By recursively applying the Laplace expansion (Theorem 4.2),\nwe can therefore compute determinants of n \u00d7 n matrices by ultimately\ncomputing determinants of 2 \u00d7 2 matrices.\nLaplace expansion\nTheorem 4.2 (Laplace Expansion). Consider a matrix A \u2208 Rn\u00d7n. Then,\nfor all j = 1, . . . , n:\n1. Expansion along column j"
  },
  {
    "vector_id": 383,
    "chunk_id": "p108_c1",
    "page_number": 108,
    "text": "rices by ultimately\ncomputing determinants of 2 \u00d7 2 matrices.\nLaplace expansion\nTheorem 4.2 (Laplace Expansion). Consider a matrix A \u2208 Rn\u00d7n. Then,\nfor all j = 1, . . . , n:\n1. Expansion along column jdet(Ak,j) is called\na minor and\n(\u22121)k+j det(Ak,j)\na cofactor. det(A) =\nnX\nk=1\n(\u22121)k+jakj det(Ak,j) . (4.12)\n2. Expansion along row j\ndet(A) =\nnX\nk=1\n(\u22121)k+jajk det(Aj,k) . (4.13)\nHere Ak,j \u2208 R(n\u22121)\u00d7(n\u22121) is the submatrix of A that we obtain when delet-\ning row k and column j.\nExample 4.3 (Laplace Expansion)\nLet us compute the determinant of\nA =\n\uf8ee\n\uf8f0\n1 2 3\n3 1 2\n0 0 1\n\uf8f9\n\uf8fb (4.14)\nusing the Laplace expansion along the first row. Applying (4.13) yields\n\f\f\f\f\f\f\n1 2 3\n3 1 2\n0 0 1\n\f\f\f\f\f\f\n= (\u22121)1+1 \u00b7 1\n\f\f\f\f\n1 2\n0 1\n\f\f\f\f\n+ (\u22121)1+2 \u00b7 2\n\f\f\f\f\n3 2\n0 1\n\f\f\f\f + (\u22121)1+3 \u00b7 3\n\f\f\f\f\n3 1\n0 0\n\f\f\f\f .\n(4.15)\nDraft (2024"
  },
  {
    "vector_id": 384,
    "chunk_id": "p108_c2",
    "page_number": 108,
    "text": "pansion along the first row. Applying (4.13) yields\n\f\f\f\f\f\f\n1 2 3\n3 1 2\n0 0 1\n\f\f\f\f\f\f\n= (\u22121)1+1 \u00b7 1\n\f\f\f\f\n1 2\n0 1\n\f\f\f\f\n+ (\u22121)1+2 \u00b7 2\n\f\f\f\f\n3 2\n0 1\n\f\f\f\f + (\u22121)1+3 \u00b7 3\n\f\f\f\f\n3 1\n0 0\n\f\f\f\f .\n(4.15)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 385,
    "chunk_id": "p109_c0",
    "page_number": 109,
    "text": "4.1 Determinant and Trace 103\nWe use (4.6) to compute the determinants of all2\u00d72 matrices and obtain\ndet(A) = 1(1 \u2212 0) \u2212 2(3 \u2212 0) + 3(0\u2212 0) = \u22125 . (4.16)\nFor completeness we can compare this result to computing the determi-\nnant using Sarrus\u2019 rule (4.7):\ndet(A) = 1\u00b71\u00b71+3\u00b70\u00b73+0\u00b72\u00b72\u22120\u00b71\u00b73\u22121\u00b70\u00b72\u22123\u00b72\u00b71 = 1\u22126 = \u22125 . (4.17)\nFor A \u2208 Rn\u00d7n the determinant exhibits the following properties:\nThe determinant of a matrix product is the product of the corresponding\ndeterminants, det(AB) = det(A)det(B).\nDeterminants are invariant to transposition, i.e., det(A) = det(A\u22a4).\nIf A is regular (invertible), then det(A\u22121) = 1\ndet(A) .\nSimilar matrices (Definition 2.22) possess the same determinant. There-\nfore, for a linear mapping \u03a6 : V \u2192 V all transformation matrices A\u03a6\nof \u03a6 have the same determinant. Thus, th"
  },
  {
    "vector_id": 386,
    "chunk_id": "p109_c1",
    "page_number": 109,
    "text": "t(A\u22121) = 1\ndet(A) .\nSimilar matrices (Definition 2.22) possess the same determinant. There-\nfore, for a linear mapping \u03a6 : V \u2192 V all transformation matrices A\u03a6\nof \u03a6 have the same determinant. Thus, the determinant is invariant to\nthe choice of basis of a linear mapping.\nAdding a multiple of a column/row to another one does not change\ndet(A).\nMultiplication of a column/row with \u03bb \u2208 R scales det(A) by \u03bb. In\nparticular, det(\u03bbA) = \u03bbn det(A).\nSwapping two rows/columns changes the sign of det(A).\nBecause of the last three properties, we can use Gaussian elimination (see\nSection 2.1) to compute det(A) by bringing A into row-echelon form.\nWe can stop Gaussian elimination when we have A in a triangular form\nwhere the elements below the diagonal are all0. Recall from (4.8) that the\ndeterminant of a"
  },
  {
    "vector_id": 387,
    "chunk_id": "p109_c2",
    "page_number": 109,
    "text": ") by bringing A into row-echelon form.\nWe can stop Gaussian elimination when we have A in a triangular form\nwhere the elements below the diagonal are all0. Recall from (4.8) that the\ndeterminant of a triangular matrix is the product of the diagonal elements.\nTheorem 4.3. A square matrix A \u2208 Rn\u00d7n has det(A) \u0338= 0 if and only if\nrk(A) = n. In other words, A is invertible if and only if it is full rank.\nWhen mathematics was mainly performed by hand, the determinant\ncalculation was considered an essential way to analyze matrix invertibil-\nity . However, contemporary approaches in machine learning use direct\nnumerical methods that superseded the explicit calculation of the deter-\nminant. For example, in Chapter 2, we learned that inverse matrices can\nbe computed by Gaussian elimination. Gaussian"
  },
  {
    "vector_id": 388,
    "chunk_id": "p109_c3",
    "page_number": 109,
    "text": "se direct\nnumerical methods that superseded the explicit calculation of the deter-\nminant. For example, in Chapter 2, we learned that inverse matrices can\nbe computed by Gaussian elimination. Gaussian elimination can thus be\nused to compute the determinant of a matrix.\nDeterminants will play an important theoretical role for the following\nsections, especially when we learn about eigenvalues and eigenvectors\n(Section 4.2) through the characteristic polynomial.\nDefinition 4.4. The trace of a square matrix A \u2208 Rn\u00d7n is defined as trace\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 389,
    "chunk_id": "p109_c4",
    "page_number": 109,
    "text": "Cambridge University Press (2020)."
  },
  {
    "vector_id": 390,
    "chunk_id": "p110_c0",
    "page_number": 110,
    "text": "104 Matrix Decompositions\ntr(A) :=\nnX\ni=1\naii , (4.18)\ni.e. , the trace is the sum of the diagonal elements of A.\nThe trace satisfies the following properties:\ntr(A + B) = tr(A) + tr(B) for A, B \u2208 Rn\u00d7n\ntr(\u03b1A) = \u03b1tr(A) , \u03b1\u2208 R for A \u2208 Rn\u00d7n\ntr(In) = n\ntr(AB) = tr(BA) for A \u2208 Rn\u00d7k, B \u2208 Rk\u00d7n\nIt can be shown that only one function satisfies these four properties to-\ngether \u2013 the trace (Gohberg et al., 2012).\nThe properties of the trace of matrix products are more general. Specif-\nically , the trace is invariant under cyclic permutations, i.e.,The trace is\ninvariant under\ncyclic permutations. tr(AKL) = tr(KLA) (4.19)\nfor matrices A \u2208 Ra\u00d7k, K \u2208 Rk\u00d7l, L \u2208 Rl\u00d7a. This property generalizes to\nproducts of an arbitrary number of matrices. As a special case of (4.19), it\nfollows that for two vectors x, y"
  },
  {
    "vector_id": 391,
    "chunk_id": "p110_c1",
    "page_number": 110,
    "text": "= tr(KLA) (4.19)\nfor matrices A \u2208 Ra\u00d7k, K \u2208 Rk\u00d7l, L \u2208 Rl\u00d7a. This property generalizes to\nproducts of an arbitrary number of matrices. As a special case of (4.19), it\nfollows that for two vectors x, y \u2208 Rn\ntr(xy\u22a4) = tr(y\u22a4x) = y\u22a4x \u2208 R. (4.20)\nGiven a linear mapping \u03a6 : V \u2192 V , where V is a vector space, we\ndefine the trace of this map by using the trace of matrix representation\nof \u03a6. For a given basis of V , we can describe \u03a6 by means of the transfor-\nmation matrix A. Then the trace of \u03a6 is the trace of A. For a different\nbasis of V , it holds that the corresponding transformation matrix B of \u03a6\ncan be obtained by a basis change of the form S\u22121AS for suitable S (see\nSection 2.7.2). For the corresponding trace of \u03a6, this means\ntr(B) = tr(S\u22121AS)\n(4.19)\n= tr(ASS\u22121) = tr(A) . (4.21)\nHence, while"
  },
  {
    "vector_id": 392,
    "chunk_id": "p110_c2",
    "page_number": 110,
    "text": "can be obtained by a basis change of the form S\u22121AS for suitable S (see\nSection 2.7.2). For the corresponding trace of \u03a6, this means\ntr(B) = tr(S\u22121AS)\n(4.19)\n= tr(ASS\u22121) = tr(A) . (4.21)\nHence, while matrix representations of linear mappings are basis depen-\ndent the trace of a linear mapping \u03a6 is independent of the basis.\nIn this section, we covered determinants and traces as functions char-\nacterizing a square matrix. Taking together our understanding of determi-\nnants and traces we can now define an important equation describing a\nmatrix A in terms of a polynomial, which we will use extensively in the\nfollowing sections.\nDefinition 4.5 (Characteristic Polynomial). For \u03bb \u2208 R and a square ma-\ntrix A \u2208 Rn\u00d7n\npA(\u03bb) := det(A \u2212 \u03bbI) (4.22a)\n= c0 + c1\u03bb + c2\u03bb2 + \u00b7 \u00b7\u00b7+ cn\u22121\u03bbn\u22121 + (\u22121)n\u03bbn , (4.22b"
  },
  {
    "vector_id": 393,
    "chunk_id": "p110_c3",
    "page_number": 110,
    "text": "ively in the\nfollowing sections.\nDefinition 4.5 (Characteristic Polynomial). For \u03bb \u2208 R and a square ma-\ntrix A \u2208 Rn\u00d7n\npA(\u03bb) := det(A \u2212 \u03bbI) (4.22a)\n= c0 + c1\u03bb + c2\u03bb2 + \u00b7 \u00b7\u00b7+ cn\u22121\u03bbn\u22121 + (\u22121)n\u03bbn , (4.22b)\nc0, . . . , cn\u22121 \u2208 R, is the characteristic polynomial of A. In particular,characteristic\npolynomial\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 394,
    "chunk_id": "p111_c0",
    "page_number": 111,
    "text": "4.2 Eigenvalues and Eigenvectors 105\nc0 = det(A) , (4.23)\ncn\u22121 = (\u22121)n\u22121tr(A) . (4.24)\nThe characteristic polynomial (4.22a) will allow us to compute eigen-\nvalues and eigenvectors, covered in the next section.\n4.2 Eigenvalues and Eigenvectors\nWe will now get to know a new way to characterize a matrix and its associ-\nated linear mapping. Recall from Section 2.7.1 that every linear mapping\nhas a unique transformation matrix given an ordered basis. We can in-\nterpret linear mappings and their associated transformation matrices by\nperforming an \u201ceigen\u201d analysis. As we will see, the eigenvalues of a lin- Eigen is a German\nword meaning\n\u201ccharacteristic\u201d,\n\u201cself\u201d, or \u201cown\u201d.\near mapping will tell us how a special set of vectors, the eigenvectors, is\ntransformed by the linear mapping.\nDefinition 4.6"
  },
  {
    "vector_id": 395,
    "chunk_id": "p111_c1",
    "page_number": 111,
    "text": "a lin- Eigen is a German\nword meaning\n\u201ccharacteristic\u201d,\n\u201cself\u201d, or \u201cown\u201d.\near mapping will tell us how a special set of vectors, the eigenvectors, is\ntransformed by the linear mapping.\nDefinition 4.6. Let A \u2208 Rn\u00d7n be a square matrix. Then \u03bb \u2208 R is an\neigenvalue of A and x \u2208 Rn\\{0} is the corresponding eigenvector of A if eigenvalue\neigenvector\nAx = \u03bbx. (4.25)\nWe call (4.25) the eigenvalue equation. eigenvalue equation\nRemark. In the linear algebra literature and software, it is often a conven-\ntion that eigenvalues are sorted in descending order, so that the largest\neigenvalue and associated eigenvector are called the first eigenvalue and\nits associated eigenvector, and the second largest called the second eigen-\nvalue and its associated eigenvector, and so on. However, textbooks and\npubl"
  },
  {
    "vector_id": 396,
    "chunk_id": "p111_c2",
    "page_number": 111,
    "text": "eigenvector are called the first eigenvalue and\nits associated eigenvector, and the second largest called the second eigen-\nvalue and its associated eigenvector, and so on. However, textbooks and\npublications may have different or no notion of orderings. We do not want\nto presume an ordering in this book if not stated explicitly . \u2662\nThe following statements are equivalent:\n\u03bb is an eigenvalue of A \u2208 Rn\u00d7n.\nThere exists an x \u2208 Rn\\{0} with Ax = \u03bbx, or equivalently , (A \u2212\n\u03bbIn)x = 0 can be solved non-trivially , i.e.,x \u0338= 0.\nrk(A \u2212 \u03bbIn) < n.\ndet(A \u2212 \u03bbIn) = 0.\nDefinition 4.7 (Collinearity and Codirection). Two vectors that point in\nthe same direction are called codirected. Two vectors are collinear if they codirected\ncollinearpoint in the same or the opposite direction.\nRemark (Non-uniqueness of"
  },
  {
    "vector_id": 397,
    "chunk_id": "p111_c3",
    "page_number": 111,
    "text": "tion). Two vectors that point in\nthe same direction are called codirected. Two vectors are collinear if they codirected\ncollinearpoint in the same or the opposite direction.\nRemark (Non-uniqueness of eigenvectors) . If x is an eigenvector of A\nassociated with eigenvalue \u03bb, then for any c \u2208 R\\{0} it holds that cx is\nan eigenvector of A with the same eigenvalue since\nA(cx) = cAx = c\u03bbx = \u03bb(cx) . (4.26)\nThus, all vectors that are collinear to x are also eigenvectors of A.\n\u2662\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 398,
    "chunk_id": "p112_c0",
    "page_number": 112,
    "text": "106 Matrix Decompositions\nTheorem 4.8. \u03bb \u2208 R is an eigenvalue of A \u2208 Rn\u00d7n if and only if \u03bb is a\nroot of the characteristic polynomial pA(\u03bb) of A.\nDefinition 4.9. Let a square matrixA have an eigenvalue\u03bbi. The algebraicalgebraic\nmultiplicity multiplicity of \u03bbi is the number of times the root appears in the character-\nistic polynomial.\nDefinition 4.10 (Eigenspace and Eigenspectrum). For A \u2208 Rn\u00d7n, the set\nof all eigenvectors of A associated with an eigenvalue \u03bb spans a subspace\nof Rn, which is called theeigenspace of A with respect to \u03bb and is denotedeigenspace\nby E\u03bb. The set of all eigenvalues of A is called the eigenspectrum, or justeigenspectrum\nspectrum, of A.spectrum\nIf \u03bb is an eigenvalue of A \u2208 Rn\u00d7n, then the corresponding eigenspace\nE\u03bb is the solution space of the homogeneous system of"
  },
  {
    "vector_id": 399,
    "chunk_id": "p112_c1",
    "page_number": 112,
    "text": "f A is called the eigenspectrum, or justeigenspectrum\nspectrum, of A.spectrum\nIf \u03bb is an eigenvalue of A \u2208 Rn\u00d7n, then the corresponding eigenspace\nE\u03bb is the solution space of the homogeneous system of linear equations\n(A\u2212\u03bbI)x = 0. Geometrically , the eigenvector corresponding to a nonzero\neigenvalue points in a direction that is stretched by the linear mapping.\nThe eigenvalue is the factor by which it is stretched. If the eigenvalue is\nnegative, the direction of the stretching is flipped.\nExample 4.4 (The Case of the Identity Matrix)\nThe identity matrix I \u2208 Rn\u00d7n has characteristic polynomial pI(\u03bb) =\ndet(I \u2212\u03bbI) = (1\u2212\u03bb)n = 0, which has only one eigenvalue\u03bb = 1 that oc-\ncurs n times. Moreover, Ix = \u03bbx = 1x holds for all vectors x \u2208 Rn\\{0}.\nBecause of this, the sole eigenspace E1 of the identi"
  },
  {
    "vector_id": 400,
    "chunk_id": "p112_c2",
    "page_number": 112,
    "text": "I(\u03bb) =\ndet(I \u2212\u03bbI) = (1\u2212\u03bb)n = 0, which has only one eigenvalue\u03bb = 1 that oc-\ncurs n times. Moreover, Ix = \u03bbx = 1x holds for all vectors x \u2208 Rn\\{0}.\nBecause of this, the sole eigenspace E1 of the identity matrix spans n di-\nmensions, and all n standard basis vectors of Rn are eigenvectors of I.\nUseful properties regarding eigenvalues and eigenvectors include the\nfollowing:\nA matrix A and its transpose A\u22a4 possess the same eigenvalues, but not\nnecessarily the same eigenvectors.\nThe eigenspace E\u03bb is the null space of A \u2212 \u03bbI since\nAx = \u03bbx \u21d0 \u21d2Ax \u2212 \u03bbx = 0 (4.27a)\n\u21d0 \u21d2(A \u2212 \u03bbI)x = 0 \u21d0 \u21d2x \u2208 ker(A \u2212 \u03bbI). (4.27b)\nSimilar matrices (see Definition 2.22) possess the same eigenvalues.\nTherefore, a linear mapping \u03a6 has eigenvalues that are independent of\nthe choice of basis of its transformation matrix. This"
  },
  {
    "vector_id": 401,
    "chunk_id": "p112_c3",
    "page_number": 112,
    "text": "4.27b)\nSimilar matrices (see Definition 2.22) possess the same eigenvalues.\nTherefore, a linear mapping \u03a6 has eigenvalues that are independent of\nthe choice of basis of its transformation matrix. This makes eigenvalues,\ntogether with the determinant and the trace, key characteristic param-\neters of a linear mapping as they are all invariant under basis change.\nSymmetric, positive definite matrices always have positive, real eigen-\nvalues.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 402,
    "chunk_id": "p113_c0",
    "page_number": 113,
    "text": "4.2 Eigenvalues and Eigenvectors 107\nExample 4.5 (Computing Eigenvalues, Eigenvectors, and\nEigenspaces)\nLet us find the eigenvalues and eigenvectors of the 2 \u00d7 2 matrix\nA =\n\u00144 2\n1 3\n\u0015\n. (4.28)\nStep 1: Characteristic Polynomial. From our definition of the eigen-\nvector x \u0338= 0 and eigenvalue \u03bb of A, there will be a vector such that\nAx = \u03bbx, i.e., (A \u2212 \u03bbI)x = 0. Since x \u0338= 0, this requires that the kernel\n(null space) of A \u2212 \u03bbI contains more elements than just 0. This means\nthat A \u2212 \u03bbI is not invertible and therefore det(A \u2212 \u03bbI) = 0. Hence, we\nneed to compute the roots of the characteristic polynomial (4.22a) to find\nthe eigenvalues.\nStep 2: Eigenvalues. The characteristic polynomial is\npA(\u03bb) = det(A \u2212 \u03bbI) (4.29a)\n= det\n\u0012\u00144 2\n1 3\n\u0015\n\u2212\n\u0014\u03bb 0\n0 \u03bb\n\u0015\u0013\n=\n\f\f\f\f\n4 \u2212 \u03bb 2\n1 3 \u2212 \u03bb\n\f\f\f\f (4.29b)\n= (4 \u2212 \u03bb)(3"
  },
  {
    "vector_id": 403,
    "chunk_id": "p113_c1",
    "page_number": 113,
    "text": "omial (4.22a) to find\nthe eigenvalues.\nStep 2: Eigenvalues. The characteristic polynomial is\npA(\u03bb) = det(A \u2212 \u03bbI) (4.29a)\n= det\n\u0012\u00144 2\n1 3\n\u0015\n\u2212\n\u0014\u03bb 0\n0 \u03bb\n\u0015\u0013\n=\n\f\f\f\f\n4 \u2212 \u03bb 2\n1 3 \u2212 \u03bb\n\f\f\f\f (4.29b)\n= (4 \u2212 \u03bb)(3 \u2212 \u03bb) \u2212 2 \u00b7 1 . (4.29c)\nWe factorize the characteristic polynomial and obtain\np(\u03bb) = (4 \u2212 \u03bb)(3 \u2212 \u03bb) \u2212 2 \u00b7 1 = 10 \u2212 7\u03bb + \u03bb2 = (2 \u2212 \u03bb)(5 \u2212 \u03bb) (4.30)\ngiving the roots \u03bb1 = 2 and \u03bb2 = 5.\nStep 3: Eigenvectors and Eigenspaces. We find the eigenvectors that\ncorrespond to these eigenvalues by looking at vectors x such that\n\u00144 \u2212 \u03bb 2\n1 3 \u2212 \u03bb\n\u0015\nx = 0 . (4.31)\nFor \u03bb = 5 we obtain\n\u00144 \u2212 5 2\n1 3 \u2212 5\n\u0015\u0014x1\nx2\n\u0015\n=\n\u0014\u22121 2\n1 \u22122\n\u0015\u0014x1\nx2\n\u0015\n= 0 . (4.32)\nWe solve this homogeneous system and obtain a solution space\nE5 = span[\n\u00142\n1\n\u0015\n] . (4.33)\nThis eigenspace is one-dimensional as it possesses a single basis vector.\nAn"
  },
  {
    "vector_id": 404,
    "chunk_id": "p113_c2",
    "page_number": 113,
    "text": "\u0014\u22121 2\n1 \u22122\n\u0015\u0014x1\nx2\n\u0015\n= 0 . (4.32)\nWe solve this homogeneous system and obtain a solution space\nE5 = span[\n\u00142\n1\n\u0015\n] . (4.33)\nThis eigenspace is one-dimensional as it possesses a single basis vector.\nAnalogously , we find the eigenvector for\u03bb = 2 by solving the homoge-\nneous system of equations\n\u00144 \u2212 2 2\n1 3 \u2212 2\n\u0015\nx =\n\u00142 2\n1 1\n\u0015\nx = 0 . (4.34)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 405,
    "chunk_id": "p114_c0",
    "page_number": 114,
    "text": "108 Matrix Decompositions\nThis means any vector x =\n\u0014x1\nx2\n\u0015\n, where x2 = \u2212x1, such as\n\u0014 1\n\u22121\n\u0015\n, is an\neigenvector with eigenvalue 2. The corresponding eigenspace is given as\nE2 = span[\n\u0014 1\n\u22121\n\u0015\n] . (4.35)\nThe two eigenspaces E5 and E2 in Example 4.5 are one-dimensional\nas they are each spanned by a single vector. However, in other cases\nwe may have multiple identical eigenvalues (see Definition 4.9) and the\neigenspace may have more than one dimension.\nDefinition 4.11. Let \u03bbi be an eigenvalue of a square matrix A. Then the\ngeometric multiplicity of \u03bbi is the number of linearly independent eigen-geometric\nmultiplicity vectors associated with \u03bbi. In other words, it is the dimensionality of the\neigenspace spanned by the eigenvectors associated with \u03bbi.\nRemark. A specific eigenvalue\u2019s geometr"
  },
  {
    "vector_id": 406,
    "chunk_id": "p114_c1",
    "page_number": 114,
    "text": "en-geometric\nmultiplicity vectors associated with \u03bbi. In other words, it is the dimensionality of the\neigenspace spanned by the eigenvectors associated with \u03bbi.\nRemark. A specific eigenvalue\u2019s geometric multiplicity must be at least\none because every eigenvalue has at least one associated eigenvector. An\neigenvalue\u2019s geometric multiplicity cannot exceed its algebraic multiplic-\nity , but it may be lower. \u2662\nExample 4.6\nThe matrix A =\n\u00142 1\n0 2\n\u0015\nhas two repeated eigenvalues\u03bb1 = \u03bb2 = 2 and an\nalgebraic multiplicity of 2. The eigenvalue has, however, only one distinct\nunit eigenvector x1 =\n\u00141\n0\n\u0015\nand, thus, geometric multiplicity 1.\nGraphical Intuition in Two Dimensions\nLet us gain some intuition for determinants, eigenvectors, and eigenval-\nues using different linear mappings. Figure 4.4 depi"
  },
  {
    "vector_id": 407,
    "chunk_id": "p114_c2",
    "page_number": 114,
    "text": "and, thus, geometric multiplicity 1.\nGraphical Intuition in Two Dimensions\nLet us gain some intuition for determinants, eigenvectors, and eigenval-\nues using different linear mappings. Figure 4.4 depicts five transformation\nmatrices A1, . . . ,A5 and their impact on a square grid of points, centered\nat the origin:In geometry , the\narea-preserving\nproperties of this\ntype of shearing\nparallel to an axis is\nalso known as\nCavalieri\u2019s principle\nof equal areas for\nparallelograms\n(Katz, 2004).\nA1 =\n\u00141\n2 0\n0 2\n\u0015\n. The direction of the two eigenvectors correspond to the\ncanonical basis vectors inR2, i.e., to two cardinal axes. The vertical axis\nis extended by a factor of2 (eigenvalue \u03bb1 = 2), and the horizontal axis\nis compressed by factor 1\n2 (eigenvalue \u03bb2 = 1\n2 ). The mapping is area\npreserving"
  },
  {
    "vector_id": 408,
    "chunk_id": "p114_c3",
    "page_number": 114,
    "text": "e., to two cardinal axes. The vertical axis\nis extended by a factor of2 (eigenvalue \u03bb1 = 2), and the horizontal axis\nis compressed by factor 1\n2 (eigenvalue \u03bb2 = 1\n2 ). The mapping is area\npreserving (det(A1) = 1 = 2 \u00b7 1\n2 ).\nA2 =\n\u00141 1\n2\n0 1\n\u0015\ncorresponds to a shearing mapping , i.e., it shears the\npoints along the horizontal axis to the right if they are on the positive\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 409,
    "chunk_id": "p115_c0",
    "page_number": 115,
    "text": "4.2 Eigenvalues and Eigenvectors 109\nFigure 4.4\nDeterminants and\neigenspaces.\nOverview of five\nlinear mappings and\ntheir associated\ntransformation\nmatrices\nAi \u2208 R2\u00d72\nprojecting 400\ncolor-coded points\nx \u2208 R2 (left\ncolumn) onto target\npoints Aix (right\ncolumn). The\ncentral column\ndepicts the first\neigenvector,\nstretched by its\nassociated\neigenvalue \u03bb1, and\nthe second\neigenvector\nstretched by its\neigenvalue \u03bb2. Each\nrow depicts the\neffect of one of five\ntransformation\nmatrices Ai with\nrespect to the\nstandard basis.\ndet(A) = 1.0\n\u03bb1 = 2.0\n\u03bb2 = 0.5\ndet(A) = 1.0\n\u03bb1 = 1.0\n\u03bb2 = 1.0\ndet(A) = 1.0\n\u03bb1 = (0.87-0.5j)\n\u03bb2 = (0.87+0.5j)\ndet(A) = 0.0\n\u03bb1 = 0.0\n\u03bb2 = 2.0\ndet(A) = 0.75\n\u03bb1 = 0.5\n\u03bb2 = 1.5\nhalf of the vertical axis, and to the left vice versa. This mapping is area\npreserving (det(A2) = 1 ). The eig"
  },
  {
    "vector_id": 410,
    "chunk_id": "p115_c1",
    "page_number": 115,
    "text": ".87-0.5j)\n\u03bb2 = (0.87+0.5j)\ndet(A) = 0.0\n\u03bb1 = 0.0\n\u03bb2 = 2.0\ndet(A) = 0.75\n\u03bb1 = 0.5\n\u03bb2 = 1.5\nhalf of the vertical axis, and to the left vice versa. This mapping is area\npreserving (det(A2) = 1 ). The eigenvalue \u03bb1 = 1 = \u03bb2 is repeated\nand the eigenvectors are collinear (drawn here for emphasis in two\nopposite directions). This indicates that the mapping acts only along\none direction (the horizontal axis).\nA3 =\n\u0014cos(\u03c0\n6 ) \u2212sin(\u03c0\n6 )\nsin(\u03c0\n6 ) cos( \u03c0\n6 )\n\u0015\n= 1\n2\n\u0014\u221a\n3 \u22121\n1\n\u221a\n3\n\u0015\nThe matrix A3 rotates the\npoints by \u03c0\n6 rad = 30 \u25e6 counter-clockwise and has only complex eigen-\nvalues, reflecting that the mapping is a rotation (hence, no eigenvectors\nare drawn). A rotation has to be volume preserving, and so the deter-\nminant is 1. For more details on rotations, we refer to Section 3.9.\nA4 =\n\u0014 1 \u22121"
  },
  {
    "vector_id": 411,
    "chunk_id": "p115_c2",
    "page_number": 115,
    "text": "he mapping is a rotation (hence, no eigenvectors\nare drawn). A rotation has to be volume preserving, and so the deter-\nminant is 1. For more details on rotations, we refer to Section 3.9.\nA4 =\n\u0014 1 \u22121\n\u22121 1\n\u0015\nrepresents a mapping in the standard basis that col-\nlapses a two-dimensional domain onto one dimension. Since one eigen-\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 412,
    "chunk_id": "p116_c0",
    "page_number": 116,
    "text": "110 Matrix Decompositions\nvalue is 0, the space in direction of the (blue) eigenvector corresponding\nto \u03bb1 = 0 collapses, while the orthogonal (red) eigenvector stretches\nspace by a factor \u03bb2 = 2. Therefore, the area of the image is 0.\nA5 =\n\u00141 1\n21\n2 1\n\u0015\nis a shear-and-stretch mapping that scales space by 75%\nsince |det(A5)| = 3\n4 . It stretches space along the (red) eigenvector\nof \u03bb2 by a factor 1.5 and compresses it along the orthogonal (blue)\neigenvector by a factor 0.5.\nExample 4.7 (Eigenspectrum of a Biological Neural Network)\nFigure 4.5\nCaenorhabditis\nelegans neural\nnetwork (Kaiser and\nHilgetag,\n2006).(a) Sym-\nmetrized\nconnectivity matrix;\n(b) Eigenspectrum.\n0 50 100 150 200 250\nneuron index\n0\n50\n100\n150\n200\n250 neuron index\n(a) Connectivity matrix.\n0 100 200\nindex of sorted eigenval"
  },
  {
    "vector_id": 413,
    "chunk_id": "p116_c1",
    "page_number": 116,
    "text": "ilgetag,\n2006).(a) Sym-\nmetrized\nconnectivity matrix;\n(b) Eigenspectrum.\n0 50 100 150 200 250\nneuron index\n0\n50\n100\n150\n200\n250 neuron index\n(a) Connectivity matrix.\n0 100 200\nindex of sorted eigenvalue\n\u221210\n\u22125\n0\n5\n10\n15\n20\n25\neigenvalue\n (b) Eigenspectrum.\nMethods to analyze and learn from network data are an essential com-\nponent of machine learning methods. The key to understanding networks\nis the connectivity between network nodes, especially if two nodes are\nconnected to each other or not. In data science applications, it is often\nuseful to study the matrix that captures this connectivity data.\nWe build a connectivity/adjacency matrixA \u2208 R277\u00d7277 of the complete\nneural network of the worm C.Elegans. Each row/column represents one\nof the 277 neurons of this worm\u2019s brain. The connectivit"
  },
  {
    "vector_id": 414,
    "chunk_id": "p116_c2",
    "page_number": 116,
    "text": "data.\nWe build a connectivity/adjacency matrixA \u2208 R277\u00d7277 of the complete\nneural network of the worm C.Elegans. Each row/column represents one\nof the 277 neurons of this worm\u2019s brain. The connectivity matrix A has\na value of aij = 1 if neuron i talks to neuron j through a synapse, and\naij = 0 otherwise. The connectivity matrix is not symmetric, which im-\nplies that eigenvalues may not be real valued. Therefore, we compute a\nsymmetrized version of the connectivity matrix as Asym := A + A\u22a4. This\nnew matrix Asym is shown in Figure 4.5(a) and has a nonzero value aij if\nand only if two neurons are connected (white pixels), irrespective of the\ndirection of the connection. In Figure 4.5(b), we show the correspond-\ning eigenspectrum of Asym. The horizontal axis shows the index of the\neigenvalues,"
  },
  {
    "vector_id": 415,
    "chunk_id": "p116_c3",
    "page_number": 116,
    "text": "connected (white pixels), irrespective of the\ndirection of the connection. In Figure 4.5(b), we show the correspond-\ning eigenspectrum of Asym. The horizontal axis shows the index of the\neigenvalues, sorted in descending order. The vertical axis shows the corre-\nsponding eigenvalue. The S-like shape of this eigenspectrum is typical for\nmany biological neural networks. The underlying mechanism responsible\nfor this is an area of active neuroscience research.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 416,
    "chunk_id": "p117_c0",
    "page_number": 117,
    "text": "4.2 Eigenvalues and Eigenvectors 111\nTheorem 4.12. The eigenvectors x1, . . . ,xn of a matrix A \u2208 Rn\u00d7n with n\ndistinct eigenvalues \u03bb1, . . . , \u03bbn are linearly independent.\nThis theorem states that eigenvectors of a matrix with n distinct eigen-\nvalues form a basis of Rn.\nDefinition 4.13. A square matrix A \u2208 Rn\u00d7n is defective if it possesses defective\nfewer than n linearly independent eigenvectors.\nA non-defective matrix A \u2208 Rn\u00d7n does not necessarily require n dis-\ntinct eigenvalues, but it does require that the eigenvectors form a basis of\nRn. Looking at the eigenspaces of a defective matrix, it follows that the\nsum of the dimensions of the eigenspaces is less than n. Specifically , a de-\nfective matrix has at least one eigenvalue\u03bbi with an algebraic multiplicity\nm >1 and a geometric multi"
  },
  {
    "vector_id": 417,
    "chunk_id": "p117_c1",
    "page_number": 117,
    "text": "it follows that the\nsum of the dimensions of the eigenspaces is less than n. Specifically , a de-\nfective matrix has at least one eigenvalue\u03bbi with an algebraic multiplicity\nm >1 and a geometric multiplicity of less than m.\nRemark. A defective matrix cannot have n distinct eigenvalues, as distinct\neigenvalues have linearly independent eigenvectors (Theorem 4.12). \u2662\nTheorem 4.14. Given a matrix A \u2208 Rm\u00d7n, we can always obtain a sym-\nmetric, positive semidefinite matrix S \u2208 Rn\u00d7n by defining\nS := A\u22a4A. (4.36)\nRemark. If rk(A) = n, then S := A\u22a4A is symmetric, positive definite.\n\u2662\nUnderstanding why Theorem 4.14 holds is insightful for how we can\nuse symmetrized matrices: Symmetry requires S = S\u22a4, and by insert-\ning (4.36) we obtain S = A\u22a4A = A\u22a4(A\u22a4)\u22a4 = (A\u22a4A)\u22a4 = S\u22a4. More-\nover, positive semidefinit"
  },
  {
    "vector_id": 418,
    "chunk_id": "p117_c2",
    "page_number": 117,
    "text": "heorem 4.14 holds is insightful for how we can\nuse symmetrized matrices: Symmetry requires S = S\u22a4, and by insert-\ning (4.36) we obtain S = A\u22a4A = A\u22a4(A\u22a4)\u22a4 = (A\u22a4A)\u22a4 = S\u22a4. More-\nover, positive semidefiniteness (Section 3.2.3) requires that x\u22a4Sx \u2a7e 0\nand inserting (4.36) we obtain x\u22a4Sx = x\u22a4A\u22a4Ax = (x\u22a4A\u22a4)(Ax) =\n(Ax)\u22a4(Ax) \u2a7e 0, because the dot product computes a sum of squares\n(which are themselves non-negative).\nspectral theorem\nTheorem 4.15 (Spectral Theorem). If A \u2208 Rn\u00d7n is symmetric, there ex-\nists an orthonormal basis of the corresponding vector space V consisting of\neigenvectors of A, and each eigenvalue is real.\nA direct implication of the spectral theorem is that the eigendecompo-\nsition of a symmetric matrix A exists (with real eigenvalues), and that\nwe can find an ONB of eigenvectors so th"
  },
  {
    "vector_id": 419,
    "chunk_id": "p117_c3",
    "page_number": 117,
    "text": "envalue is real.\nA direct implication of the spectral theorem is that the eigendecompo-\nsition of a symmetric matrix A exists (with real eigenvalues), and that\nwe can find an ONB of eigenvectors so that A = P DP\u22a4, where D is\ndiagonal and the columns of P contain the eigenvectors.\nExample 4.8\nConsider the matrix\nA =\n\uf8ee\n\uf8f0\n3 2 2\n2 3 2\n2 2 3\n\uf8f9\n\uf8fb . (4.37)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 420,
    "chunk_id": "p118_c0",
    "page_number": 118,
    "text": "112 Matrix Decompositions\nThe characteristic polynomial of A is\npA(\u03bb) = \u2212(\u03bb \u2212 1)2(\u03bb \u2212 7) , (4.38)\nso that we obtain the eigenvalues \u03bb1 = 1 and \u03bb2 = 7 , where \u03bb1 is a\nrepeated eigenvalue. Following our standard procedure for computing\neigenvectors, we obtain the eigenspaces\nE1 = span[\n\uf8ee\n\uf8f0\n\u22121\n1\n0\n\uf8f9\n\uf8fb\n| {z }\n=:x1\n,\n\uf8ee\n\uf8f0\n\u22121\n0\n1\n\uf8f9\n\uf8fb\n| {z }\n=:x2\n], E 7 = span[\n\uf8ee\n\uf8f0\n1\n1\n1\n\uf8f9\n\uf8fb\n|{z}\n=:x3\n] . (4.39)\nWe see that x3 is orthogonal to both x1 and x2. However, since x\u22a4\n1 x2 =\n1 \u0338= 0 , they are not orthogonal. The spectral theorem (Theorem 4.15)\nstates that there exists an orthogonal basis, but the one we have is not\northogonal. However, we can construct one.\nTo construct such a basis, we exploit the fact that x1, x2 are eigenvec-\ntors associated with the same eigenvalue \u03bb. Therefore, for any \u03b1, \u03b2\u2208 R it\nhol"
  },
  {
    "vector_id": 421,
    "chunk_id": "p118_c1",
    "page_number": 118,
    "text": "is not\northogonal. However, we can construct one.\nTo construct such a basis, we exploit the fact that x1, x2 are eigenvec-\ntors associated with the same eigenvalue \u03bb. Therefore, for any \u03b1, \u03b2\u2208 R it\nholds that\nA(\u03b1x1 + \u03b2x2) = Ax1\u03b1 + Ax2\u03b2 = \u03bb(\u03b1x1 + \u03b2x2) , (4.40)\ni.e., any linear combination of x1 and x2 is also an eigenvector of A as-\nsociated with \u03bb. The Gram-Schmidt algorithm (Section 3.8.3) is a method\nfor iteratively constructing an orthogonal/orthonormal basis from a set of\nbasis vectors using such linear combinations. Therefore, even ifx1 and x2\nare not orthogonal, we can apply the Gram-Schmidt algorithm and find\neigenvectors associated with \u03bb1 = 1 that are orthogonal to each other\n(and to x3). In our example, we will obtain\nx\u2032\n1 =\n\uf8ee\n\uf8f0\n\u22121\n1\n0\n\uf8f9\n\uf8fb, x\u2032\n2 = 1\n2\n\uf8ee\n\uf8f0\n\u22121\n\u22121\n2\n\uf8f9\n\uf8fb , (4.41)\nwhic"
  },
  {
    "vector_id": 422,
    "chunk_id": "p118_c2",
    "page_number": 118,
    "text": "idt algorithm and find\neigenvectors associated with \u03bb1 = 1 that are orthogonal to each other\n(and to x3). In our example, we will obtain\nx\u2032\n1 =\n\uf8ee\n\uf8f0\n\u22121\n1\n0\n\uf8f9\n\uf8fb, x\u2032\n2 = 1\n2\n\uf8ee\n\uf8f0\n\u22121\n\u22121\n2\n\uf8f9\n\uf8fb , (4.41)\nwhich are orthogonal to each other, orthogonal tox3, and eigenvectors of\nA associated with \u03bb1 = 1.\nBefore we conclude our considerations of eigenvalues and eigenvectors\nit is useful to tie these matrix characteristics together with the concepts of\nthe determinant and the trace.\nTheorem 4.16. The determinant of a matrix A \u2208 Rn\u00d7n is the product of\nits eigenvalues, i.e.,\ndet(A) =\nnY\ni=1\n\u03bbi , (4.42)\nwhere \u03bbi \u2208 C are (possibly repeated) eigenvalues of A.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 423,
    "chunk_id": "p118_c3",
    "page_number": 118,
    "text": "\u03bbi \u2208 C are (possibly repeated) eigenvalues of A.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 424,
    "chunk_id": "p119_c0",
    "page_number": 119,
    "text": "4.2 Eigenvalues and Eigenvectors 113\nFigure 4.6\nGeometric\ninterpretation of\neigenvalues. The\neigenvectors of A\nget stretched by the\ncorresponding\neigenvalues. The\narea of the unit\nsquare changes by\n|\u03bb1\u03bb2|, the\nperimeter changes\nby a factor of\n1\n2 (|\u03bb1| + |\u03bb2|).\nx1\nx2\nv1\nv2\nA\nTheorem 4.17. The trace of a matrix A \u2208 Rn\u00d7n is the sum of its eigenval-\nues, i.e.,\ntr(A) =\nnX\ni=1\n\u03bbi , (4.43)\nwhere \u03bbi \u2208 C are (possibly repeated) eigenvalues of A.\nLet us provide a geometric intuition of these two theorems. Consider\na matrix A \u2208 R2\u00d72 that possesses two linearly independent eigenvectors\nx1, x2. For this example, we assume(x1, x2) are an ONB ofR2 so that they\nare orthogonal and the area of the square they span is 1; see Figure 4.6.\nFrom Section 4.1, we know that the determinant computes the change of\na"
  },
  {
    "vector_id": 425,
    "chunk_id": "p119_c1",
    "page_number": 119,
    "text": "xample, we assume(x1, x2) are an ONB ofR2 so that they\nare orthogonal and the area of the square they span is 1; see Figure 4.6.\nFrom Section 4.1, we know that the determinant computes the change of\narea of unit square under the transformation A. In this example, we can\ncompute the change of area explicitly: Mapping the eigenvectors using\nA gives us vectors v1 = Ax1 = \u03bb1x1 and v2 = Ax2 = \u03bb2x2, i.e., the\nnew vectors vi are scaled versions of the eigenvectors xi, and the scaling\nfactors are the corresponding eigenvalues \u03bbi. v1, v2 are still orthogonal,\nand the area of the rectangle they span is |\u03bb1\u03bb2|.\nGiven that x1, x2 (in our example) are orthonormal, we can directly\ncompute the perimeter of the unit square as 2(1 + 1). Mapping the eigen-\nvectors using A creates a rectangle whose perimeter"
  },
  {
    "vector_id": 426,
    "chunk_id": "p119_c2",
    "page_number": 119,
    "text": "|\u03bb1\u03bb2|.\nGiven that x1, x2 (in our example) are orthonormal, we can directly\ncompute the perimeter of the unit square as 2(1 + 1). Mapping the eigen-\nvectors using A creates a rectangle whose perimeter is 2(|\u03bb1| + |\u03bb2|).\nTherefore, the sum of the absolute values of the eigenvalues tells us how\nthe perimeter of the unit square changes under the transformation matrix\nA.\nExample 4.9 (Google\u2019s PageRank \u2013 Webpages as Eigenvectors)\nGoogle uses the eigenvector corresponding to the maximal eigenvalue of\na matrix A to determine the rank of a page for search. The idea for the\nPageRank algorithm, developed at Stanford University by Larry Page and\nSergey Brin in 1996, was that the importance of any web page can be ap-\nproximated by the importance of pages that link to it. For this, they write\ndown all"
  },
  {
    "vector_id": 427,
    "chunk_id": "p119_c3",
    "page_number": 119,
    "text": "d at Stanford University by Larry Page and\nSergey Brin in 1996, was that the importance of any web page can be ap-\nproximated by the importance of pages that link to it. For this, they write\ndown all web sites as a huge directed graph that shows which page links\nto which. PageRank computes the weight (importance) xi \u2a7e 0 of a web\nsite ai by counting the number of pages pointing to ai. Moreover, PageR-\nank takes into account the importance of the web sites that link toai. The\nnavigation behavior of a user is then modeled by a transition matrix A of\nthis graph that tells us with what (click) probability somebody will end up\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 428,
    "chunk_id": "p119_c4",
    "page_number": 119,
    "text": "ability somebody will end up\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 429,
    "chunk_id": "p120_c0",
    "page_number": 120,
    "text": "114 Matrix Decompositions\non a different web site. The matrix A has the property that for any ini-\ntial rank/importance vector x of a web site the sequence x, Ax, A2x, . . .\nconverges to a vector x\u2217. This vector is called the PageRank and satisfiesPageRank\nAx\u2217 = x\u2217, i.e., it is an eigenvector (with corresponding eigenvalue 1) of\nA. After normalizing x\u2217, such that \u2225x\u2217\u2225 = 1, we can interpret the entries\nas probabilities. More details and different perspectives on PageRank can\nbe found in the original technical report (Page et al., 1999).\n4.3 Cholesky Decomposition\nThere are many ways to factorize special types of matrices that we en-\ncounter often in machine learning. In the positive real numbers, we have\nthe square-root operation that gives us a decomposition of the number\ninto identical co"
  },
  {
    "vector_id": 430,
    "chunk_id": "p120_c1",
    "page_number": 120,
    "text": "e special types of matrices that we en-\ncounter often in machine learning. In the positive real numbers, we have\nthe square-root operation that gives us a decomposition of the number\ninto identical components, e.g., 9 = 3 \u00b7 3. For matrices, we need to be\ncareful that we compute a square-root-like operation on positive quanti-\nties. For symmetric, positive definite matrices (see Section 3.2.3), we can\nchoose from a number of square-root equivalent operations. The CholeskyCholesky\ndecomposition decomposition/Cholesky factorization provides a square-root equivalent op-\nCholesky\nfactorization\neration on symmetric, positive definite matrices that is useful in practice.\nTheorem 4.18 (Cholesky Decomposition). A symmetric, positive definite\nmatrix A can be factorized into a product A = LL\u22a4, where"
  },
  {
    "vector_id": 431,
    "chunk_id": "p120_c2",
    "page_number": 120,
    "text": "ion on symmetric, positive definite matrices that is useful in practice.\nTheorem 4.18 (Cholesky Decomposition). A symmetric, positive definite\nmatrix A can be factorized into a product A = LL\u22a4, where L is a lower-\ntriangular matrix with positive diagonal elements:\n\uf8ee\n\uf8ef\uf8f0\na11 \u00b7 \u00b7\u00b7 a1n\n... ... ...\nan1 \u00b7 \u00b7\u00b7 ann\n\uf8f9\n\uf8fa\uf8fb =\n\uf8ee\n\uf8ef\uf8f0\nl11 \u00b7 \u00b7\u00b7 0\n... ... ...\nln1 \u00b7 \u00b7\u00b7 lnn\n\uf8f9\n\uf8fa\uf8fb\n\uf8ee\n\uf8ef\uf8f0\nl11 \u00b7 \u00b7\u00b7 ln1\n... ... ...\n0 \u00b7 \u00b7\u00b7 lnn\n\uf8f9\n\uf8fa\uf8fb . (4.44)\nL is called the Cholesky factor of A, and L is unique.Cholesky factor\nExample 4.10 (Cholesky Factorization)\nConsider a symmetric, positive definite matrix A \u2208 R3\u00d73. We are inter-\nested in finding its Cholesky factorization A = LL\u22a4, i.e.,\nA =\n\uf8ee\n\uf8f0\na11 a21 a31\na21 a22 a32\na31 a32 a33\n\uf8f9\n\uf8fb = LL\u22a4 =\n\uf8ee\n\uf8f0\nl11 0 0\nl21 l22 0\nl31 l32 l33\n\uf8f9\n\uf8fb\n\uf8ee\n\uf8f0\nl11 l21 l31\n0 l22 l32\n0 0 l33\n\uf8f9\n\uf8fb . (4.45)\nMulti"
  },
  {
    "vector_id": 432,
    "chunk_id": "p120_c3",
    "page_number": 120,
    "text": "in finding its Cholesky factorization A = LL\u22a4, i.e.,\nA =\n\uf8ee\n\uf8f0\na11 a21 a31\na21 a22 a32\na31 a32 a33\n\uf8f9\n\uf8fb = LL\u22a4 =\n\uf8ee\n\uf8f0\nl11 0 0\nl21 l22 0\nl31 l32 l33\n\uf8f9\n\uf8fb\n\uf8ee\n\uf8f0\nl11 l21 l31\n0 l22 l32\n0 0 l33\n\uf8f9\n\uf8fb . (4.45)\nMultiplying out the right-hand side yields\nA =\n\uf8ee\n\uf8f0\nl2\n11 l21l11 l31l11\nl21l11 l2\n21 + l2\n22 l31l21 + l32l22\nl31l11 l31l21 + l32l22 l2\n31 + l2\n32 + l2\n33\n\uf8f9\n\uf8fb . (4.46)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 433,
    "chunk_id": "p121_c0",
    "page_number": 121,
    "text": "4.4 Eigendecomposition and Diagonalization 115\nComparing the left-hand side of (4.45) and the right-hand side of (4.46)\nshows that there is a simple pattern in the diagonal elements lii:\nl11 = \u221aa11 , l 22 =\nq\na22 \u2212 l2\n21 , l 33 =\nq\na33 \u2212 (l2\n31 + l2\n32) . (4.47)\nSimilarly for the elements below the diagonal ( lij, where i > j), there is\nalso a repeating pattern:\nl21 = 1\nl11\na21 , l 31 = 1\nl11\na31 , l 32 = 1\nl22\n(a32 \u2212 l31l21) . (4.48)\nThus, we constructed the Cholesky decomposition for any symmetric, pos-\nitive definite 3 \u00d7 3 matrix. The key realization is that we can backward\ncalculate what the components lij for the L should be, given the values\naij for A and previously computed values of lij.\nThe Cholesky decomposition is an important tool for the numerical\ncomputations underlying machi"
  },
  {
    "vector_id": 434,
    "chunk_id": "p121_c1",
    "page_number": 121,
    "text": "he components lij for the L should be, given the values\naij for A and previously computed values of lij.\nThe Cholesky decomposition is an important tool for the numerical\ncomputations underlying machine learning. Here, symmetric positive def-\ninite matrices require frequent manipulation, e.g., the covariance matrix\nof a multivariate Gaussian variable (see Section 6.5) is symmetric, positive\ndefinite. The Cholesky factorization of this covariance matrix allows us to\ngenerate samples from a Gaussian distribution. It also allows us to perform\na linear transformation of random variables, which is heavily exploited\nwhen computing gradients in deep stochastic models, such as the varia-\ntional auto-encoder (Jimenez Rezende et al., 2014; Kingma and Welling,\n2014). The Cholesky decomposition also a"
  },
  {
    "vector_id": 435,
    "chunk_id": "p121_c2",
    "page_number": 121,
    "text": "heavily exploited\nwhen computing gradients in deep stochastic models, such as the varia-\ntional auto-encoder (Jimenez Rezende et al., 2014; Kingma and Welling,\n2014). The Cholesky decomposition also allows us to compute determi-\nnants very efficiently . Given the Cholesky decomposition A = LL\u22a4, we\nknow that det(A) = det( L) det(L\u22a4) = det( L)2. Since L is a triangular\nmatrix, the determinant is simply the product of its diagonal entries so\nthat det(A) = Q\ni l2\nii. Thus, many numerical software packages use the\nCholesky decomposition to make computations more efficient.\n4.4 Eigendecomposition and Diagonalization\nA diagonal matrix is a matrix that has value zero on all off-diagonal ele- diagonal matrix\nments, i.e., they are of the form\nD =\n\uf8ee\n\uf8ef\uf8f0\nc1 \u00b7 \u00b7\u00b7 0\n... ... ...\n0 \u00b7 \u00b7\u00b7 cn\n\uf8f9\n\uf8fa\uf8fb. (4.49)\nThe"
  },
  {
    "vector_id": 436,
    "chunk_id": "p121_c3",
    "page_number": 121,
    "text": "d Diagonalization\nA diagonal matrix is a matrix that has value zero on all off-diagonal ele- diagonal matrix\nments, i.e., they are of the form\nD =\n\uf8ee\n\uf8ef\uf8f0\nc1 \u00b7 \u00b7\u00b7 0\n... ... ...\n0 \u00b7 \u00b7\u00b7 cn\n\uf8f9\n\uf8fa\uf8fb. (4.49)\nThey allow fast computation of determinants, powers, and inverses. The\ndeterminant is the product of its diagonal entries, a matrix power Dk is\ngiven by each diagonal element raised to the power k, and the inverse\nD\u22121 is the reciprocal of its diagonal elements if all of them are nonzero.\nIn this section, we will discuss how to transform matrices into diagonal\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 437,
    "chunk_id": "p121_c4",
    "page_number": 121,
    "text": "S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 438,
    "chunk_id": "p122_c0",
    "page_number": 122,
    "text": "116 Matrix Decompositions\nform. This is an important application of the basis change we discussed in\nSection 2.7.2 and eigenvalues from Section 4.2.\nRecall that two matrices A, D are similar (Definition 2.22) if there ex-\nists an invertible matrix P, such that D = P\u22121AP. More specifically , we\nwill look at matrices A that are similar to diagonal matrices D that con-\ntain the eigenvalues of A on the diagonal.\nDefinition 4.19 (Diagonalizable). A matrix A \u2208 Rn\u00d7n is diagonalizablediagonalizable\nif it is similar to a diagonal matrix, i.e., if there exists an invertible matrix\nP \u2208 Rn\u00d7n such that D = P\u22121AP.\nIn the following, we will see that diagonalizing a matrix A \u2208 Rn\u00d7n is\na way of expressing the same linear mapping but in another basis (see\nSection 2.6.1), which will turn out to be a basis th"
  },
  {
    "vector_id": 439,
    "chunk_id": "p122_c1",
    "page_number": 122,
    "text": "P\u22121AP.\nIn the following, we will see that diagonalizing a matrix A \u2208 Rn\u00d7n is\na way of expressing the same linear mapping but in another basis (see\nSection 2.6.1), which will turn out to be a basis that consists of the eigen-\nvectors of A.\nLet A \u2208 Rn\u00d7n, let \u03bb1, . . . , \u03bbn be a set of scalars, and let p1, . . . ,pn be a\nset of vectors in Rn. We define P := [p1, . . . ,pn] and let D \u2208 Rn\u00d7n be a\ndiagonal matrix with diagonal entries \u03bb1, . . . , \u03bbn. Then we can show that\nAP = P D (4.50)\nif and only if \u03bb1, . . . , \u03bbn are the eigenvalues of A and p1, . . . ,pn are cor-\nresponding eigenvectors of A.\nWe can see that this statement holds because\nAP = A[p1, . . . ,pn] = [Ap1, . . . ,Apn] , (4.51)\nP D= [p1, . . . ,pn]\n\uf8ee\n\uf8ef\uf8f0\n\u03bb1 0\n...\n0 \u03bbn\n\uf8f9\n\uf8fa\uf8fb = [\u03bb1p1, . . . , \u03bbnpn] . (4.52)\nThus, (4.50) implies that\nA"
  },
  {
    "vector_id": 440,
    "chunk_id": "p122_c2",
    "page_number": 122,
    "text": "We can see that this statement holds because\nAP = A[p1, . . . ,pn] = [Ap1, . . . ,Apn] , (4.51)\nP D= [p1, . . . ,pn]\n\uf8ee\n\uf8ef\uf8f0\n\u03bb1 0\n...\n0 \u03bbn\n\uf8f9\n\uf8fa\uf8fb = [\u03bb1p1, . . . , \u03bbnpn] . (4.52)\nThus, (4.50) implies that\nAp1 = \u03bb1p1 (4.53)\n...\nApn = \u03bbnpn . (4.54)\nTherefore, the columns of P must be eigenvectors of A.\nOur definition of diagonalization requires that P \u2208 Rn\u00d7n is invertible,\ni.e., P has full rank (Theorem 4.3). This requires us to have n linearly\nindependent eigenvectors p1, . . . ,pn, i.e., the pi form a basis of Rn.\nTheorem 4.20 (Eigendecomposition). A square matrix A \u2208 Rn\u00d7n can be\nfactored into\nA = P DP\u22121 , (4.55)\nwhere P \u2208 Rn\u00d7n and D is a diagonal matrix whose diagonal entries are\nthe eigenvalues of A, if and only if the eigenvectors of A form a basis of Rn.\nDraft (2024-01-15) of \u201cMathematics fo"
  },
  {
    "vector_id": 441,
    "chunk_id": "p122_c3",
    "page_number": 122,
    "text": "DP\u22121 , (4.55)\nwhere P \u2208 Rn\u00d7n and D is a diagonal matrix whose diagonal entries are\nthe eigenvalues of A, if and only if the eigenvectors of A form a basis of Rn.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 442,
    "chunk_id": "p123_c0",
    "page_number": 123,
    "text": "4.4 Eigendecomposition and Diagonalization 117\nFigure 4.7 Intuition\nbehind the\neigendecomposition\nas sequential\ntransformations.\nTop-left to\nbottom-left: P\u22121\nperforms a basis\nchange (here drawn\nin R2 and depicted\nas a rotation-like\noperation) from the\nstandard basis into\nthe eigenbasis.\nBottom-left to\nbottom-right: D\nperforms a scaling\nalong the remapped\northogonal\neigenvectors,\ndepicted here by a\ncircle being\nstretched to an\nellipse. Bottom-right\nto top-right: P\nundoes the basis\nchange (depicted as\na reverse rotation)\nand restores the\noriginal coordinate\nframe.\ne1\ne2\np1\np2\np1\np2\ne1e2\np1\np2\n\u03bb1p1\n\u03bb2p2\ne1\ne2\nAe1\nAe2\nP\u22121\nD\nP\nA\nTheorem 4.20 implies that only non-defective matrices can be diagonal-\nized and that the columns ofP are the n eigenvectors of A. For symmetric\nmatrices we can obtain e"
  },
  {
    "vector_id": 443,
    "chunk_id": "p123_c1",
    "page_number": 123,
    "text": "p1\n\u03bb2p2\ne1\ne2\nAe1\nAe2\nP\u22121\nD\nP\nA\nTheorem 4.20 implies that only non-defective matrices can be diagonal-\nized and that the columns ofP are the n eigenvectors of A. For symmetric\nmatrices we can obtain even stronger outcomes for the eigenvalue decom-\nposition.\nTheorem 4.21. A symmetric matrix S \u2208 Rn\u00d7n can always be diagonalized.\nTheorem 4.21 follows directly from the spectral theorem 4.15. More-\nover, the spectral theorem states that we can find an ONB of eigenvectors\nof Rn. This makes P an orthogonal matrix so that D = P\u22a4AP.\nRemark. The Jordan normal form of a matrix offers a decomposition that\nworks for defective matrices (Lang, 1987) but is beyond the scope of this\nbook. \u2662\nGeometric Intuition for the Eigendecomposition\nWe can interpret the eigendecomposition of a matrix as follows (see als"
  },
  {
    "vector_id": 444,
    "chunk_id": "p123_c2",
    "page_number": 123,
    "text": "works for defective matrices (Lang, 1987) but is beyond the scope of this\nbook. \u2662\nGeometric Intuition for the Eigendecomposition\nWe can interpret the eigendecomposition of a matrix as follows (see also\nFigure 4.7): Let A be the transformation matrix of a linear mapping with\nrespect to the standard basis ei (blue arrows). P\u22121 performs a basis\nchange from the standard basis into the eigenbasis. Then, the diagonal\nD scales the vectors along these axes by the eigenvalues \u03bbi. Finally ,P\ntransforms these scaled vectors back into the standard/canonical coordi-\nnates yielding \u03bbipi.\nExample 4.11 (Eigendecomposition)\nLet us compute the eigendecomposition of A = 1\n2\n\u0014 5 \u22122\n\u22122 5\n\u0015\n.\nStep 1: Compute eigenvalues and eigenvectors. The characteristic\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Publis"
  },
  {
    "vector_id": 445,
    "chunk_id": "p123_c3",
    "page_number": 123,
    "text": "decomposition)\nLet us compute the eigendecomposition of A = 1\n2\n\u0014 5 \u22122\n\u22122 5\n\u0015\n.\nStep 1: Compute eigenvalues and eigenvectors. The characteristic\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 446,
    "chunk_id": "p124_c0",
    "page_number": 124,
    "text": "118 Matrix Decompositions\npolynomial of A is\ndet(A \u2212 \u03bbI) = det\n\u0012\u00145\n2 \u2212 \u03bb \u22121\n\u22121 5\n2 \u2212 \u03bb\n\u0015\u0013\n(4.56a)\n= (5\n2 \u2212 \u03bb)2 \u2212 1 = \u03bb2 \u2212 5\u03bb + 21\n4 = (\u03bb \u2212 7\n2 )(\u03bb \u2212 3\n2 ) . (4.56b)\nTherefore, the eigenvalues of A are \u03bb1 = 7\n2 and \u03bb2 = 3\n2 (the roots of the\ncharacteristic polynomial), and the associated (normalized) eigenvectors\nare obtained via\nAp1 = 7\n2p1 , Ap2 = 3\n2p2 . (4.57)\nThis yields\np1 = 1\u221a\n2\n\u0014 1\n\u22121\n\u0015\n, p2 = 1\u221a\n2\n\u00141\n1\n\u0015\n. (4.58)\nStep 2: Check for existence. The eigenvectors p1, p2 form a basis of R2.\nTherefore, A can be diagonalized.\nStep 3: Construct the matrix P to diagonalize A. We collect the eigen-\nvectors of A in P so that\nP = [p1, p2] = 1\u221a\n2\n\u0014 1 1\n\u22121 1\n\u0015\n. (4.59)\nWe then obtain\nP\u22121AP =\n\u00147\n2 0\n0 3\n2\n\u0015\n= D . (4.60)\nEquivalently , we get (exploiting that P\u22121 = P\u22a4 since the eigenvectorsFigure 4"
  },
  {
    "vector_id": 447,
    "chunk_id": "p124_c1",
    "page_number": 124,
    "text": "n-\nvectors of A in P so that\nP = [p1, p2] = 1\u221a\n2\n\u0014 1 1\n\u22121 1\n\u0015\n. (4.59)\nWe then obtain\nP\u22121AP =\n\u00147\n2 0\n0 3\n2\n\u0015\n= D . (4.60)\nEquivalently , we get (exploiting that P\u22121 = P\u22a4 since the eigenvectorsFigure 4.7 visualizes\nthe\neigendecomposition\nof A =\n\u0014 5 \u22122\n\u22122 5\n\u0015\nas a sequence of\nlinear\ntransformations.\np1 and p2 in this example form an ONB)\n1\n2\n\u0014 5 \u22122\n\u22122 5\n\u0015\n| {z }\nA\n= 1\u221a\n2\n\u0014 1 1\n\u22121 1\n\u0015\n| {z }\nP\n\u00147\n2 0\n0 3\n2\n\u0015\n| {z }\nD\n1\u221a\n2\n\u00141 \u22121\n1 1\n\u0015\n| {z }\nP\u22121\n. (4.61)\nDiagonal matrices D can efficiently be raised to a power. Therefore,\nwe can find a matrix power for a matrix A \u2208 Rn\u00d7n via the eigenvalue\ndecomposition (if it exists) so that\nAk = (P DP\u22121)k = P DkP\u22121 . (4.62)\nComputing Dk is efficient because we apply this operation individually\nto any diagonal element.\nAssume that the eigendecomposition A = P"
  },
  {
    "vector_id": 448,
    "chunk_id": "p124_c2",
    "page_number": 124,
    "text": "ition (if it exists) so that\nAk = (P DP\u22121)k = P DkP\u22121 . (4.62)\nComputing Dk is efficient because we apply this operation individually\nto any diagonal element.\nAssume that the eigendecomposition A = P DP\u22121 exists. Then,\ndet(A) = det(P DP\u22121) = det(P) det(D) det(P\u22121) (4.63a)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 449,
    "chunk_id": "p125_c0",
    "page_number": 125,
    "text": "4.5 Singular Value Decomposition 119\n= det(D) =\nY\ni\ndii (4.63b)\nallows for an efficient computation of the determinant of A.\nThe eigenvalue decomposition requires square matrices. It would be\nuseful to perform a decomposition on general matrices. In the next sec-\ntion, we introduce a more general matrix decomposition technique, the\nsingular value decomposition.\n4.5 Singular Value Decomposition\nThe singular value decomposition (SVD) of a matrix is a central matrix\ndecomposition method in linear algebra. It has been referred to as the\n\u201cfundamental theorem of linear algebra\u201d (Strang, 1993) because it can be\napplied to all matrices, not only to square matrices, and it always exists.\nMoreover, as we will explore in the following, the SVD of a matrix A,\nwhich represents a linear mapping \u03a6 : V \u2192"
  },
  {
    "vector_id": 450,
    "chunk_id": "p125_c1",
    "page_number": 125,
    "text": "e it can be\napplied to all matrices, not only to square matrices, and it always exists.\nMoreover, as we will explore in the following, the SVD of a matrix A,\nwhich represents a linear mapping \u03a6 : V \u2192 W, quantifies the change\nbetween the underlying geometry of these two vector spaces. We recom-\nmend the work by Kalman (1996) and Roy and Banerjee (2014) for a\ndeeper overview of the mathematics of the SVD.\nSVD theorem\nTheorem 4.22 (SVD Theorem). Let A \u2208 Rm\u00d7n be a rectangular matrix of\nrank r \u2208 [0, min(m, n)]. The SVD of A is a decomposition of the form SVD\nsingular value\ndecomposition\n= UA V\u22a4\u03a3\nm\nn\nm\nm\nm\nn\nn\nn\n(4.64)\nwith an orthogonal matrixU \u2208 Rm\u00d7m with column vectorsui, i = 1, . . . , m,\nand an orthogonal matrix V \u2208 Rn\u00d7n with column vectors vj, j = 1, . . . , n.\nMoreover, \u03a3 is an m \u00d7 n matr"
  },
  {
    "vector_id": 451,
    "chunk_id": "p125_c2",
    "page_number": 125,
    "text": "n\nm\nm\nm\nn\nn\nn\n(4.64)\nwith an orthogonal matrixU \u2208 Rm\u00d7m with column vectorsui, i = 1, . . . , m,\nand an orthogonal matrix V \u2208 Rn\u00d7n with column vectors vj, j = 1, . . . , n.\nMoreover, \u03a3 is an m \u00d7 n matrix with \u03a3ii = \u03c3i \u2a7e 0 and \u03a3ij = 0, i\u0338= j.\nThe diagonal entries \u03c3i, i = 1, . . . , r, of \u03a3 are called the singular values, singular values\nui are called the left-singular vectors, and vj are called the right-singular left-singular vectors\nright-singular\nvectors\nvectors. By convention, the singular values are ordered, i.e., \u03c31 \u2a7e \u03c32 \u2a7e\n\u03c3r \u2a7e 0.\nThe singular value matrix \u03a3 is unique, but it requires some attention. singular value\nmatrixObserve that the \u03a3 \u2208 Rm\u00d7n is rectangular. In particular,\u03a3 is of the same\nsize as A. This means that \u03a3 has a diagonal submatrix that contains the\nsingular values and ne"
  },
  {
    "vector_id": 452,
    "chunk_id": "p125_c3",
    "page_number": 125,
    "text": "attention. singular value\nmatrixObserve that the \u03a3 \u2208 Rm\u00d7n is rectangular. In particular,\u03a3 is of the same\nsize as A. This means that \u03a3 has a diagonal submatrix that contains the\nsingular values and needs additional zero padding. Specifically , ifm > n,\nthen the matrix \u03a3 has diagonal structure up to row n and then consists of\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 453,
    "chunk_id": "p126_c0",
    "page_number": 126,
    "text": "120 Matrix Decompositions\nFigure 4.8 Intuition\nbehind the SVD of a\nmatrix A \u2208 R3\u00d72\nas sequential\ntransformations.\nTop-left to\nbottom-left: V \u22a4\nperforms a basis\nchange in R2.\nBottom-left to\nbottom-right: \u03a3\nscales and maps\nfrom R2 to R3. The\nellipse in the\nbottom-right lives in\nR3. The third\ndimension is\northogonal to the\nsurface of the\nelliptical disk.\nBottom-right to\ntop-right: U\nperforms a basis\nchange within R3.\nv2\nv1\n \u03c32u2\n\u03c31u1\ne2\ne1\n\u03c32e2\n\u03c31e1\nA\nV\u22a4\n\u03a3\nU\n0\u22a4 row vectors from n + 1 to m below so that\n\u03a3 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u03c31 0 0\n0 ... 0\n0 0 \u03c3n\n0 . . . 0\n... ...\n0 . . . 0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n. (4.65)\nIf m < n, the matrix \u03a3 has a diagonal structure up to column m and\ncolumns that consist of 0 from m + 1 to n:\n\u03a3 =\n\uf8ee\n\uf8ef\uf8f0\n\u03c31 0 0 0 . . .0\n0 ... 0 ... ...\n0 0 \u03c3m 0 . . .0\n\uf8f9\n\uf8fa\uf8fb . (4.66)\nRemark. The SVD exists for"
  },
  {
    "vector_id": 454,
    "chunk_id": "p126_c1",
    "page_number": 126,
    "text": ", the matrix \u03a3 has a diagonal structure up to column m and\ncolumns that consist of 0 from m + 1 to n:\n\u03a3 =\n\uf8ee\n\uf8ef\uf8f0\n\u03c31 0 0 0 . . .0\n0 ... 0 ... ...\n0 0 \u03c3m 0 . . .0\n\uf8f9\n\uf8fa\uf8fb . (4.66)\nRemark. The SVD exists for any matrix A \u2208 Rm\u00d7n. \u2662\n4.5.1 Geometric Intuitions for the SVD\nThe SVD offers geometric intuitions to describe a transformation matrix\nA. In the following, we will discuss the SVD as sequential linear trans-\nformations performed on the bases. In Example 4.12, we will then apply\ntransformation matrices of the SVD to a set of vectors inR2, which allows\nus to visualize the effect of each transformation more clearly .\nThe SVD of a matrix can be interpreted as a decomposition of a corre-\nsponding linear mapping (recall Section 2.7.1) \u03a6 : Rn \u2192 Rm into three\noperations; see Figure 4.8. The SVD intuiti"
  },
  {
    "vector_id": 455,
    "chunk_id": "p126_c2",
    "page_number": 126,
    "text": "n more clearly .\nThe SVD of a matrix can be interpreted as a decomposition of a corre-\nsponding linear mapping (recall Section 2.7.1) \u03a6 : Rn \u2192 Rm into three\noperations; see Figure 4.8. The SVD intuition follows superficially a simi-\nlar structure to our eigendecomposition intuition, see Figure 4.7: Broadly\nspeaking, the SVD performs a basis change via V \u22a4 followed by a scal-\ning and augmentation (or reduction) in dimensionality via the singular\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 456,
    "chunk_id": "p127_c0",
    "page_number": 127,
    "text": "4.5 Singular Value Decomposition 121\nvalue matrix \u03a3. Finally , it performs a second basis change viaU. The SVD\nentails a number of important details and caveats, which is why we will\nreview our intuition in more detail. It is useful to review\nbasis changes\n(Section 2.7.2),\northogonal matrices\n(Definition 3.8) and\northonormal bases\n(Section 3.5).\nAssume we are given a transformation matrix of a linear mapping \u03a6 :\nRn \u2192 Rm with respect to the standard bases B and C of Rn and Rm,\nrespectively . Moreover, assume a second basis\u02dcB of Rn and \u02dcC of Rm. Then\n1. The matrix V performs a basis change in the domain Rn from \u02dcB (rep-\nresented by the red and orange vectors v1 and v2 in the top-left of Fig-\nure 4.8) to the standard basis B. V \u22a4 = V \u22121 performs a basis change\nfrom B to \u02dcB. The red and orange"
  },
  {
    "vector_id": 457,
    "chunk_id": "p127_c1",
    "page_number": 127,
    "text": "e domain Rn from \u02dcB (rep-\nresented by the red and orange vectors v1 and v2 in the top-left of Fig-\nure 4.8) to the standard basis B. V \u22a4 = V \u22121 performs a basis change\nfrom B to \u02dcB. The red and orange vectors are now aligned with the\ncanonical basis in the bottom-left of Figure 4.8.\n2. Having changed the coordinate system to \u02dcB, \u03a3 scales the new coordi-\nnates by the singular values \u03c3i (and adds or deletes dimensions), i.e.,\n\u03a3 is the transformation matrix of \u03a6 with respect to \u02dcB and \u02dcC, rep-\nresented by the red and orange vectors being stretched and lying in\nthe e1-e2 plane, which is now embedded in a third dimension in the\nbottom-right of Figure 4.8.\n3. U performs a basis change in the codomainRm from \u02dcC into the canoni-\ncal basis of Rm, represented by a rotation of the red and orange vect"
  },
  {
    "vector_id": 458,
    "chunk_id": "p127_c2",
    "page_number": 127,
    "text": "ed in a third dimension in the\nbottom-right of Figure 4.8.\n3. U performs a basis change in the codomainRm from \u02dcC into the canoni-\ncal basis of Rm, represented by a rotation of the red and orange vectors\nout of the e1-e2 plane. This is shown in the top-right of Figure 4.8.\nThe SVD expresses a change of basis in both the domain and codomain.\nThis is in contrast with the eigendecomposition that operates within the\nsame vector space, where the same basis change is applied and then un-\ndone. What makes the SVD special is that these two different bases are\nsimultaneously linked by the singular value matrix \u03a3.\nExample 4.12 (Vectors and the SVD)\nConsider a mapping of a square grid of vectors X \u2208R2 that fit in a box of\nsize 2 \u00d7 2 centered at the origin. Using the standard basis, we map these\nvecto"
  },
  {
    "vector_id": 459,
    "chunk_id": "p127_c3",
    "page_number": 127,
    "text": "e matrix \u03a3.\nExample 4.12 (Vectors and the SVD)\nConsider a mapping of a square grid of vectors X \u2208R2 that fit in a box of\nsize 2 \u00d7 2 centered at the origin. Using the standard basis, we map these\nvectors using\nA =\n\uf8ee\n\uf8f0\n1 \u22120.8\n0 1\n1 0\n\uf8f9\n\uf8fb = U\u03a3V \u22a4 (4.67a)\n=\n\uf8ee\n\uf8f0\n\u22120.79 0 \u22120.62\n0.38 \u22120.78 \u22120.49\n\u22120.48 \u22120.62 0 .62\n\uf8f9\n\uf8fb\n\uf8ee\n\uf8f0\n1.62 0\n0 1 .0\n0 0\n\uf8f9\n\uf8fb\n\u0014\u22120.78 0 .62\n\u22120.62 \u22120.78\n\u0015\n. (4.67b)\nWe start with a set of vectors X (colored dots; see top-left panel of Fig-\nure 4.9) arranged in a grid. We then apply V \u22a4 \u2208 R2\u00d72, which rotates X.\nThe rotated vectors are shown in the bottom-left panel of Figure 4.9. We\nnow map these vectors using the singular value matrix\u03a3 to the codomain\nR3 (see the bottom-right panel in Figure 4.9). Note that all vectors lie in\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published"
  },
  {
    "vector_id": 460,
    "chunk_id": "p127_c4",
    "page_number": 127,
    "text": "p these vectors using the singular value matrix\u03a3 to the codomain\nR3 (see the bottom-right panel in Figure 4.9). Note that all vectors lie in\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 461,
    "chunk_id": "p128_c0",
    "page_number": 128,
    "text": "122 Matrix Decompositions\nthe x1-x2 plane. The third coordinate is always 0. The vectors in the x1-x2\nplane have been stretched by the singular values.\nThe direct mapping of the vectors X by A to the codomain R3 equals\nthe transformation of X by U\u03a3V \u22a4, where U performs a rotation within\nthe codomain R3 so that the mapped vectors are no longer restricted to\nthe x1-x2 plane; they still are on a plane as shown in the top-right panel\nof Figure 4.9.\nFigure 4.9 SVD and\nmapping of vectors\n(represented by\ndiscs). The panels\nfollow the same\nanti-clockwise\nstructure of\nFigure 4.8.\n\u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5\nx1\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\n1.5\nx2\nx1\n-1.5 -0.5\n0.5\n1.5\nx2\n-1.5\n-0.5\n0.5\n1.5\nx3\n-1.0\n-0.5\n0.0\n0.5\n1.0\n\u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5\nx1\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\n1.5\nx2\nx1\n-1.5 -0.5 0.5 1."
  },
  {
    "vector_id": 462,
    "chunk_id": "p128_c1",
    "page_number": 128,
    "text": "1.0 1.5\nx1\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\n1.5\nx2\nx1\n-1.5 -0.5\n0.5\n1.5\nx2\n-1.5\n-0.5\n0.5\n1.5\nx3\n-1.0\n-0.5\n0.0\n0.5\n1.0\n\u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5\nx1\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\n1.5\nx2\nx1\n-1.5 -0.5 0.5 1.5\nx2\n-1.5\n-0.5\n0.5\n1.5\nx3\n0\n4.5.2 Construction of the SVD\nWe will next discuss why the SVD exists and show how to compute it\nin detail. The SVD of a general matrix shares some similarities with the\neigendecomposition of a square matrix.\nRemark. Compare the eigendecomposition of an SPD matrix\nS = S\u22a4 = P DP\u22a4 (4.68)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 463,
    "chunk_id": "p128_c2",
    "page_number": 128,
    "text": "om."
  },
  {
    "vector_id": 464,
    "chunk_id": "p129_c0",
    "page_number": 129,
    "text": "4.5 Singular Value Decomposition 123\nwith the corresponding SVD\nS = U\u03a3V \u22a4 . (4.69)\nIf we set\nU = P = V , D = \u03a3 , (4.70)\nwe see that the SVD of SPD matrices is their eigendecomposition. \u2662\nIn the following, we will explore why Theorem 4.22 holds and how\nthe SVD is constructed. Computing the SVD of A \u2208 Rm\u00d7n is equivalent\nto finding two sets of orthonormal bases U = ( u1, . . . ,um) and V =\n(v1, . . . ,vn) of the codomain Rm and the domain Rn, respectively . From\nthese ordered bases, we will construct the matrices U and V .\nOur plan is to start with constructing the orthonormal set of right-\nsingular vectors v1, . . . ,vn \u2208 Rn. We then construct the orthonormal set\nof left-singular vectors u1, . . . ,um \u2208 Rm. Thereafter, we will link the two\nand require that the orthogonality of the vi is pres"
  },
  {
    "vector_id": 465,
    "chunk_id": "p129_c1",
    "page_number": 129,
    "text": "lar vectors v1, . . . ,vn \u2208 Rn. We then construct the orthonormal set\nof left-singular vectors u1, . . . ,um \u2208 Rm. Thereafter, we will link the two\nand require that the orthogonality of the vi is preserved under the trans-\nformation of A. This is important because we know that the images Avi\nform a set of orthogonal vectors. We will then normalize these images by\nscalar factors, which will turn out to be the singular values.\nLet us begin with constructing the right-singular vectors. The spectral\ntheorem (Theorem 4.15) tells us that the eigenvectors of a symmetric\nmatrix form an ONB, which also means it can be diagonalized. More-\nover, from Theorem 4.14 we can always construct a symmetric, positive\nsemidefinite matrix A\u22a4A \u2208 Rn\u00d7n from any rectangular matrix A \u2208\nRm\u00d7n. Thus, we can always diag"
  },
  {
    "vector_id": 466,
    "chunk_id": "p129_c2",
    "page_number": 129,
    "text": "means it can be diagonalized. More-\nover, from Theorem 4.14 we can always construct a symmetric, positive\nsemidefinite matrix A\u22a4A \u2208 Rn\u00d7n from any rectangular matrix A \u2208\nRm\u00d7n. Thus, we can always diagonalize A\u22a4A and obtain\nA\u22a4A = P DP\u22a4 = P\n\uf8ee\n\uf8ef\uf8f0\n\u03bb1 \u00b7 \u00b7\u00b7 0\n... ... ...\n0 \u00b7 \u00b7\u00b7 \u03bbn\n\uf8f9\n\uf8fa\uf8fbP\u22a4 , (4.71)\nwhere P is an orthogonal matrix, which is composed of the orthonormal\neigenbasis. The \u03bbi \u2a7e 0 are the eigenvalues of A\u22a4A. Let us assume the\nSVD of A exists and inject (4.64) into (4.71). This yields\nA\u22a4A = (U\u03a3V \u22a4)\u22a4(U\u03a3V \u22a4) = V \u03a3\u22a4U\u22a4U\u03a3V \u22a4 , (4.72)\nwhere U, V are orthogonal matrices. Therefore, with U\u22a4U = I we ob-\ntain\nA\u22a4A = V \u03a3\u22a4\u03a3V \u22a4 = V\n\uf8ee\n\uf8ef\uf8f0\n\u03c32\n1 0 0\n0 ... 0\n0 0 \u03c32\nn\n\uf8f9\n\uf8fa\uf8fbV \u22a4 . (4.73)\nComparing now (4.71) and (4.73), we identify\nV \u22a4 = P\u22a4 , (4.74)\n\u03c32\ni = \u03bbi . (4.75)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S."
  },
  {
    "vector_id": 467,
    "chunk_id": "p129_c3",
    "page_number": 129,
    "text": "-\ntain\nA\u22a4A = V \u03a3\u22a4\u03a3V \u22a4 = V\n\uf8ee\n\uf8ef\uf8f0\n\u03c32\n1 0 0\n0 ... 0\n0 0 \u03c32\nn\n\uf8f9\n\uf8fa\uf8fbV \u22a4 . (4.73)\nComparing now (4.71) and (4.73), we identify\nV \u22a4 = P\u22a4 , (4.74)\n\u03c32\ni = \u03bbi . (4.75)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 468,
    "chunk_id": "p130_c0",
    "page_number": 130,
    "text": "124 Matrix Decompositions\nTherefore, the eigenvectors ofA\u22a4A that compose P are the right-singular\nvectors V of A (see (4.74)). The eigenvalues of A\u22a4A are the squared\nsingular values of \u03a3 (see (4.75)).\nTo obtain the left-singular vectors U, we follow a similar procedure.\nWe start by computing the SVD of the symmetric matrix AA\u22a4 \u2208 Rm\u00d7m\n(instead of the previous A\u22a4A \u2208 Rn\u00d7n). The SVD of A yields\nAA\u22a4 = (U\u03a3V \u22a4)(U\u03a3V \u22a4)\u22a4 = U\u03a3V \u22a4V \u03a3\u22a4U\u22a4 (4.76a)\n= U\n\uf8ee\n\uf8ef\uf8f0\n\u03c32\n1 0 0\n0 ... 0\n0 0 \u03c32\nm\n\uf8f9\n\uf8fa\uf8fbU\u22a4 . (4.76b)\nThe spectral theorem tells us that AA\u22a4 = SDS\u22a4 can be diagonalized\nand we can find an ONB of eigenvectors of AA\u22a4, which are collected in\nS. The orthonormal eigenvectors of AA\u22a4 are the left-singular vectors U\nand form an orthonormal basis in the codomain of the SVD.\nThis leaves the question of the structure of"
  },
  {
    "vector_id": 469,
    "chunk_id": "p130_c1",
    "page_number": 130,
    "text": "\u22a4, which are collected in\nS. The orthonormal eigenvectors of AA\u22a4 are the left-singular vectors U\nand form an orthonormal basis in the codomain of the SVD.\nThis leaves the question of the structure of the matrix \u03a3. Since AA\u22a4\nand A\u22a4A have the same nonzero eigenvalues (see page 106), the nonzero\nentries of the \u03a3 matrices in the SVD for both cases have to be the same.\nThe last step is to link up all the parts we touched upon so far. We have\nan orthonormal set of right-singular vectors in V . To finish the construc-\ntion of the SVD, we connect them with the orthonormal vectors U. To\nreach this goal, we use the fact the images of the vi under A have to be\northogonal, too. We can show this by using the results from Section 3.4.\nWe require that the inner product between Avi and Avj must be 0 for\ni"
  },
  {
    "vector_id": 470,
    "chunk_id": "p130_c2",
    "page_number": 130,
    "text": ", we use the fact the images of the vi under A have to be\northogonal, too. We can show this by using the results from Section 3.4.\nWe require that the inner product between Avi and Avj must be 0 for\ni \u0338= j. For any two orthogonal eigenvectors vi, vj, i \u0338= j, it holds that\n(Avi)\u22a4(Avj) = v\u22a4\ni (A\u22a4A)vj = v\u22a4\ni (\u03bbjvj) = \u03bbjv\u22a4\ni vj = 0 . (4.77)\nFor the case m \u2a7e r, it holds that {Av1, . . . ,Avr} is a basis of an r-\ndimensional subspace of Rm.\nTo complete the SVD construction, we need left-singular vectors that\nare orthonormal: We normalize the images of the right-singular vectors\nAvi and obtain\nui := Avi\n\u2225Avi\u2225 = 1\u221a\u03bbi\nAvi = 1\n\u03c3i\nAvi , (4.78)\nwhere the last equality was obtained from (4.75) and (4.76b), showing\nus that the eigenvalues of AA\u22a4 are such that \u03c32\ni = \u03bbi.\nTherefore, the eigenvectors of A\u22a4"
  },
  {
    "vector_id": 471,
    "chunk_id": "p130_c3",
    "page_number": 130,
    "text": "Avi\n\u2225Avi\u2225 = 1\u221a\u03bbi\nAvi = 1\n\u03c3i\nAvi , (4.78)\nwhere the last equality was obtained from (4.75) and (4.76b), showing\nus that the eigenvalues of AA\u22a4 are such that \u03c32\ni = \u03bbi.\nTherefore, the eigenvectors of A\u22a4A, which we know are the right-\nsingular vectors vi, and their normalized images underA, the left-singular\nvectors ui, form two self-consistent ONBs that are connected through the\nsingular value matrix \u03a3.\nLet us rearrange (4.78) to obtain the singular value equationsingular value\nequation\nAvi = \u03c3iui , i = 1, . . . , r . (4.79)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 472,
    "chunk_id": "p130_c4",
    "page_number": 130,
    "text": "s://mml-book.com."
  },
  {
    "vector_id": 473,
    "chunk_id": "p131_c0",
    "page_number": 131,
    "text": "4.5 Singular Value Decomposition 125\nThis equation closely resembles the eigenvalue equation (4.25), but the\nvectors on the left- and the right-hand sides are not the same.\nFor n < m, (4.79) holds only for i \u2a7d n, but (4.79) says nothing about\nthe ui for i > n. However, we know by construction that they are or-\nthonormal. Conversely , form < n, (4.79) holds only fori \u2a7d m. Fori > m,\nwe have Avi = 0 and we still know that the vi form an orthonormal set.\nThis means that the SVD also supplies an orthonormal basis of the kernel\n(null space) of A, the set of vectors x with Ax = 0 (see Section 2.7.3).\nConcatenating the vi as the columns of V and the ui as the columns of\nU yields\nAV = U\u03a3 , (4.80)\nwhere \u03a3 has the same dimensions as A and a diagonal structure for rows\n1, . . . , r. Hence, right-multi"
  },
  {
    "vector_id": 474,
    "chunk_id": "p131_c1",
    "page_number": 131,
    "text": "Concatenating the vi as the columns of V and the ui as the columns of\nU yields\nAV = U\u03a3 , (4.80)\nwhere \u03a3 has the same dimensions as A and a diagonal structure for rows\n1, . . . , r. Hence, right-multiplying with V \u22a4 yields A = U\u03a3V \u22a4, which is\nthe SVD of A.\nExample 4.13 (Computing the SVD)\nLet us find the singular value decomposition of\nA =\n\u0014 1 0 1\n\u22122 1 0\n\u0015\n. (4.81)\nThe SVD requires us to compute the right-singular vectorsvj, the singular\nvalues \u03c3k, and the left-singular vectors ui.\nStep 1: Right-singular vectors as the eigenbasis of A\u22a4A.\nWe start by computing\nA\u22a4A =\n\uf8ee\n\uf8f0\n1 \u22122\n0 1\n1 0\n\uf8f9\n\uf8fb\n\u0014 1 0 1\n\u22122 1 0\n\u0015\n=\n\uf8ee\n\uf8f0\n5 \u22122 1\n\u22122 1 0\n1 0 1\n\uf8f9\n\uf8fb . (4.82)\nWe compute the singular values and right-singular vectors vj through\nthe eigenvalue decomposition of A\u22a4A, which is given as\nA\u22a4A =\n\uf8ee\n\uf8ef\uf8f0\n5\u221a\n30 0 \u22121\u221a\n6\n\u22122"
  },
  {
    "vector_id": 475,
    "chunk_id": "p131_c2",
    "page_number": 131,
    "text": "\u22122 1 0\n\u0015\n=\n\uf8ee\n\uf8f0\n5 \u22122 1\n\u22122 1 0\n1 0 1\n\uf8f9\n\uf8fb . (4.82)\nWe compute the singular values and right-singular vectors vj through\nthe eigenvalue decomposition of A\u22a4A, which is given as\nA\u22a4A =\n\uf8ee\n\uf8ef\uf8f0\n5\u221a\n30 0 \u22121\u221a\n6\n\u22122\u221a\n30\n1\u221a\n5\n\u22122\u221a\n6\n1\u221a\n30\n2\u221a\n5\n1\u221a\n6\n\uf8f9\n\uf8fa\uf8fb\n\uf8ee\n\uf8f0\n6 0 0\n0 1 0\n0 0 0\n\uf8f9\n\uf8fb\n\uf8ee\n\uf8ef\uf8f0\n5\u221a\n30\n\u22122\u221a\n30\n1\u221a\n30\n0 1\u221a\n5\n2\u221a\n5\n\u22121\u221a\n6\n\u22122\u221a\n6\n1\u221a\n6\n\uf8f9\n\uf8fa\uf8fb = P DP\u22a4 ,\n(4.83)\nand we obtain the right-singular vectors as the columns of P so that\nV = P =\n\uf8ee\n\uf8ef\uf8f0\n5\u221a\n30 0 \u22121\u221a\n6\n\u22122\u221a\n30\n1\u221a\n5\n\u22122\u221a\n6\n1\u221a\n30\n2\u221a\n5\n1\u221a\n6\n\uf8f9\n\uf8fa\uf8fb . (4.84)\nStep 2: Singular-value matrix.\nAs the singular values \u03c3i are the square roots of the eigenvalues of\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 476,
    "chunk_id": "p131_c3",
    "page_number": 131,
    "text": "roth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 477,
    "chunk_id": "p132_c0",
    "page_number": 132,
    "text": "126 Matrix Decompositions\nA\u22a4A we obtain them straight from D. Since rk(A) = 2 , there are only\ntwo nonzero singular values: \u03c31 =\n\u221a\n6 and \u03c32 = 1 . The singular value\nmatrix must be the same size as A, and we obtain\n\u03a3 =\n\u0014\u221a\n6 0 0\n0 1 0\n\u0015\n. (4.85)\nStep 3: Left-singular vectors as the normalized image of the right-\nsingular vectors.\nWe find the left-singular vectors by computing the image of the right-\nsingular vectors under A and normalizing them by dividing them by their\ncorresponding singular value. We obtain\nu1 = 1\n\u03c31\nAv1 = 1\u221a\n6\n\u0014 1 0 1\n\u22122 1 0\n\u0015\n\uf8ee\n\uf8ef\uf8f0\n5\u221a\n30\n\u22122\u221a\n30\n1\u221a\n30\n\uf8f9\n\uf8fa\uf8fb =\n\"\n1\u221a\n5\n\u2212 2\u221a\n5\n#\n, (4.86)\nu2 = 1\n\u03c32\nAv2 = 1\n1\n\u0014 1 0 1\n\u22122 1 0\n\u0015\uf8ee\n\uf8f0\n0\n1\u221a\n5\n2\u221a\n5\n\uf8f9\n\uf8fb =\n\"\n2\u221a\n5\n1\u221a\n5\n#\n, (4.87)\nU = [u1, u2] = 1\u221a\n5\n\u0014 1 2\n\u22122 1\n\u0015\n. (4.88)\nNote that on a computer the approach illustrated here has poor numeric"
  },
  {
    "vector_id": 478,
    "chunk_id": "p132_c1",
    "page_number": 132,
    "text": "(4.86)\nu2 = 1\n\u03c32\nAv2 = 1\n1\n\u0014 1 0 1\n\u22122 1 0\n\u0015\uf8ee\n\uf8f0\n0\n1\u221a\n5\n2\u221a\n5\n\uf8f9\n\uf8fb =\n\"\n2\u221a\n5\n1\u221a\n5\n#\n, (4.87)\nU = [u1, u2] = 1\u221a\n5\n\u0014 1 2\n\u22122 1\n\u0015\n. (4.88)\nNote that on a computer the approach illustrated here has poor numerical\nbehavior, and the SVD ofA is normally computed without resorting to the\neigenvalue decomposition of A\u22a4A.\n4.5.3 Eigenvalue Decomposition vs. Singular Value Decomposition\nLet us consider the eigendecomposition A = P DP\u22121 and the SVD A =\nU\u03a3V \u22a4 and review the core elements of the past sections.\nThe SVD always exists for any matrixRm\u00d7n. The eigendecomposition is\nonly defined for square matrices Rn\u00d7n and only exists if we can find a\nbasis of eigenvectors of Rn.\nThe vectors in the eigendecomposition matrix P are not necessarily\northogonal, i.e., the change of basis is not a simple rotation and sca"
  },
  {
    "vector_id": 479,
    "chunk_id": "p132_c2",
    "page_number": 132,
    "text": "and only exists if we can find a\nbasis of eigenvectors of Rn.\nThe vectors in the eigendecomposition matrix P are not necessarily\northogonal, i.e., the change of basis is not a simple rotation and scaling.\nOn the other hand, the vectors in the matrices U and V in the SVD are\northonormal, so they do represent rotations.\nBoth the eigendecomposition and the SVD are compositions of three\nlinear mappings:\n1. Change of basis in the domain\n2. Independent scaling of each new basis vector and mapping from do-\nmain to codomain\n3. Change of basis in the codomain\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 480,
    "chunk_id": "p132_c3",
    "page_number": 132,
    "text": "hine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 481,
    "chunk_id": "p133_c0",
    "page_number": 133,
    "text": "4.5 Singular Value Decomposition 127\nFigure 4.10 Movie\nratings of three\npeople for four\nmovies and its SVD\ndecomposition.5 4 15 5 00 0 51 0 4\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\uf8f9\n\uf8fa\uf8fa\uf8fb\nAli\nBeatrix\nChandra\nStar WarsBlade RunnerAmelieDelicatessen\n=\n\u22120.6710 0.0236 0.4647\u22120.5774\u22120.7197 0.2054\u22120.4759 0.4619\u22120.0939\u22120.7705\u22120.5268\u22120.3464\u22120.1515\u22120.6030 0.5293\u22120.5774\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n9.6438 0 00 6.3639 00 0 0.70560 0 0\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u22120.7367\u22120.6515\u22120.18110.0852 0.1762\u22120.98070.6708\u22120.7379\u22120.0743\n\uf8ee\n\uf8ef\uf8f0\n\uf8f9\n\uf8fa\uf8fb\nA key difference between the eigendecomposition and the SVD is that\nin the SVD, domain and codomain can be vector spaces of different\ndimensions.\nIn the SVD, the left- and right-singular vector matrices U and V are\ngenerally not inverse of each other (they perform basis changes in dif-\nferent vector spaces). In the eigendecomposition,"
  },
  {
    "vector_id": 482,
    "chunk_id": "p133_c1",
    "page_number": 133,
    "text": "ensions.\nIn the SVD, the left- and right-singular vector matrices U and V are\ngenerally not inverse of each other (they perform basis changes in dif-\nferent vector spaces). In the eigendecomposition, the basis change ma-\ntrices P and P\u22121 are inverses of each other.\nIn the SVD, the entries in the diagonal matrix \u03a3 are all real and non-\nnegative, which is not generally true for the diagonal matrix in the\neigendecomposition.\nThe SVD and the eigendecomposition are closely related through their\nprojections\n\u2013 The left-singular vectors of A are eigenvectors of AA\u22a4\n\u2013 The right-singular vectors of A are eigenvectors of A\u22a4A.\n\u2013 The nonzero singular values of A are the square roots of the nonzero\neigenvalues of both AA\u22a4 and A\u22a4A.\nFor symmetric matrices A \u2208 Rn\u00d7n, the eigenvalue decomposition and\nthe SVD"
  },
  {
    "vector_id": 483,
    "chunk_id": "p133_c2",
    "page_number": 133,
    "text": "e eigenvectors of A\u22a4A.\n\u2013 The nonzero singular values of A are the square roots of the nonzero\neigenvalues of both AA\u22a4 and A\u22a4A.\nFor symmetric matrices A \u2208 Rn\u00d7n, the eigenvalue decomposition and\nthe SVD are one and the same, which follows from the spectral theo-\nrem 4.15.\nExample 4.14 (Finding Structure in Movie Ratings and Consumers)\nLet us add a practical interpretation of the SVD by analyzing data on\npeople and their preferred movies. Consider three viewers (Ali, Beatrix,\nChandra) rating four different movies ( Star Wars, Blade Runner, Amelie,\nDelicatessen). Their ratings are values between 0 (worst) and 5 (best) and\nencoded in a data matrix A \u2208 R4\u00d73 as shown in Figure 4.10. Each row\nrepresents a movie and each column a user. Thus, the column vectors of\nmovie ratings, one for each viewer,"
  },
  {
    "vector_id": 484,
    "chunk_id": "p133_c3",
    "page_number": 133,
    "text": "(worst) and 5 (best) and\nencoded in a data matrix A \u2208 R4\u00d73 as shown in Figure 4.10. Each row\nrepresents a movie and each column a user. Thus, the column vectors of\nmovie ratings, one for each viewer, are xAli, xBeatrix, xChandra.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 485,
    "chunk_id": "p134_c0",
    "page_number": 134,
    "text": "128 Matrix Decompositions\nFactoring A using the SVD offers us a way to capture the relationships\nof how people rate movies, and especially if there is a structure linking\nwhich people like which movies. Applying the SVD to our data matrix A\nmakes a number of assumptions:\n1. All viewers rate movies consistently using the same linear mapping.\n2. There are no errors or noise in the ratings.\n3. We interpret the left-singular vectors ui as stereotypical movies and\nthe right-singular vectors vj as stereotypical viewers.\nWe then make the assumption that any viewer\u2019s specific movie preferences\ncan be expressed as a linear combination of the vj. Similarly , any movie\u2019s\nlike-ability can be expressed as a linear combination of the ui. Therefore,\na vector in the domain of the SVD can be interpreted as"
  },
  {
    "vector_id": 486,
    "chunk_id": "p134_c1",
    "page_number": 134,
    "text": "expressed as a linear combination of the vj. Similarly , any movie\u2019s\nlike-ability can be expressed as a linear combination of the ui. Therefore,\na vector in the domain of the SVD can be interpreted as a viewer in the\n\u201cspace\u201d of stereotypical viewers, and a vector in the codomain of the SVD\ncorrespondingly as a movie in the \u201cspace\u201d of stereotypical movies. Let usThese two \u201cspaces\u201d\nare only\nmeaningfully\nspanned by the\nrespective viewer\nand movie data if\nthe data itself covers\na sufficient diversity\nof viewers and\nmovies.\ninspect the SVD of our movie-user matrix. The first left-singular vector u1\nhas large absolute values for the two science fiction movies and a large\nfirst singular value (red shading in Figure 4.10). Thus, this groups a type\nof users with a specific set of movies (science fi"
  },
  {
    "vector_id": 487,
    "chunk_id": "p134_c2",
    "page_number": 134,
    "text": "has large absolute values for the two science fiction movies and a large\nfirst singular value (red shading in Figure 4.10). Thus, this groups a type\nof users with a specific set of movies (science fiction theme). Similarly , the\nfirst right-singular v1 shows large absolute values for Ali and Beatrix, who\ngive high ratings to science fiction movies (green shading in Figure 4.10).\nThis suggests that v1 reflects the notion of a science fiction lover.\nSimilarly ,u2, seems to capture a French art house film theme, andv2 in-\ndicates that Chandra is close to an idealized lover of such movies. An ide-\nalized science fiction lover is a purist and only loves science fiction movies,\nso a science fiction loverv1 gives a rating of zero to everything but science\nfiction themed\u2014this logic is implied by"
  },
  {
    "vector_id": 488,
    "chunk_id": "p134_c3",
    "page_number": 134,
    "text": "-\nalized science fiction lover is a purist and only loves science fiction movies,\nso a science fiction loverv1 gives a rating of zero to everything but science\nfiction themed\u2014this logic is implied by the diagonal substructure for the\nsingular value matrix \u03a3. A specific movie is therefore represented by how\nit decomposes (linearly) into its stereotypical movies. Likewise, a person\nwould be represented by how they decompose (via linear combination)\ninto movie themes.\nIt is worth to briefly discuss SVD terminology and conventions, as there\nare different versions used in the literature. While these differences can\nbe confusing, the mathematics remains invariant to them.\nFor convenience in notation and abstraction, we use an SVD notation\nwhere the SVD is described as having two square left- and"
  },
  {
    "vector_id": 489,
    "chunk_id": "p134_c4",
    "page_number": 134,
    "text": "e differences can\nbe confusing, the mathematics remains invariant to them.\nFor convenience in notation and abstraction, we use an SVD notation\nwhere the SVD is described as having two square left- and right-singular\nvector matrices, but a non-square singular value matrix. Our defini-\ntion (4.64) for the SVD is sometimes called the full SVD.full SVD\nSome authors define the SVD a bit differently and focus on square sin-\ngular matrices. Then, for A \u2208 Rm\u00d7n and m \u2a7e n,\nA\nm\u00d7n\n= U\nm\u00d7n\n\u03a3\nn\u00d7n\nV \u22a4\nn\u00d7n\n. (4.89)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 490,
    "chunk_id": "p135_c0",
    "page_number": 135,
    "text": "4.6 Matrix Approximation 129\nSometimes this formulation is called thereduced SVD (e.g., Datta (2010)) reduced SVD\nor the SVD (e.g., Press et al. (2007)). This alternative format changes\nmerely how the matrices are constructed but leaves the mathematical\nstructure of the SVD unchanged. The convenience of this alternative\nformulation is that \u03a3 is diagonal, as in the eigenvalue decomposition.\nIn Section 4.6, we will learn about matrix approximation techniques\nusing the SVD, which is also called the truncated SVD. truncated SVD\nIt is possible to define the SVD of a rank- r matrix A so that U is an\nm \u00d7 r matrix, \u03a3 a diagonal matrix r \u00d7 r, and V an r \u00d7 n matrix.\nThis construction is very similar to our definition, and ensures that the\ndiagonal matrix \u03a3 has only nonzero entries along the diagonal"
  },
  {
    "vector_id": 491,
    "chunk_id": "p135_c1",
    "page_number": 135,
    "text": "m \u00d7 r matrix, \u03a3 a diagonal matrix r \u00d7 r, and V an r \u00d7 n matrix.\nThis construction is very similar to our definition, and ensures that the\ndiagonal matrix \u03a3 has only nonzero entries along the diagonal. The\nmain convenience of this alternative notation is that \u03a3 is diagonal, as\nin the eigenvalue decomposition.\nA restriction that the SVD for A only applies to m \u00d7 n matrices with\nm > nis practically unnecessary . Whenm < n, the SVD decomposition\nwill yield \u03a3 with more zero columns than rows and, consequently , the\nsingular values \u03c3m+1, . . . , \u03c3n are 0.\nThe SVD is used in a variety of applications in machine learning from\nleast-squares problems in curve fitting to solving systems of linear equa-\ntions. These applications harness various important properties of the SVD,\nits relation to the ran"
  },
  {
    "vector_id": 492,
    "chunk_id": "p135_c2",
    "page_number": 135,
    "text": "s in machine learning from\nleast-squares problems in curve fitting to solving systems of linear equa-\ntions. These applications harness various important properties of the SVD,\nits relation to the rank of a matrix, and its ability to approximate matrices\nof a given rank with lower-rank matrices. Substituting a matrix with its\nSVD has often the advantage of making calculation more robust to nu-\nmerical rounding errors. As we will explore in the next section, the SVD\u2019s\nability to approximate matrices with \u201csimpler\u201d matrices in a principled\nmanner opens up machine learning applications ranging from dimension-\nality reduction and topic modeling to data compression and clustering.\n4.6 Matrix Approximation\nWe considered the SVD as a way to factorize A = U\u03a3V \u22a4 \u2208 Rm\u00d7n into\nthe product of three mat"
  },
  {
    "vector_id": 493,
    "chunk_id": "p135_c3",
    "page_number": 135,
    "text": "om dimension-\nality reduction and topic modeling to data compression and clustering.\n4.6 Matrix Approximation\nWe considered the SVD as a way to factorize A = U\u03a3V \u22a4 \u2208 Rm\u00d7n into\nthe product of three matrices, where U \u2208 Rm\u00d7m and V \u2208 Rn\u00d7n are or-\nthogonal and \u03a3 contains the singular values on its main diagonal. Instead\nof doing the full SVD factorization, we will now investigate how the SVD\nallows us to represent a matrixA as a sum of simpler (low-rank) matrices\nAi, which lends itself to a matrix approximation scheme that is cheaper\nto compute than the full SVD.\nWe construct a rank-1 matrix Ai \u2208 Rm\u00d7n as\nAi := uiv\u22a4\ni , (4.90)\nwhich is formed by the outer product of the ith orthogonal column vector\nof U and V . Figure 4.11 shows an image of Stonehenge, which can be\nrepresented by a matrix A \u2208 R1"
  },
  {
    "vector_id": 494,
    "chunk_id": "p135_c4",
    "page_number": 135,
    "text": "m\u00d7n as\nAi := uiv\u22a4\ni , (4.90)\nwhich is formed by the outer product of the ith orthogonal column vector\nof U and V . Figure 4.11 shows an image of Stonehenge, which can be\nrepresented by a matrix A \u2208 R1432\u00d71910, and some outer products Ai, as\ndefined in (4.90).\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 495,
    "chunk_id": "p136_c0",
    "page_number": 136,
    "text": "130 Matrix Decompositions\nFigure 4.11 Image\nprocessing with the\nSVD. (a) The\noriginal grayscale\nimage is a\n1, 432 \u00d7 1, 910\nmatrix of values\nbetween 0 (black)\nand 1 (white).\n(b)\u2013(f) Rank-1\nmatrices\nA1, . . . ,A5 and\ntheir corresponding\nsingular values\n\u03c31, . . . , \u03c35. The\ngrid-like structure of\neach rank-1 matrix\nis imposed by the\nouter-product of the\nleft and\nright-singular\nvectors.\n(a) Original image A.\n (b) A1, \u03c31 \u2248 228, 052.\n (c) A2, \u03c32 \u2248 40, 647.\n(d) A3, \u03c33 \u2248 26, 125.\n (e) A4, \u03c34 \u2248 20, 232.\n (f) A5, \u03c35 \u2248 15, 436.\nA matrix A \u2208 Rm\u00d7n of rank r can be written as a sum of rank-1 matrices\nAi so that\nA =\nrX\ni=1\n\u03c3iuiv\u22a4\ni =\nrX\ni=1\n\u03c3iAi , (4.91)\nwhere the outer-product matrices Ai are weighted by the ith singular\nvalue \u03c3i. We can see why (4.91) holds: The diagonal structure of the\nsingular value"
  },
  {
    "vector_id": 496,
    "chunk_id": "p136_c1",
    "page_number": 136,
    "text": "hat\nA =\nrX\ni=1\n\u03c3iuiv\u22a4\ni =\nrX\ni=1\n\u03c3iAi , (4.91)\nwhere the outer-product matrices Ai are weighted by the ith singular\nvalue \u03c3i. We can see why (4.91) holds: The diagonal structure of the\nsingular value matrix \u03a3 multiplies only matching left- and right-singular\nvectors uiv\u22a4\ni and scales them by the corresponding singular value \u03c3i. All\nterms \u03a3ijuiv\u22a4\nj vanish for i \u0338= j because \u03a3 is a diagonal matrix. Any terms\ni > rvanish because the corresponding singular values are 0.\nIn (4.90), we introduced rank- 1 matrices Ai. We summed up the r in-\ndividual rank-1 matrices to obtain a rank- r matrix A; see (4.91). If the\nsum does not run over all matrices Ai, i = 1 , . . . , r, but only up to an\nintermediate value k < r, we obtain a rank-k approximationrank-k\napproximation\nbA(k) :=\nkX\ni=1\n\u03c3iuiv\u22a4\ni =\nkX\ni"
  },
  {
    "vector_id": 497,
    "chunk_id": "p136_c2",
    "page_number": 136,
    "text": "4.91). If the\nsum does not run over all matrices Ai, i = 1 , . . . , r, but only up to an\nintermediate value k < r, we obtain a rank-k approximationrank-k\napproximation\nbA(k) :=\nkX\ni=1\n\u03c3iuiv\u22a4\ni =\nkX\ni=1\n\u03c3iAi (4.92)\nof A with rk( bA(k)) = k. Figure 4.12 shows low-rank approximations\nbA(k) of an original image A of Stonehenge. The shape of the rocks be-\ncomes increasingly visible and clearly recognizable in the rank- 5 approx-\nimation. While the original image requires 1, 432 \u00b7 1, 910 = 2 , 735, 120\nnumbers, the rank-5 approximation requires us only to store the five sin-\ngular values and the five left- and right-singular vectors (1, 432 and 1, 910-\ndimensional each) for a total of5\u00b7(1, 432+1 , 910+1) = 16 , 715 numbers\n\u2013 just above 0.6% of the original.\nTo measure the difference (error) bet"
  },
  {
    "vector_id": 498,
    "chunk_id": "p136_c3",
    "page_number": 136,
    "text": "ve left- and right-singular vectors (1, 432 and 1, 910-\ndimensional each) for a total of5\u00b7(1, 432+1 , 910+1) = 16 , 715 numbers\n\u2013 just above 0.6% of the original.\nTo measure the difference (error) betweenA and its rank-k approxima-\ntion bA(k), we need the notion of a norm. In Section 3.1, we already used\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 499,
    "chunk_id": "p137_c0",
    "page_number": 137,
    "text": "4.6 Matrix Approximation 131\nFigure 4.12 Image\nreconstruction with\nthe SVD. (a)\nOriginal image.\n(b)\u2013(f) Image\nreconstruction using\nthe low-rank\napproximation of\nthe SVD, where the\nrank-k\napproximation is\ngiven by bA(k) =Pk\ni=1 \u03c3iAi.\n(a) Original image A.\n (b) Rank-1 approximation bA(1).\n(c) Rank-2 approximation bA(2).\n(d) Rank-3 approximation bA(3).\n(e) Rank-4 approximation bA(4).\n(f) Rank-5 approximation bA(5).\nnorms on vectors that measure the length of a vector. By analogy we can\nalso define norms on matrices.\nDefinition 4.23 (Spectral Norm of a Matrix). For x \u2208 Rn\\{0}, the spectral spectral norm\nnorm of a matrix A \u2208 Rm\u00d7n is defined as\n\u2225A\u22252 := max\nx\n\u2225Ax\u22252\n\u2225x\u22252\n. (4.93)\nWe introduce the notation of a subscript in the matrix norm (left-hand\nside), similar to the Euclidean norm for vectors"
  },
  {
    "vector_id": 500,
    "chunk_id": "p137_c1",
    "page_number": 137,
    "text": "l norm\nnorm of a matrix A \u2208 Rm\u00d7n is defined as\n\u2225A\u22252 := max\nx\n\u2225Ax\u22252\n\u2225x\u22252\n. (4.93)\nWe introduce the notation of a subscript in the matrix norm (left-hand\nside), similar to the Euclidean norm for vectors (right-hand side), which\nhas subscript 2. The spectral norm (4.93) determines how long any vector\nx can at most become when multiplied by A.\nTheorem 4.24. The spectral norm of A is its largest singular value \u03c31.\nWe leave the proof of this theorem as an exercise.\nEckart-Young\ntheoremTheorem 4.25 (Eckart-Young Theorem (Eckart and Young, 1936)). Con-\nsider a matrix A \u2208 Rm\u00d7n of rank r and let B \u2208 Rm\u00d7n be a matrix of rank\nk. For any k \u2a7d r with bA(k) = Pk\ni=1 \u03c3iuiv\u22a4\ni it holds that\nbA(k) = argminrk(B)=k \u2225A \u2212 B\u22252 , (4.94)\r\r\rA \u2212 bA(k)\n\r\r\r\n2\n= \u03c3k+1 . (4.95)\nThe Eckart-Young theorem states explicitly h"
  },
  {
    "vector_id": 501,
    "chunk_id": "p137_c2",
    "page_number": 137,
    "text": "n be a matrix of rank\nk. For any k \u2a7d r with bA(k) = Pk\ni=1 \u03c3iuiv\u22a4\ni it holds that\nbA(k) = argminrk(B)=k \u2225A \u2212 B\u22252 , (4.94)\r\r\rA \u2212 bA(k)\n\r\r\r\n2\n= \u03c3k+1 . (4.95)\nThe Eckart-Young theorem states explicitly how much error we intro-\nduce by approximating A using a rank- k approximation. We can inter-\npret the rank- k approximation obtained with the SVD as a projection of\nthe full-rank matrix A onto a lower-dimensional space of rank-at-most- k\nmatrices. Of all possible projections, the SVD minimizes the error (with\nrespect to the spectral norm) between A and any rank-k approximation.\nWe can retrace some of the steps to understand why (4.95) should hold.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 502,
    "chunk_id": "p137_c3",
    "page_number": 137,
    "text": "of the steps to understand why (4.95) should hold.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 503,
    "chunk_id": "p138_c0",
    "page_number": 138,
    "text": "132 Matrix Decompositions\nWe observe that the difference between A \u2212 bA(k) is a matrix containing\nthe sum of the remaining rank-1 matrices\nA \u2212 bA(k) =\nrX\ni=k+1\n\u03c3iuiv\u22a4\ni . (4.96)\nBy Theorem 4.24, we immediately obtain\u03c3k+1 as the spectral norm of the\ndifference matrix. Let us have a closer look at (4.94). If we assume that\nthere is another matrix B with rk(B) \u2a7d k, such that\n\u2225A \u2212 B\u22252 <\n\r\r\rA \u2212 bA(k)\n\r\r\r\n2\n, (4.97)\nthen there exists an at least (n \u2212 k)-dimensional null space Z \u2286 Rn, such\nthat x \u2208 Z implies that Bx = 0. Then it follows that\n\u2225Ax\u22252 = \u2225(A \u2212 B)x\u22252 , (4.98)\nand by using a version of the Cauchy-Schwartz inequality (3.17) that en-\ncompasses norms of matrices, we obtain\n\u2225Ax\u22252 \u2a7d \u2225A \u2212 B\u22252 \u2225x\u22252 < \u03c3k+1 \u2225x\u22252 . (4.99)\nHowever, there exists a (k + 1)-dimensional subspace where \u2225Ax\u22252 \u2a7e\n\u03c3k+1 \u2225x\u2225"
  },
  {
    "vector_id": 504,
    "chunk_id": "p138_c1",
    "page_number": 138,
    "text": "Cauchy-Schwartz inequality (3.17) that en-\ncompasses norms of matrices, we obtain\n\u2225Ax\u22252 \u2a7d \u2225A \u2212 B\u22252 \u2225x\u22252 < \u03c3k+1 \u2225x\u22252 . (4.99)\nHowever, there exists a (k + 1)-dimensional subspace where \u2225Ax\u22252 \u2a7e\n\u03c3k+1 \u2225x\u22252, which is spanned by the right-singular vectors vj, j\u2a7d k + 1 of\nA. Adding up dimensions of these two spaces yields a number greater than\nn, as there must be a nonzero vector in both spaces. This is a contradiction\nof the rank-nullity theorem (Theorem 2.24) in Section 2.7.3.\nThe Eckart-Young theorem implies that we can use SVD to reduce a\nrank-r matrix A to a rank- k matrix bA in a principled, optimal (in the\nspectral norm sense) manner. We can interpret the approximation ofA by\na rank-k matrix as a form of lossy compression. Therefore, the low-rank\napproximation of a matrix appears in many m"
  },
  {
    "vector_id": 505,
    "chunk_id": "p138_c2",
    "page_number": 138,
    "text": "timal (in the\nspectral norm sense) manner. We can interpret the approximation ofA by\na rank-k matrix as a form of lossy compression. Therefore, the low-rank\napproximation of a matrix appears in many machine learning applications,\ne.g., image processing, noise filtering, and regularization of ill-posed prob-\nlems. Furthermore, it plays a key role in dimensionality reduction and\nprincipal component analysis, as we will see in Chapter 10.\nExample 4.15 (Finding Structure in Movie Ratings and Consumers\n(continued))\nComing back to our movie-rating example, we can now apply the con-\ncept of low-rank approximations to approximate the original data matrix.\nRecall that our first singular value captures the notion of science fiction\ntheme in movies and science fiction lovers. Thus, by using only the"
  },
  {
    "vector_id": 506,
    "chunk_id": "p138_c3",
    "page_number": 138,
    "text": "approximations to approximate the original data matrix.\nRecall that our first singular value captures the notion of science fiction\ntheme in movies and science fiction lovers. Thus, by using only the first\nsingular value term in a rank-1 decomposition of the movie-rating matrix,\nwe obtain the predicted ratings\nA1 = u1v\u22a4\n1 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22120.6710\n\u22120.7197\n\u22120.0939\n\u22120.1515\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u0002\n\u22120.7367 \u22120.6515 \u22120.1811\n\u0003\n(4.100a)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 507,
    "chunk_id": "p139_c0",
    "page_number": 139,
    "text": "4.7 Matrix Phylogeny 133\n=\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n0.4943 0 .4372 0 .1215\n0.5302 0 .4689 0 .1303\n0.0692 0 .0612 0 .0170\n0.1116 0 .0987 0 .0274\n\uf8f9\n\uf8fa\uf8fa\uf8fb . (4.100b)\nThis first rank- 1 approximation A1 is insightful: it tells us that Ali and\nBeatrix like science fiction movies, such as Star Wars and Bladerunner\n(entries have values > 0.4), but fails to capture the ratings of the other\nmovies by Chandra. This is not surprising, as Chandra\u2019s type of movies is\nnot captured by the first singular value. The second singular value gives\nus a better rank-1 approximation for those movie-theme lovers:\nA2 = u2v\u22a4\n2 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n0.0236\n0.2054\n\u22120.7705\n\u22120.6030\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u0002\n0.0852 0 .1762 \u22120.9807\n\u0003\n(4.101a)\n=\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n0.0020 0 .0042 \u22120.0231\n0.0175 0 .0362 \u22120.2014\n\u22120.0656 \u22120.1358 0 .7556\n\u22120.0514 \u22120.1063 0 .5914\n\uf8f9\n\uf8fa\uf8fa\uf8fb . (4.101b)\nIn this second"
  },
  {
    "vector_id": 508,
    "chunk_id": "p139_c1",
    "page_number": 139,
    "text": "6\n0.2054\n\u22120.7705\n\u22120.6030\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u0002\n0.0852 0 .1762 \u22120.9807\n\u0003\n(4.101a)\n=\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n0.0020 0 .0042 \u22120.0231\n0.0175 0 .0362 \u22120.2014\n\u22120.0656 \u22120.1358 0 .7556\n\u22120.0514 \u22120.1063 0 .5914\n\uf8f9\n\uf8fa\uf8fa\uf8fb . (4.101b)\nIn this second rank- 1 approximation A2, we capture Chandra\u2019s ratings\nand movie types well, but not the science fiction movies. This leads us to\nconsider the rank-2 approximation bA(2), where we combine the first two\nrank-1 approximations\nbA(2) = \u03c31A1 + \u03c32A2 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n4.7801 4 .2419 1 .0244\n5.2252 4 .7522 \u22120.0250\n0.2493 \u22120.2743 4 .9724\n0.7495 0 .2756 4 .0278\n\uf8f9\n\uf8fa\uf8fa\uf8fb . (4.102)\nbA(2) is similar to the original movie ratings table\nA =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n5 4 1\n5 5 0\n0 0 5\n1 0 4\n\uf8f9\n\uf8fa\uf8fa\uf8fb , (4.103)\nand this suggests that we can ignore the contribution of A3. We can in-\nterpret this so that in the data table there is no evidence of a"
  },
  {
    "vector_id": 509,
    "chunk_id": "p139_c2",
    "page_number": 139,
    "text": "ratings table\nA =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n5 4 1\n5 5 0\n0 0 5\n1 0 4\n\uf8f9\n\uf8fa\uf8fa\uf8fb , (4.103)\nand this suggests that we can ignore the contribution of A3. We can in-\nterpret this so that in the data table there is no evidence of a third movie-\ntheme/movie-lovers category . This also means that the entire space of\nmovie-themes/movie-lovers in our example is a two-dimensional space\nspanned by science fiction and French art house movies and lovers.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 510,
    "chunk_id": "p140_c0",
    "page_number": 140,
    "text": "134 Matrix Decompositions\nFigure 4.13 A\nfunctional\nphylogeny of\nmatrices\nencountered in\nmachine learning.\nReal matrices\u2203Pseudo-inverse\u2203SVD\nSquare\u2203Determinant\u2203Trace\nNonsquare\nDefective Singular\nNon-defective(diagonalizable)\nSingular\nNormal Non-normal\nSymmetriceigenvalues\u2208R\nPositive definite\nCholeskyeigenvalues>0\nDiagonal\nIdentitymatrix\n\u2203Inverse Matrix\nRegular(invertible)\nOrthogonalRotation\nRn\u00d7n Rn\u00d7m\nNo basis of\neigenvectors\nBasis of\neigenvectors\nA\u22a4A=AA\u22a4 A\u22a4A\u0338=AA\u22a4\nColumns are\northogonal\neigenvectors\nA\u22a4A=AA\n\u22a4\n=I\ndet\n\u0338= 0\ndet\n\u0338= 0\ndet = 0\n4.7 Matrix Phylogeny\nThe word\n\u201cphylogenetic\u201d\ndescribes how we\ncapture the\nrelationships among\nindividuals or\ngroups and derived\nfrom the Greek\nwords for \u201ctribe\u201d\nand \u201csource\u201d.\nIn Chapters 2 and 3, we covered the basics of linear algebra and analytic\ngeometry ."
  },
  {
    "vector_id": 511,
    "chunk_id": "p140_c1",
    "page_number": 140,
    "text": "e\ncapture the\nrelationships among\nindividuals or\ngroups and derived\nfrom the Greek\nwords for \u201ctribe\u201d\nand \u201csource\u201d.\nIn Chapters 2 and 3, we covered the basics of linear algebra and analytic\ngeometry . In this chapter, we looked at fundamental characteristics of ma-\ntrices and linear mappings. Figure 4.13 depicts the phylogenetic tree of\nrelationships between different types of matrices (black arrows indicating\n\u201cis a subset of\u201d) and the covered operations we can perform on them (in\nblue). We consider all real matrices A \u2208 Rn\u00d7m. For non-square matrices\n(where n \u0338= m), the SVD always exists, as we saw in this chapter. Focus-\ning on square matrices A \u2208 Rn\u00d7n, the determinant informs us whether a\nsquare matrix possesses an inverse matrix, i.e., whether it belongs to the\nclass of regular, invertib"
  },
  {
    "vector_id": 512,
    "chunk_id": "p140_c2",
    "page_number": 140,
    "text": "saw in this chapter. Focus-\ning on square matrices A \u2208 Rn\u00d7n, the determinant informs us whether a\nsquare matrix possesses an inverse matrix, i.e., whether it belongs to the\nclass of regular, invertible matrices. If the squaren \u00d7 n matrix possesses n\nlinearly independent eigenvectors, then the matrix is non-defective and an\neigendecomposition exists (Theorem 4.12). We know that repeated eigen-\nvalues may result in defective matrices, which cannot be diagonalized.\nNon-singular and non-defective matrices are not the same. For exam-\nple, a rotation matrix will be invertible (determinant is nonzero) but not\ndiagonalizable in the real numbers (eigenvalues are not guaranteed to be\nreal numbers).\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 513,
    "chunk_id": "p140_c3",
    "page_number": 140,
    "text": "o) but not\ndiagonalizable in the real numbers (eigenvalues are not guaranteed to be\nreal numbers).\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 514,
    "chunk_id": "p141_c0",
    "page_number": 141,
    "text": "4.8 Further Reading 135\nWe dive further into the branch of non-defective squaren \u00d7 n matrices.\nA is normal if the condition A\u22a4A = AA\u22a4 holds. Moreover, if the more\nrestrictive condition holds that A\u22a4A = AA\u22a4 = I, then A is called or-\nthogonal (see Definition 3.8). The set of orthogonal matrices is a subset of\nthe regular (invertible) matrices and satisfies A\u22a4 = A\u22121.\nNormal matrices have a frequently encountered subset, the symmetric\nmatrices S \u2208 Rn\u00d7n, which satisfy S = S\u22a4. Symmetric matrices have only\nreal eigenvalues. A subset of the symmetric matrices consists of the pos-\nitive definite matrices P that satisfy the condition of x\u22a4P x> 0 for all\nx \u2208 Rn\\{0}. In this case, a unique Cholesky decomposition exists (Theo-\nrem 4.18). Positive definite matrices have only positive eigenvalues and\nare"
  },
  {
    "vector_id": 515,
    "chunk_id": "p141_c1",
    "page_number": 141,
    "text": "es P that satisfy the condition of x\u22a4P x> 0 for all\nx \u2208 Rn\\{0}. In this case, a unique Cholesky decomposition exists (Theo-\nrem 4.18). Positive definite matrices have only positive eigenvalues and\nare always invertible (i.e., have a nonzero determinant).\nAnother subset of symmetric matrices consists of the diagonal matrices\nD. Diagonal matrices are closed under multiplication and addition, but do\nnot necessarily form a group (this is only the case if all diagonal entries\nare nonzero so that the matrix is invertible). A special diagonal matrix is\nthe identity matrix I.\n4.8 Further Reading\nMost of the content in this chapter establishes underlying mathematics\nand connects them to methods for studying mappings, many of which are\nat the heart of machine learning at the level of underpinning so"
  },
  {
    "vector_id": 516,
    "chunk_id": "p141_c2",
    "page_number": 141,
    "text": "of the content in this chapter establishes underlying mathematics\nand connects them to methods for studying mappings, many of which are\nat the heart of machine learning at the level of underpinning software so-\nlutions and building blocks for almost all machine learning theory . Matrix\ncharacterization using determinants, eigenspectra, and eigenspaces pro-\nvides fundamental features and conditions for categorizing and analyzing\nmatrices. This extends to all forms of representations of data and map-\npings involving data, as well as judging the numerical stability of compu-\ntational operations on such matrices (Press et al., 2007).\nDeterminants are fundamental tools in order to invert matrices and\ncompute eigenvalues \u201cby hand\u201d. However, for almost all but the smallest\ninstances, numerical co"
  },
  {
    "vector_id": 517,
    "chunk_id": "p141_c3",
    "page_number": 141,
    "text": "n such matrices (Press et al., 2007).\nDeterminants are fundamental tools in order to invert matrices and\ncompute eigenvalues \u201cby hand\u201d. However, for almost all but the smallest\ninstances, numerical computation by Gaussian elimination outperforms\ndeterminants (Press et al., 2007). Determinants remain nevertheless a\npowerful theoretical concept, e.g., to gain intuition about the orientation\nof a basis based on the sign of the determinant. Eigenvectors can be used\nto perform basis changes to transform data into the coordinates of mean-\ningful orthogonal, feature vectors. Similarly , matrix decomposition meth-\nods, such as the Cholesky decomposition, reappear often when we com-\npute or simulate random events (Rubinstein and Kroese, 2016). Therefore,\nthe Cholesky decomposition enables us to com"
  },
  {
    "vector_id": 518,
    "chunk_id": "p141_c4",
    "page_number": 141,
    "text": "osition meth-\nods, such as the Cholesky decomposition, reappear often when we com-\npute or simulate random events (Rubinstein and Kroese, 2016). Therefore,\nthe Cholesky decomposition enables us to compute the reparametrization\ntrick where we want to perform continuous differentiation over random\nvariables, e.g., in variational autoencoders (Jimenez Rezende et al., 2014;\nKingma and Welling, 2014).\nEigendecomposition is fundamental in enabling us to extract mean-\ningful and interpretable information that characterizes linear mappings.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 519,
    "chunk_id": "p141_c5",
    "page_number": 141,
    "text": "Cambridge University Press (2020)."
  },
  {
    "vector_id": 520,
    "chunk_id": "p142_c0",
    "page_number": 142,
    "text": "136 Matrix Decompositions\nTherefore, the eigendecomposition underlies a general class of machine\nlearning algorithms called spectral methods that perform eigendecomposi-\ntion of a positive-definite kernel. These spectral decomposition methods\nencompass classical approaches to statistical data analysis, such as the\nfollowing:\nprincipal component\nanalysis Principal component analysis(PCA (Pearson, 1901), see also Chapter 10),\nin which a low-dimensional subspace, which explains most of the vari-\nability in the data, is sought.Fisher discriminant\nanalysis Fisher discriminant analysis, which aims to determine a separating hy-\nperplane for data classification (Mika et al., 1999).multidimensional\nscaling Multidimensional scaling (MDS) (Carroll and Chang, 1970).\nThe computational efficiency of the"
  },
  {
    "vector_id": 521,
    "chunk_id": "p142_c1",
    "page_number": 142,
    "text": "o determine a separating hy-\nperplane for data classification (Mika et al., 1999).multidimensional\nscaling Multidimensional scaling (MDS) (Carroll and Chang, 1970).\nThe computational efficiency of these methods typically comes from find-\ning the best rank- k approximation to a symmetric, positive semidefinite\nmatrix. More contemporary examples of spectral methods have different\norigins, but each of them requires the computation of the eigenvectors\nand eigenvalues of a positive-definite kernel, such as Isomap (TenenbaumIsomap\net al., 2000), Laplacian eigenmaps (Belkin and Niyogi, 2003), HessianLaplacian\neigenmaps\nHessian eigenmaps\neigenmaps (Donoho and Grimes, 2003), and spectral clustering (Shi and\nspectral clustering\nMalik, 2000). The core computations of these are generally underpinned\nb"
  },
  {
    "vector_id": 522,
    "chunk_id": "p142_c2",
    "page_number": 142,
    "text": "Laplacian\neigenmaps\nHessian eigenmaps\neigenmaps (Donoho and Grimes, 2003), and spectral clustering (Shi and\nspectral clustering\nMalik, 2000). The core computations of these are generally underpinned\nby low-rank matrix approximation techniques (Belabbas and Wolfe, 2009)\nas we encountered here via the SVD.\nThe SVD allows us to discover some of the same kind of information as\nthe eigendecomposition. However, the SVD is more generally applicable\nto non-square matrices and data tables. These matrix factorization meth-\nods become relevant whenever we want to identify heterogeneity in data\nwhen we want to perform data compression by approximation, e.g., in-\nstead of storingn\u00d7m values just storing(n+m)k values, or when we want\nto perform data pre-processing, e.g., to decorrelate predictor variable"
  },
  {
    "vector_id": 523,
    "chunk_id": "p142_c3",
    "page_number": 142,
    "text": "nt to perform data compression by approximation, e.g., in-\nstead of storingn\u00d7m values just storing(n+m)k values, or when we want\nto perform data pre-processing, e.g., to decorrelate predictor variables of\na design matrix (Ormoneit et al., 2001). The SVD operates on matrices,\nwhich we can interpret as rectangular arrays with two indices (rows and\ncolumns). The extension of matrix-like structure to higher-dimensional\narrays are called tensors. It turns out that the SVD is the special case of\na more general family of decompositions that operate on such tensors\n(Kolda and Bader, 2009). SVD-like operations and low-rank approxima-\ntions on tensors are, for example, theTucker decomposition(Tucker, 1966)Tucker\ndecomposition or the CP decomposition (Carroll and Chang, 1970).\nCP decomposition The SV"
  },
  {
    "vector_id": 524,
    "chunk_id": "p142_c4",
    "page_number": 142,
    "text": "erations and low-rank approxima-\ntions on tensors are, for example, theTucker decomposition(Tucker, 1966)Tucker\ndecomposition or the CP decomposition (Carroll and Chang, 1970).\nCP decomposition The SVD low-rank approximation is frequently used in machine learn-\ning for computational efficiency reasons. This is because it reduces the\namount of memory and operations with nonzero multiplications we need\nto perform on potentially very large matrices of data (Trefethen and Bau III,\n1997). Moreover, low-rank approximations are used to operate on ma-\ntrices that may contain missing values as well as for purposes of lossy\ncompression and dimensionality reduction (Moonen and De Moor, 1995;\nMarkovsky, 2011).\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 525,
    "chunk_id": "p142_c5",
    "page_number": 142,
    "text": "for purposes of lossy\ncompression and dimensionality reduction (Moonen and De Moor, 1995;\nMarkovsky, 2011).\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 526,
    "chunk_id": "p143_c0",
    "page_number": 143,
    "text": "Exercises 137\nExercises\n4.1 Compute the determinant using the Laplace expansion (using the first row)\nand the Sarrus rule for\nA =\n\uf8ee\n\uf8f0\n1 3 5\n2 4 6\n0 2 4\n\uf8f9\n\uf8fb .\n4.2 Compute the following determinant efficiently:\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n2 0 1 2 0\n2 \u22121 0 1 1\n0 1 2 1 2\n\u22122 0 2 \u22121 2\n2 0 0 1 1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n.\n4.3 Compute the eigenspaces of\na.\nA :=\n\u0014\n1 0\n1 1\n\u0015\nb.\nB :=\n\u0014\n\u22122 2\n2 1\n\u0015\n4.4 Compute all eigenspaces of\nA =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n0 \u22121 1 1\n\u22121 1 \u22122 3\n2 \u22121 0 0\n1 \u22121 1 0\n\uf8f9\n\uf8fa\uf8fa\uf8fb .\n4.5 Diagonalizability of a matrix is unrelated to its invertibility . Determine for\nthe following four matrices whether they are diagonalizable and/or invert-\nible\n\u0014\n1 0\n0 1\n\u0015\n,\n\u0014\n1 0\n0 0\n\u0015\n,\n\u0014\n1 1\n0 1\n\u0015\n,\n\u0014\n0 1\n0 0\n\u0015\n.\n4.6 Compute the eigenspaces of the following transformation matrices. Are they\ndiagonalizable?\na. For\nA =\n\uf8ee\n\uf8f0\n2 3 0\n1 4 3\n0 0 1\n\uf8f9\n\uf8fb\nb. For\nA ="
  },
  {
    "vector_id": 527,
    "chunk_id": "p143_c1",
    "page_number": 143,
    "text": "e\n\u0014\n1 0\n0 1\n\u0015\n,\n\u0014\n1 0\n0 0\n\u0015\n,\n\u0014\n1 1\n0 1\n\u0015\n,\n\u0014\n0 1\n0 0\n\u0015\n.\n4.6 Compute the eigenspaces of the following transformation matrices. Are they\ndiagonalizable?\na. For\nA =\n\uf8ee\n\uf8f0\n2 3 0\n1 4 3\n0 0 1\n\uf8f9\n\uf8fb\nb. For\nA =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 1 0 0\n0 0 0 0\n0 0 0 0\n0 0 0 0\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 528,
    "chunk_id": "p144_c0",
    "page_number": 144,
    "text": "138 Matrix Decompositions\n4.7 Are the following matrices diagonalizable? If yes, determine their diagonal\nform and a basis with respect to which the transformation matrices are di-\nagonal. If no, give reasons why they are not diagonalizable.\na.\nA =\n\u0014\n0 1\n\u22128 4\n\u0015\nb.\nA =\n\uf8ee\n\uf8f0\n1 1 1\n1 1 1\n1 1 1\n\uf8f9\n\uf8fb\nc.\nA =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n5 4 2 1\n0 1 \u22121 \u22121\n\u22121 \u22121 3 0\n1 1 \u22121 2\n\uf8f9\n\uf8fa\uf8fa\uf8fb\nd.\nA =\n\uf8ee\n\uf8f0\n5 \u22126 \u22126\n\u22121 4 2\n3 \u22126 \u22124\n\uf8f9\n\uf8fb\n4.8 Find the SVD of the matrix\nA =\n\u0014\n3 2 2\n2 3 \u22122\n\u0015\n.\n4.9 Find the singular value decomposition of\nA =\n\u0014\n2 2\n\u22121 1\n\u0015\n.\n4.10 Find the rank-1 approximation of\nA =\n\u0014\n3 2 2\n2 3 \u22122\n\u0015\n4.11 Show that for any A \u2208 Rm\u00d7n the matrices A\u22a4A and AA\u22a4 possess the\nsame nonzero eigenvalues.\n4.12 Show that for x \u0338= 0 Theorem 4.24 holds, i.e., show that\nmax\nx\n\u2225Ax\u22252\n\u2225x\u22252\n= \u03c31 ,\nwhere \u03c31 is the largest singular value of A \u2208 Rm\u00d7n.\nD"
  },
  {
    "vector_id": 529,
    "chunk_id": "p144_c1",
    "page_number": 144,
    "text": "e matrices A\u22a4A and AA\u22a4 possess the\nsame nonzero eigenvalues.\n4.12 Show that for x \u0338= 0 Theorem 4.24 holds, i.e., show that\nmax\nx\n\u2225Ax\u22252\n\u2225x\u22252\n= \u03c31 ,\nwhere \u03c31 is the largest singular value of A \u2208 Rm\u00d7n.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 530,
    "chunk_id": "p145_c0",
    "page_number": 145,
    "text": "5\nVector Calculus\nMany algorithms in machine learning optimize an objective function with\nrespect to a set of desired model parameters that control how well a model\nexplains the data: Finding good parameters can be phrased as an opti-\nmization problem (see Sections 8.2 and 8.3). Examples include: (i) lin-\near regression (see Chapter 9), where we look at curve-fitting problems\nand optimize linear weight parameters to maximize the likelihood; (ii)\nneural-network auto-encoders for dimensionality reduction and data com-\npression, where the parameters are the weights and biases of each layer,\nand where we minimize a reconstruction error by repeated application of\nthe chain rule; and (iii) Gaussian mixture models (see Chapter 11) for\nmodeling data distributions, where we optimize the location an"
  },
  {
    "vector_id": 531,
    "chunk_id": "p145_c1",
    "page_number": 145,
    "text": "here we minimize a reconstruction error by repeated application of\nthe chain rule; and (iii) Gaussian mixture models (see Chapter 11) for\nmodeling data distributions, where we optimize the location and shape\nparameters of each mixture component to maximize the likelihood of the\nmodel. Figure 5.1 illustrates some of these problems, which we typically\nsolve by using optimization algorithms that exploit gradient information\n(Section 7.1). Figure 5.2 gives an overview of how concepts in this chap-\nter are related and how they are connected to other chapters of the book.\nCentral to this chapter is the concept of a function. A function f is\na quantity that relates two quantities to each other. In this book, these\nquantities are typically inputs x \u2208 RD and targets (function values) f(x),\nwhich we"
  },
  {
    "vector_id": 532,
    "chunk_id": "p145_c2",
    "page_number": 145,
    "text": "the concept of a function. A function f is\na quantity that relates two quantities to each other. In this book, these\nquantities are typically inputs x \u2208 RD and targets (function values) f(x),\nwhich we assume are real-valued if not stated otherwise. Here RD is the\ndomain of f, and the function values f(x) are the image/codomain of f. domain\nimage/codomain\nFigure 5.1 Vector\ncalculus plays a\ncentral role in (a)\nregression (curve\nfitting) and (b)\ndensity estimation,\ni.e., modeling data\ndistributions.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE\n(a) Regression problem: Find parameters,\nsuch that the curve explains the observations\n(crosses) well.\n\u221210 \u22125 0 5 10\nx1\n\u221210\n\u22125\n0\n5\n10\nx2\n(b) Density estimation with a Gaussian mixture\nmodel: Find means and covariances, such that\nthe data (dots) can be"
  },
  {
    "vector_id": 533,
    "chunk_id": "p145_c3",
    "page_number": 145,
    "text": "he curve explains the observations\n(crosses) well.\n\u221210 \u22125 0 5 10\nx1\n\u221210\n\u22125\n0\n5\n10\nx2\n(b) Density estimation with a Gaussian mixture\nmodel: Find means and covariances, such that\nthe data (dots) can be explained well.\n139\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 534,
    "chunk_id": "p146_c0",
    "page_number": 146,
    "text": "140 Vector Calculus\nFigure 5.2 A mind\nmap of the concepts\nintroduced in this\nchapter, along with\nwhen they are used\nin other parts of the\nbook.\nDifference quotient\nPartial derivatives\nJacobianHessian\nTaylor series\nChapter 7Optimization\nChapter 6Probability\nChapter 9Regression\nChapter 10Dimensionalityreduction\nChapter 11Density estimation\nChapter 12Classification\ndefinescollected inused in\nused in\nused in\nused in\nused in\nused in\nused in\nSection 2.7.3 provides much more detailed discussion in the context of\nlinear functions. We often write\nf : RD \u2192 R (5.1a)\nx 7\u2192 f(x) (5.1b)\nto specify a function, where (5.1a) specifies that f is a mapping from\nRD to R and (5.1b) specifies the explicit assignment of an input x to\na function value f(x). A function f assigns every input x exactly one\nfunction v"
  },
  {
    "vector_id": 535,
    "chunk_id": "p146_c1",
    "page_number": 146,
    "text": ", where (5.1a) specifies that f is a mapping from\nRD to R and (5.1b) specifies the explicit assignment of an input x to\na function value f(x). A function f assigns every input x exactly one\nfunction value f(x).\nExample 5.1\nRecall the dot product as a special case of an inner product (Section 3.2).\nIn the previous notation, the function f(x) = x\u22a4x, x \u2208 R2, would be\nspecified as\nf : R2 \u2192 R (5.2a)\nx 7\u2192 x2\n1 + x2\n2 . (5.2b)\nIn this chapter, we will discuss how to compute gradients of functions,\nwhich is often essential to facilitate learning in machine learning models\nsince the gradient points in the direction of steepest ascent. Therefore,\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 536,
    "chunk_id": "p146_c2",
    "page_number": 146,
    "text": "the direction of steepest ascent. Therefore,\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 537,
    "chunk_id": "p147_c0",
    "page_number": 147,
    "text": "5.1 Differentiation of Univariate Functions 141\nFigure 5.3 The\naverage incline of a\nfunction f between\nx0 and x0 + \u03b4x is\nthe incline of the\nsecant (blue)\nthrough f(x0) and\nf(x0 + \u03b4x) and\ngiven by \u03b4y/\u03b4x.\n\u03b4y\n\u03b4x\nf(x)\nx\ny\nf(x0)\nf(x0 + \u03b4x)\nvector calculus is one of the fundamental mathematical tools we need in\nmachine learning. Throughout this book, we assume that functions are\ndifferentiable. With some additional technical definitions, which we do\nnot cover here, many of the approaches presented can be extended to\nsub-differentials (functions that are continuous but not differentiable at\ncertain points). We will look at an extension to the case of functions with\nconstraints in Chapter 7.\n5.1 Differentiation of Univariate Functions\nIn the following, we briefly revisit differentiation of a univa"
  },
  {
    "vector_id": 538,
    "chunk_id": "p147_c1",
    "page_number": 147,
    "text": "oints). We will look at an extension to the case of functions with\nconstraints in Chapter 7.\n5.1 Differentiation of Univariate Functions\nIn the following, we briefly revisit differentiation of a univariate function,\nwhich may be familiar from high school mathematics. We start with the\ndifference quotient of a univariate function y = f(x), x, y\u2208 R, which we\nwill subsequently use to define derivatives.\nDefinition 5.1 (Difference Quotient). The difference quotient difference quotient\n\u03b4y\n\u03b4x := f(x + \u03b4x) \u2212 f(x)\n\u03b4x (5.3)\ncomputes the slope of the secant line through two points on the graph of\nf. In Figure 5.3, these are the points with x-coordinates x0 and x0 + \u03b4x.\nThe difference quotient can also be considered the average slope of f\nbetween x and x + \u03b4x if we assume f to be a linear function. I"
  },
  {
    "vector_id": 539,
    "chunk_id": "p147_c2",
    "page_number": 147,
    "text": "Figure 5.3, these are the points with x-coordinates x0 and x0 + \u03b4x.\nThe difference quotient can also be considered the average slope of f\nbetween x and x + \u03b4x if we assume f to be a linear function. In the limit\nfor \u03b4x \u2192 0, we obtain the tangent of f at x, if f is differentiable. The\ntangent is then the derivative of f at x.\nDefinition 5.2 (Derivative). More formally , forh >0 the derivative of f derivative\nat x is defined as the limit\ndf\ndx := lim\nh\u21920\nf(x + h) \u2212 f(x)\nh , (5.4)\nand the secant in Figure 5.3 becomes a tangent.\nThe derivative of f points in the direction of steepest ascent of f.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 540,
    "chunk_id": "p147_c3",
    "page_number": 147,
    "text": "\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 541,
    "chunk_id": "p148_c0",
    "page_number": 148,
    "text": "142 Vector Calculus\nExample 5.2 (Derivative of a Polynomial)\nWe want to compute the derivative of f(x) = xn, n\u2208 N. We may already\nknow that the answer will be nxn\u22121, but we want to derive this result\nusing the definition of the derivative as the limit of the difference quotient.\nUsing the definition of the derivative in (5.4), we obtain\ndf\ndx = lim\nh\u21920\nf(x + h) \u2212 f(x)\nh (5.5a)\n= lim\nh\u21920\n(x + h)n \u2212 xn\nh (5.5b)\n= lim\nh\u21920\nPn\ni=0\n\u0000n\ni\n\u0001\nxn\u2212ihi \u2212 xn\nh . (5.5c)\nWe see thatxn =\n\u0000n\n0\n\u0001\nxn\u22120h0. By starting the sum at1, the xn-term cancels,\nand we obtain\ndf\ndx = lim\nh\u21920\nPn\ni=1\n\u0000n\ni\n\u0001\nxn\u2212ihi\nh (5.6a)\n= lim\nh\u21920\nnX\ni=1\n \nn\ni\n!\nxn\u2212ihi\u22121 (5.6b)\n= lim\nh\u21920\n \nn\n1\n!\nxn\u22121 +\nnX\ni=2\n \nn\ni\n!\nxn\u2212ihi\u22121\n| {z }\n\u21920 as h\u21920\n(5.6c)\n= n!\n1!(n \u2212 1)!xn\u22121 = nxn\u22121 . (5.6d)\n5.1.1 Taylor Series\nThe Taylor series is a represent"
  },
  {
    "vector_id": 542,
    "chunk_id": "p148_c1",
    "page_number": 148,
    "text": "im\nh\u21920\nnX\ni=1\n \nn\ni\n!\nxn\u2212ihi\u22121 (5.6b)\n= lim\nh\u21920\n \nn\n1\n!\nxn\u22121 +\nnX\ni=2\n \nn\ni\n!\nxn\u2212ihi\u22121\n| {z }\n\u21920 as h\u21920\n(5.6c)\n= n!\n1!(n \u2212 1)!xn\u22121 = nxn\u22121 . (5.6d)\n5.1.1 Taylor Series\nThe Taylor series is a representation of a function f as an infinite sum of\nterms. These terms are determined using derivatives of f evaluated at x0.\nDefinition 5.3 (Taylor Polynomial). The Taylor polynomial of degree n ofTaylor polynomial\nf : R \u2192 R at x0 is defined asWe define t0 := 1\nfor all t \u2208 R.\nTn(x) :=\nnX\nk=0\nf(k)(x0)\nk! (x \u2212 x0)k , (5.7)\nwhere f(k)(x0) is the kth derivative of f at x0 (which we assume exists)\nand f(k)(x0)\nk! are the coefficients of the polynomial.\nDefinition 5.4 (Taylor Series). For a smooth functionf \u2208 C\u221e, f : R \u2192 R,\nthe Taylor series of f at x0 is defined asTaylor series\nDraft (2024-01-15) of \u201cMath"
  },
  {
    "vector_id": 543,
    "chunk_id": "p148_c2",
    "page_number": 148,
    "text": ")\nk! are the coefficients of the polynomial.\nDefinition 5.4 (Taylor Series). For a smooth functionf \u2208 C\u221e, f : R \u2192 R,\nthe Taylor series of f at x0 is defined asTaylor series\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 544,
    "chunk_id": "p149_c0",
    "page_number": 149,
    "text": "5.1 Differentiation of Univariate Functions 143\nT\u221e(x) =\n\u221eX\nk=0\nf(k)(x0)\nk! (x \u2212 x0)k . (5.8)\nFor x0 = 0 , we obtain the Maclaurin series as a special instance of the f \u2208 C\u221e means that\nf is continuously\ndifferentiable\ninfinitely many\ntimes.\nMaclaurin series\nTaylor series. If f(x) = T\u221e(x), then f is called analytic.\nanalytic\nRemark. In general, a Taylor polynomial of degree n is an approximation\nof a function, which does not need to be a polynomial. The Taylor poly-\nnomial is similar to f in a neighborhood around x0. However, a Taylor\npolynomial of degree n is an exact representation of a polynomial f of\ndegree k \u2a7d n since all derivatives f(i), i > kvanish. \u2662\nExample 5.3 (Taylor Polynomial)\nWe consider the polynomial\nf(x) = x4 (5.9)\nand seek the Taylor polynomial T6, evaluated at x0 = 1. We"
  },
  {
    "vector_id": 545,
    "chunk_id": "p149_c1",
    "page_number": 149,
    "text": "mial f of\ndegree k \u2a7d n since all derivatives f(i), i > kvanish. \u2662\nExample 5.3 (Taylor Polynomial)\nWe consider the polynomial\nf(x) = x4 (5.9)\nand seek the Taylor polynomial T6, evaluated at x0 = 1. We start by com-\nputing the coefficients f(k)(1) for k = 0, . . . ,6:\nf(1) = 1 (5.10)\nf\u2032(1) = 4 (5.11)\nf\u2032\u2032(1) = 12 (5.12)\nf(3)(1) = 24 (5.13)\nf(4)(1) = 24 (5.14)\nf(5)(1) = 0 (5.15)\nf(6)(1) = 0 (5.16)\nTherefore, the desired Taylor polynomial is\nT6(x) =\n6X\nk=0\nf(k)(x0)\nk! (x \u2212 x0)k (5.17a)\n= 1 + 4(x \u2212 1) + 6(x \u2212 1)2 + 4(x \u2212 1)3 + (x \u2212 1)4 + 0. (5.17b)\nMultiplying out and re-arranging yields\nT6(x) = (1 \u2212 4 + 6\u2212 4 + 1) +x(4 \u2212 12 + 12\u2212 4)\n+ x2(6 \u2212 12 + 6) +x3(4 \u2212 4) + x4 (5.18a)\n= x4 = f(x) , (5.18b)\ni.e., we obtain an exact representation of the original function.\n\u00a92024 M. P. Deisenroth, A. A. Faisal"
  },
  {
    "vector_id": 546,
    "chunk_id": "p149_c2",
    "page_number": 149,
    "text": "\u2212 4 + 6\u2212 4 + 1) +x(4 \u2212 12 + 12\u2212 4)\n+ x2(6 \u2212 12 + 6) +x3(4 \u2212 4) + x4 (5.18a)\n= x4 = f(x) , (5.18b)\ni.e., we obtain an exact representation of the original function.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 547,
    "chunk_id": "p150_c0",
    "page_number": 150,
    "text": "144 Vector Calculus\nFigure 5.4 Taylor\npolynomials. The\noriginal function\nf(x) =\nsin(x) + cos(x)\n(black, solid) is\napproximated by\nTaylor polynomials\n(dashed) around\nx0 = 0.\nHigher-order Taylor\npolynomials\napproximate the\nfunction f better\nand more globally .\nT10 is already\nsimilar to f in\n[\u22124, 4].\n\u22124 \u22122 0 2 4\nx\n\u22122\n0\n2\n4\ny\nf\nT0\nT1\nT5\nT10\nExample 5.4 (Taylor Series)\nConsider the function in Figure 5.4 given by\nf(x) = sin(x) + cos(x) \u2208 C\u221e . (5.19)\nWe seek a Taylor series expansion of f at x0 = 0, which is the Maclaurin\nseries expansion of f. We obtain the following derivatives:\nf(0) = sin(0) + cos(0) = 1 (5.20)\nf\u2032(0) = cos(0) \u2212 sin(0) = 1 (5.21)\nf\u2032\u2032(0) = \u2212sin(0) \u2212 cos(0) = \u22121 (5.22)\nf(3)(0) = \u2212cos(0) + sin(0) = \u22121 (5.23)\nf(4)(0) = sin(0) + cos(0) = f(0) = 1 (5.24)\n...\nWe can see a pattern her"
  },
  {
    "vector_id": 548,
    "chunk_id": "p150_c1",
    "page_number": 150,
    "text": "s(0) = 1 (5.20)\nf\u2032(0) = cos(0) \u2212 sin(0) = 1 (5.21)\nf\u2032\u2032(0) = \u2212sin(0) \u2212 cos(0) = \u22121 (5.22)\nf(3)(0) = \u2212cos(0) + sin(0) = \u22121 (5.23)\nf(4)(0) = sin(0) + cos(0) = f(0) = 1 (5.24)\n...\nWe can see a pattern here: The coefficients in our Taylor series are only\n\u00b11 (since sin(0) = 0), each of which occurs twice before switching to the\nother one. Furthermore, f(k+4)(0) = f(k)(0).\nTherefore, the full Taylor series expansion of f at x0 = 0 is given by\nT\u221e(x) =\n\u221eX\nk=0\nf(k)(x0)\nk! (x \u2212 x0)k (5.25a)\n= 1 + x \u2212 1\n2!x2 \u2212 1\n3!x3 + 1\n4!x4 + 1\n5!x5 \u2212 \u00b7 \u00b7 \u00b7 (5.25b)\n= 1 \u2212 1\n2!x2 + 1\n4!x4 \u2213 \u00b7 \u00b7 \u00b7+ x \u2212 1\n3!x3 + 1\n5!x5 \u2213 \u00b7 \u00b7\u00b7 (5.25c)\n=\n\u221eX\nk=0\n(\u22121)k 1\n(2k)!x2k +\n\u221eX\nk=0\n(\u22121)k 1\n(2k + 1)!x2k+1 (5.25d)\n= cos(x) + sin(x) , (5.25e)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 549,
    "chunk_id": "p150_c2",
    "page_number": 150,
    "text": "\u00b7\u00b7 (5.25c)\n=\n\u221eX\nk=0\n(\u22121)k 1\n(2k)!x2k +\n\u221eX\nk=0\n(\u22121)k 1\n(2k + 1)!x2k+1 (5.25d)\n= cos(x) + sin(x) , (5.25e)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 550,
    "chunk_id": "p151_c0",
    "page_number": 151,
    "text": "5.1 Differentiation of Univariate Functions 145\nwhere we used the power series representations power series\nrepresentation\ncos(x) =\n\u221eX\nk=0\n(\u22121)k 1\n(2k)!x2k , (5.26)\nsin(x) =\n\u221eX\nk=0\n(\u22121)k 1\n(2k + 1)!x2k+1 . (5.27)\nFigure 5.4 shows the corresponding first Taylor polynomials Tn for n =\n0, 1, 5, 10.\nRemark. A Taylor series is a special case of a power series\nf(x) =\n\u221eX\nk=0\nak(x \u2212 c)k (5.28)\nwhere ak are coefficients and c is a constant, which has the special form\nin Definition 5.4. \u2662\n5.1.2 Differentiation Rules\nIn the following, we briefly state basic differentiation rules, where we\ndenote the derivative of f by f\u2032.\nProduct rule: (f(x)g(x))\u2032 = f\u2032(x)g(x) + f(x)g\u2032(x) (5.29)\nQuotient rule:\n\u0012f(x)\ng(x)\n\u0013\u2032\n= f\u2032(x)g(x) \u2212 f(x)g\u2032(x)\n(g(x))2 (5.30)\nSum rule: (f(x) + g(x))\u2032 = f\u2032(x) + g\u2032(x) (5.31)\nChain ru"
  },
  {
    "vector_id": 551,
    "chunk_id": "p151_c1",
    "page_number": 151,
    "text": "vative of f by f\u2032.\nProduct rule: (f(x)g(x))\u2032 = f\u2032(x)g(x) + f(x)g\u2032(x) (5.29)\nQuotient rule:\n\u0012f(x)\ng(x)\n\u0013\u2032\n= f\u2032(x)g(x) \u2212 f(x)g\u2032(x)\n(g(x))2 (5.30)\nSum rule: (f(x) + g(x))\u2032 = f\u2032(x) + g\u2032(x) (5.31)\nChain rule:\n\u0000\ng(f(x))\n\u0001\u2032\n= (g \u25e6 f)\u2032(x) = g\u2032(f(x))f\u2032(x) (5.32)\nHere, g \u25e6 f denotes function composition x 7\u2192 f(x) 7\u2192 g(f(x)).\nExample 5.5 (Chain Rule)\nLet us compute the derivative of the function h(x) = (2x + 1)4 using the\nchain rule. With\nh(x) = (2x + 1)4 = g(f(x)) , (5.33)\nf(x) = 2x + 1, (5.34)\ng(f) = f4 , (5.35)\nwe obtain the derivatives of f and g as\nf\u2032(x) = 2 , (5.36)\ng\u2032(f) = 4f3 , (5.37)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 552,
    "chunk_id": "p151_c2",
    "page_number": 151,
    "text": "Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 553,
    "chunk_id": "p152_c0",
    "page_number": 152,
    "text": "146 Vector Calculus\nsuch that the derivative of h is given as\nh\u2032(x) = g\u2032(f)f\u2032(x) = (4f3) \u00b7 2\n(5.34)\n= 4(2 x + 1)3 \u00b7 2 = 8(2x + 1)3 , (5.38)\nwhere we used the chain rule (5.32) and substituted the definition of f\nin (5.34) in g\u2032(f).\n5.2 Partial Differentiation and Gradients\nDifferentiation as discussed in Section 5.1 applies to functions f of a\nscalar variable x \u2208 R. In the following, we consider the general case\nwhere the function f depends on one or more variables x \u2208 Rn, e.g.,\nf(x) = f(x1, x2). The generalization of the derivative to functions of sev-\neral variables is the gradient.\nWe find the gradient of the function f with respect to x by varying one\nvariable at a time and keeping the others constant. The gradient is then\nthe collection of these partial derivatives.\nDefinition 5.5 (Pa"
  },
  {
    "vector_id": 554,
    "chunk_id": "p152_c1",
    "page_number": 152,
    "text": "the gradient of the function f with respect to x by varying one\nvariable at a time and keeping the others constant. The gradient is then\nthe collection of these partial derivatives.\nDefinition 5.5 (Partial Derivative). For a function f : Rn \u2192 R, x 7\u2192\nf(x), x \u2208 Rn of n variables x1, . . . , xn we define the partial derivatives aspartial derivative\n\u2202f\n\u2202x1\n= lim\nh\u21920\nf(x1 + h, x2, . . . , xn) \u2212 f(x)\nh\n...\n\u2202f\n\u2202xn\n= lim\nh\u21920\nf(x1, . . . , xn\u22121, xn + h) \u2212 f(x)\nh\n(5.39)\nand collect them in the row vector\n\u2207xf = gradf = df\ndx =\n\u0014\u2202f (x)\n\u2202x1\n\u2202f (x)\n\u2202x2\n\u00b7 \u00b7\u00b7 \u2202f (x)\n\u2202xn\n\u0015\n\u2208 R1\u00d7n , (5.40)\nwhere n is the number of variables and 1 is the dimension of the image/\nrange/codomain of f. Here, we defined the column vectorx = [x1, . . . , xn]\u22a4\n\u2208 Rn. The row vector in (5.40) is called the gradient of f or the Jacob"
  },
  {
    "vector_id": 555,
    "chunk_id": "p152_c2",
    "page_number": 152,
    "text": "r of variables and 1 is the dimension of the image/\nrange/codomain of f. Here, we defined the column vectorx = [x1, . . . , xn]\u22a4\n\u2208 Rn. The row vector in (5.40) is called the gradient of f or the Jacobiangradient\nJacobian and is the generalization of the derivative from Section 5.1.\nRemark. This definition of the Jacobian is a special case of the general\ndefinition of the Jacobian for vector-valued functions as the collection of\npartial derivatives. We will get back to this in Section 5.3. \u2662We can use results\nfrom scalar\ndifferentiation: Each\npartial derivative is\na derivative with\nrespect to a scalar.\nExample 5.6 (Partial Derivatives Using the Chain Rule)\nFor f(x, y) = (x + 2y3)2, we obtain the partial derivatives\n\u2202f (x, y)\n\u2202x = 2(x + 2y3) \u2202\n\u2202x (x + 2y3) = 2(x + 2y3) , (5.41)\nDraft (2024-0"
  },
  {
    "vector_id": 556,
    "chunk_id": "p152_c3",
    "page_number": 152,
    "text": "scalar.\nExample 5.6 (Partial Derivatives Using the Chain Rule)\nFor f(x, y) = (x + 2y3)2, we obtain the partial derivatives\n\u2202f (x, y)\n\u2202x = 2(x + 2y3) \u2202\n\u2202x (x + 2y3) = 2(x + 2y3) , (5.41)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 557,
    "chunk_id": "p153_c0",
    "page_number": 153,
    "text": "5.2 Partial Differentiation and Gradients 147\n\u2202f (x, y)\n\u2202y = 2(x + 2y3) \u2202\n\u2202y (x + 2y3) = 12(x + 2y3)y2 . (5.42)\nwhere we used the chain rule (5.32) to compute the partial derivatives.\nRemark (Gradient as a Row Vector). It is not uncommon in the literature\nto define the gradient vector as a column vector, following the conven-\ntion that vectors are generally column vectors. The reason why we define\nthe gradient vector as a row vector is twofold: First, we can consistently\ngeneralize the gradient to vector-valued functions f : Rn \u2192 Rm (then\nthe gradient becomes a matrix). Second, we can immediately apply the\nmulti-variate chain rule without paying attention to the dimension of the\ngradient. We will discuss both points in Section 5.3. \u2662\nExample 5.7 (Gradient)\nFor f(x1, x2) = x2\n1x2 + x1x3\n2 \u2208"
  },
  {
    "vector_id": 558,
    "chunk_id": "p153_c1",
    "page_number": 153,
    "text": "ely apply the\nmulti-variate chain rule without paying attention to the dimension of the\ngradient. We will discuss both points in Section 5.3. \u2662\nExample 5.7 (Gradient)\nFor f(x1, x2) = x2\n1x2 + x1x3\n2 \u2208 R, the partial derivatives (i.e., the deriva-\ntives of f with respect to x1 and x2) are\n\u2202f (x1, x2)\n\u2202x1\n= 2x1x2 + x3\n2 (5.43)\n\u2202f (x1, x2)\n\u2202x2\n= x2\n1 + 3x1x2\n2 (5.44)\nand the gradient is then\ndf\ndx =\n\u0014\u2202f (x1, x2)\n\u2202x1\n\u2202f (x1, x2)\n\u2202x2\n\u0015\n=\n\u0002\n2x1x2 + x3\n2 x2\n1 + 3x1x2\n2\n\u0003\n\u2208 R1\u00d72 .\n(5.45)\n5.2.1 Basic Rules of Partial Differentiation\nProduct rule:\n(fg)\u2032 = f\u2032g + fg\u2032,\nSum rule:\n(f + g)\u2032 = f\u2032 + g\u2032,\nChain rule:\n(g(f))\u2032 = g\u2032(f)f\u2032\nIn the multivariate case, wherex \u2208 Rn, the basic differentiation rules that\nwe know from school (e.g., sum rule, product rule, chain rule; see also\nSection 5.1.2) still apply ."
  },
  {
    "vector_id": 559,
    "chunk_id": "p153_c2",
    "page_number": 153,
    "text": "ule:\n(g(f))\u2032 = g\u2032(f)f\u2032\nIn the multivariate case, wherex \u2208 Rn, the basic differentiation rules that\nwe know from school (e.g., sum rule, product rule, chain rule; see also\nSection 5.1.2) still apply . However, when we compute derivatives with re-\nspect to vectors x \u2208 Rn we need to pay attention: Our gradients now\ninvolve vectors and matrices, and matrix multiplication is not commuta-\ntive (Section 2.2.1), i.e., the order matters.\nHere are the general product rule, sum rule, and chain rule:\nProduct rule: \u2202\n\u2202x\n\u0000\nf(x)g(x)\n\u0001\n= \u2202f\n\u2202xg(x) + f(x) \u2202g\n\u2202x (5.46)\nSum rule: \u2202\n\u2202x\n\u0000\nf(x) + g(x)\n\u0001\n= \u2202f\n\u2202x + \u2202g\n\u2202x (5.47)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 560,
    "chunk_id": "p153_c3",
    "page_number": 153,
    "text": "g\n\u2202x (5.47)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 561,
    "chunk_id": "p154_c0",
    "page_number": 154,
    "text": "148 Vector Calculus\nChain rule: \u2202\n\u2202x(g \u25e6 f)(x) = \u2202\n\u2202x\n\u0000\ng(f(x))\n\u0001\n= \u2202g\n\u2202f\n\u2202f\n\u2202x (5.48)\nLet us have a closer look at the chain rule. The chain rule (5.48) resem-This is only an\nintuition, but not\nmathematically\ncorrect since the\npartial derivative is\nnot a fraction.\nbles to some degree the rules for matrix multiplication where we said that\nneighboring dimensions have to match for matrix multiplication to be de-\nfined; see Section 2.2.1. If we go from left to right, the chain rule exhibits\nsimilar properties: \u2202f shows up in the \u201cdenominator\u201d of the first factor\nand in the \u201cnumerator\u201d of the second factor. If we multiply the factors to-\ngether, multiplication is defined, i.e., the dimensions of\u2202f match, and \u2202f\n\u201ccancels\u201d, such that \u2202g/\u2202 x remains.\n5.2.2 Chain Rule\nConsider a function f : R2 \u2192"
  },
  {
    "vector_id": 562,
    "chunk_id": "p154_c1",
    "page_number": 154,
    "text": "nd factor. If we multiply the factors to-\ngether, multiplication is defined, i.e., the dimensions of\u2202f match, and \u2202f\n\u201ccancels\u201d, such that \u2202g/\u2202 x remains.\n5.2.2 Chain Rule\nConsider a function f : R2 \u2192 R of two variables x1, x2. Furthermore,\nx1(t) and x2(t) are themselves functions of t. To compute the gradient of\nf with respect to t, we need to apply the chain rule (5.48) for multivariate\nfunctions as\ndf\ndt =\nh\n\u2202f\n\u2202x1\n\u2202f\n\u2202x2\ni\"\n\u2202x1(t)\n\u2202t\u2202x2(t)\n\u2202t\n#\n= \u2202f\n\u2202x1\n\u2202x1\n\u2202t + \u2202f\n\u2202x2\n\u2202x2\n\u2202t , (5.49)\nwhere d denotes the gradient and \u2202 partial derivatives.\nExample 5.8\nConsider f(x1, x2) = x2\n1 + 2x2, where x1 = sin t and x2 = cos t, then\ndf\ndt = \u2202f\n\u2202x1\n\u2202x1\n\u2202t + \u2202f\n\u2202x2\n\u2202x2\n\u2202t (5.50a)\n= 2 sint\u2202 sin t\n\u2202t + 2\u2202 cos t\n\u2202t (5.50b)\n= 2 sint cos t \u2212 2 sint = 2 sint(cos t \u2212 1) (5.50c)\nis the corresponding derivati"
  },
  {
    "vector_id": 563,
    "chunk_id": "p154_c2",
    "page_number": 154,
    "text": "x1 = sin t and x2 = cos t, then\ndf\ndt = \u2202f\n\u2202x1\n\u2202x1\n\u2202t + \u2202f\n\u2202x2\n\u2202x2\n\u2202t (5.50a)\n= 2 sint\u2202 sin t\n\u2202t + 2\u2202 cos t\n\u2202t (5.50b)\n= 2 sint cos t \u2212 2 sint = 2 sint(cos t \u2212 1) (5.50c)\nis the corresponding derivative of f with respect to t.\nIf f(x1, x2) is a function of x1 and x2, where x1(s, t) and x2(s, t) are\nthemselves functions of two variables s and t, the chain rule yields the\npartial derivatives\n\u2202f\n\u2202s = \u2202f\n\u2202x1\n\u2202x1\n\u2202s + \u2202f\n\u2202x2\n\u2202x2\n\u2202s , (5.51)\n\u2202f\n\u2202t = \u2202f\n\u2202x1\n\u2202x1\n\u2202t + \u2202f\n\u2202x2\n\u2202x2\n\u2202t , (5.52)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 564,
    "chunk_id": "p155_c0",
    "page_number": 155,
    "text": "5.3 Gradients of Vector-Valued Functions 149\nand the gradient is obtained by the matrix multiplication\ndf\nd(s, t) = \u2202f\n\u2202x\n\u2202x\n\u2202(s, t) =\nh \u2202f\n\u2202x1\n\u2202f\n\u2202x2\ni\n| {z }\n=\n\u2202f\n\u2202x\n\uf8ee\n\uf8ef\uf8f0\n\u2202x1\n\u2202s\n\u2202x1\n\u2202t\n\u2202x2\n\u2202s\n\u2202x2\n\u2202t\n\uf8f9\n\uf8fa\uf8fb\n| {z }\n=\n\u2202x\n\u2202(s, t)\n. (5.53)\nThis compact way of writing the chain rule as a matrix multiplication only The chain rule can\nbe written as a\nmatrix\nmultiplication.\nmakes sense if the gradient is defined as a row vector. Otherwise, we will\nneed to start transposing gradients for the matrix dimensions to match.\nThis may still be straightforward as long as the gradient is a vector or a\nmatrix; however, when the gradient becomes a tensor (we will discuss this\nin the following), the transpose is no longer a triviality .\nRemark (Verifying the Correctness of a Gradient Implementation) . The\ndefin"
  },
  {
    "vector_id": 565,
    "chunk_id": "p155_c1",
    "page_number": 155,
    "text": "wever, when the gradient becomes a tensor (we will discuss this\nin the following), the transpose is no longer a triviality .\nRemark (Verifying the Correctness of a Gradient Implementation) . The\ndefinition of the partial derivatives as the limit of the corresponding dif-\nference quotient (see (5.39)) can be exploited when numerically checking\nthe correctness of gradients in computer programs: When we compute Gradient checking\ngradients and implement them, we can use finite differences to numer-\nically test our computation and implementation: We choose the value h\nto be small (e.g., h = 10\u22124) and compare the finite-difference approxima-\ntion from (5.39) with our (analytic) implementation of the gradient. If the\nerror is small, our gradient implementation is probably correct. \u201cSmall\u201d\ncould m"
  },
  {
    "vector_id": 566,
    "chunk_id": "p155_c2",
    "page_number": 155,
    "text": "nd compare the finite-difference approxima-\ntion from (5.39) with our (analytic) implementation of the gradient. If the\nerror is small, our gradient implementation is probably correct. \u201cSmall\u201d\ncould mean that\nqP\ni(dhi\u2212d fi)2\nP\ni(dhi+d fi)2 < 10\u22126, where dhi is the finite-difference\napproximation and d fi is the analytic gradient of f with respect to the ith\nvariable xi. \u2662\n5.3 Gradients of Vector-Valued Functions\nThus far, we discussed partial derivatives and gradients of functions f :\nRn \u2192 R mapping to the real numbers. In the following, we will generalize\nthe concept of the gradient to vector-valued functions (vector fields) f :\nRn \u2192 Rm, where n \u2a7e 1 and m >1.\nFor a function f : Rn \u2192 Rm and a vector x = [x1, . . . , xn]\u22a4 \u2208 Rn, the\ncorresponding vector of function values is given as\nf(x) ="
  },
  {
    "vector_id": 567,
    "chunk_id": "p155_c3",
    "page_number": 155,
    "text": "-valued functions (vector fields) f :\nRn \u2192 Rm, where n \u2a7e 1 and m >1.\nFor a function f : Rn \u2192 Rm and a vector x = [x1, . . . , xn]\u22a4 \u2208 Rn, the\ncorresponding vector of function values is given as\nf(x) =\n\uf8ee\n\uf8ef\uf8f0\nf1(x)\n...\nfm(x)\n\uf8f9\n\uf8fa\uf8fb \u2208 Rm . (5.54)\nWriting the vector-valued function in this way allows us to view a vector-\nvalued function f : Rn \u2192 Rm as a vector of functions [f1, . . . , fm]\u22a4,\nfi : Rn \u2192 R that map onto R. The differentiation rules for every fi are\nexactly the ones we discussed in Section 5.2.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 568,
    "chunk_id": "p155_c4",
    "page_number": 155,
    "text": "."
  },
  {
    "vector_id": 569,
    "chunk_id": "p156_c0",
    "page_number": 156,
    "text": "150 Vector Calculus\nTherefore, the partial derivative of a vector-valued function f : Rn \u2192\nRm with respect to xi \u2208 R, i = 1, . . . n, is given as the vector\n\u2202f\n\u2202xi\n=\n\uf8ee\n\uf8ef\uf8f0\n\u2202f1\n\u2202xi\n...\n\u2202fm\n\u2202xi\n\uf8f9\n\uf8fa\uf8fb =\n\uf8ee\n\uf8ef\uf8f0\nlimh\u21920\nf1(x1,...,xi\u22121,xi+h,xi+1,...xn)\u2212f1(x)\nh\n...\nlimh\u21920\nfm(x1,...,xi\u22121,xi+h,xi+1,...xn)\u2212fm(x)\nh\n\uf8f9\n\uf8fa\uf8fb \u2208 Rm .\n(5.55)\nFrom (5.40), we know that the gradient of f with respect to a vector is\nthe row vector of the partial derivatives. In (5.55), every partial derivative\n\u2202f/\u2202xi is itself a column vector. Therefore, we obtain the gradient of f :\nRn \u2192 Rm with respect to x \u2208 Rn by collecting these partial derivatives:\ndf(x)\ndx = \u2202f(x)\n\u2202x1\n\u00b7 \u00b7\u00b7\u2202f(x)\n\u2202xn\n\u0014 \u0015\n(5.56a)\n=\n\u2202f1(x)\n\u2202x1\n\u00b7 \u00b7\u00b7 \u2202f1(x)\n\u2202xn\n... ...\n\u2202fm(x)\n\u2202x1\n\u00b7 \u00b7\u00b7\u2202fm(x)\n\u2202xn\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u2208 Rm\u00d7n . (5.56b)\nDefinition 5.6 (Jacobian). The collecti"
  },
  {
    "vector_id": 570,
    "chunk_id": "p156_c1",
    "page_number": 156,
    "text": "tial derivatives:\ndf(x)\ndx = \u2202f(x)\n\u2202x1\n\u00b7 \u00b7\u00b7\u2202f(x)\n\u2202xn\n\u0014 \u0015\n(5.56a)\n=\n\u2202f1(x)\n\u2202x1\n\u00b7 \u00b7\u00b7 \u2202f1(x)\n\u2202xn\n... ...\n\u2202fm(x)\n\u2202x1\n\u00b7 \u00b7\u00b7\u2202fm(x)\n\u2202xn\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u2208 Rm\u00d7n . (5.56b)\nDefinition 5.6 (Jacobian). The collection of all first-order partial deriva-\ntives of a vector-valued function f : Rn \u2192 Rm is called the Jacobian. TheJacobian\nJacobian J is an m \u00d7 n matrix, which we define and arrange as follows:The gradient of a\nfunction\nf : Rn \u2192 Rm is a\nmatrix of size\nm \u00d7 n.\nJ = \u2207xf = df(x)\ndx =\n\u0014\u2202f(x)\n\u2202x1\n\u00b7 \u00b7\u00b7 \u2202f(x)\n\u2202xn\n\u0015\n(5.57)\n=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u2202f1(x)\n\u2202x1\n\u00b7 \u00b7\u00b7 \u2202f1(x)\n\u2202xn\n... ...\n\u2202fm(x)\n\u2202x1\n\u00b7 \u00b7\u00b7 \u2202fm(x)\n\u2202xn\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, (5.58)\nx =\n\uf8ee\n\uf8ef\uf8f0\nx1\n...\nxn\n\uf8f9\n\uf8fa\uf8fb , J (i, j) = \u2202fi\n\u2202xj\n. (5.59)\nAs a special case of (5.58), a function f : Rn \u2192 R1, which maps a\nvector x \u2208 Rn onto a scalar (e.g., f(x) = Pn\ni=1 xi), possesses a Jacobian\nth"
  },
  {
    "vector_id": 571,
    "chunk_id": "p156_c2",
    "page_number": 156,
    "text": "8)\nx =\n\uf8ee\n\uf8ef\uf8f0\nx1\n...\nxn\n\uf8f9\n\uf8fa\uf8fb , J (i, j) = \u2202fi\n\u2202xj\n. (5.59)\nAs a special case of (5.58), a function f : Rn \u2192 R1, which maps a\nvector x \u2208 Rn onto a scalar (e.g., f(x) = Pn\ni=1 xi), possesses a Jacobian\nthat is a row vector (matrix of dimension 1 \u00d7 n); see (5.40).\nRemark. In this book, we use the numerator layout of the derivative, i.e.,numerator layout\nthe derivative df/dx of f \u2208 Rm with respect to x \u2208 Rn is an m \u00d7\nn matrix, where the elements of f define the rows and the elements of\nx define the columns of the corresponding Jacobian; see (5.58). There\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 572,
    "chunk_id": "p156_c3",
    "page_number": 156,
    "text": "e Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 573,
    "chunk_id": "p157_c0",
    "page_number": 157,
    "text": "5.3 Gradients of Vector-Valued Functions 151\nFigure 5.5 The\ndeterminant of the\nJacobian of f can\nbe used to compute\nthe magnifier\nbetween the blue\nand orange area.\nb1\nb2 c1 c2\nf(\u00b7)\nexists also thedenominator layout, which is the transpose of the numerator denominator layout\nlayout. In this book, we will use the numerator layout. \u2662\nWe will see how the Jacobian is used in the change-of-variable method\nfor probability distributions in Section 6.7. The amount of scaling due to\nthe transformation of a variable is provided by the determinant.\nIn Section 4.1, we saw that the determinant can be used to compute\nthe area of a parallelogram. If we are given two vectors b1 = [1 , 0]\u22a4,\nb2 = [0, 1]\u22a4 as the sides of the unit square (blue; see Figure 5.5), the area\nof this square is\n\f\f\f\fdet\n\u0012\u00141 0\n0 1\n\u0015\u0013"
  },
  {
    "vector_id": 574,
    "chunk_id": "p157_c1",
    "page_number": 157,
    "text": "o compute\nthe area of a parallelogram. If we are given two vectors b1 = [1 , 0]\u22a4,\nb2 = [0, 1]\u22a4 as the sides of the unit square (blue; see Figure 5.5), the area\nof this square is\n\f\f\f\fdet\n\u0012\u00141 0\n0 1\n\u0015\u0013\f\f\f\f = 1 . (5.60)\nIf we take a parallelogram with the sides c1 = [ \u22122, 1]\u22a4, c2 = [1 , 1]\u22a4\n(orange in Figure 5.5), its area is given as the absolute value of the deter-\nminant (see Section 4.1)\n\f\f\f\fdet\n\u0012\u0014\u22122 1\n1 1\n\u0015\u0013\f\f\f\f = | \u22123| = 3 , (5.61)\ni.e., the area of this is exactly three times the area of the unit square.\nWe can find this scaling factor by finding a mapping that transforms the\nunit square into the other square. In linear algebra terms, we effectively\nperform a variable transformation from (b1, b2) to (c1, c2). In our case,\nthe mapping is linear and the absolute value of the determinant o"
  },
  {
    "vector_id": 575,
    "chunk_id": "p157_c2",
    "page_number": 157,
    "text": "to the other square. In linear algebra terms, we effectively\nperform a variable transformation from (b1, b2) to (c1, c2). In our case,\nthe mapping is linear and the absolute value of the determinant of this\nmapping gives us exactly the scaling factor we are looking for.\nWe will describe two approaches to identify this mapping. First, we ex-\nploit that the mapping is linear so that we can use the tools from Chapter 2\nto identify this mapping. Second, we will find the mapping using partial\nderivatives using the tools we have been discussing in this chapter.\nApproach 1 To get started with the linear algebra approach, we\nidentify both {b1, b2} and {c1, c2} as bases of R2 (see Section 2.6.1 for a\nrecap). What we effectively perform is a change of basis from (b1, b2) to\n(c1, c2), and we are look"
  },
  {
    "vector_id": 576,
    "chunk_id": "p157_c3",
    "page_number": 157,
    "text": "ear algebra approach, we\nidentify both {b1, b2} and {c1, c2} as bases of R2 (see Section 2.6.1 for a\nrecap). What we effectively perform is a change of basis from (b1, b2) to\n(c1, c2), and we are looking for the transformation matrix that implements\nthe basis change. Using results from Section 2.7.2, we identify the desired\nbasis change matrix as\nJ =\n\u0014\u22122 1\n1 1\n\u0015\n, (5.62)\nsuch that Jb1 = c1 and Jb2 = c2. The absolute value of the determi-\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 577,
    "chunk_id": "p158_c0",
    "page_number": 158,
    "text": "152 Vector Calculus\nnant of J, which yields the scaling factor we are looking for, is given as\n|det(J)| = 3, i.e., the area of the square spanned by(c1, c2) is three times\ngreater than the area spanned by (b1, b2).\nApproach 2 The linear algebra approach works for linear trans-\nformations; for nonlinear transformations (which become relevant in Sec-\ntion 6.7), we follow a more general approach using partial derivatives.\nFor this approach, we consider a function f : R2 \u2192 R2 that performs a\nvariable transformation. In our example, f maps the coordinate represen-\ntation of any vector x \u2208 R2 with respect to (b1, b2) onto the coordinate\nrepresentation y \u2208 R2 with respect to (c1, c2). We want to identify the\nmapping so that we can compute how an area (or volume) changes when\nit is being transform"
  },
  {
    "vector_id": 578,
    "chunk_id": "p158_c1",
    "page_number": 158,
    "text": "espect to (b1, b2) onto the coordinate\nrepresentation y \u2208 R2 with respect to (c1, c2). We want to identify the\nmapping so that we can compute how an area (or volume) changes when\nit is being transformed by f. For this, we need to find out how f(x)\nchanges if we modify x a bit. This question is exactly answered by the\nJacobian matrix df\ndx \u2208 R2\u00d72. Since we can write\ny1 = \u22122x1 + x2 (5.63)\ny2 = x1 + x2 (5.64)\nwe obtain the functional relationship between x and y, which allows us\nto get the partial derivatives\n\u2202y1\n\u2202x1\n= \u22122 , \u2202y1\n\u2202x2\n= 1 , \u2202y2\n\u2202x1\n= 1 , \u2202y2\n\u2202x2\n= 1 (5.65)\nand compose the Jacobian as\nJ =\n\uf8ee\n\uf8ef\uf8f0\n\u2202y1\n\u2202x1\n\u2202y1\n\u2202x2\n\u2202y2\n\u2202x1\n\u2202y2\n\u2202x2\n\uf8f9\n\uf8fa\uf8fb =\n\u0014\u22122 1\n1 1\n\u0015\n. (5.66)\nThe Jacobian represents the coordinate transformation we are lookingGeometrically , the\nJacobian\ndeterminant gives\nthe magnificat"
  },
  {
    "vector_id": 579,
    "chunk_id": "p158_c2",
    "page_number": 158,
    "text": "s\nJ =\n\uf8ee\n\uf8ef\uf8f0\n\u2202y1\n\u2202x1\n\u2202y1\n\u2202x2\n\u2202y2\n\u2202x1\n\u2202y2\n\u2202x2\n\uf8f9\n\uf8fa\uf8fb =\n\u0014\u22122 1\n1 1\n\u0015\n. (5.66)\nThe Jacobian represents the coordinate transformation we are lookingGeometrically , the\nJacobian\ndeterminant gives\nthe magnification/\nscaling factor when\nwe transform an\narea or volume.\nfor. It is exact if the coordinate transformation is linear (as in our case),\nand (5.66) recovers exactly the basis change matrix in (5.62). If the co-\nordinate transformation is nonlinear, the Jacobian approximates this non-\nlinear transformation locally with a linear one. The absolute value of the\nJacobian determinant |det(J)| is the factor by which areas or volumes are\nJacobian\ndeterminant\nscaled when coordinates are transformed. Our case yields |det(J)| = 3.\nThe Jacobian determinant and variable transformations will become\nrelevant i"
  },
  {
    "vector_id": 580,
    "chunk_id": "p158_c3",
    "page_number": 158,
    "text": "r by which areas or volumes are\nJacobian\ndeterminant\nscaled when coordinates are transformed. Our case yields |det(J)| = 3.\nThe Jacobian determinant and variable transformations will become\nrelevant in Section 6.7 when we transform random variables and prob-\nability distributions. These transformations are extremely relevant in ma-Figure 5.6\nDimensionality of\n(partial) derivatives.\nf(x)\nx\n\u2202f\n\u2202x\nchine learning in the context of training deep neural networks using the\nreparametrization trick, also called infinite perturbation analysis.\nIn this chapter, we encountered derivatives of functions. Figure 5.6 sum-\nmarizes the dimensions of those derivatives. If f : R \u2192 R the gradient is\nsimply a scalar (top-left entry). For f : RD \u2192 R the gradient is a 1 \u00d7 D\nrow vector (top-right entry). For f : R"
  },
  {
    "vector_id": 581,
    "chunk_id": "p158_c4",
    "page_number": 158,
    "text": "gure 5.6 sum-\nmarizes the dimensions of those derivatives. If f : R \u2192 R the gradient is\nsimply a scalar (top-left entry). For f : RD \u2192 R the gradient is a 1 \u00d7 D\nrow vector (top-right entry). For f : R \u2192 RE, the gradient is an E \u00d7 1\ncolumn vector, and for f : RD \u2192 RE the gradient is an E \u00d7 D matrix.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 582,
    "chunk_id": "p159_c0",
    "page_number": 159,
    "text": "5.3 Gradients of Vector-Valued Functions 153\nExample 5.9 (Gradient of a Vector-Valued Function)\nWe are given\nf(x) = Ax, f(x) \u2208 RM , A \u2208 RM\u00d7N , x \u2208 RN .\nTo compute the gradient df/dx we first determine the dimension of\ndf/dx: Since f : RN \u2192 RM , it follows that df/dx \u2208 RM\u00d7N . Second,\nto compute the gradient we determine the partial derivatives of f with\nrespect to every xj:\nfi(x) =\nNX\nj=1\nAijxj =\u21d2 \u2202fi\n\u2202xj\n= Aij (5.67)\nWe collect the partial derivatives in the Jacobian and obtain the gradient\ndf\ndx =\n\uf8ee\n\uf8ef\uf8f0\n\u2202f1\n\u2202x1\n\u00b7 \u00b7\u00b7 \u2202f1\n\u2202xN\n... ...\n\u2202fM\n\u2202x1\n\u00b7 \u00b7\u00b7 \u2202fM\n\u2202xN\n\uf8f9\n\uf8fa\uf8fb =\n\uf8ee\n\uf8ef\uf8f0\nA11 \u00b7 \u00b7\u00b7 A1N\n... ...\nAM1 \u00b7 \u00b7\u00b7 AMN\n\uf8f9\n\uf8fa\uf8fb = A \u2208 RM\u00d7N . (5.68)\nExample 5.10 (Chain Rule)\nConsider the function h : R \u2192 R, h(t) = (f \u25e6 g)(t) with\nf : R2 \u2192 R (5.69)\ng : R \u2192 R2 (5.70)\nf(x) = exp(x1x2\n2) , (5.71)\nx =\n\u0014x1\nx2\n\u0015\n= g(t) =\n\u0014t"
  },
  {
    "vector_id": 583,
    "chunk_id": "p159_c1",
    "page_number": 159,
    "text": "AMN\n\uf8f9\n\uf8fa\uf8fb = A \u2208 RM\u00d7N . (5.68)\nExample 5.10 (Chain Rule)\nConsider the function h : R \u2192 R, h(t) = (f \u25e6 g)(t) with\nf : R2 \u2192 R (5.69)\ng : R \u2192 R2 (5.70)\nf(x) = exp(x1x2\n2) , (5.71)\nx =\n\u0014x1\nx2\n\u0015\n= g(t) =\n\u0014t cos t\nt sin t\n\u0015\n(5.72)\nand compute the gradient of h with respect to t. Since f : R2 \u2192 R and\ng : R \u2192 R2 we note that\n\u2202f\n\u2202x \u2208 R1\u00d72 , \u2202g\n\u2202t \u2208 R2\u00d71 . (5.73)\nThe desired gradient is computed by applying the chain rule:\ndh\ndt = \u2202f\n\u2202x\n\u2202x\n\u2202t =\n\u0014 \u2202f\n\u2202x1\n\u2202f\n\u2202x2\n\u0015\n\uf8ee\n\uf8ef\uf8f0\n\u2202x1\n\u2202t\u2202x2\n\u2202t\n\uf8f9\n\uf8fa\uf8fb (5.74a)\n=\n\u0002\nexp(x1x2\n2)x2\n2 2 exp(x1x2\n2)x1x2\n\u0003\u0014cos t \u2212 t sin t\nsin t + t cos t\n\u0015\n(5.74b)\n= exp(x1x2\n2)\n\u0000\nx2\n2(cos t \u2212 t sin t) + 2x1x2(sin t + t cos t)\n\u0001\n, (5.74c)\nwhere x1 = t cos t and x2 = t sin t; see (5.72).\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 584,
    "chunk_id": "p159_c2",
    "page_number": 159,
    "text": "sin t) + 2x1x2(sin t + t cos t)\n\u0001\n, (5.74c)\nwhere x1 = t cos t and x2 = t sin t; see (5.72).\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 585,
    "chunk_id": "p160_c0",
    "page_number": 160,
    "text": "154 Vector Calculus\nExample 5.11 (Gradient of a Least-Squares Loss in a Linear Model)\nLet us consider the linear modelWe will discuss this\nmodel in much\nmore detail in\nChapter 9 in the\ncontext of linear\nregression, where\nwe need derivatives\nof the least-squares\nloss L with respect\nto the parameters \u03b8.\ny = \u03a6\u03b8 , (5.75)\nwhere \u03b8 \u2208 RD is a parameter vector, \u03a6 \u2208 RN\u00d7D are input features and\ny \u2208 RN are the corresponding observations. We define the functions\nL(e) := \u2225e\u22252 , (5.76)\ne(\u03b8) := y \u2212 \u03a6\u03b8 . (5.77)\nWe seek \u2202L\n\u2202\u03b8 , and we will use the chain rule for this purpose. L is called a\nleast-squares loss function.least-squares loss\nBefore we start our calculation, we determine the dimensionality of the\ngradient as\n\u2202L\n\u2202\u03b8 \u2208 R1\u00d7D . (5.78)\nThe chain rule allows us to compute the gradient as\n\u2202L\n\u2202\u03b8 = \u2202L\n\u2202e\n\u2202e"
  },
  {
    "vector_id": 586,
    "chunk_id": "p160_c1",
    "page_number": 160,
    "text": "nction.least-squares loss\nBefore we start our calculation, we determine the dimensionality of the\ngradient as\n\u2202L\n\u2202\u03b8 \u2208 R1\u00d7D . (5.78)\nThe chain rule allows us to compute the gradient as\n\u2202L\n\u2202\u03b8 = \u2202L\n\u2202e\n\u2202e\n\u2202\u03b8 , (5.79)\nwhere the dth element is given bydLdtheta =\nnp.einsum(\n\u2019n,nd\u2019,\ndLde,dedtheta) \u2202L\n\u2202\u03b8 [1, d] =\nNX\nn=1\n\u2202L\n\u2202e [n]\u2202e\n\u2202\u03b8[n, d] . (5.80)\nWe know that \u2225e\u22252 = e\u22a4e (see Section 3.2) and determine\n\u2202L\n\u2202e = 2e\u22a4 \u2208 R1\u00d7N . (5.81)\nFurthermore, we obtain\n\u2202e\n\u2202\u03b8 = \u2212\u03a6 \u2208 RN\u00d7D , (5.82)\nsuch that our desired derivative is\n\u2202L\n\u2202\u03b8 = \u22122e\u22a4\u03a6\n(5.77)\n= \u22122(y\u22a4 \u2212 \u03b8\u22a4\u03a6\u22a4)| {z }\n1\u00d7N\n\u03a6|{z}\nN\u00d7D\n\u2208 R1\u00d7D . (5.83)\nRemark. We would have obtained the same result without using the chain\nrule by immediately looking at the function\nL2(\u03b8) := \u2225y \u2212 \u03a6\u03b8\u22252 = (y \u2212 \u03a6\u03b8)\u22a4(y \u2212 \u03a6\u03b8) . (5.84)\nThis approach is still practical for simple functio"
  },
  {
    "vector_id": 587,
    "chunk_id": "p160_c2",
    "page_number": 160,
    "text": "ld have obtained the same result without using the chain\nrule by immediately looking at the function\nL2(\u03b8) := \u2225y \u2212 \u03a6\u03b8\u22252 = (y \u2212 \u03a6\u03b8)\u22a4(y \u2212 \u03a6\u03b8) . (5.84)\nThis approach is still practical for simple functions like L2 but becomes\nimpractical for deep function compositions. \u2662\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 588,
    "chunk_id": "p161_c0",
    "page_number": 161,
    "text": "5.4 Gradients of Matrices 155\nFigure 5.7\nVisualization of\ngradient\ncomputation of a\nmatrix with respect\nto a vector. We are\ninterested in\ncomputing the\ngradient of\nA \u2208 R4\u00d72 with\nrespect to a vector\nx \u2208 R3. We know\nthat gradient\ndA\ndx \u2208 R4\u00d72\u00d73. We\nfollow two\nequivalent\napproaches to arrive\nthere: (a) collating\npartial derivatives\ninto a Jacobian\ntensor;\n(b) flattening of the\nmatrix into a vector,\ncomputing the\nJacobian matrix,\nre-shaping into a\nJacobian tensor.\nA\u2208R4\u00d72 x\u2208R3\n\u2202A\n\u2202x1\n\u2208R4\u00d72\n\u2202A\n\u2202x2\n\u2208R4\u00d72\n\u2202A\n\u2202x3\n\u2208R4\u00d72\nx1\nx2\nx3\ndA\ndx\u2208R4\u00d72\u00d73\n4\n2\n3\nPartial derivatives:\ncollate\n(a) Approach 1: We compute the partial derivative\n\u2202A\n\u2202x1\n, \u2202A\n\u2202x2\n, \u2202A\n\u2202x3\n, each of which is a 4 \u00d7 2 matrix, and col-\nlate them in a 4 \u00d7 2 \u00d7 3 tensor.\nA\u2208R4\u00d72 x\u2208R3\nx1\nx2\nx3\ndA\ndx\u2208R4\u00d72\u00d73\nre-shape re-shapegradient\nA\u2208R4\u00d72 \u02dcA\u2208R8\nd\u02dc"
  },
  {
    "vector_id": 589,
    "chunk_id": "p161_c1",
    "page_number": 161,
    "text": "the partial derivative\n\u2202A\n\u2202x1\n, \u2202A\n\u2202x2\n, \u2202A\n\u2202x3\n, each of which is a 4 \u00d7 2 matrix, and col-\nlate them in a 4 \u00d7 2 \u00d7 3 tensor.\nA\u2208R4\u00d72 x\u2208R3\nx1\nx2\nx3\ndA\ndx\u2208R4\u00d72\u00d73\nre-shape re-shapegradient\nA\u2208R4\u00d72 \u02dcA\u2208R8\nd\u02dcA\ndx\u2208R8\u00d73\n(b) Approach 2: We re-shape (flatten)A \u2208 R4\u00d72 into a vec-\ntor \u02dcA \u2208 R8. Then, we compute the gradient d \u02dcA\ndx \u2208 R8\u00d73.\nWe obtain the gradient tensor by re-shaping this gradient as\nillustrated above.\n5.4 Gradients of Matrices We can think of a\ntensor as a\nmultidimensional\narray .\nWe will encounter situations where we need to take gradients of matrices\nwith respect to vectors (or other matrices), which results in a multidimen-\nsional tensor. We can think of this tensor as a multidimensional array that\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Pres"
  },
  {
    "vector_id": 590,
    "chunk_id": "p161_c2",
    "page_number": 161,
    "text": "ces), which results in a multidimen-\nsional tensor. We can think of this tensor as a multidimensional array that\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 591,
    "chunk_id": "p162_c0",
    "page_number": 162,
    "text": "156 Vector Calculus\ncollects partial derivatives. For example, if we compute the gradient of an\nm \u00d7 n matrix A with respect to a p \u00d7 q matrix B, the resulting Jacobian\nwould be (m\u00d7n)\u00d7(p\u00d7q), i.e., a four-dimensional tensorJ, whose entries\nare given as Jijkl = \u2202Aij/\u2202Bkl.\nSince matrices represent linear mappings, we can exploit the fact that\nthere is a vector-space isomorphism (linear, invertible mapping) between\nthe space Rm\u00d7n of m \u00d7 n matrices and the space Rmn of mn vectors.\nTherefore, we can re-shape our matrices into vectors of lengths mn and\npq, respectively . The gradient using thesemn vectors results in a Jacobian\nof size mn \u00d7 pq. Figure 5.7 visualizes both approaches. In practical ap-Matrices can be\ntransformed into\nvectors by stacking\nthe columns of the\nmatrix\n(\u201cflattening\u201d).\nplicat"
  },
  {
    "vector_id": 592,
    "chunk_id": "p162_c1",
    "page_number": 162,
    "text": "tors results in a Jacobian\nof size mn \u00d7 pq. Figure 5.7 visualizes both approaches. In practical ap-Matrices can be\ntransformed into\nvectors by stacking\nthe columns of the\nmatrix\n(\u201cflattening\u201d).\nplications, it is often desirable to re-shape the matrix into a vector and\ncontinue working with this Jacobian matrix: The chain rule (5.48) boils\ndown to simple matrix multiplication, whereas in the case of a Jacobian\ntensor, we will need to pay more attention to what dimensions we need\nto sum out.\nExample 5.12 (Gradient of Vectors with Respect to Matrices)\nLet us consider the following example, where\nf = Ax, f \u2208 RM , A \u2208 RM\u00d7N , x \u2208 RN (5.85)\nand where we seek the gradientdf/dA. Let us start again by determining\nthe dimension of the gradient as\ndf\ndA \u2208 RM\u00d7(M\u00d7N) . (5.86)\nBy definition, the gradient"
  },
  {
    "vector_id": 593,
    "chunk_id": "p162_c2",
    "page_number": 162,
    "text": "f = Ax, f \u2208 RM , A \u2208 RM\u00d7N , x \u2208 RN (5.85)\nand where we seek the gradientdf/dA. Let us start again by determining\nthe dimension of the gradient as\ndf\ndA \u2208 RM\u00d7(M\u00d7N) . (5.86)\nBy definition, the gradient is the collection of the partial derivatives:\ndf\ndA =\n\uf8ee\n\uf8ef\uf8f0\n\u2202f1\n\u2202A\n...\n\u2202fM\n\u2202A\n\uf8f9\n\uf8fa\uf8fb , \u2202fi\n\u2202A \u2208 R1\u00d7(M\u00d7N) . (5.87)\nTo compute the partial derivatives, it will be helpful to explicitly write out\nthe matrix vector multiplication:\nfi =\nNX\nj=1\nAijxj, i = 1, . . . , M , (5.88)\nand the partial derivatives are then given as\n\u2202fi\n\u2202Aiq\n= xq . (5.89)\nThis allows us to compute the partial derivatives of fi with respect to a\nrow of A, which is given as\n\u2202fi\n\u2202Ai,:\n= x\u22a4 \u2208 R1\u00d71\u00d7N , (5.90)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 594,
    "chunk_id": "p162_c3",
    "page_number": 162,
    "text": "espect to a\nrow of A, which is given as\n\u2202fi\n\u2202Ai,:\n= x\u22a4 \u2208 R1\u00d71\u00d7N , (5.90)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 595,
    "chunk_id": "p163_c0",
    "page_number": 163,
    "text": "5.4 Gradients of Matrices 157\n\u2202fi\n\u2202Ak\u0338=i,:\n= 0\u22a4 \u2208 R1\u00d71\u00d7N (5.91)\nwhere we have to pay attention to the correct dimensionality . Since fi\nmaps onto R and each row of A is of size 1 \u00d7 N, we obtain a 1 \u00d7 1 \u00d7 N-\nsized tensor as the partial derivative of fi with respect to a row of A.\nWe stack the partial derivatives (5.91) and get the desired gradient\nin (5.87) via\n\u2202fi\n\u2202A =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0\u22a4\n...\n0\u22a4\nx\u22a4\n0\u22a4\n...\n0\u22a4\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u2208 R1\u00d7(M\u00d7N) . (5.92)\nExample 5.13 (Gradient of Matrices with Respect to Matrices)\nConsider a matrix R \u2208 RM\u00d7N and f : RM\u00d7N \u2192 RN\u00d7N with\nf(R) = R\u22a4R =: K \u2208 RN\u00d7N , (5.93)\nwhere we seek the gradient dK/dR.\nTo solve this hard problem, let us first write down what we already\nknow: The gradient has the dimensions\ndK\ndR \u2208 R(N\u00d7N)\u00d7(M\u00d7N) , (5.94)\nwhich is a tensor. Moreover,\ndKpq\ndR \u2208 R1\u00d7"
  },
  {
    "vector_id": 596,
    "chunk_id": "p163_c1",
    "page_number": 163,
    "text": "k the gradient dK/dR.\nTo solve this hard problem, let us first write down what we already\nknow: The gradient has the dimensions\ndK\ndR \u2208 R(N\u00d7N)\u00d7(M\u00d7N) , (5.94)\nwhich is a tensor. Moreover,\ndKpq\ndR \u2208 R1\u00d7M\u00d7N (5.95)\nfor p, q= 1 , . . . , N, where Kpq is the (p, q)th entry of K = f(R). De-\nnoting the ith column of R by ri, every entry of K is given by the dot\nproduct of two columns of R, i.e.,\nKpq = r\u22a4\np rq =\nMX\nm=1\nRmpRmq . (5.96)\nWhen we now compute the partial derivative \u2202Kpq\n\u2202Rij\nwe obtain\n\u2202Kpq\n\u2202Rij\n=\nMX\nm=1\n\u2202\n\u2202Rij\nRmpRmq = \u2202pqij , (5.97)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 597,
    "chunk_id": "p163_c2",
    "page_number": 163,
    "text": "d by Cambridge University Press (2020)."
  },
  {
    "vector_id": 598,
    "chunk_id": "p164_c0",
    "page_number": 164,
    "text": "158 Vector Calculus\n\u2202pqij =\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\nRiq if j = p, p\u0338= q\nRip if j = q, p\u0338= q\n2Riq if j = p, p= q\n0 otherwise\n. (5.98)\nFrom (5.94), we know that the desired gradient has the dimension (N \u00d7\nN) \u00d7 (M \u00d7 N), and every single entry of this tensor is given by \u2202pqij\nin (5.98), where p, q, j= 1, . . . , Nand i = 1, . . . , M.\n5.5 Useful Identities for Computing Gradients\nIn the following, we list some useful gradients that are frequently required\nin a machine learning context (Petersen and Pedersen, 2012). Here, we\nuse tr(\u00b7) as the trace (see Definition 4.4), det(\u00b7) as the determinant (see\nSection 4.1) and f(X)\u22121 as the inverse of f(X), assuming it exists.\n\u2202\n\u2202X f(X)\u22a4 =\n\u0012\u2202f(X)\n\u2202X\n\u0013\u22a4\n(5.99)\n\u2202\n\u2202X tr(f(X)) = tr\n\u0012\u2202f(X)\n\u2202X\n\u0013\n(5.100)\n\u2202\n\u2202X det(f(X)) = det(f(X))tr\n\u0012\nf(X)\u22121 \u2202f(X)\n\u2202X\n\u0013\n(5.101)\n\u2202\n\u2202X f(X)\u22121 = \u2212"
  },
  {
    "vector_id": 599,
    "chunk_id": "p164_c1",
    "page_number": 164,
    "text": "and f(X)\u22121 as the inverse of f(X), assuming it exists.\n\u2202\n\u2202X f(X)\u22a4 =\n\u0012\u2202f(X)\n\u2202X\n\u0013\u22a4\n(5.99)\n\u2202\n\u2202X tr(f(X)) = tr\n\u0012\u2202f(X)\n\u2202X\n\u0013\n(5.100)\n\u2202\n\u2202X det(f(X)) = det(f(X))tr\n\u0012\nf(X)\u22121 \u2202f(X)\n\u2202X\n\u0013\n(5.101)\n\u2202\n\u2202X f(X)\u22121 = \u2212f(X)\u22121 \u2202f(X)\n\u2202X f(X)\u22121 (5.102)\n\u2202a\u22a4X\u22121b\n\u2202X = \u2212(X\u22121)\u22a4ab\u22a4(X\u22121)\u22a4 (5.103)\n\u2202x\u22a4a\n\u2202x = a\u22a4 (5.104)\n\u2202a\u22a4x\n\u2202x = a\u22a4 (5.105)\n\u2202a\u22a4Xb\n\u2202X = ab\u22a4 (5.106)\n\u2202x\u22a4Bx\n\u2202x = x\u22a4(B + B\u22a4) (5.107)\n\u2202\n\u2202s(x \u2212 As)\u22a4W(x \u2212 As) = \u22122(x \u2212 As)\u22a4W A for symmetric W\n(5.108)\nRemark. In this book, we only cover traces and transposes of matrices.\nHowever, we have seen that derivatives can be higher-dimensional ten-\nsors, in which case the usual trace and transpose are not defined. In these\ncases, the trace of aD\u00d7D\u00d7E\u00d7F tensor would be anE\u00d7F-dimensional\nmatrix. This is a special case of a tensor contraction. Similarly , when we\nDraft (2024-01-1"
  },
  {
    "vector_id": 600,
    "chunk_id": "p164_c2",
    "page_number": 164,
    "text": "race and transpose are not defined. In these\ncases, the trace of aD\u00d7D\u00d7E\u00d7F tensor would be anE\u00d7F-dimensional\nmatrix. This is a special case of a tensor contraction. Similarly , when we\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 601,
    "chunk_id": "p165_c0",
    "page_number": 165,
    "text": "5.6 Backpropagation and Automatic Differentiation 159\n\u201ctranspose\u201d a tensor, we mean swapping the first two dimensions. Specif-\nically , in (5.99) through (5.102), we require tensor-related computations\nwhen we work with multivariate functions f(\u00b7) and compute derivatives\nwith respect to matrices (and choose not to vectorize them as discussed in\nSection 5.4). \u2662\n5.6 Backpropagation and Automatic Differentiation\nA good discussion\nabout\nbackpropagation\nand the chain rule is\navailable at a blog\nby Tim Vieira at\nhttps://tinyurl.\ncom/ycfm2yrw.\nIn many machine learning applications, we find good model parameters\nby performing gradient descent (Section 7.1), which relies on the fact\nthat we can compute the gradient of a learning objective with respect\nto the parameters of the model. For a given obj"
  },
  {
    "vector_id": 602,
    "chunk_id": "p165_c1",
    "page_number": 165,
    "text": "parameters\nby performing gradient descent (Section 7.1), which relies on the fact\nthat we can compute the gradient of a learning objective with respect\nto the parameters of the model. For a given objective function, we can\nobtain the gradient with respect to the model parameters using calculus\nand applying the chain rule; see Section 5.2.2. We already had a taste in\nSection 5.3 when we looked at the gradient of a squared loss with respect\nto the parameters of a linear regression model.\nConsider the function\nf(x) =\nq\nx2 + exp(x2) + cos\n\u0000\nx2 + exp(x2)\n\u0001\n. (5.109)\nBy application of the chain rule, and noting that differentiation is linear,\nwe compute the gradient\ndf\ndx = 2x + 2x exp(x2)\n2\np\nx2 + exp(x2) \u2212 sin\n\u0000\nx2 + exp(x2)\n\u0001\u0000\n2x + 2x exp(x2)\n\u0001\n= 2x\n \n1\n2\np\nx2 + exp(x2) \u2212 sin\n\u0000\nx2 + exp(x2)"
  },
  {
    "vector_id": 603,
    "chunk_id": "p165_c2",
    "page_number": 165,
    "text": "e, and noting that differentiation is linear,\nwe compute the gradient\ndf\ndx = 2x + 2x exp(x2)\n2\np\nx2 + exp(x2) \u2212 sin\n\u0000\nx2 + exp(x2)\n\u0001\u0000\n2x + 2x exp(x2)\n\u0001\n= 2x\n \n1\n2\np\nx2 + exp(x2) \u2212 sin\n\u0000\nx2 + exp(x2)\n\u0001\n!\n\u0000\n1 + exp(x2)\n\u0001\n.\n(5.110)\nWriting out the gradient in this explicit way is often impractical since it\noften results in a very lengthy expression for a derivative. In practice,\nit means that, if we are not careful, the implementation of the gradient\ncould be significantly more expensive than computing the function, which\nimposes unnecessary overhead. For training deep neural network mod-\nels, the backpropagation algorithm (Kelley, 1960; Bryson, 1961; Dreyfus, backpropagation\n1962; Rumelhart et al., 1986) is an efficient way to compute the gradient\nof an error function with respect to the pa"
  },
  {
    "vector_id": 604,
    "chunk_id": "p165_c3",
    "page_number": 165,
    "text": "he backpropagation algorithm (Kelley, 1960; Bryson, 1961; Dreyfus, backpropagation\n1962; Rumelhart et al., 1986) is an efficient way to compute the gradient\nof an error function with respect to the parameters of the model.\n5.6.1 Gradients in a Deep Network\nAn area where the chain rule is used to an extreme is deep learning, where\nthe function value y is computed as a many-level function composition\ny = (fK \u25e6 fK\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6f1)(x) = fK(fK\u22121(\u00b7 \u00b7\u00b7(f1(x)) \u00b7 \u00b7\u00b7)) , (5.111)\nwhere x are the inputs (e.g., images), y are the observations (e.g., class\nlabels), and every functionfi, i = 1, . . . , K, possesses its own parameters.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 605,
    "chunk_id": "p165_c4",
    "page_number": 165,
    "text": "esses its own parameters.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 606,
    "chunk_id": "p166_c0",
    "page_number": 166,
    "text": "160 Vector Calculus\nFigure 5.8 Forward\npass in a multi-layer\nneural network to\ncompute the loss L\nas a function of the\ninputs x and the\nparameters Ai, bi.\nx fK\nA0,b0 AK\u22121,bK\u22121\nLfK\u22121\nAK\u22122,bK\u22122\nf1\nA1,b1\nIn neural networks with multiple layers, we have functions fi(xi\u22121) =We discuss the case,\nwhere the activation\nfunctions are\nidentical in each\nlayer to unclutter\nnotation.\n\u03c3(Ai\u22121xi\u22121 + bi\u22121) in the ith layer. Here xi\u22121 is the output of layer i \u2212 1\nand \u03c3 an activation function, such as the logistic sigmoid 1\n1+e\u2212x , tanh or a\nrectified linear unit (ReLU). In order to train these models, we require the\ngradient of a loss function L with respect to all model parameters Aj, bj\nfor j = 1, . . . , K. This also requires us to compute the gradient of L with\nrespect to the inputs of each layer. For ex"
  },
  {
    "vector_id": 607,
    "chunk_id": "p166_c1",
    "page_number": 166,
    "text": "the\ngradient of a loss function L with respect to all model parameters Aj, bj\nfor j = 1, . . . , K. This also requires us to compute the gradient of L with\nrespect to the inputs of each layer. For example, if we have inputs x and\nobservations y and a network structure defined by\nf0 := x (5.112)\nfi := \u03c3i(Ai\u22121fi\u22121 + bi\u22121) , i = 1, . . . , K , (5.113)\nsee also Figure 5.8 for a visualization, we may be interested in finding\nAj, bj for j = 0, . . . , K\u2212 1, such that the squared loss\nL(\u03b8) = \u2225y \u2212 fK(\u03b8, x)\u22252 (5.114)\nis minimized, where \u03b8 = {A0, b0, . . . ,AK\u22121, bK\u22121}.\nTo obtain the gradients with respect to the parameter set \u03b8, we require\nthe partial derivatives of L with respect to the parameters \u03b8j = {Aj, bj}\nof each layer j = 0, . . . , K\u2212 1. The chain rule allows us to determine the\npartial d"
  },
  {
    "vector_id": 608,
    "chunk_id": "p166_c2",
    "page_number": 166,
    "text": "pect to the parameter set \u03b8, we require\nthe partial derivatives of L with respect to the parameters \u03b8j = {Aj, bj}\nof each layer j = 0, . . . , K\u2212 1. The chain rule allows us to determine the\npartial derivatives asA more in-depth\ndiscussion about\ngradients of neural\nnetworks can be\nfound in Justin\nDomke\u2019s lecture\nnotes\nhttps://tinyurl.\ncom/yalcxgtv.\n\u2202L\n\u2202\u03b8K\u22121\n= \u2202L\n\u2202fK\n\u2202fK\n\u2202\u03b8K\u22121\n(5.115)\n\u2202L\n\u2202\u03b8K\u22122\n= \u2202L\n\u2202fK\n\u2202fK\n\u2202fK\u22121\n\u2202fK\u22121\n\u2202\u03b8K\u22122\n(5.116)\n\u2202L\n\u2202\u03b8K\u22123\n= \u2202L\n\u2202fK\n\u2202fK\n\u2202fK\u22121\n\u2202fK\u22121\n\u2202fK\u22122\n\u2202fK\u22122\n\u2202\u03b8K\u22123\n(5.117)\n\u2202L\n\u2202\u03b8i\n= \u2202L\n\u2202fK\n\u2202fK\n\u2202fK\u22121\n\u00b7 \u00b7\u00b7\u2202fi+2\n\u2202fi+1\n\u2202fi+1\n\u2202\u03b8i\n(5.118)\nThe orange terms are partial derivatives of the output of a layer with\nrespect to its inputs, whereas the blue terms are partial derivatives of\nthe output of a layer with respect to its parameters. Assuming, we have\nalready computed the partial"
  },
  {
    "vector_id": 609,
    "chunk_id": "p166_c3",
    "page_number": 166,
    "text": "he output of a layer with\nrespect to its inputs, whereas the blue terms are partial derivatives of\nthe output of a layer with respect to its parameters. Assuming, we have\nalready computed the partial derivatives\u2202L/\u2202\u03b8i+1, then most of the com-\nputation can be reused to compute \u2202L/\u2202\u03b8i. The additional terms that we\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 610,
    "chunk_id": "p167_c0",
    "page_number": 167,
    "text": "5.6 Backpropagation and Automatic Differentiation 161\nFigure 5.9\nBackward pass in a\nmulti-layer neural\nnetwork to compute\nthe gradients of the\nloss function.\nx fK\nA0,b0 AK\u22121,bK\u22121\nLfK\u22121\nAK\u22122,bK\u22122\nf1\nA1,b1\nFigure 5.10 Simple\ngraph illustrating\nthe flow of data\nfrom x to y via\nsome intermediate\nvariables a, b.\nx a b y\nneed to compute are indicated by the boxes. Figure 5.9 visualizes that the\ngradients are passed backward through the network.\n5.6.2 Automatic Differentiation\nIt turns out that backpropagation is a special case of a general technique\nin numerical analysis called automatic differentiation. We can think of au- automatic\ndifferentiationtomatic differentation as a set of techniques to numerically (in contrast to\nsymbolically) evaluate the exact (up to machine precision) gradient of a"
  },
  {
    "vector_id": 611,
    "chunk_id": "p167_c1",
    "page_number": 167,
    "text": "tion. We can think of au- automatic\ndifferentiationtomatic differentation as a set of techniques to numerically (in contrast to\nsymbolically) evaluate the exact (up to machine precision) gradient of a\nfunction by working with intermediate variables and applying the chain\nrule. Automatic differentiation applies a series of elementary arithmetic Automatic\ndifferentiation is\ndifferent from\nsymbolic\ndifferentiation and\nnumerical\napproximations of\nthe gradient, e.g., by\nusing finite\ndifferences.\noperations, e.g., addition and multiplication and elementary functions,\ne.g., sin, cos, exp, log. By applying the chain rule to these operations, the\ngradient of quite complicated functions can be computed automatically .\nAutomatic differentiation applies to general computer programs and has\nforward and"
  },
  {
    "vector_id": 612,
    "chunk_id": "p167_c2",
    "page_number": 167,
    "text": "lying the chain rule to these operations, the\ngradient of quite complicated functions can be computed automatically .\nAutomatic differentiation applies to general computer programs and has\nforward and reverse modes. Baydin et al. (2018) give a great overview of\nautomatic differentiation in machine learning.\nFigure 5.10 shows a simple graph representing the data flow from in-\nputs x to outputs y via some intermediate variables a, b. If we were to\ncompute the derivative dy/dx, we would apply the chain rule and obtain\ndy\ndx = dy\ndb\ndb\nda\nda\ndx . (5.119)\nIntuitively , the forward and reverse mode differ in the order of multipli- In the general case,\nwe work with\nJacobians, which\ncan be vectors,\nmatrices, or tensors.\ncation. Due to the associativity of matrix multiplication, we can choose\nbetwe"
  },
  {
    "vector_id": 613,
    "chunk_id": "p167_c3",
    "page_number": 167,
    "text": "differ in the order of multipli- In the general case,\nwe work with\nJacobians, which\ncan be vectors,\nmatrices, or tensors.\ncation. Due to the associativity of matrix multiplication, we can choose\nbetween\ndy\ndx =\n\u0012dy\ndb\ndb\nda\n\u0013 da\ndx , (5.120)\ndy\ndx = dy\ndb\n\u0012db\nda\nda\ndx\n\u0013\n. (5.121)\nEquation (5.120) would be the reverse mode because gradients are prop- reverse mode\nagated backward through the graph, i.e., reverse to the data flow. Equa-\ntion (5.121) would be the forward mode, where the gradients flow with forward mode\nthe data from left to right through the graph.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 614,
    "chunk_id": "p167_c4",
    "page_number": 167,
    "text": "isal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 615,
    "chunk_id": "p168_c0",
    "page_number": 168,
    "text": "162 Vector Calculus\nIn the following, we will focus on reverse mode automatic differentia-\ntion, which is backpropagation. In the context of neural networks, where\nthe input dimensionality is often much higher than the dimensionality of\nthe labels, the reverse mode is computationally significantly cheaper than\nthe forward mode. Let us start with an instructive example.\nExample 5.14\nConsider the function\nf(x) =\nq\nx2 + exp(x2) + cos\n\u0000\nx2 + exp(x2)\n\u0001\n(5.122)\nfrom (5.109). If we were to implement a function f on a computer, we\nwould be able to save some computation by using intermediate variables:intermediate\nvariables\na = x2 , (5.123)\nb = exp(a) , (5.124)\nc = a + b , (5.125)\nd = \u221ac , (5.126)\ne = cos(c) , (5.127)\nf = d + e . (5.128)\nFigure 5.11\nComputation graph\nwith inputs x,\nfunction values"
  },
  {
    "vector_id": 616,
    "chunk_id": "p168_c1",
    "page_number": 168,
    "text": "intermediate\nvariables\na = x2 , (5.123)\nb = exp(a) , (5.124)\nc = a + b , (5.125)\nd = \u221ac , (5.126)\ne = cos(c) , (5.127)\nf = d + e . (5.128)\nFigure 5.11\nComputation graph\nwith inputs x,\nfunction values f,\nand intermediate\nvariables a, b, c, d, e.\nx (\u00b7)2 a\nexp(\u00b7) b\n+ c\n\u221a\u00b7\ncos(\u00b7)\nd\ne\n+ f\nThis is the same kind of thinking process that occurs when applying\nthe chain rule. Note that the preceding set of equations requires fewer\noperations than a direct implementation of the function f(x) as defined\nin (5.109). The corresponding computation graph in Figure 5.11 shows\nthe flow of data and computations required to obtain the function value\nf.\nThe set of equations that include intermediate variables can be thought\nof as a computation graph, a representation that is widely used in imple-\nmentations of"
  },
  {
    "vector_id": 617,
    "chunk_id": "p168_c2",
    "page_number": 168,
    "text": "required to obtain the function value\nf.\nThe set of equations that include intermediate variables can be thought\nof as a computation graph, a representation that is widely used in imple-\nmentations of neural network software libraries. We can directly compute\nthe derivatives of the intermediate variables with respect to their corre-\nsponding inputs by recalling the definition of the derivative of elementary\nfunctions. We obtain the following:\n\u2202a\n\u2202x = 2x (5.129)\n\u2202b\n\u2202a = exp(a) (5.130)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 618,
    "chunk_id": "p169_c0",
    "page_number": 169,
    "text": "5.6 Backpropagation and Automatic Differentiation 163\n\u2202c\n\u2202a = 1 = \u2202c\n\u2202b (5.131)\n\u2202d\n\u2202c = 1\n2\u221ac (5.132)\n\u2202e\n\u2202c = \u2212sin(c) (5.133)\n\u2202f\n\u2202d = 1 = \u2202f\n\u2202e . (5.134)\nBy looking at the computation graph in Figure 5.11, we can compute\n\u2202f/\u2202x by working backward from the output and obtain\n\u2202f\n\u2202c = \u2202f\n\u2202d\n\u2202d\n\u2202c + \u2202f\n\u2202e\n\u2202e\n\u2202c (5.135)\n\u2202f\n\u2202b = \u2202f\n\u2202c\n\u2202c\n\u2202b (5.136)\n\u2202f\n\u2202a = \u2202f\n\u2202b\n\u2202b\n\u2202a + \u2202f\n\u2202c\n\u2202c\n\u2202a (5.137)\n\u2202f\n\u2202x = \u2202f\n\u2202a\n\u2202a\n\u2202x . (5.138)\nNote that we implicitly applied the chain rule to obtain \u2202f/\u2202x . By substi-\ntuting the results of the derivatives of the elementary functions, we get\n\u2202f\n\u2202c = 1 \u00b7 1\n2\u221ac + 1 \u00b7 (\u2212sin(c)) (5.139)\n\u2202f\n\u2202b = \u2202f\n\u2202c \u00b7 1 (5.140)\n\u2202f\n\u2202a = \u2202f\n\u2202b exp(a) + \u2202f\n\u2202c \u00b7 1 (5.141)\n\u2202f\n\u2202x = \u2202f\n\u2202a \u00b7 2x . (5.142)\nBy thinking of each of the derivatives above as a variable, we observe\nthat the computation requ"
  },
  {
    "vector_id": 619,
    "chunk_id": "p169_c1",
    "page_number": 169,
    "text": "(5.139)\n\u2202f\n\u2202b = \u2202f\n\u2202c \u00b7 1 (5.140)\n\u2202f\n\u2202a = \u2202f\n\u2202b exp(a) + \u2202f\n\u2202c \u00b7 1 (5.141)\n\u2202f\n\u2202x = \u2202f\n\u2202a \u00b7 2x . (5.142)\nBy thinking of each of the derivatives above as a variable, we observe\nthat the computation required for calculating the derivative is of similar\ncomplexity as the computation of the function itself. This is quite counter-\nintuitive since the mathematical expression for the derivative \u2202f\n\u2202x (5.110)\nis significantly more complicated than the mathematical expression of the\nfunction f(x) in (5.109).\nAutomatic differentiation is a formalization of Example 5.14. Letx1, . . . , xd\nbe the input variables to the function, xd+1, . . . , xD\u22121 be the intermediate\nvariables, and xD the output variable. Then the computation graph can be\nexpressed as follows:\nFor i = d + 1, . . . , D: xi = gi(xPa(xi))"
  },
  {
    "vector_id": 620,
    "chunk_id": "p169_c2",
    "page_number": 169,
    "text": "iables to the function, xd+1, . . . , xD\u22121 be the intermediate\nvariables, and xD the output variable. Then the computation graph can be\nexpressed as follows:\nFor i = d + 1, . . . , D: xi = gi(xPa(xi)) , (5.143)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 621,
    "chunk_id": "p170_c0",
    "page_number": 170,
    "text": "164 Vector Calculus\nwhere the gi(\u00b7) are elementary functions and xPa(xi) are the parent nodes\nof the variable xi in the graph. Given a function defined in this way , we\ncan use the chain rule to compute the derivative of the function in a step-\nby-step fashion. Recall that by definition f = xD and hence\n\u2202f\n\u2202xD\n= 1 . (5.144)\nFor other variables xi, we apply the chain rule\n\u2202f\n\u2202xi\n=\nX\nxj:xi\u2208Pa(xj)\n\u2202f\n\u2202xj\n\u2202xj\n\u2202xi\n=\nX\nxj:xi\u2208Pa(xj)\n\u2202f\n\u2202xj\n\u2202gj\n\u2202xi\n, (5.145)\nwhere Pa(xj) is the set of parent nodes of xj in the computation graph.\nEquation (5.143) is the forward propagation of a function, whereas (5.145)Auto-differentiation\nin reverse mode\nrequires a parse\ntree.\nis the backpropagation of the gradient through the computation graph.\nFor neural network training, we backpropagate the error of the predic"
  },
  {
    "vector_id": 622,
    "chunk_id": "p170_c1",
    "page_number": 170,
    "text": ")Auto-differentiation\nin reverse mode\nrequires a parse\ntree.\nis the backpropagation of the gradient through the computation graph.\nFor neural network training, we backpropagate the error of the prediction\nwith respect to the label.\nThe automatic differentiation approach above works whenever we have\na function that can be expressed as a computation graph, where the ele-\nmentary functions are differentiable. In fact, the function may not even be\na mathematical function but a computer program. However, not all com-\nputer programs can be automatically differentiated, e.g., if we cannot find\ndifferential elementary functions. Programming structures, such as for\nloops and if statements, require more care as well.\n5.7 Higher-Order Derivatives\nSo far, we have discussed gradients, i.e., first-order"
  },
  {
    "vector_id": 623,
    "chunk_id": "p170_c2",
    "page_number": 170,
    "text": "ential elementary functions. Programming structures, such as for\nloops and if statements, require more care as well.\n5.7 Higher-Order Derivatives\nSo far, we have discussed gradients, i.e., first-order derivatives. Some-\ntimes, we are interested in derivatives of higher order, e.g., when we want\nto use Newton\u2019s Method for optimization, which requires second-order\nderivatives (Nocedal and Wright, 2006). In Section 5.1.1, we discussed\nthe Taylor series to approximate functions using polynomials. In the mul-\ntivariate case, we can do exactly the same. In the following, we will do\nexactly this. But let us start with some notation.\nConsider a function f : R2 \u2192 R of two variables x, y. We use the\nfollowing notation for higher-order partial derivatives (and for gradients):\n\u22022f\n\u2202x2 is the second pa"
  },
  {
    "vector_id": 624,
    "chunk_id": "p170_c3",
    "page_number": 170,
    "text": "let us start with some notation.\nConsider a function f : R2 \u2192 R of two variables x, y. We use the\nfollowing notation for higher-order partial derivatives (and for gradients):\n\u22022f\n\u2202x2 is the second partial derivative of f with respect to x.\n\u2202nf\n\u2202xn is the nth partial derivative of f with respect to x.\n\u22022f\n\u2202y\u2202x = \u2202\n\u2202y\n\u0000\u2202f\n\u2202x\n\u0001\nis the partial derivative obtained by first partial differ-\nentiating with respect to x and then with respect to y.\n\u22022f\n\u2202x\u2202y is the partial derivative obtained by first partial differentiating by\ny and then x.\nThe Hessian is the collection of all second-order partial derivatives.Hessian\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 625,
    "chunk_id": "p170_c4",
    "page_number": 170,
    "text": "vatives.Hessian\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 626,
    "chunk_id": "p171_c0",
    "page_number": 171,
    "text": "5.8 Linearization and Multivariate Taylor Series 165\nFigure 5.12 Linear\napproximation of a\nfunction. The\noriginal function f\nis linearized at\nx0 = \u22122 using a\nfirst-order Taylor\nseries expansion.\n\u22124 \u22122 0 2 4\nx\n\u22122\n\u22121\n0\n1\nf(x)\nf(x)\nf(x0) f(x0) + f\u2032(x0)(x \u2212 x0)\nIf f(x, y) is a twice (continuously) differentiable function, then\n\u22022f\n\u2202x\u2202y = \u22022f\n\u2202y\u2202x , (5.146)\ni.e., the order of differentiation does not matter, and the corresponding\nHessian matrix Hessian matrix\nH =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u22022f\n\u2202x2\n\u22022f\n\u2202x\u2202y\n\u22022f\n\u2202x\u2202y\n\u22022f\n\u2202y2\n\uf8f9\n\uf8fa\uf8fa\uf8fb (5.147)\nis symmetric. The Hessian is denoted as\u22072\nx,yf(x, y). Generally , forx \u2208 Rn\nand f : Rn \u2192 R, the Hessian is an n \u00d7 n matrix. The Hessian measures\nthe curvature of the function locally around (x, y).\nRemark (Hessian of a Vector Field). If f : Rn \u2192 Rm is a vector field, the\nHessian is"
  },
  {
    "vector_id": 627,
    "chunk_id": "p171_c1",
    "page_number": 171,
    "text": "Rn \u2192 R, the Hessian is an n \u00d7 n matrix. The Hessian measures\nthe curvature of the function locally around (x, y).\nRemark (Hessian of a Vector Field). If f : Rn \u2192 Rm is a vector field, the\nHessian is an (m \u00d7 n \u00d7 n)-tensor. \u2662\n5.8 Linearization and Multivariate Taylor Series\nThe gradient \u2207f of a function f is often used for a locally linear approxi-\nmation of f around x0:\nf(x) \u2248 f(x0) + (\u2207xf)(x0)(x \u2212 x0) . (5.148)\nHere (\u2207xf)(x0) is the gradient of f with respect to x, evaluated at x0.\nFigure 5.12 illustrates the linear approximation of a functionf at an input\nx0. The original function is approximated by a straight line. This approx-\nimation is locally accurate, but the farther we move away from x0 the\nworse the approximation gets. Equation (5.148) is a special case of a mul-\ntivariate Taylor"
  },
  {
    "vector_id": 628,
    "chunk_id": "p171_c2",
    "page_number": 171,
    "text": "mated by a straight line. This approx-\nimation is locally accurate, but the farther we move away from x0 the\nworse the approximation gets. Equation (5.148) is a special case of a mul-\ntivariate Taylor series expansion of f at x0, where we consider only the\nfirst two terms. We discuss the more general case in the following, which\nwill allow for better approximations.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 629,
    "chunk_id": "p172_c0",
    "page_number": 172,
    "text": "166 Vector Calculus\nFigure 5.13\nVisualizing outer\nproducts. Outer\nproducts of vectors\nincrease the\ndimensionality of\nthe array by 1 per\nterm. (a) The outer\nproduct of two\nvectors results in a\nmatrix; (b) the\nouter product of\nthree vectors yields\na third-order tensor.\n(a) Given a vector \u03b4 \u2208 R4, we obtain the outer product \u03b42 := \u03b4 \u2297 \u03b4 = \u03b4\u03b4\u22a4 \u2208\nR4\u00d74 as a matrix.\n(b) An outer product \u03b43 := \u03b4 \u2297 \u03b4 \u2297 \u03b4 \u2208 R4\u00d74\u00d74 results in a third-order tensor (\u201cthree-\ndimensional matrix\u201d), i.e., an array with three indexes.\nDefinition 5.7 (Multivariate Taylor Series). We consider a function\nf : RD \u2192 R (5.149)\nx 7\u2192 f(x) , x \u2208 RD , (5.150)\nthat is smooth at x0. When we define the difference vector \u03b4 := x \u2212 x0,\nthe multivariate Taylor series of f at (x0) is defined asmultivariate Taylor\nseries\nf(x) =\n\u221eX\nk=0\nDk\nxf(x0)"
  },
  {
    "vector_id": 630,
    "chunk_id": "p172_c1",
    "page_number": 172,
    "text": ") , x \u2208 RD , (5.150)\nthat is smooth at x0. When we define the difference vector \u03b4 := x \u2212 x0,\nthe multivariate Taylor series of f at (x0) is defined asmultivariate Taylor\nseries\nf(x) =\n\u221eX\nk=0\nDk\nxf(x0)\nk! \u03b4k , (5.151)\nwhere Dk\nxf(x0) is the k-th (total) derivative of f with respect to x, eval-\nuated at x0.\nDefinition 5.8 (Taylor Polynomial). The Taylor polynomial of degree n ofTaylor polynomial\nf at x0 contains the first n + 1 components of the series in (5.151) and is\ndefined as\nTn(x) =\nnX\nk=0\nDk\nxf(x0)\nk! \u03b4k . (5.152)\nIn (5.151) and (5.152), we used the slightly sloppy notation of \u03b4k,\nwhich is not defined for vectors x \u2208 RD, D >1, and k > 1. Note that\nboth Dk\nxf and \u03b4k are k-th order tensors, i.e., k-dimensional arrays. TheA vector can be\nimplemented as a\none-dimensional\narray , a matrix"
  },
  {
    "vector_id": 631,
    "chunk_id": "p172_c2",
    "page_number": 172,
    "text": "s not defined for vectors x \u2208 RD, D >1, and k > 1. Note that\nboth Dk\nxf and \u03b4k are k-th order tensors, i.e., k-dimensional arrays. TheA vector can be\nimplemented as a\none-dimensional\narray , a matrix as a\ntwo-dimensional\narray .\nkth-order tensor \u03b4k \u2208 R\nk times\nz }| {\nD\u00d7D\u00d7...\u00d7D is obtained as a k-fold outer product,\ndenoted by \u2297, of the vector \u03b4 \u2208 RD. For example,\n\u03b42 := \u03b4 \u2297 \u03b4 = \u03b4\u03b4\u22a4 , \u03b42[i, j] = \u03b4[i]\u03b4[j] (5.153)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 632,
    "chunk_id": "p173_c0",
    "page_number": 173,
    "text": "5.8 Linearization and Multivariate Taylor Series 167\n\u03b43 := \u03b4 \u2297 \u03b4 \u2297 \u03b4 , \u03b43[i, j, k] = \u03b4[i]\u03b4[j]\u03b4[k] . (5.154)\nFigure 5.13 visualizes two such outer products. In general, we obtain the\nterms\nDk\nxf(x0)\u03b4k =\nDX\ni1=1\n\u00b7 \u00b7\u00b7\nDX\nik=1\nDk\nxf(x0)[i1, . . . , ik]\u03b4[i1] \u00b7 \u00b7\u00b7\u03b4[ik] (5.155)\nin the Taylor series, where Dk\nxf(x0)\u03b4k contains k-th order polynomials.\nNow that we defined the Taylor series for vector fields, let us explicitly\nwrite down the first terms Dk\nxf(x0)\u03b4k of the Taylor series expansion for\nk = 0, . . . ,3 and \u03b4 := x \u2212 x0: np.einsum(\n\u2019i,i\u2019,Df1,d)\nnp.einsum(\n\u2019ij,i,j\u2019,\nDf2,d,d)\nnp.einsum(\n\u2019ijk,i,j,k\u2019,\nDf3,d,d,d)\nk = 0 : D0\nxf(x0)\u03b40 = f(x0) \u2208 R (5.156)\nk = 1 : D1\nxf(x0)\u03b41 = \u2207xf(x0)| {z }\n1\u00d7D\n\u03b4|{z}\nD\u00d71\n=\nDX\ni=1\n\u2207xf(x0)[i]\u03b4[i] \u2208 R (5.157)\nk = 2 : D2\nxf(x0)\u03b42 = tr\n\u0000\nH(x0)| {z }\nD\u00d7D\n\u03b4|{z}\nD\u00d71\n\u03b4\u22a4\n|{"
  },
  {
    "vector_id": 633,
    "chunk_id": "p173_c1",
    "page_number": 173,
    "text": ",k\u2019,\nDf3,d,d,d)\nk = 0 : D0\nxf(x0)\u03b40 = f(x0) \u2208 R (5.156)\nk = 1 : D1\nxf(x0)\u03b41 = \u2207xf(x0)| {z }\n1\u00d7D\n\u03b4|{z}\nD\u00d71\n=\nDX\ni=1\n\u2207xf(x0)[i]\u03b4[i] \u2208 R (5.157)\nk = 2 : D2\nxf(x0)\u03b42 = tr\n\u0000\nH(x0)| {z }\nD\u00d7D\n\u03b4|{z}\nD\u00d71\n\u03b4\u22a4\n|{z}\n1\u00d7D\n\u0001\n= \u03b4\u22a4H(x0)\u03b4 (5.158)\n=\nDX\ni=1\nDX\nj=1\nH[i, j]\u03b4[i]\u03b4[j] \u2208 R (5.159)\nk = 3 : D3\nxf(x0)\u03b43 =\nDX\ni=1\nDX\nj=1\nDX\nk=1\nD3\nxf(x0)[i, j, k]\u03b4[i]\u03b4[j]\u03b4[k] \u2208 R\n(5.160)\nHere, H(x0) is the Hessian of f evaluated at x0.\nExample 5.15 (Taylor Series Expansion of a Function with Two Vari-\nables)\nConsider the function\nf(x, y) = x2 + 2xy + y3 . (5.161)\nWe want to compute the Taylor series expansion of f at (x0, y0) = (1, 2).\nBefore we start, let us discuss what to expect: The function in (5.161) is\na polynomial of degree 3. We are looking for a Taylor series expansion,\nwhich itself is a linear combination of po"
  },
  {
    "vector_id": 634,
    "chunk_id": "p173_c2",
    "page_number": 173,
    "text": "= (1, 2).\nBefore we start, let us discuss what to expect: The function in (5.161) is\na polynomial of degree 3. We are looking for a Taylor series expansion,\nwhich itself is a linear combination of polynomials. Therefore, we do not\nexpect the Taylor series expansion to contain terms of fourth or higher\norder to express a third-order polynomial. This means that it should be\nsufficient to determine the first four terms of (5.151) for an exact alterna-\ntive representation of (5.161).\nTo determine the Taylor series expansion, we start with the constant\nterm and the first-order derivatives, which are given by\nf(1, 2) = 13 (5.162)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 635,
    "chunk_id": "p173_c3",
    "page_number": 173,
    "text": "re given by\nf(1, 2) = 13 (5.162)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 636,
    "chunk_id": "p174_c0",
    "page_number": 174,
    "text": "168 Vector Calculus\n\u2202f\n\u2202x = 2x + 2y =\u21d2 \u2202f\n\u2202x (1, 2) = 6 (5.163)\n\u2202f\n\u2202y = 2x + 3y2 =\u21d2 \u2202f\n\u2202y (1, 2) = 14 . (5.164)\nTherefore, we obtain\nD1\nx,yf(1, 2) = \u2207x,yf(1, 2) =\nh\n\u2202f\n\u2202x (1, 2) \u2202f\n\u2202y (1, 2)\ni\n=\n\u0002\n6 14\n\u0003\n\u2208 R1\u00d72\n(5.165)\nsuch that\nD1\nx,yf(1, 2)\n1! \u03b4 =\n\u0002\n6 14\n\u0003\u0014x \u2212 1\ny \u2212 2\n\u0015\n= 6(x \u2212 1) + 14(y \u2212 2) . (5.166)\nNote that D1\nx,yf(1, 2)\u03b4 contains only linear terms, i.e., first-order polyno-\nmials.\nThe second-order partial derivatives are given by\n\u22022f\n\u2202x2 = 2 =\u21d2 \u22022f\n\u2202x2 (1, 2) = 2 (5.167)\n\u22022f\n\u2202y2 = 6y =\u21d2 \u22022f\n\u2202y2 (1, 2) = 12 (5.168)\n\u22022f\n\u2202y\u2202x = 2 =\u21d2 \u22022f\n\u2202y\u2202x (1, 2) = 2 (5.169)\n\u22022f\n\u2202x\u2202y = 2 =\u21d2 \u22022f\n\u2202x\u2202y (1, 2) = 2 . (5.170)\nWhen we collect the second-order partial derivatives, we obtain the Hes-\nsian\nH =\n\" \u22022f\n\u2202x2\n\u22022f\n\u2202x\u2202y\n\u22022f\n\u2202y\u2202x\n\u22022f\n\u2202y2\n#\n=\n\u00142 2\n2 6 y\n\u0015\n, (5.171)\nsuch that\nH(1, 2) =\n\u00142 2\n2 12\n\u0015\n\u2208 R2\u00d7"
  },
  {
    "vector_id": 637,
    "chunk_id": "p174_c1",
    "page_number": 174,
    "text": "2) = 2 . (5.170)\nWhen we collect the second-order partial derivatives, we obtain the Hes-\nsian\nH =\n\" \u22022f\n\u2202x2\n\u22022f\n\u2202x\u2202y\n\u22022f\n\u2202y\u2202x\n\u22022f\n\u2202y2\n#\n=\n\u00142 2\n2 6 y\n\u0015\n, (5.171)\nsuch that\nH(1, 2) =\n\u00142 2\n2 12\n\u0015\n\u2208 R2\u00d72 . (5.172)\nTherefore, the next term of the Taylor-series expansion is given by\nD2\nx,yf(1, 2)\n2! \u03b42 = 1\n2\u03b4\u22a4H(1, 2)\u03b4 (5.173a)\n= 1\n2\n\u0002\nx \u2212 1 y \u2212 2\n\u0003\u00142 2\n2 12\n\u0015\u0014x \u2212 1\ny \u2212 2\n\u0015\n(5.173b)\n= (x \u2212 1)2 + 2(x \u2212 1)(y \u2212 2) + 6(y \u2212 2)2 . (5.173c)\nHere, D2\nx,yf(1, 2)\u03b42 contains only quadratic terms, i.e., second-order poly-\nnomials.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 638,
    "chunk_id": "p174_c2",
    "page_number": 174,
    "text": "ook.com."
  },
  {
    "vector_id": 639,
    "chunk_id": "p175_c0",
    "page_number": 175,
    "text": "5.8 Linearization and Multivariate Taylor Series 169\nThe third-order derivatives are obtained as\nD3\nx,yf =\nh\n\u2202H\n\u2202x\n\u2202H\n\u2202y\ni\n\u2208 R2\u00d72\u00d72 , (5.174)\nD3\nx,yf[:, :, 1] = \u2202H\n\u2202x =\n\" \u22023f\n\u2202x3\n\u22023f\n\u2202x2\u2202y\n\u22023f\n\u2202x\u2202y\u2202x\n\u22023f\n\u2202x\u2202y2\n#\n, (5.175)\nD3\nx,yf[:, :, 2] = \u2202H\n\u2202y =\n\" \u22023f\n\u2202y\u2202x2\n\u22023f\n\u2202y\u2202x\u2202y\n\u22023f\n\u2202y2\u2202x\n\u22023f\n\u2202y3\n#\n. (5.176)\nSince most second-order partial derivatives in the Hessian in (5.171) are\nconstant, the only nonzero third-order partial derivative is\n\u22023f\n\u2202y3 = 6 =\u21d2 \u22023f\n\u2202y3 (1, 2) = 6 . (5.177)\nHigher-order derivatives and the mixed derivatives of degree 3 (e.g.,\n\u2202f 3\n\u2202x2\u2202y ) vanish, such that\nD3\nx,yf[:, :, 1] =\n\u00140 0\n0 0\n\u0015\n, D 3\nx,yf[:, :, 2] =\n\u00140 0\n0 6\n\u0015\n(5.178)\nand\nD3\nx,yf(1, 2)\n3! \u03b43 = (y \u2212 2)3 , (5.179)\nwhich collects all cubic terms of the Taylor series. Overall, the (exact)\nTaylor series expansion of f"
  },
  {
    "vector_id": 640,
    "chunk_id": "p175_c1",
    "page_number": 175,
    "text": "\u00140 0\n0 0\n\u0015\n, D 3\nx,yf[:, :, 2] =\n\u00140 0\n0 6\n\u0015\n(5.178)\nand\nD3\nx,yf(1, 2)\n3! \u03b43 = (y \u2212 2)3 , (5.179)\nwhich collects all cubic terms of the Taylor series. Overall, the (exact)\nTaylor series expansion of f at (x0, y0) = (1, 2) is\nf(x) = f(1, 2) +D1\nx,yf(1, 2)\u03b4 + D2\nx,yf(1, 2)\n2! \u03b42 + D3\nx,yf(1, 2)\n3! \u03b43\n(5.180a)\n= f(1, 2) + \u2202f (1, 2)\n\u2202x (x \u2212 1) + \u2202f (1, 2)\n\u2202y (y \u2212 2)\n+ 1\n2!\n\u0012\u22022f(1, 2)\n\u2202x2 (x \u2212 1)2 + \u22022f(1, 2)\n\u2202y2 (y \u2212 2)2\n+ 2\u22022f(1, 2)\n\u2202x\u2202y (x \u2212 1)(y \u2212 2)\n\u0013\n+ 1\n6\n\u22023f(1, 2)\n\u2202y3 (y \u2212 2)3 (5.180b)\n= 13 + 6(x \u2212 1) + 14(y \u2212 2)\n+ (x \u2212 1)2 + 6(y \u2212 2)2 + 2(x \u2212 1)(y \u2212 2) + (y \u2212 2)3 . (5.180c)\nIn this case, we obtained an exact Taylor series expansion of the polyno-\nmial in (5.161), i.e., the polynomial in (5.180c) is identical to the original\npolynomial in (5.161). In this particular example, this result"
  },
  {
    "vector_id": 641,
    "chunk_id": "p175_c2",
    "page_number": 175,
    "text": "e obtained an exact Taylor series expansion of the polyno-\nmial in (5.161), i.e., the polynomial in (5.180c) is identical to the original\npolynomial in (5.161). In this particular example, this result is not sur-\nprising since the original function was a third-order polynomial, which\nwe expressed through a linear combination of constant terms, first-order,\nsecond-order, and third-order polynomials in (5.180c).\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 642,
    "chunk_id": "p176_c0",
    "page_number": 176,
    "text": "170 Vector Calculus\n5.9 Further Reading\nFurther details of matrix differentials, along with a short review of the\nrequired linear algebra, can be found in Magnus and Neudecker (2007).\nAutomatic differentiation has had a long history , and we refer to Griewank\nand Walther (2003), Griewank and Walther (2008), and Elliott (2009)\nand the references therein.\nIn machine learning (and other disciplines), we often need to compute\nexpectations, i.e., we need to solve integrals of the form\nEx[f(x)] =\nZ\nf(x)p(x)dx. (5.181)\nEven if p(x) is in a convenient form (e.g., Gaussian), this integral gen-\nerally cannot be solved analytically . The Taylor series expansion of f is\none way of finding an approximate solution: Assuming p(x) = N\n\u0000\n\u00b5, \u03a3\n\u0001\nis Gaussian, then the first-order Taylor series expansion arou"
  },
  {
    "vector_id": 643,
    "chunk_id": "p176_c1",
    "page_number": 176,
    "text": "annot be solved analytically . The Taylor series expansion of f is\none way of finding an approximate solution: Assuming p(x) = N\n\u0000\n\u00b5, \u03a3\n\u0001\nis Gaussian, then the first-order Taylor series expansion around \u00b5 locally\nlinearizes the nonlinear function f. For linear functions, we can compute\nthe mean (and the covariance) exactly if p(x) is Gaussian distributed (see\nSection 6.5). This property is heavily exploited by the extended Kalmanextended Kalman\nfilter filter (Maybeck, 1979) for online state estimation in nonlinear dynami-\ncal systems (also called \u201cstate-space models\u201d). Other deterministic ways\nto approximate the integral in (5.181) are the unscented transform (Julierunscented transform\nand Uhlmann, 1997), which does not require any gradients, or theLaplaceLaplace\napproximation approximatio"
  },
  {
    "vector_id": 644,
    "chunk_id": "p176_c2",
    "page_number": 176,
    "text": "to approximate the integral in (5.181) are the unscented transform (Julierunscented transform\nand Uhlmann, 1997), which does not require any gradients, or theLaplaceLaplace\napproximation approximation (MacKay, 2003; Bishop, 2006; Murphy, 2012), which uses\na second-order Taylor series expansion (requiring the Hessian) for a local\nGaussian approximation of p(x) around its mode.\nExercises\n5.1 Compute the derivative f\u2032(x) for\nf(x) = log(x4) sin(x3) .\n5.2 Compute the derivative f\u2032(x) of the logistic sigmoid\nf(x) = 1\n1 + exp(\u2212x) .\n5.3 Compute the derivative f\u2032(x) of the function\nf(x) = exp(\u2212 1\n2\u03c32 (x \u2212 \u00b5)2) ,\nwhere \u00b5, \u03c3\u2208 R are constants.\n5.4 Compute the Taylor polynomials Tn, n = 0, . . . ,5 of f(x) = sin( x) + cos(x)\nat x0 = 0.\n5.5 Consider the following functions:\nf1(x) = sin(x1) cos(x2) , x"
  },
  {
    "vector_id": 645,
    "chunk_id": "p176_c3",
    "page_number": 176,
    "text": "(x \u2212 \u00b5)2) ,\nwhere \u00b5, \u03c3\u2208 R are constants.\n5.4 Compute the Taylor polynomials Tn, n = 0, . . . ,5 of f(x) = sin( x) + cos(x)\nat x0 = 0.\n5.5 Consider the following functions:\nf1(x) = sin(x1) cos(x2) , x \u2208 R2\nf2(x, y) = x\u22a4y , x, y \u2208 Rn\nf3(x) = xx\u22a4 , x \u2208 Rn\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 646,
    "chunk_id": "p177_c0",
    "page_number": 177,
    "text": "Exercises 171\na. What are the dimensions of \u2202fi\n\u2202x ?\nb. Compute the Jacobians.\n5.6 Differentiate f with respect to t and g with respect to X, where\nf(t) = sin(log(t\u22a4t)) , t \u2208 RD\ng(X) = tr(AXB) , A \u2208 RD\u00d7E, X \u2208 RE\u00d7F , B \u2208 RF\u00d7D ,\nwhere tr(\u00b7) denotes the trace.\n5.7 Compute the derivatives df/dx of the following functions by using the chain\nrule. Provide the dimensions of every single partial derivative. Describe your\nsteps in detail.\na.\nf(z) = log(1 + z) , z = x\u22a4x, x \u2208 RD\nb.\nf(z) = sin(z) , z = Ax + b, A \u2208 RE\u00d7D, x \u2208 RD, b \u2208 RE\nwhere sin(\u00b7) is applied to every element of z.\n5.8 Compute the derivatives df/dx of the following functions. Describe your\nsteps in detail.\na. Use the chain rule. Provide the dimensions of every single partial deriva-\ntive.\nf(z) = exp(\u22121\n2 z)\nz = g(y) = y\u22a4S\u22121y\ny = h(x) ="
  },
  {
    "vector_id": 647,
    "chunk_id": "p177_c1",
    "page_number": 177,
    "text": "ves df/dx of the following functions. Describe your\nsteps in detail.\na. Use the chain rule. Provide the dimensions of every single partial deriva-\ntive.\nf(z) = exp(\u22121\n2 z)\nz = g(y) = y\u22a4S\u22121y\ny = h(x) = x \u2212 \u00b5\nwhere x, \u00b5 \u2208 RD, S \u2208 RD\u00d7D.\nb.\nf(x) = tr(xx\u22a4 + \u03c32I) , x \u2208 RD\nHere tr(A) is the trace of A, i.e., the sum of the diagonal elements Aii.\nHint: Explicitly write out the outer product.\nc. Use the chain rule. Provide the dimensions of every single partial deriva-\ntive. You do not need to compute the product of the partial derivatives\nexplicitly .\nf = tanh(z) \u2208 RM\nz = Ax + b, x \u2208 RN , A \u2208 RM\u00d7N , b \u2208 RM .\nHere, tanh is applied to every component of z.\n5.9 We define\ng(x, z, \u03bd) := log p(x, z) \u2212 log q(z, \u03bd)\nz := t(\u03f5, \u03bd)\nfor differentiable functions p, q, tand x \u2208 RD, z \u2208 RE, \u03bd \u2208 RF , \u03f5 \u2208 RG. By\nus"
  },
  {
    "vector_id": 648,
    "chunk_id": "p177_c2",
    "page_number": 177,
    "text": "\u2208 RM .\nHere, tanh is applied to every component of z.\n5.9 We define\ng(x, z, \u03bd) := log p(x, z) \u2212 log q(z, \u03bd)\nz := t(\u03f5, \u03bd)\nfor differentiable functions p, q, tand x \u2208 RD, z \u2208 RE, \u03bd \u2208 RF , \u03f5 \u2208 RG. By\nusing the chain rule, compute the gradient\nd\nd\u03bd g(x, z, \u03bd) .\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 649,
    "chunk_id": "p178_c0",
    "page_number": 178,
    "text": "6\nProbability and Distributions\nProbability , loosely speaking, concerns the study of uncertainty . Probabil-\nity can be thought of as the fraction of times an event occurs, or as a degree\nof belief about an event. We then would like to use this probability to mea-\nsure the chance of something occurring in an experiment. As mentioned\nin Chapter 1, we often quantify uncertainty in the data, uncertainty in the\nmachine learning model, and uncertainty in the predictions produced by\nthe model. Quantifying uncertainty requires the idea of arandom variable,random variable\nwhich is a function that maps outcomes of random experiments to a set of\nproperties that we are interested in. Associated with the random variable\nis a function that measures the probability that a particular outcome (or\nset of"
  },
  {
    "vector_id": 650,
    "chunk_id": "p178_c1",
    "page_number": 178,
    "text": "s outcomes of random experiments to a set of\nproperties that we are interested in. Associated with the random variable\nis a function that measures the probability that a particular outcome (or\nset of outcomes) will occur; this is called the probability distribution.probability\ndistribution Probability distributions are used as a building block for other con-\ncepts, such as probabilistic modeling (Section 8.4), graphical models (Sec-\ntion 8.5), and model selection (Section 8.6). In the next section, we present\nthe three concepts that define a probability space (the sample space, the\nevents, and the probability of an event) and how they are related to a\nfourth concept called the random variable. The presentation is deliber-\nately slightly hand wavy since a rigorous presentation may occlude t"
  },
  {
    "vector_id": 651,
    "chunk_id": "p178_c2",
    "page_number": 178,
    "text": "the probability of an event) and how they are related to a\nfourth concept called the random variable. The presentation is deliber-\nately slightly hand wavy since a rigorous presentation may occlude the\nintuition behind the concepts. An outline of the concepts presented in this\nchapter are shown in Figure 6.1.\n6.1 Construction of a Probability Space\nThe theory of probability aims at defining a mathematical structure to\ndescribe random outcomes of experiments. For example, when tossing a\nsingle coin, we cannot determine the outcome, but by doing a large num-\nber of coin tosses, we can observe a regularity in the average outcome.\nUsing this mathematical structure of probability , the goal is to perform\nautomated reasoning, and in this sense, probability generalizes logical\nreasoning (Jaynes,"
  },
  {
    "vector_id": 652,
    "chunk_id": "p178_c3",
    "page_number": 178,
    "text": "regularity in the average outcome.\nUsing this mathematical structure of probability , the goal is to perform\nautomated reasoning, and in this sense, probability generalizes logical\nreasoning (Jaynes, 2003).\n6.1.1 Philosophical Issues\nWhen constructing automated reasoning systems, classical Boolean logic\ndoes not allow us to express certain forms of plausible reasoning. Consider\n172\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 653,
    "chunk_id": "p178_c4",
    "page_number": 178,
    "text": "l use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 654,
    "chunk_id": "p179_c0",
    "page_number": 179,
    "text": "6.1 Construction of a Probability Space 173\nFigure 6.1 A mind\nmap of the concepts\nrelated to random\nvariables and\nprobability\ndistributions, as\ndescribed in this\nchapter.\nRandom variable& distribution\nSum ruleProduct rule\nBayes\u2019 Theorem\nSummary statistics\nMean Variance\nTransformations\nIndependence\nInner product\nGaussian\nBernoulli\nBeta\nSufficient statistics\nExponential family\nChapter 9Regression\nChapter 10Dimensionalityreduction\nChapter 11Density estimation\nProperty\nSimilarity\nExample\nExample\nConjugate\nPropertyFinite\nthe following scenario: We observe that A is false. We find B becomes\nless plausible, although no conclusion can be drawn from classical logic.\nWe observe that B is true. It seems A becomes more plausible. We use\nthis form of reasoning daily . We are waiting for a friend, and c"
  },
  {
    "vector_id": 655,
    "chunk_id": "p179_c1",
    "page_number": 179,
    "text": "usible, although no conclusion can be drawn from classical logic.\nWe observe that B is true. It seems A becomes more plausible. We use\nthis form of reasoning daily . We are waiting for a friend, and consider\nthree possibilities: H1, she is on time; H2, she has been delayed by traffic;\nand H3, she has been abducted by aliens. When we observe our friend\nis late, we must logically rule out H1. We also tend to consider H2 to be\nmore likely , though we are not logically required to do so. Finally , we may\nconsider H3 to be possible, but we continue to consider it quite unlikely .\nHow do we conclude H2 is the most plausible answer? Seen in this way , \u201cFor plausible\nreasoning it is\nnecessary to extend\nthe discrete true and\nfalse values of truth\nto continuous\nplausibilities\u201d\n(Jaynes, 2003).\nprobab"
  },
  {
    "vector_id": 656,
    "chunk_id": "p179_c2",
    "page_number": 179,
    "text": "H2 is the most plausible answer? Seen in this way , \u201cFor plausible\nreasoning it is\nnecessary to extend\nthe discrete true and\nfalse values of truth\nto continuous\nplausibilities\u201d\n(Jaynes, 2003).\nprobability theory can be considered a generalization of Boolean logic. In\nthe context of machine learning, it is often applied in this way to formalize\nthe design of automated reasoning systems. Further arguments about how\nprobability theory is the foundation of reasoning systems can be found\nin Pearl (1988).\nThe philosophical basis of probability and how it should be somehow\nrelated to what we think should be true (in the logical sense) was studied\nby Cox (Jaynes, 2003). Another way to think about it is that if we are\nprecise about our common sense we end up constructing probabilities.\nE. T. Jayne"
  },
  {
    "vector_id": 657,
    "chunk_id": "p179_c3",
    "page_number": 179,
    "text": "hould be true (in the logical sense) was studied\nby Cox (Jaynes, 2003). Another way to think about it is that if we are\nprecise about our common sense we end up constructing probabilities.\nE. T. Jaynes (1922\u20131998) identified three mathematical criteria, which\nmust apply to all plausibilities:\n1. The degrees of plausibility are represented by real numbers.\n2. These numbers must be based on the rules of common sense.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 658,
    "chunk_id": "p180_c0",
    "page_number": 180,
    "text": "174 Probability and Distributions\n3. The resulting reasoning must be consistent, with the three following\nmeanings of the word \u201cconsistent\u201d:\n(a) Consistency or non-contradiction: When the same result can be\nreached through different means, the same plausibility value must\nbe found in all cases.\n(b) Honesty: All available data must be taken into account.\n(c) Reproducibility: If our state of knowledge about two problems are\nthe same, then we must assign the same degree of plausibility to\nboth of them.\nThe Cox\u2013Jaynes theorem proves these plausibilities to be sufficient to\ndefine the universal mathematical rules that apply to plausibility p, up to\ntransformation by an arbitrary monotonic function. Crucially , these rules\nare the rules of probability .\nRemark. In machine learning and statistics"
  },
  {
    "vector_id": 659,
    "chunk_id": "p180_c1",
    "page_number": 180,
    "text": "hematical rules that apply to plausibility p, up to\ntransformation by an arbitrary monotonic function. Crucially , these rules\nare the rules of probability .\nRemark. In machine learning and statistics, there are two major interpre-\ntations of probability: the Bayesian and frequentist interpretations (Bishop,\n2006; Efron and Hastie, 2016). The Bayesian interpretation uses probabil-\nity to specify the degree of uncertainty that the user has about an event. It\nis sometimes referred to as \u201csubjective probability\u201d or \u201cdegree of belief\u201d.\nThe frequentist interpretation considers the relative frequencies of events\nof interest to the total number of events that occurred. The probability of\nan event is defined as the relative frequency of the event in the limit when\none has infinite data. \u2662\nSome mac"
  },
  {
    "vector_id": 660,
    "chunk_id": "p180_c2",
    "page_number": 180,
    "text": "ies of events\nof interest to the total number of events that occurred. The probability of\nan event is defined as the relative frequency of the event in the limit when\none has infinite data. \u2662\nSome machine learning texts on probabilistic models use lazy notation\nand jargon, which is confusing. This text is no exception. Multiple distinct\nconcepts are all referred to as \u201cprobability distribution\u201d, and the reader\nhas to often disentangle the meaning from the context. One trick to help\nmake sense of probability distributions is to check whether we are trying\nto model something categorical (a discrete random variable) or some-\nthing continuous (a continuous random variable). The kinds of questions\nwe tackle in machine learning are closely related to whether we are con-\nsidering categorical or c"
  },
  {
    "vector_id": 661,
    "chunk_id": "p180_c3",
    "page_number": 180,
    "text": "ete random variable) or some-\nthing continuous (a continuous random variable). The kinds of questions\nwe tackle in machine learning are closely related to whether we are con-\nsidering categorical or continuous models.\n6.1.2 Probability and Random Variables\nThere are three distinct ideas that are often confused when discussing\nprobabilities. First is the idea of a probability space, which allows us to\nquantify the idea of a probability . However, we mostly do not work directly\nwith this basic probability space. Instead, we work with random variables\n(the second idea), which transfers the probability to a more convenient\n(often numerical) space. The third idea is the idea of a distribution or law\nassociated with a random variable. We will introduce the first two ideas\nin this section and exp"
  },
  {
    "vector_id": 662,
    "chunk_id": "p180_c4",
    "page_number": 180,
    "text": "ility to a more convenient\n(often numerical) space. The third idea is the idea of a distribution or law\nassociated with a random variable. We will introduce the first two ideas\nin this section and expand on the third idea in Section 6.2.\nModern probability is based on a set of axioms proposed by Kolmogorov\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 663,
    "chunk_id": "p181_c0",
    "page_number": 181,
    "text": "6.1 Construction of a Probability Space 175\n(Grinstead and Snell, 1997; Jaynes, 2003) that introduce the three con-\ncepts of sample space, event space, and probability measure. The prob-\nability space models a real-world process (referred to as an experiment)\nwith random outcomes.\nThe sample space \u2126\nThe sample space is the set of all possible outcomes of the experiment, sample space\nusually denoted by \u2126. For example, two successive coin tosses have\na sample space of {hh, tt, ht, th }, where \u201ch\u201d denotes \u201cheads\u201d and \u201ct\u201d\ndenotes \u201ctails\u201d.\nThe event space A\nThe event space is the space of potential results of the experiment. A event space\nsubset A of the sample space \u2126 is in the event space A if at the end\nof the experiment we can observe whether a particular outcome \u03c9 \u2208 \u2126\nis in A. The event sp"
  },
  {
    "vector_id": 664,
    "chunk_id": "p181_c1",
    "page_number": 181,
    "text": "results of the experiment. A event space\nsubset A of the sample space \u2126 is in the event space A if at the end\nof the experiment we can observe whether a particular outcome \u03c9 \u2208 \u2126\nis in A. The event space A is obtained by considering the collection of\nsubsets of \u2126, and for discrete probability distributions (Section 6.2.1)\nA is often the power set of \u2126.\nThe probability P\nWith each event A \u2208 A, we associate a numberP(A) that measures the\nprobability or degree of belief that the event will occur. P(A) is called\nthe probability of A. probability\nThe probability of a single event must lie in the interval [0, 1], and the\ntotal probability over all outcomes in the sample space \u2126 must be 1, i.e.,\nP(\u2126) = 1. Given a probability space(\u2126, A, P), we want to use it to model\nsome real-world phenomenon. I"
  },
  {
    "vector_id": 665,
    "chunk_id": "p181_c2",
    "page_number": 181,
    "text": "terval [0, 1], and the\ntotal probability over all outcomes in the sample space \u2126 must be 1, i.e.,\nP(\u2126) = 1. Given a probability space(\u2126, A, P), we want to use it to model\nsome real-world phenomenon. In machine learning, we often avoid explic-\nitly referring to the probability space, but instead refer to probabilities on\nquantities of interest, which we denote by T . In this book, we refer to T\nas the target space and refer to elements of T as states. We introduce a target space\nfunction X : \u2126 \u2192 Tthat takes an element of\u2126 (an outcome) and returns\na particular quantity of interest x, a value in T . This association/mapping\nfrom \u2126 to T is called arandom variable. For example, in the case of tossing random variable\ntwo coins and counting the number of heads, a random variable X maps\nto the thr"
  },
  {
    "vector_id": 666,
    "chunk_id": "p181_c3",
    "page_number": 181,
    "text": "T . This association/mapping\nfrom \u2126 to T is called arandom variable. For example, in the case of tossing random variable\ntwo coins and counting the number of heads, a random variable X maps\nto the three possible outcomes: X(hh) = 2, X(ht) = 1, X(th) = 1, and\nX(tt) = 0. In this particular case, T = {0, 1, 2}, and it is the probabilities\non elements ofT that we are interested in. For a finite sample space\u2126 and The name \u201crandom\nvariable\u201d is a great\nsource of\nmisunderstanding\nas it is neither\nrandom nor is it a\nvariable. It is a\nfunction.\nfinite T , the function corresponding to a random variable is essentially a\nlookup table. For any subset S \u2286 T, we associate PX(S) \u2208 [0, 1] (the\nprobability) to a particular event occurring corresponding to the random\nvariable X. Example 6.1 provides a concre"
  },
  {
    "vector_id": 667,
    "chunk_id": "p181_c4",
    "page_number": 181,
    "text": "is essentially a\nlookup table. For any subset S \u2286 T, we associate PX(S) \u2208 [0, 1] (the\nprobability) to a particular event occurring corresponding to the random\nvariable X. Example 6.1 provides a concrete illustration of the terminol-\nogy .\nRemark. The aforementioned sample space \u2126 unfortunately is referred\nto by different names in different books. Another common name for \u2126\nis \u201cstate space\u201d (Jacod and Protter, 2004), but state space is sometimes\nreserved for referring to states in a dynamical system (Hasselblatt and\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 668,
    "chunk_id": "p181_c5",
    "page_number": 181,
    "text": "ty Press (2020)."
  },
  {
    "vector_id": 669,
    "chunk_id": "p182_c0",
    "page_number": 182,
    "text": "176 Probability and Distributions\nKatok, 2003). Other names sometimes used to describe \u2126 are: \u201csample\ndescription space\u201d, \u201cpossibility space,\u201d and \u201cevent space\u201d. \u2662\nExample 6.1\nWe assume that the reader is already familiar with computing probabilitiesThis toy example is\nessentially a biased\ncoin flip example. of intersections and unions of sets of events. A gentler introduction to\nprobability with many examples can be found in chapter 2 of Walpole\net al. (2011).\nConsider a statistical experiment where we model a funfair game con-\nsisting of drawing two coins from a bag (with replacement). There are\ncoins from USA (denoted as $) and UK (denoted as \u00a3) in the bag, and\nsince we draw two coins from the bag, there are four outcomes in total.\nThe state space or sample space \u2126 of this experiment is"
  },
  {
    "vector_id": 670,
    "chunk_id": "p182_c1",
    "page_number": 182,
    "text": "are\ncoins from USA (denoted as $) and UK (denoted as \u00a3) in the bag, and\nsince we draw two coins from the bag, there are four outcomes in total.\nThe state space or sample space \u2126 of this experiment is then ($, $), ($,\n\u00a3), (\u00a3, $), (\u00a3, \u00a3). Let us assume that the composition of the bag of coins is\nsuch that a draw returns at random a $ with probability 0.3.\nThe event we are interested in is the total number of times the repeated\ndraw returns $. Let us define a random variable X that maps the sample\nspace \u2126 to T , which denotes the number of times we draw $ out of the\nbag. We can see from the preceding sample space we can get zero $, one $,\nor two $s, and thereforeT = {0, 1, 2}. The random variableX (a function\nor lookup table) can be represented as a table like the following:\nX(($, $)) = 2 (6"
  },
  {
    "vector_id": 671,
    "chunk_id": "p182_c2",
    "page_number": 182,
    "text": "eding sample space we can get zero $, one $,\nor two $s, and thereforeT = {0, 1, 2}. The random variableX (a function\nor lookup table) can be represented as a table like the following:\nX(($, $)) = 2 (6.1)\nX(($, \u00a3)) = 1 (6.2)\nX((\u00a3, $)) = 1 (6.3)\nX((\u00a3, \u00a3)) = 0 . (6.4)\nSince we return the first coin we draw before drawing the second, this\nimplies that the two draws are independent of each other, which we will\ndiscuss in Section 6.4.5. Note that there are two experimental outcomes,\nwhich map to the same event, where only one of the draws returns $.\nTherefore, the probability mass function (Section 6.2.1) of X is given by\nP(X = 2) = P(($, $))\n= P($) \u00b7 P($)\n= 0.3 \u00b7 0.3 = 0.09 (6.5)\nP(X = 1) = P(($, \u00a3) \u222a (\u00a3, $))\n= P(($, \u00a3)) + P((\u00a3, $))\n= 0.3 \u00b7 (1 \u2212 0.3) + (1\u2212 0.3) \u00b7 0.3 = 0.42 (6.6)\nP(X = 0) = P(("
  },
  {
    "vector_id": 672,
    "chunk_id": "p182_c3",
    "page_number": 182,
    "text": "6.2.1) of X is given by\nP(X = 2) = P(($, $))\n= P($) \u00b7 P($)\n= 0.3 \u00b7 0.3 = 0.09 (6.5)\nP(X = 1) = P(($, \u00a3) \u222a (\u00a3, $))\n= P(($, \u00a3)) + P((\u00a3, $))\n= 0.3 \u00b7 (1 \u2212 0.3) + (1\u2212 0.3) \u00b7 0.3 = 0.42 (6.6)\nP(X = 0) = P((\u00a3, \u00a3))\n= P(\u00a3) \u00b7 P(\u00a3)\n= (1 \u2212 0.3) \u00b7 (1 \u2212 0.3) = 0.49 . (6.7)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 673,
    "chunk_id": "p183_c0",
    "page_number": 183,
    "text": "6.1 Construction of a Probability Space 177\nIn the calculation, we equated two different concepts, the probability\nof the output of X and the probability of the samples in \u2126. For example,\nin (6.7) we say P(X = 0) = P((\u00a3, \u00a3)). Consider the random variable\nX : \u2126 \u2192 Tand a subset S \u2286 T(for example, a single element of T ,\nsuch as the outcome that one head is obtained when tossing two coins).\nLet X\u22121(S) be the pre-image of S by X, i.e., the set of elements of \u2126 that\nmap to S under X; {\u03c9 \u2208 \u2126 : X(\u03c9) \u2208 S}. One way to understand the\ntransformation of probability from events in \u2126 via the random variable\nX is to associate it with the probability of the pre-image of S (Jacod and\nProtter, 2004). For S \u2286 T, we have the notation\nPX(S) = P(X \u2208 S) = P(X\u22121(S)) = P({\u03c9 \u2208 \u2126 : X(\u03c9) \u2208 S}) . (6.8)\nThe left-hand s"
  },
  {
    "vector_id": 674,
    "chunk_id": "p183_c1",
    "page_number": 183,
    "text": "X is to associate it with the probability of the pre-image of S (Jacod and\nProtter, 2004). For S \u2286 T, we have the notation\nPX(S) = P(X \u2208 S) = P(X\u22121(S)) = P({\u03c9 \u2208 \u2126 : X(\u03c9) \u2208 S}) . (6.8)\nThe left-hand side of (6.8) is the probability of the set of possible outcomes\n(e.g., number of $ = 1) that we are interested in. Via the random variable\nX, which maps states to outcomes, we see in the right-hand side of (6.8)\nthat this is the probability of the set of states (in\u2126) that have the property\n(e.g., $\u00a3, \u00a3$). We say that a random variable X is distributed according\nto a particular probability distribution PX, which defines the probability\nmapping between the event and the probability of the outcome of the\nrandom variable. In other words, the functionPX or equivalently P \u25e6X\u22121\nis the law or distribu"
  },
  {
    "vector_id": 675,
    "chunk_id": "p183_c2",
    "page_number": 183,
    "text": "tion PX, which defines the probability\nmapping between the event and the probability of the outcome of the\nrandom variable. In other words, the functionPX or equivalently P \u25e6X\u22121\nis the law or distribution of random variable X. law\ndistributionRemark. The target space, that is, the range T of the random variable X,\nis used to indicate the kind of probability space, i.e., aT random variable.\nWhen T is finite or countably infinite, this is called a discrete random\nvariable (Section 6.2.1). For continuous random variables (Section 6.2.2),\nwe only consider T = R or T = RD. \u2662\n6.1.3 Statistics\nProbability theory and statistics are often presented together, but they con-\ncern different aspects of uncertainty . One way of contrasting them is by the\nkinds of problems that are considered. Using proba"
  },
  {
    "vector_id": 676,
    "chunk_id": "p183_c3",
    "page_number": 183,
    "text": "ility theory and statistics are often presented together, but they con-\ncern different aspects of uncertainty . One way of contrasting them is by the\nkinds of problems that are considered. Using probability , we can consider\na model of some process, where the underlying uncertainty is captured\nby random variables, and we use the rules of probability to derive what\nhappens. In statistics, we observe that something has happened and try\nto figure out the underlying process that explains the observations. In this\nsense, machine learning is close to statistics in its goals to construct a\nmodel that adequately represents the process that generated the data. We\ncan use the rules of probability to obtain a \u201cbest-fitting\u201d model for some\ndata.\nAnother aspect of machine learning systems is that we ar"
  },
  {
    "vector_id": 677,
    "chunk_id": "p183_c4",
    "page_number": 183,
    "text": "adequately represents the process that generated the data. We\ncan use the rules of probability to obtain a \u201cbest-fitting\u201d model for some\ndata.\nAnother aspect of machine learning systems is that we are interested\nin generalization error (see Chapter 8). This means that we are actually\ninterested in the performance of our system on instances that we will\nobserve in future, which are not identical to the instances that we have\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 678,
    "chunk_id": "p184_c0",
    "page_number": 184,
    "text": "178 Probability and Distributions\nseen so far. This analysis of future performance relies on probability and\nstatistics, most of which is beyond what will be presented in this chapter.\nThe interested reader is encouraged to look at the books by Boucheron\net al. (2013) and Shalev-Shwartz and Ben-David (2014). We will see more\nabout statistics in Chapter 8.\n6.2 Discrete and Continuous Probabilities\nLet us focus our attention on ways to describe the probability of an event\nas introduced in Section 6.1. Depending on whether the target space is dis-\ncrete or continuous, the natural way to refer to distributions is different.\nWhen the target space T is discrete, we can specify the probability that a\nrandom variable X takes a particular value x \u2208 T, denoted as P(X = x).\nThe expression P(X = x) fo"
  },
  {
    "vector_id": 679,
    "chunk_id": "p184_c1",
    "page_number": 184,
    "text": "distributions is different.\nWhen the target space T is discrete, we can specify the probability that a\nrandom variable X takes a particular value x \u2208 T, denoted as P(X = x).\nThe expression P(X = x) for a discrete random variable X is known as\nthe probability mass function. When the target space T is continuous, e.g.,probability mass\nfunction the real line R, it is more natural to specify the probability that a random\nvariable X is in an interval, denoted by P(a \u2a7d X \u2a7d b) for a < b. By con-\nvention, we specify the probability that a random variable X is less than\na particular value x, denoted by P(X \u2a7d x). The expression P(X \u2a7d x) for\na continuous random variable X is known as the cumulative distributioncumulative\ndistribution function function. We will discuss continuous random variables in S"
  },
  {
    "vector_id": 680,
    "chunk_id": "p184_c2",
    "page_number": 184,
    "text": "P(X \u2a7d x). The expression P(X \u2a7d x) for\na continuous random variable X is known as the cumulative distributioncumulative\ndistribution function function. We will discuss continuous random variables in Section 6.2.2.\nWe will revisit the nomenclature and contrast discrete and continuous\nrandom variables in Section 6.2.3.\nRemark. We will use the phraseunivariate distribution to refer to distribu-univariate\ntions of a single random variable (whose states are denoted by non-bold\nx). We will refer to distributions of more than one random variable as\nmultivariate distributions, and will usually consider a vector of randommultivariate\nvariables (whose states are denoted by bold x). \u2662\n6.2.1 Discrete Probabilities\nWhen the target space is discrete, we can imagine the probability distri-\nbution of mult"
  },
  {
    "vector_id": 681,
    "chunk_id": "p184_c3",
    "page_number": 184,
    "text": "r a vector of randommultivariate\nvariables (whose states are denoted by bold x). \u2662\n6.2.1 Discrete Probabilities\nWhen the target space is discrete, we can imagine the probability distri-\nbution of multiple random variables as filling out a (multidimensional)\narray of numbers. Figure 6.2 shows an example. The target space of the\njoint probability is the Cartesian product of the target spaces of each of\nthe random variables. We define the joint probability as the entry of bothjoint probability\nvalues jointly\nP(X = xi, Y= yj) = nij\nN , (6.9)\nwhere nij is the number of events with state xi and yj and N the total\nnumber of events. The joint probability is the probability of the intersec-\ntion of both events, that is, P(X = xi, Y= yj) = P(X = xi \u2229 Y = yj).\nFigure 6.2 illustrates theprobability ma"
  },
  {
    "vector_id": 682,
    "chunk_id": "p184_c4",
    "page_number": 184,
    "text": "nd N the total\nnumber of events. The joint probability is the probability of the intersec-\ntion of both events, that is, P(X = xi, Y= yj) = P(X = xi \u2229 Y = yj).\nFigure 6.2 illustrates theprobability mass function(pmf) of a discrete prob-probability mass\nfunction ability distribution. For two random variables X and Y , the probability\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 683,
    "chunk_id": "p185_c0",
    "page_number": 185,
    "text": "6.2 Discrete and Continuous Probabilities 179\nFigure 6.2\nVisualization of a\ndiscrete bivariate\nprobability mass\nfunction, with\nrandom variables X\nand Y . This\ndiagram is adapted\nfrom Bishop (2006).\nX\nx1 x2 x3 x4 x5\nY\ny3\ny2\ny1\nnij\norj\nciz}|{\nthat X = x and Y = y is (lazily) written as p(x, y) and is called the joint\nprobability . One can think of a probability as a function that takes state\nx and y and returns a real number, which is the reason we write p(x, y).\nThe marginal probability that X takes the value x irrespective of the value marginal probability\nof random variable Y is (lazily) written as p(x). We write X \u223c p(x) to\ndenote that the random variable X is distributed according to p(x). If we\nconsider only the instances where X = x, then the fraction of instances\n(the conditional pro"
  },
  {
    "vector_id": 684,
    "chunk_id": "p185_c1",
    "page_number": 185,
    "text": "ten as p(x). We write X \u223c p(x) to\ndenote that the random variable X is distributed according to p(x). If we\nconsider only the instances where X = x, then the fraction of instances\n(the conditional probability) for which Y = y is written (lazily) as p(y |x). conditional\nprobability\nExample 6.2\nConsider two random variablesX and Y , where X has five possible states\nand Y has three possible states, as shown in Figure 6.2. We denote by nij\nthe number of events with state X = xi and Y = yj, and denote by\nN the total number of events. The value ci is the sum of the individual\nfrequencies for the ith column, that is, ci = P3\nj=1 nij. Similarly , the value\nrj is the row sum, that is, rj = P5\ni=1 nij. Using these definitions, we can\ncompactly express the distribution of X and Y .\nThe probability di"
  },
  {
    "vector_id": 685,
    "chunk_id": "p185_c2",
    "page_number": 185,
    "text": "column, that is, ci = P3\nj=1 nij. Similarly , the value\nrj is the row sum, that is, rj = P5\ni=1 nij. Using these definitions, we can\ncompactly express the distribution of X and Y .\nThe probability distribution of each random variable, the marginal\nprobability , can be seen as the sum over a row or column\nP(X = xi) = ci\nN =\nP3\nj=1 nij\nN (6.10)\nand\nP(Y = yj) = rj\nN =\nP5\ni=1 nij\nN , (6.11)\nwhere ci and rj are the ith column and jth row of the probability table,\nrespectively . By convention, for discrete random variables with a finite\nnumber of events, we assume that probabilties sum up to one, that is,\n5X\ni=1\nP(X = xi) = 1 and\n3X\nj=1\nP(Y = yj) = 1 . (6.12)\nThe conditional probability is the fraction of a row or column in a par-\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Ca"
  },
  {
    "vector_id": 686,
    "chunk_id": "p185_c3",
    "page_number": 185,
    "text": "hat is,\n5X\ni=1\nP(X = xi) = 1 and\n3X\nj=1\nP(Y = yj) = 1 . (6.12)\nThe conditional probability is the fraction of a row or column in a par-\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 687,
    "chunk_id": "p186_c0",
    "page_number": 186,
    "text": "180 Probability and Distributions\nticular cell. For example, the conditional probability of Y given X is\nP(Y = yj |X = xi) = nij\nci\n, (6.13)\nand the conditional probability of X given Y is\nP(X = xi |Y = yj) = nij\nrj\n. (6.14)\nIn machine learning, we use discrete probability distributions to model\ncategorical variables, i.e., variables that take a finite set of unordered val-categorical variable\nues. They could be categorical features, such as the degree taken at uni-\nversity when used for predicting the salary of a person, or categorical la-\nbels, such as letters of the alphabet when doing handwriting recognition.\nDiscrete distributions are also often used to construct probabilistic models\nthat combine a finite number of continuous distributions (Chapter 11).\n6.2.2 Continuous Probabilities"
  },
  {
    "vector_id": 688,
    "chunk_id": "p186_c1",
    "page_number": 186,
    "text": "writing recognition.\nDiscrete distributions are also often used to construct probabilistic models\nthat combine a finite number of continuous distributions (Chapter 11).\n6.2.2 Continuous Probabilities\nWe consider real-valued random variables in this section, i.e., we consider\ntarget spaces that are intervals of the real lineR. In this book, we pretend\nthat we can perform operations on real random variables as if we have dis-\ncrete probability spaces with finite states. However, this simplification is\nnot precise for two situations: when we repeat something infinitely often,\nand when we want to draw a point from an interval. The first situation\narises when we discuss generalization errors in machine learning (Chap-\nter 8). The second situation arises when we want to discuss continuous\ndistri"
  },
  {
    "vector_id": 689,
    "chunk_id": "p186_c2",
    "page_number": 186,
    "text": "draw a point from an interval. The first situation\narises when we discuss generalization errors in machine learning (Chap-\nter 8). The second situation arises when we want to discuss continuous\ndistributions, such as the Gaussian (Section 6.5). For our purposes, the\nlack of precision allows for a briefer introduction to probability .\nRemark. In continuous spaces, there are two additional technicalities,\nwhich are counterintuitive. First, the set of all subsets (used to define\nthe event space A in Section 6.1) is not well behaved enough. A needs\nto be restricted to behave well under set complements, set intersections,\nand set unions. Second, the size of a set (which in discrete spaces can be\nobtained by counting the elements) turns out to be tricky . The size of a\nset is called its measure."
  },
  {
    "vector_id": 690,
    "chunk_id": "p186_c3",
    "page_number": 186,
    "text": "ents, set intersections,\nand set unions. Second, the size of a set (which in discrete spaces can be\nobtained by counting the elements) turns out to be tricky . The size of a\nset is called its measure. For example, the cardinality of discrete sets, themeasure\nlength of an interval in R, and the volume of a region in Rd are all mea-\nsures. Sets that behave well under set operations and additionally have\na topology are called a Borel \u03c3-algebra. Betancourt details a careful con-Borel \u03c3-algebra\nstruction of probability spaces from set theory without being bogged down\nin technicalities; see https://tinyurl.com/yb3t6mfd. For a more pre-\ncise construction, we refer to Billingsley (1995) and Jacod and Protter\n(2004).\nIn this book, we consider real-valued random variables with their cor-\nDraft (2024"
  },
  {
    "vector_id": 691,
    "chunk_id": "p186_c4",
    "page_number": 186,
    "text": "tinyurl.com/yb3t6mfd. For a more pre-\ncise construction, we refer to Billingsley (1995) and Jacod and Protter\n(2004).\nIn this book, we consider real-valued random variables with their cor-\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 692,
    "chunk_id": "p187_c0",
    "page_number": 187,
    "text": "6.2 Discrete and Continuous Probabilities 181\nresponding Borel \u03c3-algebra. We consider random variables with values in\nRD to be a vector of real-valued random variables. \u2662\nDefinition 6.1 (Probability Density Function). A function f : RD \u2192 R is\ncalled a probability density function (pdf) if probability density\nfunction\npdf1. \u2200x \u2208 RD : f(x) \u2a7e 0\n2. Its integral exists and\nZ\nRD\nf(x)dx = 1 . (6.15)\nFor probability mass functions (pmf) of discrete random variables, the\nintegral in (6.15) is replaced with a sum (6.12).\nObserve that the probability density function is any function f that is\nnon-negative and integrates to one. We associate a random variable X\nwith this function f by\nP(a \u2a7d X \u2a7d b) =\nZ b\na\nf(x)dx , (6.16)\nwhere a, b\u2208 R and x \u2208 R are outcomes of the continuous random vari-\nable X. State"
  },
  {
    "vector_id": 693,
    "chunk_id": "p187_c1",
    "page_number": 187,
    "text": "e and integrates to one. We associate a random variable X\nwith this function f by\nP(a \u2a7d X \u2a7d b) =\nZ b\na\nf(x)dx , (6.16)\nwhere a, b\u2208 R and x \u2208 R are outcomes of the continuous random vari-\nable X. States x \u2208 RD are defined analogously by considering a vector\nof x \u2208 R. This association (6.16) is called the law or distribution of the law\nrandom variable X. P(X = x) is a set of\nmeasure zero.Remark. In contrast to discrete random variables, the probability of a con-\ntinuous random variable X taking a particular value P(X = x) is zero.\nThis is like trying to specify an interval in (6.16) where a = b. \u2662\nDefinition 6.2 (Cumulative Distribution Function). A cumulative distribu- cumulative\ndistribution functiontion function (cdf) of a multivariate real-valued random variable X with\nstates x \u2208 RD is g"
  },
  {
    "vector_id": 694,
    "chunk_id": "p187_c2",
    "page_number": 187,
    "text": "\u2662\nDefinition 6.2 (Cumulative Distribution Function). A cumulative distribu- cumulative\ndistribution functiontion function (cdf) of a multivariate real-valued random variable X with\nstates x \u2208 RD is given by\nFX(x) = P(X1 \u2a7d x1, . . . , XD \u2a7d xD) , (6.17)\nwhere X = [X1, . . . , XD]\u22a4, x = [x1, . . . , xD]\u22a4, and the right-hand side\nrepresents the probability that random variableXi takes the value smaller\nthan or equal to xi.\nThere are cdfs,\nwhich do not have\ncorresponding pdfs.\nThe cdf can be expressed also as the integral of the probability density\nfunction f(x) so that\nFX(x) =\nZ x1\n\u2212\u221e\n\u00b7 \u00b7\u00b7\nZ xD\n\u2212\u221e\nf(z1, . . . , zD)dz1 \u00b7 \u00b7\u00b7dzD . (6.18)\nRemark. We reiterate that there are in fact two distinct concepts when\ntalking about distributions. First is the idea of a pdf (denoted by f(x)),\nwhich is a non"
  },
  {
    "vector_id": 695,
    "chunk_id": "p187_c3",
    "page_number": 187,
    "text": "\u221e\nf(z1, . . . , zD)dz1 \u00b7 \u00b7\u00b7dzD . (6.18)\nRemark. We reiterate that there are in fact two distinct concepts when\ntalking about distributions. First is the idea of a pdf (denoted by f(x)),\nwhich is a nonnegative function that sums to one. Second is the law of a\nrandom variable X, that is, the association of a random variable X with\nthe pdf f(x). \u2662\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 696,
    "chunk_id": "p188_c0",
    "page_number": 188,
    "text": "182 Probability and Distributions\nFigure 6.3\nExamples of\n(a) discrete and\n(b) continuous\nuniform\ndistributions. See\nExample 6.3 for\ndetails of the\ndistributions.\n\u22121 0 1 2\nz\n0.0\n0.5\n1.0\n1.5\n2.0\nP(Z = z)\n(a) Discrete distribution\n\u22121 0 1 2\nx\n0.0\n0.5\n1.0\n1.5\n2.0\np(x) (b) Continuous distribution\nFor most of this book, we will not use the notation f(x) and FX(x) as\nwe mostly do not need to distinguish between the pdf and cdf. However,\nwe will need to be careful about pdfs and cdfs in Section 6.7.\n6.2.3 Contrasting Discrete and Continuous Distributions\nRecall from Section 6.1.2 that probabilities are positive and the total prob-\nability sums up to one. For discrete random variables (see (6.12)), this\nimplies that the probability of each state must lie in the interval [0, 1].\nHowever, for continuo"
  },
  {
    "vector_id": 697,
    "chunk_id": "p188_c1",
    "page_number": 188,
    "text": "positive and the total prob-\nability sums up to one. For discrete random variables (see (6.12)), this\nimplies that the probability of each state must lie in the interval [0, 1].\nHowever, for continuous random variables the normalization (see (6.15))\ndoes not imply that the value of the density is less than or equal to 1 for\nall values. We illustrate this in Figure 6.3 using the uniform distributionuniform distribution\nfor both discrete and continuous random variables.\nExample 6.3\nWe consider two examples of the uniform distribution, where each state is\nequally likely to occur. This example illustrates some differences between\ndiscrete and continuous probability distributions.\nLet Z be a discrete uniform random variable with three states {z =\n\u22121.1, z= 0.3, z= 1.5}. The probability mass fun"
  },
  {
    "vector_id": 698,
    "chunk_id": "p188_c2",
    "page_number": 188,
    "text": "lustrates some differences between\ndiscrete and continuous probability distributions.\nLet Z be a discrete uniform random variable with three states {z =\n\u22121.1, z= 0.3, z= 1.5}. The probability mass function can be representedThe actual values of\nthese states are not\nmeaningful here,\nand we deliberately\nchose numbers to\ndrive home the\npoint that we do not\nwant to use (and\nshould ignore) the\nordering of the\nstates.\nas a table of probability values:\nz\nP(Z=z)\n\u22121.1\n13\n0.3\n13\n1.5\n13\nAlternatively , we can think of this as a graph (Figure 6.3(a)), where we\nuse the fact that the states can be located on the x-axis, and the y-axis\nrepresents the probability of a particular state. The y-axis in Figure 6.3(a)\nis deliberately extended so that is it the same as in Figure 6.3(b).\nLet X be a continuous ra"
  },
  {
    "vector_id": 699,
    "chunk_id": "p188_c3",
    "page_number": 188,
    "text": "n the x-axis, and the y-axis\nrepresents the probability of a particular state. The y-axis in Figure 6.3(a)\nis deliberately extended so that is it the same as in Figure 6.3(b).\nLet X be a continuous random variable taking values in the range0.9 \u2a7d\nX \u2a7d 1.6, as represented by Figure 6.3(b). Observe that the height of the\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 700,
    "chunk_id": "p189_c0",
    "page_number": 189,
    "text": "6.3 Sum Rule, Product Rule, and Bayes\u2019 Theorem 183\nTable 6.1\nNomenclature for\nprobability\ndistributions.\nType \u201cPoint probability\u201d \u201cInterval probability\u201d\nDiscrete P(X = x) Not applicable\nProbability mass function\nContinuous p(x) P(X \u2a7d x)\nProbability density function Cumulative distribution function\ndensity can be greater than 1. However, it needs to hold that\nZ 1.6\n0.9\np(x)dx = 1 . (6.19)\nRemark. There is an additional subtlety with regards to discrete prob-\nability distributions. The states z1, . . . , zd do not in principle have any\nstructure, i.e., there is usually no way to compare them, for example\nz1 = red , z2 = green , z3 = blue . However, in many machine learning\napplications discrete states take numerical values, e.g., z1 = \u22121.1, z2 =\n0.3, z3 = 1.5, where we could say z1 < z2 < z3"
  },
  {
    "vector_id": 701,
    "chunk_id": "p189_c1",
    "page_number": 189,
    "text": "r example\nz1 = red , z2 = green , z3 = blue . However, in many machine learning\napplications discrete states take numerical values, e.g., z1 = \u22121.1, z2 =\n0.3, z3 = 1.5, where we could say z1 < z2 < z3. Discrete states that as-\nsume numerical values are particularly useful because we often consider\nexpected values (Section 6.4.1) of random variables. \u2662\nUnfortunately , machine learning literature uses notation and nomen-\nclature that hides the distinction between the sample space \u2126, the target\nspace T , and the random variable X. For a value x of the set of possible\noutcomes of the random variable X, i.e., x \u2208 T, p(x) denotes the prob- We think of the\noutcome x as the\nargument that\nresults in the\nprobability p(x).\nability that random variable X has the outcome x. For discrete random\nvariable"
  },
  {
    "vector_id": 702,
    "chunk_id": "p189_c2",
    "page_number": 189,
    "text": "le X, i.e., x \u2208 T, p(x) denotes the prob- We think of the\noutcome x as the\nargument that\nresults in the\nprobability p(x).\nability that random variable X has the outcome x. For discrete random\nvariables, this is written as P(X = x), which is known as the probabil-\nity mass function. The pmf is often referred to as the \u201cdistribution\u201d. For\ncontinuous variables, p(x) is called the probability density function (often\nreferred to as a density). To muddy things even further, the cumulative\ndistribution function P(X \u2a7d x) is often also referred to as the \u201cdistribu-\ntion\u201d. In this chapter, we will use the notationX to refer to both univariate\nand multivariate random variables, and denote the states by x and x re-\nspectively . We summarize the nomenclature in Table 6.1.\nRemark. We will be using the e"
  },
  {
    "vector_id": 703,
    "chunk_id": "p189_c3",
    "page_number": 189,
    "text": "he notationX to refer to both univariate\nand multivariate random variables, and denote the states by x and x re-\nspectively . We summarize the nomenclature in Table 6.1.\nRemark. We will be using the expression \u201cprobability distribution\u201d not\nonly for discrete probability mass functions but also for continuous proba-\nbility density functions, although this is technically incorrect. In line with\nmost machine learning literature, we also rely on context to distinguish\nthe different uses of the phrase probability distribution. \u2662\n6.3 Sum Rule, Product Rule, and Bayes\u2019 Theorem\nWe think of probability theory as an extension to logical reasoning. As we\ndiscussed in Section 6.1.1, the rules of probability presented here follow\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge U"
  },
  {
    "vector_id": 704,
    "chunk_id": "p189_c4",
    "page_number": 189,
    "text": "theory as an extension to logical reasoning. As we\ndiscussed in Section 6.1.1, the rules of probability presented here follow\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 705,
    "chunk_id": "p190_c0",
    "page_number": 190,
    "text": "184 Probability and Distributions\nnaturally from fulfilling the desiderata (Jaynes, 2003, chapter 2). Prob-\nabilistic modeling (Section 8.4) provides a principled foundation for de-\nsigning machine learning methods. Once we have defined probability dis-\ntributions (Section 6.2) corresponding to the uncertainties of the data and\nour problem, it turns out that there are only two fundamental rules, the\nsum rule and the product rule.\nRecall from (6.9) that p(x, y) is the joint distribution of the two ran-\ndom variables x, y. The distributions p(x) and p(y) are the correspond-\ning marginal distributions, and p(y |x) is the conditional distribution ofy\ngiven x. Given the definitions of the marginal and conditional probability\nfor discrete and continuous random variables in Section 6.2, we can no"
  },
  {
    "vector_id": 706,
    "chunk_id": "p190_c1",
    "page_number": 190,
    "text": "tions, and p(y |x) is the conditional distribution ofy\ngiven x. Given the definitions of the marginal and conditional probability\nfor discrete and continuous random variables in Section 6.2, we can now\npresent the two fundamental rules in probability theory .These two rules\narise\nnaturally (Jaynes,\n2003) from the\nrequirements we\ndiscussed in\nSection 6.1.1.\nThe first rule, the sum rule, states that\nsum rule\np(x) =\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\nX\ny\u2208Y\np(x, y) if y is discrete\nZ\nY\np(x, y)dy if y is continuous\n, (6.20)\nwhere Y are the states of the target space of random variable Y . This\nmeans that we sum out (or integrate out) the set of statesy of the random\nvariable Y . The sum rule is also known as the marginalization property.marginalization\nproperty The sum rule relates the joint distribution to a marginal"
  },
  {
    "vector_id": 707,
    "chunk_id": "p190_c2",
    "page_number": 190,
    "text": "egrate out) the set of statesy of the random\nvariable Y . The sum rule is also known as the marginalization property.marginalization\nproperty The sum rule relates the joint distribution to a marginal distribution. In\ngeneral, when the joint distribution contains more than two random vari-\nables, the sum rule can be applied to any subset of the random variables,\nresulting in a marginal distribution of potentially more than one random\nvariable. More concretely , ifx = [x1, . . . , xD]\u22a4, we obtain the marginal\np(xi) =\nZ\np(x1, . . . , xD)dx\\i (6.21)\nby repeated application of the sum rule where we integrate/sum out all\nrandom variables except xi, which is indicated by \\i, which reads \u201call\nexcept i.\u201d\nRemark. Many of the computational challenges of probabilistic modeling\nare due to the applicati"
  },
  {
    "vector_id": 708,
    "chunk_id": "p190_c3",
    "page_number": 190,
    "text": "integrate/sum out all\nrandom variables except xi, which is indicated by \\i, which reads \u201call\nexcept i.\u201d\nRemark. Many of the computational challenges of probabilistic modeling\nare due to the application of the sum rule. When there are many variables\nor discrete variables with many states, the sum rule boils down to per-\nforming a high-dimensional sum or integral. Performing high-dimensional\nsums or integrals is generally computationally hard, in the sense that there\nis no known polynomial-time algorithm to calculate them exactly . \u2662\nThe second rule, known as theproduct rule, relates the joint distributionproduct rule\nto the conditional distribution via\np(x, y) = p(y |x)p(x) . (6.22)\nThe product rule can be interpreted as the fact that every joint distribu-\ntion of two random variables can"
  },
  {
    "vector_id": 709,
    "chunk_id": "p190_c4",
    "page_number": 190,
    "text": "distributionproduct rule\nto the conditional distribution via\np(x, y) = p(y |x)p(x) . (6.22)\nThe product rule can be interpreted as the fact that every joint distribu-\ntion of two random variables can be factorized (written as a product)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 710,
    "chunk_id": "p191_c0",
    "page_number": 191,
    "text": "6.3 Sum Rule, Product Rule, and Bayes\u2019 Theorem 185\nof two other distributions. The two factors are the marginal distribu-\ntion of the first random variable p(x), and the conditional distribution\nof the second random variable given the first p(y |x). Since the ordering\nof random variables is arbitrary in p(x, y), the product rule also implies\np(x, y) = p(x|y)p(y). To be precise, (6.22) is expressed in terms of the\nprobability mass functions for discrete random variables. For continuous\nrandom variables, the product rule is expressed in terms of the probability\ndensity functions (Section 6.2.3).\nIn machine learning and Bayesian statistics, we are often interested in\nmaking inferences of unobserved (latent) random variables given that we\nhave observed other random variables. Let us assume we"
  },
  {
    "vector_id": 711,
    "chunk_id": "p191_c1",
    "page_number": 191,
    "text": "In machine learning and Bayesian statistics, we are often interested in\nmaking inferences of unobserved (latent) random variables given that we\nhave observed other random variables. Let us assume we have some prior\nknowledge p(x) about an unobserved random variable x and some rela-\ntionship p(y |x) between x and a second random variable y, which we\ncan observe. If we observe y, we can use Bayes\u2019 theorem to draw some\nconclusions about x given the observed values of y. Bayes\u2019 theorem (also Bayes\u2019 theorem\nBayes\u2019 rule or Bayes\u2019 law) Bayes\u2019 rule\nBayes\u2019 law\np(x|y)| {z }\nposterior\n=\nlikelihood\nz }| {\np(y |x)\nprior\nz}|{\np(x)\np(y)|{z}\nevidence\n(6.23)\nis a direct consequence of the product rule in (6.22) since\np(x, y) = p(x|y)p(y) (6.24)\nand\np(x, y) = p(y |x)p(x) (6.25)\nso that\np(x|y)p(y) = p(y |x)"
  },
  {
    "vector_id": 712,
    "chunk_id": "p191_c2",
    "page_number": 191,
    "text": "{\np(y |x)\nprior\nz}|{\np(x)\np(y)|{z}\nevidence\n(6.23)\nis a direct consequence of the product rule in (6.22) since\np(x, y) = p(x|y)p(y) (6.24)\nand\np(x, y) = p(y |x)p(x) (6.25)\nso that\np(x|y)p(y) = p(y |x)p(x) \u21d0 \u21d2p(x|y) = p(y |x)p(x)\np(y) . (6.26)\nIn (6.23), p(x) is the prior, which encapsulates our subjective prior prior\nknowledge of the unobserved (latent) variable x before observing any\ndata. We can choose any prior that makes sense to us, but it is critical to\nensure that the prior has a nonzero pdf (or pmf) on all plausible x, even\nif they are very rare.\nThe likelihood p(y |x) describes how x and y are related, and in the likelihood\nThe likelihood is\nsometimes also\ncalled the\n\u201cmeasurement\nmodel\u201d.\ncase of discrete probability distributions, it is the probability of the data y\nif we were to"
  },
  {
    "vector_id": 713,
    "chunk_id": "p191_c3",
    "page_number": 191,
    "text": "and y are related, and in the likelihood\nThe likelihood is\nsometimes also\ncalled the\n\u201cmeasurement\nmodel\u201d.\ncase of discrete probability distributions, it is the probability of the data y\nif we were to know the latent variable x. Note that the likelihood is not a\ndistribution in x, but only in y. We call p(y |x) either the \u201clikelihood of\nx (given y)\u201d or the \u201cprobability of y given x\u201d but never the likelihood of\ny (MacKay, 2003).\nThe posterior p(x|y) is the quantity of interest in Bayesian statistics posterior\nbecause it expresses exactly what we are interested in, i.e., what we know\nabout x after having observed y.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 714,
    "chunk_id": "p191_c4",
    "page_number": 191,
    "text": "r having observed y.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 715,
    "chunk_id": "p192_c0",
    "page_number": 192,
    "text": "186 Probability and Distributions\nThe quantity\np(y) :=\nZ\np(y |x)p(x)dx = EX[p(y |x)] (6.27)\nis the marginal likelihood/evidence. The right-hand side of (6.27) uses themarginal likelihood\nevidence expectation operator which we define in Section 6.4.1. By definition, the\nmarginal likelihood integrates the numerator of (6.23) with respect to the\nlatent variable x. Therefore, the marginal likelihood is independent of\nx, and it ensures that the posterior p(x|y) is normalized. The marginal\nlikelihood can also be interpreted as the expected likelihood where we\ntake the expectation with respect to the prior p(x). Beyond normalization\nof the posterior, the marginal likelihood also plays an important role in\nBayesian model selection, as we will discuss in Section 8.6. Due to the\nintegration in (8.44"
  },
  {
    "vector_id": 716,
    "chunk_id": "p192_c1",
    "page_number": 192,
    "text": "prior p(x). Beyond normalization\nof the posterior, the marginal likelihood also plays an important role in\nBayesian model selection, as we will discuss in Section 8.6. Due to the\nintegration in (8.44), the evidence is often hard to compute.Bayes\u2019 theorem is\nalso called the\n\u201cprobabilistic\ninverse.\u201d\nBayes\u2019 theorem (6.23) allows us to invert the relationship between x\nand y given by the likelihood. Therefore, Bayes\u2019 theorem is sometimes\ncalled the probabilistic inverse. We will discuss Bayes\u2019 theorem further inprobabilistic inverse\nSection 8.4.\nRemark. In Bayesian statistics, the posterior distribution is the quantity\nof interest as it encapsulates all available information from the prior and\nthe data. Instead of carrying the posterior around, it is possible to focus\non some statistic of the"
  },
  {
    "vector_id": 717,
    "chunk_id": "p192_c2",
    "page_number": 192,
    "text": "ibution is the quantity\nof interest as it encapsulates all available information from the prior and\nthe data. Instead of carrying the posterior around, it is possible to focus\non some statistic of the posterior, such as the maximum of the posterior,\nwhich we will discuss in Section 8.3. However, focusing on some statistic\nof the posterior leads to loss of information. If we think in a bigger con-\ntext, then the posterior can be used within a decision-making system, and\nhaving the full posterior can be extremely useful and lead to decisions that\nare robust to disturbances. For example, in the context of model-based re-\ninforcement learning, Deisenroth et al. (2015) show that using the full\nposterior distribution of plausible transition functions leads to very fast\n(data/sample efficient) le"
  },
  {
    "vector_id": 718,
    "chunk_id": "p192_c3",
    "page_number": 192,
    "text": "ontext of model-based re-\ninforcement learning, Deisenroth et al. (2015) show that using the full\nposterior distribution of plausible transition functions leads to very fast\n(data/sample efficient) learning, whereas focusing on the maximum of\nthe posterior leads to consistent failures. Therefore, having the full pos-\nterior can be very useful for a downstream task. In Chapter 9, we will\ncontinue this discussion in the context of linear regression. \u2662\n6.4 Summary Statistics and Independence\nWe are often interested in summarizing sets of random variables and com-\nparing pairs of random variables. A statistic of a random variable is a de-\nterministic function of that random variable. The summary statistics of a\ndistribution provide one useful view of how a random variable behaves,\nand as the n"
  },
  {
    "vector_id": 719,
    "chunk_id": "p192_c4",
    "page_number": 192,
    "text": "A statistic of a random variable is a de-\nterministic function of that random variable. The summary statistics of a\ndistribution provide one useful view of how a random variable behaves,\nand as the name suggests, provide numbers that summarize and charac-\nterize the distribution. We describe the mean and the variance, two well-\nknown summary statistics. Then we discuss two ways to compare a pair\nof random variables: first, how to say that two random variables are inde-\npendent; and second, how to compute an inner product between them.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 720,
    "chunk_id": "p192_c5",
    "page_number": 192,
    "text": "Feedback:https://mml-book.com."
  },
  {
    "vector_id": 721,
    "chunk_id": "p193_c0",
    "page_number": 193,
    "text": "6.4 Summary Statistics and Independence 187\n6.4.1 Means and Covariances\nMean and (co)variance are often useful to describe properties of probabil-\nity distributions (expected values and spread). We will see in Section 6.6\nthat there is a useful family of distributions (called the exponential fam-\nily), where the statistics of the random variable capture all possible infor-\nmation.\nThe concept of the expected value is central to machine learning, and\nthe foundational concepts of probability itself can be derived from the\nexpected value (Whittle, 2000).\nDefinition 6.3 (Expected Value). The expected value of a function g : R \u2192 expected value\nR of a univariate continuous random variable X \u223c p(x) is given by\nEX[g(x)] =\nZ\nX\ng(x)p(x)dx . (6.28)\nCorrespondingly , the expected value of a functiong"
  },
  {
    "vector_id": 722,
    "chunk_id": "p193_c1",
    "page_number": 193,
    "text": "ted value of a function g : R \u2192 expected value\nR of a univariate continuous random variable X \u223c p(x) is given by\nEX[g(x)] =\nZ\nX\ng(x)p(x)dx . (6.28)\nCorrespondingly , the expected value of a functiong of a discrete random\nvariable X \u223c p(x) is given by\nEX[g(x)] =\nX\nx\u2208X\ng(x)p(x) , (6.29)\nwhere X is the set of possible outcomes (the target space) of the random\nvariable X.\nIn this section, we consider discrete random variables to have numerical\noutcomes. This can be seen by observing that the function g takes real\nnumbers as inputs. The expected value\nof a function of a\nrandom variable is\nsometimes referred\nto as the law of the\nunconscious\nstatistician (Casella\nand Berger, 2002,\nSection 2.2).\nRemark. We consider multivariate random variables X as a finite vector\nof univariate random variables ["
  },
  {
    "vector_id": 723,
    "chunk_id": "p193_c2",
    "page_number": 193,
    "text": "referred\nto as the law of the\nunconscious\nstatistician (Casella\nand Berger, 2002,\nSection 2.2).\nRemark. We consider multivariate random variables X as a finite vector\nof univariate random variables [X1, . . . , XD]\u22a4. For multivariate random\nvariables, we define the expected value element wise\nEX[g(x)] =\n\uf8ee\n\uf8ef\uf8f0\nEX1 [g(x1)]\n...\nEXD [g(xD)]\n\uf8f9\n\uf8fa\uf8fb \u2208 RD , (6.30)\nwhere the subscript EXd indicates that we are taking the expected value\nwith respect to the dth element of the vector x. \u2662\nDefinition 6.3 defines the meaning of the notation EX as the operator\nindicating that we should take the integral with respect to the probabil-\nity density (for continuous distributions) or the sum over all states (for\ndiscrete distributions). The definition of the mean (Definition 6.4), is a\nspecial case of the expec"
  },
  {
    "vector_id": 724,
    "chunk_id": "p193_c3",
    "page_number": 193,
    "text": "respect to the probabil-\nity density (for continuous distributions) or the sum over all states (for\ndiscrete distributions). The definition of the mean (Definition 6.4), is a\nspecial case of the expected value, obtained by choosing g to be the iden-\ntity function.\nDefinition 6.4 (Mean). The mean of a random variable X with states mean\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 725,
    "chunk_id": "p194_c0",
    "page_number": 194,
    "text": "188 Probability and Distributions\nx \u2208 RD is an average and is defined as\nEX[x] =\n\uf8ee\n\uf8ef\uf8f0\nEX1 [x1]\n...\nEXD [xD]\n\uf8f9\n\uf8fa\uf8fb \u2208 RD , (6.31)\nwhere\nEXd[xd] :=\n\uf8f1\n\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\nZ\nX\nxdp(xd)dxd if X is a continuous random variable\nX\nxi\u2208X\nxip(xd = xi) if X is a discrete random variable\n(6.32)\nfor d = 1 , . . . , D, where the subscript d indicates the corresponding di-\nmension of x. The integral and sum are over the states X of the target\nspace of the random variable X.\nIn one dimension, there are two other intuitive notions of \u201caverage\u201d,\nwhich are the median and the mode. The median is the \u201cmiddle\u201d value ifmedian\nwe sort the values, i.e.,50% of the values are greater than the median and\n50% are smaller than the median. This idea can be generalized to contin-\nuous values by considering the value where the cdf (Defin"
  },
  {
    "vector_id": 726,
    "chunk_id": "p194_c1",
    "page_number": 194,
    "text": "rt the values, i.e.,50% of the values are greater than the median and\n50% are smaller than the median. This idea can be generalized to contin-\nuous values by considering the value where the cdf (Definition 6.2) is0.5.\nFor distributions, which are asymmetric or have long tails, the median\nprovides an estimate of a typical value that is closer to human intuition\nthan the mean value. Furthermore, the median is more robust to outliers\nthan the mean. The generalization of the median to higher dimensions is\nnon-trivial as there is no obvious way to \u201csort\u201d in more than one dimen-\nsion (Hallin et al., 2010; Kong and Mizera, 2012). The mode is the mostmode\nfrequently occurring value. For a discrete random variable, the mode is\ndefined as the value of x having the highest frequency of occurrence. Fo"
  },
  {
    "vector_id": 727,
    "chunk_id": "p194_c2",
    "page_number": 194,
    "text": "2010; Kong and Mizera, 2012). The mode is the mostmode\nfrequently occurring value. For a discrete random variable, the mode is\ndefined as the value of x having the highest frequency of occurrence. For\na continuous random variable, the mode is defined as a peak in the density\np(x). A particular density p(x) may have more than one mode, and fur-\nthermore there may be a very large number of modes in high-dimensional\ndistributions. Therefore, finding all the modes of a distribution can be\ncomputationally challenging.\nExample 6.4\nConsider the two-dimensional distribution illustrated in Figure 6.4:\np(x) = 0.4 N\n\u0012\nx\n\f\f\f\f\n\u001410\n2\n\u0015\n,\n\u00141 0\n0 1\n\u0015\u0013\n+ 0.6 N\n\u0012\nx\n\f\f\f\f\n\u00140\n0\n\u0015\n,\n\u00148.4 2 .0\n2.0 1 .7\n\u0015\u0013\n.\n(6.33)\nWe will define the Gaussian distribution N\n\u0000\n\u00b5, \u03c32\u0001\nin Section 6.5. Also\nshown is its correspondin"
  },
  {
    "vector_id": 728,
    "chunk_id": "p194_c3",
    "page_number": 194,
    "text": "p(x) = 0.4 N\n\u0012\nx\n\f\f\f\f\n\u001410\n2\n\u0015\n,\n\u00141 0\n0 1\n\u0015\u0013\n+ 0.6 N\n\u0012\nx\n\f\f\f\f\n\u00140\n0\n\u0015\n,\n\u00148.4 2 .0\n2.0 1 .7\n\u0015\u0013\n.\n(6.33)\nWe will define the Gaussian distribution N\n\u0000\n\u00b5, \u03c32\u0001\nin Section 6.5. Also\nshown is its corresponding marginal distribution in each dimension. Ob-\nserve that the distribution is bimodal (has two modes), but one of the\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 729,
    "chunk_id": "p195_c0",
    "page_number": 195,
    "text": "6.4 Summary Statistics and Independence 189\nmarginal distributions is unimodal (has one mode). The horizontal bi-\nmodal univariate distribution illustrates that the mean and median can\nbe different from each other. While it is tempting to define the two-\ndimensional median to be the concatenation of the medians in each di-\nmension, the fact that we cannot define an ordering of two-dimensional\npoints makes it difficult. When we say \u201ccannot define an ordering\u201d, we\nmean that there is more than one way to define the relation < so that\u00143\n0\n\u0015\n<\n\u00142\n3\n\u0015\n.\nFigure 6.4\nIllustration of the\nmean, mode, and\nmedian for a\ntwo-dimensional\ndataset, as well as\nits marginal\ndensities.\nMean\nModes\nMedian\nRemark. The expected value (Definition 6.3) is a linear operator. For ex-\nample, given a real-valued functio"
  },
  {
    "vector_id": 730,
    "chunk_id": "p195_c1",
    "page_number": 195,
    "text": "median for a\ntwo-dimensional\ndataset, as well as\nits marginal\ndensities.\nMean\nModes\nMedian\nRemark. The expected value (Definition 6.3) is a linear operator. For ex-\nample, given a real-valued functionf(x) = ag(x)+ bh(x) where a, b\u2208 R\nand x \u2208 RD, we obtain\nEX[f(x)] =\nZ\nf(x)p(x)dx (6.34a)\n=\nZ\n[ag(x) + bh(x)]p(x)dx (6.34b)\n= a\nZ\ng(x)p(x)dx + b\nZ\nh(x)p(x)dx (6.34c)\n= aEX[g(x)] + bEX[h(x)] . (6.34d)\n\u2662\nFor two random variables, we may wish to characterize their correspon-\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 731,
    "chunk_id": "p196_c0",
    "page_number": 196,
    "text": "190 Probability and Distributions\ndence to each other. The covariance intuitively represents the notion of\nhow dependent random variables are to one another.\nDefinition 6.5 (Covariance (Univariate)) . The covariance between twocovariance\nunivariate random variables X, Y \u2208 R is given by the expected product\nof their deviations from their respective means, i.e.,\nCovX,Y [x, y] := EX,Y\n\u0002\n(x \u2212 EX[x])(y \u2212 EY [y])\n\u0003\n. (6.35)\nTerminology: The\ncovariance of\nmultivariate random\nvariables Cov[x, y]\nis sometimes\nreferred to as\ncross-covariance,\nwith covariance\nreferring to\nCov[x, x].\nRemark. When the random variable associated with the expectation or\ncovariance is clear by its arguments, the subscript is often suppressed (for\nexample, EX[x] is often written as E[x]). \u2662\nBy using the linearity of expect"
  },
  {
    "vector_id": 732,
    "chunk_id": "p196_c1",
    "page_number": 196,
    "text": "om variable associated with the expectation or\ncovariance is clear by its arguments, the subscript is often suppressed (for\nexample, EX[x] is often written as E[x]). \u2662\nBy using the linearity of expectations, the expression in Definition 6.5\ncan be rewritten as the expected value of the product minus the product\nof the expected values, i.e.,\nCov[x, y] = E[xy] \u2212 E[x]E[y] . (6.36)\nThe covariance of a variable with itselfCov[x, x] is called the variance andvariance\nis denoted by VX[x]. The square root of the variance is called thestandardstandard deviation\ndeviation and is often denoted by \u03c3(x). The notion of covariance can be\ngeneralized to multivariate random variables.\nDefinition 6.6 (Covariance (Multivariate)). If we consider two multivari-\nate random variables X and Y with states x \u2208 RD a"
  },
  {
    "vector_id": 733,
    "chunk_id": "p196_c2",
    "page_number": 196,
    "text": "he notion of covariance can be\ngeneralized to multivariate random variables.\nDefinition 6.6 (Covariance (Multivariate)). If we consider two multivari-\nate random variables X and Y with states x \u2208 RD and y \u2208 RE respec-\ntively , thecovariance between X and Y is defined ascovariance\nCov[x, y] = E[xy\u22a4] \u2212 E[x]E[y]\u22a4 = Cov[y, x]\u22a4 \u2208 RD\u00d7E . (6.37)\nDefinition 6.6 can be applied with the same multivariate random vari-\nable in both arguments, which results in a useful concept that intuitively\ncaptures the \u201cspread\u201d of a random variable. For a multivariate random\nvariable, the variance describes the relation between individual dimen-\nsions of the random variable.\nDefinition 6.7 (Variance). The variance of a random variable X withvariance\nstates x \u2208 RD and a mean vector \u00b5 \u2208 RD is defined as\nVX[x] = CovX["
  },
  {
    "vector_id": 734,
    "chunk_id": "p196_c3",
    "page_number": 196,
    "text": "n between individual dimen-\nsions of the random variable.\nDefinition 6.7 (Variance). The variance of a random variable X withvariance\nstates x \u2208 RD and a mean vector \u00b5 \u2208 RD is defined as\nVX[x] = CovX[x, x] (6.38a)\n= EX[(x \u2212 \u00b5)(x \u2212 \u00b5)\u22a4] = EX[xx\u22a4] \u2212 EX[x]EX[x]\u22a4 (6.38b)\n=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nCov[x1, x1] Cov[ x1, x2] . . .Cov[x1, xD]\nCov[x2, x1] Cov[ x2, x2] . . .Cov[x2, xD]\n... ... ... ...\nCov[xD, x1] . . . . . . Cov[xD, xD]\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb . (6.38c)\nThe D \u00d7 D matrix in (6.38c) is called the covariance matrix of the mul-covariance matrix\ntivariate random variable X. The covariance matrix is symmetric and pos-\nitive semidefinite and tells us something about the spread of the data. On\nits diagonal, the covariance matrix contains the variances of themarginalsmarginal\nDraft (2024-01-15) of \u201cMathematics for Machine Lea"
  },
  {
    "vector_id": 735,
    "chunk_id": "p196_c4",
    "page_number": 196,
    "text": "midefinite and tells us something about the spread of the data. On\nits diagonal, the covariance matrix contains the variances of themarginalsmarginal\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 736,
    "chunk_id": "p197_c0",
    "page_number": 197,
    "text": "6.4 Summary Statistics and Independence 191\nFigure 6.5\nTwo-dimensional\ndatasets with\nidentical means and\nvariances along\neach axis (colored\nlines) but with\ndifferent\ncovariances.\n\u22125 0 5\nx\n\u22122\n0\n2\n4\n6\ny\n(a) x and y are negatively correlated.\n\u22125 0 5\nx\n\u22122\n0\n2\n4\n6\ny\n (b) x and y are positively correlated.\np(xi) =\nZ\np(x1, . . . , xD)dx\\i , (6.39)\nwhere \u201c\\i\u201d denotes \u201call variables but i\u201d. The off-diagonal entries are the\ncross-covariance terms Cov[xi, xj] for i, j= 1, . . . , D, i\u0338= j. cross-covariance\nRemark. In this book, we generally assume that covariance matrices are\npositive definite to enable better intuition. We therefore do not discuss\ncorner cases that result in positive semidefinite (low-rank) covariance ma-\ntrices. \u2662\nWhen we want to compare the covariances between different pairs of\nr"
  },
  {
    "vector_id": 737,
    "chunk_id": "p197_c1",
    "page_number": 197,
    "text": "better intuition. We therefore do not discuss\ncorner cases that result in positive semidefinite (low-rank) covariance ma-\ntrices. \u2662\nWhen we want to compare the covariances between different pairs of\nrandom variables, it turns out that the variance of each random variable\naffects the value of the covariance. The normalized version of covariance\nis called the correlation.\nDefinition 6.8 (Correlation). The correlation between two random vari- correlation\nables X, Yis given by\ncorr[x, y] = Cov[x, y]p\nV[x]V[y] \u2208 [\u22121, 1] . (6.40)\nThe correlation matrix is the covariance matrix of standardized random\nvariables, x/\u03c3(x). In other words, each random variable is divided by its\nstandard deviation (the square root of the variance) in the correlation\nmatrix.\nThe covariance (and correlation) indicate how"
  },
  {
    "vector_id": 738,
    "chunk_id": "p197_c2",
    "page_number": 197,
    "text": "variables, x/\u03c3(x). In other words, each random variable is divided by its\nstandard deviation (the square root of the variance) in the correlation\nmatrix.\nThe covariance (and correlation) indicate how two random variables\nare related; see Figure 6.5. Positive correlation corr[x, y] means that when\nx grows, then y is also expected to grow. Negative correlation means that\nas x increases, then y decreases.\n6.4.2 Empirical Means and Covariances\nThe definitions in Section 6.4.1 are often also called the population mean population mean\nand covarianceand covariance, as it refers to the true statistics for the population. In ma-\nchine learning, we need to learn from empirical observations of data. Con-\nsider a random variable X. There are two conceptual steps to go from\n\u00a92024 M. P. Deisenroth, A."
  },
  {
    "vector_id": 739,
    "chunk_id": "p197_c3",
    "page_number": 197,
    "text": "s for the population. In ma-\nchine learning, we need to learn from empirical observations of data. Con-\nsider a random variable X. There are two conceptual steps to go from\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 740,
    "chunk_id": "p198_c0",
    "page_number": 198,
    "text": "192 Probability and Distributions\npopulation statistics to the realization of empirical statistics. First, we use\nthe fact that we have a finite dataset (of size N) to construct an empirical\nstatistic that is a function of a finite number of identical random variables,\nX1, . . . , XN . Second, we observe the data, that is, we look at the realiza-\ntion x1, . . . , xN of each of the random variables and apply the empirical\nstatistic.\nSpecifically , for the mean (Definition 6.4), given a particular dataset we\ncan obtain an estimate of the mean, which is called the empirical mean orempirical mean\nsample mean. The same holds for the empirical covariance.sample mean\nDefinition 6.9 (Empirical Mean and Covariance). The empirical mean vec-empirical mean\ntor is the arithmetic average of the observat"
  },
  {
    "vector_id": 741,
    "chunk_id": "p198_c1",
    "page_number": 198,
    "text": "sample mean. The same holds for the empirical covariance.sample mean\nDefinition 6.9 (Empirical Mean and Covariance). The empirical mean vec-empirical mean\ntor is the arithmetic average of the observations for each variable, and it\nis defined as\n\u00afx := 1\nN\nNX\nn=1\nxn , (6.41)\nwhere xn \u2208 RD.\nSimilar to the empirical mean, theempirical covariance matrix is aD\u00d7Dempirical covariance\nmatrix\n\u03a3 := 1\nN\nNX\nn=1\n(xn \u2212 \u00afx)(xn \u2212 \u00afx)\u22a4. (6.42)\nThroughout the\nbook, we use the\nempirical\ncovariance, which is\na biased estimate.\nThe unbiased\n(sometimes called\ncorrected)\ncovariance has the\nfactor N \u2212 1 in the\ndenominator\ninstead of N.\nTo compute the statistics for a particular dataset, we would use the\nrealizations (observations) x1, . . . ,xN and use (6.41) and (6.42). Em-\npirical covariance matrices are symmetr"
  },
  {
    "vector_id": 742,
    "chunk_id": "p198_c2",
    "page_number": 198,
    "text": "ator\ninstead of N.\nTo compute the statistics for a particular dataset, we would use the\nrealizations (observations) x1, . . . ,xN and use (6.41) and (6.42). Em-\npirical covariance matrices are symmetric, positive semidefinite (see Sec-\ntion 3.2.3).\n6.4.3 Three Expressions for the Variance\nWe now focus on a single random variable X and use the preceding em-\npirical formulas to derive three possible expressions for the variance. The\nThe derivations are\nexercises at the end\nof this chapter.\nfollowing derivation is the same for the population variance, except that\nwe need to take care of integrals. The standard definition of variance, cor-\nresponding to the definition of covariance (Definition 6.5), is the expec-\ntation of the squared deviation of a random variable X from its expected\nvalue \u00b5,"
  },
  {
    "vector_id": 743,
    "chunk_id": "p198_c3",
    "page_number": 198,
    "text": ". The standard definition of variance, cor-\nresponding to the definition of covariance (Definition 6.5), is the expec-\ntation of the squared deviation of a random variable X from its expected\nvalue \u00b5, i.e.,\nVX[x] := EX[(x \u2212 \u00b5)2] . (6.43)\nThe expectation in (6.43) and the mean \u00b5 = EX(x) are computed us-\ning (6.32), depending on whether X is a discrete or continuous random\nvariable. The variance as expressed in (6.43) is the mean of a new random\nvariable Z := (X \u2212 \u00b5)2.\nWhen estimating the variance in (6.43) empirically , we need to resort\nto a two-pass algorithm: one pass through the data to calculate the mean\n\u00b5 using (6.41), and then a second pass using this estimate \u02c6\u00b5 calculate the\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 744,
    "chunk_id": "p198_c4",
    "page_number": 198,
    "text": "culate the mean\n\u00b5 using (6.41), and then a second pass using this estimate \u02c6\u00b5 calculate the\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 745,
    "chunk_id": "p199_c0",
    "page_number": 199,
    "text": "6.4 Summary Statistics and Independence 193\nvariance. It turns out that we can avoid two passes by rearranging the\nterms. The formula in (6.43) can be converted to the so-called raw-score raw-score formula\nfor varianceformula for variance:\nVX[x] = EX[x2] \u2212 (EX[x])\n2\n. (6.44)\nThe expression in (6.44) can be remembered as \u201cthe mean of the square\nminus the square of the mean\u201d. It can be calculated empirically in one pass\nthrough data since we can accumulate xi (to calculate the mean) and x2\ni\nsimultaneously , wherexi is the ith observation. Unfortunately , if imple- If the two terms\nin (6.44) are huge\nand approximately\nequal, we may\nsuffer from an\nunnecessary loss of\nnumerical precision\nin floating-point\narithmetic.\nmented in this way , it can be numerically unstable. The raw-score version\nof"
  },
  {
    "vector_id": 746,
    "chunk_id": "p199_c1",
    "page_number": 199,
    "text": "huge\nand approximately\nequal, we may\nsuffer from an\nunnecessary loss of\nnumerical precision\nin floating-point\narithmetic.\nmented in this way , it can be numerically unstable. The raw-score version\nof the variance can be useful in machine learning, e.g., when deriving the\nbias\u2013variance decomposition (Bishop, 2006).\nA third way to understand the variance is that it is a sum of pairwise dif-\nferences between all pairs of observations. Consider a sample x1, . . . , xN\nof realizations of random variable X, and we compute the squared differ-\nence between pairs of xi and xj. By expanding the square, we can show\nthat the sum of N2 pairwise differences is the empirical variance of the\nobservations:\n1\nN2\nNX\ni,j=1\n(xi \u2212 xj)2 = 2\n\uf8ee\n\uf8f0 1\nN\nNX\ni=1\nx2\ni \u2212\n \n1\nN\nNX\ni=1\nxi\n!2\uf8f9\n\uf8fb . (6.45)\nWe see that (6.45)"
  },
  {
    "vector_id": 747,
    "chunk_id": "p199_c2",
    "page_number": 199,
    "text": "we can show\nthat the sum of N2 pairwise differences is the empirical variance of the\nobservations:\n1\nN2\nNX\ni,j=1\n(xi \u2212 xj)2 = 2\n\uf8ee\n\uf8f0 1\nN\nNX\ni=1\nx2\ni \u2212\n \n1\nN\nNX\ni=1\nxi\n!2\uf8f9\n\uf8fb . (6.45)\nWe see that (6.45) is twice the raw-score expression (6.44). This means\nthat we can express the sum of pairwise distances (of which there are N2\nof them) as a sum of deviations from the mean (of which there areN). Ge-\nometrically , this means that there is an equivalence between the pairwise\ndistances and the distances from the center of the set of points. From a\ncomputational perspective, this means that by computing the mean ( N\nterms in the summation), and then computing the variance (again N\nterms in the summation), we can obtain an expression (left-hand side\nof (6.45)) that has N2 terms.\n6.4.4 Sums and Tra"
  },
  {
    "vector_id": 748,
    "chunk_id": "p199_c3",
    "page_number": 199,
    "text": "ing the mean ( N\nterms in the summation), and then computing the variance (again N\nterms in the summation), we can obtain an expression (left-hand side\nof (6.45)) that has N2 terms.\n6.4.4 Sums and Transformations of Random Variables\nWe may want to model a phenomenon that cannot be well explained by\ntextbook distributions (we introduce some in Sections 6.5 and 6.6), and\nhence may perform simple manipulations of random variables (such as\nadding two random variables).\nConsider two random variables X, Ywith states x, y \u2208 RD. Then:\nE[x + y] = E[x] + E[y] (6.46)\nE[x \u2212 y] = E[x] \u2212 E[y] (6.47)\nV[x + y] = V[x] + V[y] + Cov[x, y] + Cov[y, x] (6.48)\nV[x \u2212 y] = V[x] + V[y] \u2212 Cov[x, y] \u2212 Cov[y, x] . (6.49)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 749,
    "chunk_id": "p199_c4",
    "page_number": 199,
    "text": "] = V[x] + V[y] + Cov[x, y] + Cov[y, x] (6.48)\nV[x \u2212 y] = V[x] + V[y] \u2212 Cov[x, y] \u2212 Cov[y, x] . (6.49)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 750,
    "chunk_id": "p200_c0",
    "page_number": 200,
    "text": "194 Probability and Distributions\nMean and (co)variance exhibit some useful properties when it comes\nto affine transformation of random variables. Consider a random variable\nX with mean \u00b5 and covariance matrix \u03a3 and a (deterministic) affine\ntransformation y = Ax + b of x. Then y is itself a random variable\nwhose mean vector and covariance matrix are given by\nEY [y] = EX[Ax + b] = AEX[x] + b = A\u00b5 + b, (6.50)\nVY [y] = VX[Ax + b] = VX[Ax] = AVX[x]A\u22a4 = A\u03a3A\u22a4 , (6.51)\nrespectively . Furthermore,This can be shown\ndirectly by using the\ndefinition of the\nmean and\ncovariance.\nCov[x, y] = E[x(Ax + b)\u22a4] \u2212 E[x]E[Ax + b]\u22a4 (6.52a)\n= E[x]b\u22a4 + E[xx\u22a4]A\u22a4 \u2212 \u00b5b\u22a4 \u2212 \u00b5\u00b5\u22a4A\u22a4 (6.52b)\n= \u00b5b\u22a4 \u2212 \u00b5b\u22a4 +\n\u0000\nE[xx\u22a4] \u2212 \u00b5\u00b5\u22a4\u0001\nA\u22a4 (6.52c)\n(6.38b)\n= \u03a3A\u22a4 , (6.52d)\nwhere \u03a3 = E[xx\u22a4] \u2212 \u00b5\u00b5\u22a4 is the covariance of X.\n6.4.5 Statistical Inde"
  },
  {
    "vector_id": 751,
    "chunk_id": "p200_c1",
    "page_number": 200,
    "text": "E[x]E[Ax + b]\u22a4 (6.52a)\n= E[x]b\u22a4 + E[xx\u22a4]A\u22a4 \u2212 \u00b5b\u22a4 \u2212 \u00b5\u00b5\u22a4A\u22a4 (6.52b)\n= \u00b5b\u22a4 \u2212 \u00b5b\u22a4 +\n\u0000\nE[xx\u22a4] \u2212 \u00b5\u00b5\u22a4\u0001\nA\u22a4 (6.52c)\n(6.38b)\n= \u03a3A\u22a4 , (6.52d)\nwhere \u03a3 = E[xx\u22a4] \u2212 \u00b5\u00b5\u22a4 is the covariance of X.\n6.4.5 Statistical Independence\nDefinition 6.10 (Independence). Two random variables X, Yare statis-statistical\nindependence tically independent if and only if\np(x, y) = p(x)p(y) . (6.53)\nIntuitively , two random variablesX and Y are independent if the value\nof y (once known) does not add any additional information about x (and\nvice versa). If X, Yare (statistically) independent, then\np(y |x) = p(y)\np(x|y) = p(x)\nVX,Y [x + y] = VX[x] + VY [y]\nCovX,Y [x, y] = 0\nThe last point may not hold in converse, i.e., two random variables can\nhave covariance zero but are not statistically independent. To understand\nwhy , recall"
  },
  {
    "vector_id": 752,
    "chunk_id": "p200_c2",
    "page_number": 200,
    "text": "x + y] = VX[x] + VY [y]\nCovX,Y [x, y] = 0\nThe last point may not hold in converse, i.e., two random variables can\nhave covariance zero but are not statistically independent. To understand\nwhy , recall that covariance measures only linear dependence. Therefore,\nrandom variables that are nonlinearly dependent could have covariance\nzero.\nExample 6.5\nConsider a random variable X with zero mean ( EX[x] = 0 ) and also\nEX[x3] = 0 . Let y = x2 (hence, Y is dependent on X) and consider the\ncovariance (6.36) between X and Y . But this gives\nCov[x, y] = E[xy] \u2212 E[x]E[y] = E[x3] = 0 . (6.54)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 753,
    "chunk_id": "p200_c3",
    "page_number": 200,
    "text": "1-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 754,
    "chunk_id": "p201_c0",
    "page_number": 201,
    "text": "6.4 Summary Statistics and Independence 195\nIn machine learning, we often consider problems that can be mod-\neled as independent and identically distributed (i.i.d.) random variables, independent and\nidentically\ndistributed\ni.i.d.\nX1, . . . , XN . For more than two random variables, the word \u201cindepen-\ndent\u201d (Definition 6.10) usually refers to mutually independent random\nvariables, where all subsets are independent (see Pollard (2002, chap-\nter 4) and Jacod and Protter (2004, chapter 3)). The phrase \u201cidentically\ndistributed\u201d means that all the random variables are from the same distri-\nbution.\nAnother concept that is important in machine learning is conditional\nindependence.\nDefinition 6.11 (Conditional Independence). Two random variables X\nand Y are conditionally independent given Z if and"
  },
  {
    "vector_id": 755,
    "chunk_id": "p201_c1",
    "page_number": 201,
    "text": "Another concept that is important in machine learning is conditional\nindependence.\nDefinition 6.11 (Conditional Independence). Two random variables X\nand Y are conditionally independent given Z if and only if conditionally\nindependent\np(x, y |z) = p(x|z)p(y |z) for all z \u2208 Z, (6.55)\nwhere Z is the set of states of random variable Z. We write X \u22a5 \u22a5Y |Z to\ndenote that X is conditionally independent of Y given Z.\nDefinition 6.11 requires that the relation in (6.55) must hold true for\nevery value of z. The interpretation of (6.55) can be understood as \u201cgiven\nknowledge about z, the distribution of x and y factorizes\u201d. Independence\ncan be cast as a special case of conditional independence if we writeX \u22a5 \u22a5\nY | \u2205. By using the product rule of probability (6.22), we can expand the\nleft-hand side of"
  },
  {
    "vector_id": 756,
    "chunk_id": "p201_c2",
    "page_number": 201,
    "text": "x and y factorizes\u201d. Independence\ncan be cast as a special case of conditional independence if we writeX \u22a5 \u22a5\nY | \u2205. By using the product rule of probability (6.22), we can expand the\nleft-hand side of (6.55) to obtain\np(x, y |z) = p(x|y, z)p(y |z) . (6.56)\nBy comparing the right-hand side of (6.55) with (6.56), we see thatp(y |z)\nappears in both of them so that\np(x|y, z) = p(x|z) . (6.57)\nEquation (6.57) provides an alternative definition of conditional indepen-\ndence, i.e., X \u22a5 \u22a5Y |Z. This alternative presentation provides the inter-\npretation \u201cgiven that we know z, knowledge about y does not change our\nknowledge of x\u201d.\n6.4.6 Inner Products of Random Variables\nRecall the definition of inner products from Section 3.2. We can define an Inner products\nbetween\nmultivariate random\nvariables ca"
  },
  {
    "vector_id": 757,
    "chunk_id": "p201_c3",
    "page_number": 201,
    "text": "change our\nknowledge of x\u201d.\n6.4.6 Inner Products of Random Variables\nRecall the definition of inner products from Section 3.2. We can define an Inner products\nbetween\nmultivariate random\nvariables can be\ntreated in a similar\nfashion\ninner product between random variables, which we briefly describe in this\nsection. If we have two uncorrelated random variables X, Y, then\nV[x + y] = V[x] + V[y] . (6.58)\nSince variances are measured in squared units, this looks very much like\nthe Pythagorean theorem for right triangles c2 = a2 + b2.\nIn the following, we see whether we can find a geometric interpreta-\ntion of the variance relation of uncorrelated random variables in (6.58).\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 758,
    "chunk_id": "p201_c4",
    "page_number": 201,
    "text": "eta-\ntion of the variance relation of uncorrelated random variables in (6.58).\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 759,
    "chunk_id": "p202_c0",
    "page_number": 202,
    "text": "196 Probability and Distributions\nFigure 6.6\nGeometry of\nrandom variables. If\nrandom variables X\nand Y are\nuncorrelated, they\nare orthogonal\nvectors in a\ncorresponding\nvector space, and\nthe Pythagorean\ntheorem applies.\n\u221a\nvar[y]\n\u221a\nvar[x]\n\u221a\nvar[x + y] =\n\u221a\nvar[x] + var[y]\na\nc\nb\nRandom variables can be considered vectors in a vector space, and we\ncan define inner products to obtain geometric properties of random vari-\nables (Eaton, 2007). If we define\n\u27e8X, Y\u27e9 := Cov[x, y] (6.59)\nfor zero mean random variablesX and Y , we obtain an inner product. We\nsee that the covariance is symmetric, positive definite, and linear in eitherCov[x, x] = 0 \u21d0\u21d2\nx = 0 argument. The length of a random variable is\nCov[\u03b1x + z, y] =\n\u03b1 Cov[x, y] +\nCov[z, y] for \u03b1 \u2208 R. \u2225X\u2225 =\nq\nCov[x, x] =\nq\nV[x] = \u03c3[x] , (6.60)\ni.e., its"
  },
  {
    "vector_id": 760,
    "chunk_id": "p202_c1",
    "page_number": 202,
    "text": "inite, and linear in eitherCov[x, x] = 0 \u21d0\u21d2\nx = 0 argument. The length of a random variable is\nCov[\u03b1x + z, y] =\n\u03b1 Cov[x, y] +\nCov[z, y] for \u03b1 \u2208 R. \u2225X\u2225 =\nq\nCov[x, x] =\nq\nV[x] = \u03c3[x] , (6.60)\ni.e., its standard deviation. The \u201clonger\u201d the random variable, the more\nuncertain it is; and a random variable with length 0 is deterministic.\nIf we look at the angle \u03b8 between two random variables X, Y, we get\ncos \u03b8 = \u27e8X, Y\u27e9\n\u2225X\u2225 \u2225Y \u2225 = Cov[x, y]p\nV[x]V[y] , (6.61)\nwhich is the correlation (Definition 6.8) between the two random vari-\nables. This means that we can think of correlation as the cosine of the\nangle between two random variables when we consider them geometri-\ncally . We know from Definition 3.7 thatX \u22a5 Y \u21d0 \u21d2 \u27e8X, Y\u27e9 = 0. In our\ncase, this means that X and Y are orthogonal if and only ifCov[x"
  },
  {
    "vector_id": 761,
    "chunk_id": "p202_c2",
    "page_number": 202,
    "text": "angle between two random variables when we consider them geometri-\ncally . We know from Definition 3.7 thatX \u22a5 Y \u21d0 \u21d2 \u27e8X, Y\u27e9 = 0. In our\ncase, this means that X and Y are orthogonal if and only ifCov[x, y] = 0,\ni.e., they are uncorrelated. Figure 6.6 illustrates this relationship.\nRemark. While it is tempting to use the Euclidean distance (constructed\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 762,
    "chunk_id": "p203_c0",
    "page_number": 203,
    "text": "6.5 Gaussian Distribution 197\nFigure 6.7\nGaussian\ndistribution of two\nrandom variables x1\nand x2.\nx1\n\u22121 0 1\nx2\n\u22125.0\u22122.50.02.55.07.5\np(x1, x\n2)\n0.00\n0.05\n0.10\n0.15\n0.20\nfrom the preceding definition of inner products) to compare probability\ndistributions, it is unfortunately not the best way to obtain distances be-\ntween distributions. Recall that the probability mass (or density) is posi-\ntive and needs to add up to 1. These constraints mean that distributions\nlive on something called a statistical manifold. The study of this space of\nprobability distributions is called information geometry . Computing dis-\ntances between distributions are often done using Kullback-Leibler diver-\ngence, which is a generalization of distances that account for properties of\nthe statistical manifold. Just lik"
  },
  {
    "vector_id": 763,
    "chunk_id": "p203_c1",
    "page_number": 203,
    "text": "Computing dis-\ntances between distributions are often done using Kullback-Leibler diver-\ngence, which is a generalization of distances that account for properties of\nthe statistical manifold. Just like the Euclidean distance is a special case of\na metric (Section 3.3), the Kullback-Leibler divergence is a special case of\ntwo more general classes of divergences called Bregman divergences and\nf-divergences. The study of divergences is beyond the scope of this book,\nand we refer for more details to the recent book by Amari (2016), one of\nthe founders of the field of information geometry . \u2662\n6.5 Gaussian Distribution\nThe Gaussian distribution is the most well-studied probability distribution\nfor continuous-valued random variables. It is also referred to as thenormal normal distribution\ndistrib"
  },
  {
    "vector_id": 764,
    "chunk_id": "p203_c2",
    "page_number": 203,
    "text": "aussian Distribution\nThe Gaussian distribution is the most well-studied probability distribution\nfor continuous-valued random variables. It is also referred to as thenormal normal distribution\ndistribution. Its importance originates from the fact that it has many com- The Gaussian\ndistribution arises\nnaturally when we\nconsider sums of\nindependent and\nidentically\ndistributed random\nvariables. This is\nknown as the\ncentral limit\ntheorem (Grinstead\nand Snell, 1997).\nputationally convenient properties, which we will be discussing in the fol-\nlowing. In particular, we will use it to define the likelihood and prior for\nlinear regression (Chapter 9), and consider a mixture of Gaussians for\ndensity estimation (Chapter 11).\nThere are many other areas of machine learning that also benefit from\nusing"
  },
  {
    "vector_id": 765,
    "chunk_id": "p203_c3",
    "page_number": 203,
    "text": "ihood and prior for\nlinear regression (Chapter 9), and consider a mixture of Gaussians for\ndensity estimation (Chapter 11).\nThere are many other areas of machine learning that also benefit from\nusing a Gaussian distribution, for example Gaussian processes, variational\ninference, and reinforcement learning. It is also widely used in other ap-\nplication areas such as signal processing (e.g., Kalman filter), control (e.g.,\nlinear quadratic regulator), and statistics (e.g., hypothesis testing).\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 766,
    "chunk_id": "p204_c0",
    "page_number": 204,
    "text": "198 Probability and Distributions\nFigure 6.8\nGaussian\ndistributions\noverlaid with 100\nsamples. (a) One-\ndimensional case;\n(b) two-dimensional\ncase.\n\u22125.0 \u22122.5 0.0 2.5 5.0 7.5\nx\n0.00\n0.05\n0.10\n0.15\n0.20\n p(x)\nMean\nSample\n2\u03c3\n(a) Univariate (one-dimensional) Gaussian;\nThe red cross shows the mean and the red\nline shows the extent of the variance.\n\u22121 0 1\nx1\n\u22124\n\u22122\n0\n2\n4\n6\n8\nx2\nMean\nSample\n(b) Multivariate (two-dimensional) Gaus-\nsian, viewed from top. The red cross shows\nthe mean and the colored lines show the con-\ntour lines of the density .\nFor a univariate random variable, the Gaussian distribution has a den-\nsity that is given by\np(x |\u00b5, \u03c32) = 1\u221a\n2\u03c0\u03c32\nexp\n\u0012\n\u2212(x \u2212 \u00b5)2\n2\u03c32\n\u0013\n. (6.62)\nThe multivariate Gaussian distribution is fully characterized by a meanmultivariate\nGaussian\ndistribution\nmean"
  },
  {
    "vector_id": 767,
    "chunk_id": "p204_c1",
    "page_number": 204,
    "text": "on has a den-\nsity that is given by\np(x |\u00b5, \u03c32) = 1\u221a\n2\u03c0\u03c32\nexp\n\u0012\n\u2212(x \u2212 \u00b5)2\n2\u03c32\n\u0013\n. (6.62)\nThe multivariate Gaussian distribution is fully characterized by a meanmultivariate\nGaussian\ndistribution\nmean vector\nvector \u00b5 and a covariance matrix \u03a3 and defined as\ncovariance matrix p(x|\u00b5, \u03a3) = (2\u03c0)\u2212D\n2 |\u03a3|\u22121\n2 exp\n\u0000\n\u2212 1\n2 (x \u2212 \u00b5)\u22a4\u03a3\u22121(x \u2212 \u00b5)\n\u0001\n, (6.63)\nwhere x \u2208 RD. We write p(x) = N\n\u0000\nx|\u00b5, \u03a3\n\u0001\nor X \u223c N\n\u0000\n\u00b5, \u03a3\n\u0001\n. Fig-Also known as a\nmultivariate normal\ndistribution.\nure 6.7 shows a bivariate Gaussian (mesh), with the corresponding con-\ntour plot. Figure 6.8 shows a univariate Gaussian and a bivariate Gaussian\nwith corresponding samples. The special case of the Gaussian with zero\nmean and identity covariance, that is, \u00b5 = 0 and \u03a3 = I, is referred to as\nthe standard normal distribution.standard norm"
  },
  {
    "vector_id": 768,
    "chunk_id": "p204_c2",
    "page_number": 204,
    "text": "Gaussian\nwith corresponding samples. The special case of the Gaussian with zero\nmean and identity covariance, that is, \u00b5 = 0 and \u03a3 = I, is referred to as\nthe standard normal distribution.standard normal\ndistribution Gaussians are widely used in statistical estimation and machine learn-\ning as they have closed-form expressions for marginal and conditional dis-\ntributions. In Chapter 9, we use these closed-form expressions extensively\nfor linear regression. A major advantage of modeling with Gaussian ran-\ndom variables is that variable transformations (Section 6.7) are often not\nneeded. Since the Gaussian distribution is fully specified by its mean and\ncovariance, we often can obtain the transformed distribution by applying\nthe transformation to the mean and covariance of the random variable"
  },
  {
    "vector_id": 769,
    "chunk_id": "p204_c3",
    "page_number": 204,
    "text": "e Gaussian distribution is fully specified by its mean and\ncovariance, we often can obtain the transformed distribution by applying\nthe transformation to the mean and covariance of the random variable.\n6.5.1 Marginals and Conditionals of Gaussians are Gaussians\nIn the following, we present marginalization and conditioning in the gen-\neral case of multivariate random variables. If this is confusing at first read-\ning, the reader is advised to consider two univariate random variables in-\nstead. Let X and Y be two multivariate random variables, that may have\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 770,
    "chunk_id": "p204_c4",
    "page_number": 204,
    "text": "Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 771,
    "chunk_id": "p205_c0",
    "page_number": 205,
    "text": "6.5 Gaussian Distribution 199\ndifferent dimensions. To consider the effect of applying the sum rule of\nprobability and the effect of conditioning, we explicitly write the Gaus-\nsian distribution in terms of the concatenated states [x\u22a4 y\u22a4]\u22a4 so that\np(x, y) = N\n\u0012\u0014\u00b5x\n\u00b5y\n\u0015\n,\n\u0014\u03a3xx \u03a3xy\n\u03a3yx \u03a3yy\n\u0015\u0013\n, (6.64)\nwhere \u03a3xx = Cov[ x, x] and \u03a3yy = Cov[ y, y] are the marginal covari-\nance matrices of x and y, respectively , and\u03a3xy = Cov[x, y] is the cross-\ncovariance matrix between x and y.\nThe conditional distribution p(x|y) is also Gaussian (illustrated in Fig-\nure 6.9(c)) and given by (derived in Section 2.3 of Bishop, 2006)\np(x|y) = N\n\u0000\n\u00b5x |y, \u03a3x |y\n\u0001\n(6.65)\n\u00b5x |y = \u00b5x + \u03a3xy\u03a3\u22121\nyy (y \u2212 \u00b5y) (6.66)\n\u03a3x |y = \u03a3xx \u2212 \u03a3xy\u03a3\u22121\nyy \u03a3yx . (6.67)\nNote that in the computation of the mean in (6.66), the y-value is an"
  },
  {
    "vector_id": 772,
    "chunk_id": "p205_c1",
    "page_number": 205,
    "text": "3 of Bishop, 2006)\np(x|y) = N\n\u0000\n\u00b5x |y, \u03a3x |y\n\u0001\n(6.65)\n\u00b5x |y = \u00b5x + \u03a3xy\u03a3\u22121\nyy (y \u2212 \u00b5y) (6.66)\n\u03a3x |y = \u03a3xx \u2212 \u03a3xy\u03a3\u22121\nyy \u03a3yx . (6.67)\nNote that in the computation of the mean in (6.66), the y-value is an\nobservation and no longer random.\nRemark. The conditional Gaussian distribution shows up in many places,\nwhere we are interested in posterior distributions:\nThe Kalman filter (Kalman, 1960), one of the most central algorithms\nfor state estimation in signal processing, does nothing but computing\nGaussian conditionals of joint distributions (Deisenroth and Ohlsson,\n2011; S\u00a8arkk\u00a8a, 2013).\nGaussian processes (Rasmussen and Williams, 2006), which are a prac-\ntical implementation of a distribution over functions. In a Gaussian pro-\ncess, we make assumptions of joint Gaussianity of random variables."
  },
  {
    "vector_id": 773,
    "chunk_id": "p205_c2",
    "page_number": 205,
    "text": "ocesses (Rasmussen and Williams, 2006), which are a prac-\ntical implementation of a distribution over functions. In a Gaussian pro-\ncess, we make assumptions of joint Gaussianity of random variables. By\n(Gaussian) conditioning on observed data, we can determine a poste-\nrior distribution over functions.\nLatent linear Gaussian models (Roweis and Ghahramani, 1999; Mur-\nphy, 2012), which include probabilistic principal component analysis\n(PPCA) (Tipping and Bishop, 1999). We will look at PPCA in more de-\ntail in Section 10.7.\n\u2662\nThe marginal distribution p(x) of a joint Gaussian distribution p(x, y)\n(see (6.64)) is itself Gaussian and computed by applying the sum rule\n(6.20) and given by\np(x) =\nZ\np(x, y)dy = N\n\u0000\nx|\u00b5x, \u03a3xx\n\u0001\n. (6.68)\nThe corresponding result holds for p(y), which is obtained by"
  },
  {
    "vector_id": 774,
    "chunk_id": "p205_c3",
    "page_number": 205,
    "text": "y)\n(see (6.64)) is itself Gaussian and computed by applying the sum rule\n(6.20) and given by\np(x) =\nZ\np(x, y)dy = N\n\u0000\nx|\u00b5x, \u03a3xx\n\u0001\n. (6.68)\nThe corresponding result holds for p(y), which is obtained by marginaliz-\ning with respect tox. Intuitively , looking at the joint distribution in (6.64),\nwe ignore (i.e., integrate out) everything we are not interested in. This is\nillustrated in Figure 6.9(b).\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 775,
    "chunk_id": "p206_c0",
    "page_number": 206,
    "text": "200 Probability and Distributions\nExample 6.6\nFigure 6.9\n(a) Bivariate\nGaussian;\n(b) marginal of a\njoint Gaussian\ndistribution is\nGaussian; (c) the\nconditional\ndistribution of a\nGaussian is also\nGaussian.\n\u22121 0 1\nx1\n\u22124\n\u22122\n0\n2\n4\n6\n8\nx2\nx2 = \u22121\n(a) Bivariate Gaussian.\n\u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5\nx1\n0.0\n0.2\n0.4\n0.6\np(x1)\nMean\n2\u03c3\n(b) Marginal distribution.\n\u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5\nx1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2 p(x1|x2 = \u22121)\nMean\n2\u03c3 (c) Conditional distribution.\nConsider the bivariate Gaussian distribution (illustrated in Figure 6.9):\np(x1, x2) = N\n\u0012\u00140\n2\n\u0015\n,\n\u00140.3 \u22121\n\u22121 5\n\u0015\u0013\n. (6.69)\nWe can compute the parameters of the univariate Gaussian, conditioned\non x2 = \u22121, by applying (6.66) and (6.67) to obtain the mean and vari-\nance respectively . Numerically , this is\n\u00b5x1 |x2=\u22121 = 0 + (\u22121) \u00b7"
  },
  {
    "vector_id": 776,
    "chunk_id": "p206_c1",
    "page_number": 206,
    "text": "compute the parameters of the univariate Gaussian, conditioned\non x2 = \u22121, by applying (6.66) and (6.67) to obtain the mean and vari-\nance respectively . Numerically , this is\n\u00b5x1 |x2=\u22121 = 0 + (\u22121) \u00b7 0.2 \u00b7 (\u22121 \u2212 2) = 0.6 (6.70)\nand\n\u03c32\nx1 |x2=\u22121 = 0.3 \u2212 (\u22121) \u00b7 0.2 \u00b7 (\u22121) = 0.1 . (6.71)\nTherefore, the conditional Gaussian is given by\np(x1 |x2 = \u22121) = N\n\u0000\n0.6, 0.1\n\u0001\n. (6.72)\nThe marginal distribution p(x1), in contrast, can be obtained by apply-\ning (6.68), which is essentially using the mean and variance of the random\nvariable x1, giving us\np(x1) = N\n\u0000\n0, 0.3\n\u0001\n. (6.73)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 777,
    "chunk_id": "p206_c2",
    "page_number": 206,
    "text": "athematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 778,
    "chunk_id": "p207_c0",
    "page_number": 207,
    "text": "6.5 Gaussian Distribution 201\n6.5.2 Product of Gaussian Densities\nFor linear regression (Chapter 9), we need to compute a Gaussian likeli-\nhood. Furthermore, we may wish to assume a Gaussian prior (Section 9.3).\nWe apply Bayes\u2019 Theorem to compute the posterior, which results in a mul-\ntiplication of the likelihood and the prior, that is, the multiplication of two\nGaussian densities. Theproduct of two GaussiansN\n\u0000\nx|a, A\n\u0001\nN\n\u0000\nx|b, B\n\u0001\nThe derivation is an\nexercise at the end\nof this chapter.\nis a Gaussian distribution scaled by a c \u2208 R, given by c N\n\u0000\nx|c, C\n\u0001\nwith\nC = (A\u22121 + B\u22121)\u22121 (6.74)\nc = C(A\u22121a + B\u22121b) (6.75)\nc = (2\u03c0)\u2212D\n2 |A + B|\u22121\n2 exp\n\u0000\n\u2212 1\n2 (a \u2212 b)\u22a4(A + B)\u22121(a \u2212 b)\n\u0001\n. (6.76)\nThe scaling constant c itself can be written in the form of a Gaussian\ndensity either in a or in b with"
  },
  {
    "vector_id": 779,
    "chunk_id": "p207_c1",
    "page_number": 207,
    "text": "C(A\u22121a + B\u22121b) (6.75)\nc = (2\u03c0)\u2212D\n2 |A + B|\u22121\n2 exp\n\u0000\n\u2212 1\n2 (a \u2212 b)\u22a4(A + B)\u22121(a \u2212 b)\n\u0001\n. (6.76)\nThe scaling constant c itself can be written in the form of a Gaussian\ndensity either in a or in b with an \u201cinflated\u201d covariance matrix A + B,\ni.e., c = N\n\u0000\na|b, A + B\n\u0001\n= N\n\u0000\nb|a, A + B\n\u0001\n.\nRemark. For notation convenience, we will sometimes use N\n\u0000\nx|m, S\n\u0001\nto describe the functional form of a Gaussian density even if x is not a\nrandom variable. We have just done this in the preceding demonstration\nwhen we wrote\nc = N\n\u0000\na|b, A + B\n\u0001\n= N\n\u0000\nb|a, A + B\n\u0001\n. (6.77)\nHere, neither a nor b are random variables. However, writingc in this way\nis more compact than (6.76). \u2662\n6.5.3 Sums and Linear Transformations\nIf X, Yare independent Gaussian random variables (i.e., the joint distri-\nbution is given as p"
  },
  {
    "vector_id": 780,
    "chunk_id": "p207_c2",
    "page_number": 207,
    "text": "ables. However, writingc in this way\nis more compact than (6.76). \u2662\n6.5.3 Sums and Linear Transformations\nIf X, Yare independent Gaussian random variables (i.e., the joint distri-\nbution is given as p(x, y) = p(x)p(y)) with p(x) = N\n\u0000\nx|\u00b5x, \u03a3x\n\u0001\nand\np(y) = N\n\u0000\ny |\u00b5y, \u03a3y\n\u0001\n, then x + y is also Gaussian distributed and given\nby\np(x + y) = N\n\u0000\n\u00b5x + \u00b5y, \u03a3x + \u03a3y\n\u0001\n. (6.78)\nKnowing that p(x + y) is Gaussian, the mean and covariance matrix can\nbe determined immediately using the results from (6.46) through (6.49).\nThis property will be important when we consider i.i.d. Gaussian noise\nacting on random variables, as is the case for linear regression (Chap-\nter 9).\nExample 6.7\nSince expectations are linear operations, we can obtain the weighted sum\nof independent Gaussian random variables\np(ax + by)"
  },
  {
    "vector_id": 781,
    "chunk_id": "p207_c3",
    "page_number": 207,
    "text": "variables, as is the case for linear regression (Chap-\nter 9).\nExample 6.7\nSince expectations are linear operations, we can obtain the weighted sum\nof independent Gaussian random variables\np(ax + by) = N\n\u0000\na\u00b5x + b\u00b5y, a2\u03a3x + b2\u03a3y\n\u0001\n. (6.79)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 782,
    "chunk_id": "p208_c0",
    "page_number": 208,
    "text": "202 Probability and Distributions\nRemark. A case that will be useful in Chapter 11 is the weighted sum of\nGaussian densities. This is different from the weighted sum of Gaussian\nrandom variables. \u2662\nIn Theorem 6.12, the random variable x is from a density that is a\nmixture of two densitiesp1(x) and p2(x), weighted by \u03b1. The theorem can\nbe generalized to the multivariate random variable case, since linearity of\nexpectations holds also for multivariate random variables. However, the\nidea of a squared random variable needs to be replaced by xx\u22a4.\nTheorem 6.12. Consider a mixture of two univariate Gaussian densities\np(x) = \u03b1p1(x) + (1\u2212 \u03b1)p2(x) , (6.80)\nwhere the scalar 0 < \u03b1 <1 is the mixture weight, and p1(x) and p2(x) are\nunivariate Gaussian densities (Equation (6.62)) with different parameter"
  },
  {
    "vector_id": 783,
    "chunk_id": "p208_c1",
    "page_number": 208,
    "text": "aussian densities\np(x) = \u03b1p1(x) + (1\u2212 \u03b1)p2(x) , (6.80)\nwhere the scalar 0 < \u03b1 <1 is the mixture weight, and p1(x) and p2(x) are\nunivariate Gaussian densities (Equation (6.62)) with different parameters,\ni.e., (\u00b51, \u03c32\n1) \u0338= (\u00b52, \u03c32\n2).\nThen the mean of the mixture density p(x) is given by the weighted sum\nof the means of each random variable:\nE[x] = \u03b1\u00b51 + (1 \u2212 \u03b1)\u00b52 . (6.81)\nThe variance of the mixture density p(x) is given by\nV[x] =\n\u0002\n\u03b1\u03c32\n1 + (1 \u2212 \u03b1)\u03c32\n2\n\u0003\n+\n\u0010\u0002\n\u03b1\u00b52\n1 + (1 \u2212 \u03b1)\u00b52\n2\n\u0003\n\u2212 [\u03b1\u00b51 + (1 \u2212 \u03b1)\u00b52]\n2\n\u0011\n.\n(6.82)\nProof The mean of the mixture density p(x) is given by the weighted\nsum of the means of each random variable. We apply the definition of the\nmean (Definition 6.4), and plug in our mixture (6.80), which yields\nE[x] =\nZ \u221e\n\u2212\u221e\nxp(x)dx (6.83a)\n=\nZ \u221e\n\u2212\u221e\n(\u03b1xp1(x) + (1\u2212 \u03b1)xp2(x)) dx (6.8"
  },
  {
    "vector_id": 784,
    "chunk_id": "p208_c2",
    "page_number": 208,
    "text": "eans of each random variable. We apply the definition of the\nmean (Definition 6.4), and plug in our mixture (6.80), which yields\nE[x] =\nZ \u221e\n\u2212\u221e\nxp(x)dx (6.83a)\n=\nZ \u221e\n\u2212\u221e\n(\u03b1xp1(x) + (1\u2212 \u03b1)xp2(x)) dx (6.83b)\n= \u03b1\nZ \u221e\n\u2212\u221e\nxp1(x)dx + (1 \u2212 \u03b1)\nZ \u221e\n\u2212\u221e\nxp2(x)dx (6.83c)\n= \u03b1\u00b51 + (1 \u2212 \u03b1)\u00b52 . (6.83d)\nTo compute the variance, we can use the raw-score version of the vari-\nance from (6.44), which requires an expression of the expectation of the\nsquared random variable. Here we use the definition of an expectation of\na function (the square) of a random variable (Definition 6.3),\nE[x2] =\nZ \u221e\n\u2212\u221e\nx2p(x)dx (6.84a)\n=\nZ \u221e\n\u2212\u221e\n\u0000\n\u03b1x2p1(x) + (1\u2212 \u03b1)x2p2(x)\n\u0001\ndx (6.84b)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 785,
    "chunk_id": "p208_c3",
    "page_number": 208,
    "text": "Z \u221e\n\u2212\u221e\n\u0000\n\u03b1x2p1(x) + (1\u2212 \u03b1)x2p2(x)\n\u0001\ndx (6.84b)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 786,
    "chunk_id": "p209_c0",
    "page_number": 209,
    "text": "6.5 Gaussian Distribution 203\n= \u03b1\nZ \u221e\n\u2212\u221e\nx2p1(x)dx + (1 \u2212 \u03b1)\nZ \u221e\n\u2212\u221e\nx2p2(x)dx (6.84c)\n= \u03b1(\u00b52\n1 + \u03c32\n1) + (1\u2212 \u03b1)(\u00b52\n2 + \u03c32\n2) , (6.84d)\nwhere in the last equality , we again used the raw-score version of the\nvariance (6.44) giving \u03c32 = E[x2] \u2212 \u00b52. This is rearranged such that the\nexpectation of a squared random variable is the sum of the squared mean\nand the variance.\nTherefore, the variance is given by subtracting (6.83d) from (6.84d),\nV[x] = E[x2] \u2212 (E[x])2 (6.85a)\n= \u03b1(\u00b52\n1 + \u03c32\n1) + (1\u2212 \u03b1)(\u00b52\n2 + \u03c32\n2) \u2212 (\u03b1\u00b51 + (1 \u2212 \u03b1)\u00b52)2 (6.85b)\n=\n\u0002\n\u03b1\u03c32\n1 + (1 \u2212 \u03b1)\u03c32\n2\n\u0003\n+\n\u0010\u0002\n\u03b1\u00b52\n1 + (1 \u2212 \u03b1)\u00b52\n2\n\u0003\n\u2212 [\u03b1\u00b51 + (1 \u2212 \u03b1)\u00b52]\n2\n\u0011\n. (6.85c)\nRemark. The preceding derivation holds for any density , but since the\nGaussian is fully determined by the mean and variance, the mixture den-\nsity can be determined in close"
  },
  {
    "vector_id": 787,
    "chunk_id": "p209_c1",
    "page_number": 209,
    "text": "(1 \u2212 \u03b1)\u00b52]\n2\n\u0011\n. (6.85c)\nRemark. The preceding derivation holds for any density , but since the\nGaussian is fully determined by the mean and variance, the mixture den-\nsity can be determined in closed form. \u2662\nFor a mixture density , the individual components can be considered\nto be conditional distributions (conditioned on the component identity).\nEquation (6.85c) is an example of the conditional variance formula, also\nknown as the law of total variance, which generally states that for two ran- law of total variance\ndom variables X and Y it holds thatVX[x] = EY [VX[x|y]]+VY [EX[x|y]],\ni.e., the (total) variance ofX is the expected conditional variance plus the\nvariance of a conditional mean.\nWe consider in Example 6.17 a bivariate standard Gaussian random\nvariable X and performed a linear"
  },
  {
    "vector_id": 788,
    "chunk_id": "p209_c2",
    "page_number": 209,
    "text": "he (total) variance ofX is the expected conditional variance plus the\nvariance of a conditional mean.\nWe consider in Example 6.17 a bivariate standard Gaussian random\nvariable X and performed a linear transformation Ax on it. The outcome\nis a Gaussian random variable with mean zero and covariance AA\u22a4. Ob-\nserve that adding a constant vector will change the mean of the distribu-\ntion, without affecting its variance, that is, the random variable x + \u00b5 is\nGaussian with mean \u00b5 and identity covariance. Hence, any linear/affine\ntransformation of a Gaussian random variable is Gaussian distributed. Any linear/affine\ntransformation of a\nGaussian random\nvariable is also\nGaussian\ndistributed.\nConsider a Gaussian distributed random variable X \u223c N\n\u0000\n\u00b5, \u03a3\n\u0001\n. For\na given matrix A of appropriate shape, l"
  },
  {
    "vector_id": 789,
    "chunk_id": "p209_c3",
    "page_number": 209,
    "text": "y linear/affine\ntransformation of a\nGaussian random\nvariable is also\nGaussian\ndistributed.\nConsider a Gaussian distributed random variable X \u223c N\n\u0000\n\u00b5, \u03a3\n\u0001\n. For\na given matrix A of appropriate shape, let Y be a random variable such\nthat y = Ax is a transformed version of x. We can compute the mean of\ny by exploiting that the expectation is a linear operator (6.50) as follows:\nE[y] = E[Ax] = AE[x] = A\u00b5. (6.86)\nSimilarly the variance of y can be found by using (6.51):\nV[y] = V[Ax] = AV[x]A\u22a4 = A\u03a3A\u22a4 . (6.87)\nThis means that the random variable y is distributed according to\np(y) = N\n\u0000\ny |A\u00b5, A\u03a3A\u22a4\u0001\n. (6.88)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 790,
    "chunk_id": "p209_c4",
    "page_number": 209,
    "text": "(6.88)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 791,
    "chunk_id": "p210_c0",
    "page_number": 210,
    "text": "204 Probability and Distributions\nLet us now consider the reverse transformation: when we know that a\nrandom variable has a mean that is a linear transformation of another\nrandom variable. For a given full rank matrixA \u2208 RM\u00d7N , where M \u2a7e N,\nlet y \u2208 RM be a Gaussian random variable with mean Ax, i.e.,\np(y) = N\n\u0000\ny |Ax, \u03a3\n\u0001\n. (6.89)\nWhat is the corresponding probability distribution p(x)? If A is invert-\nible, then we can write x = A\u22121y and apply the transformation in the\nprevious paragraph. However, in general A is not invertible, and we use\nan approach similar to that of the pseudo-inverse (3.57). That is, we pre-\nmultiply both sides with A\u22a4 and then invert A\u22a4A, which is symmetric\nand positive definite, giving us the relation\ny = Ax \u21d0 \u21d2(A\u22a4A)\u22121A\u22a4y = x. (6.90)\nHence, x is a linear transforma"
  },
  {
    "vector_id": 792,
    "chunk_id": "p210_c1",
    "page_number": 210,
    "text": "57). That is, we pre-\nmultiply both sides with A\u22a4 and then invert A\u22a4A, which is symmetric\nand positive definite, giving us the relation\ny = Ax \u21d0 \u21d2(A\u22a4A)\u22121A\u22a4y = x. (6.90)\nHence, x is a linear transformation of y, and we obtain\np(x) = N\n\u0000\nx|(A\u22a4A)\u22121A\u22a4y, (A\u22a4A)\u22121A\u22a4\u03a3A(A\u22a4A)\u22121\u0001\n. (6.91)\n6.5.4 Sampling from Multivariate Gaussian Distributions\nWe will not explain the subtleties of random sampling on a computer, and\nthe interested reader is referred to Gentle (2004). In the case of a mul-\ntivariate Gaussian, this process consists of three stages: first, we need a\nsource of pseudo-random numbers that provide a uniform sample in the\ninterval [0,1]; second, we use a non-linear transformation such as the\nBox-M\u00a8uller transform (Devroye, 1986) to obtain a sample from a univari-\nate Gaussian; and third, we c"
  },
  {
    "vector_id": 793,
    "chunk_id": "p210_c2",
    "page_number": 210,
    "text": "de a uniform sample in the\ninterval [0,1]; second, we use a non-linear transformation such as the\nBox-M\u00a8uller transform (Devroye, 1986) to obtain a sample from a univari-\nate Gaussian; and third, we collate a vector of these samples to obtain a\nsample from a multivariate standard normal N\n\u0000\n0, I\n\u0001\n.\nFor a general multivariate Gaussian, that is, where the mean is non\nzero and the covariance is not the identity matrix, we use the proper-\nties of linear transformations of a Gaussian random variable. Assume we\nare interested in generating samples xi, i= 1, . . . , n,from a multivariate\nGaussian distribution with mean \u00b5 and covariance matrix \u03a3. We wouldTo compute the\nCholesky\nfactorization of a\nmatrix, it is required\nthat the matrix is\nsymmetric and\npositive definite\n(Section 3.2.3).\nCovariance"
  },
  {
    "vector_id": 794,
    "chunk_id": "p210_c3",
    "page_number": 210,
    "text": "stribution with mean \u00b5 and covariance matrix \u03a3. We wouldTo compute the\nCholesky\nfactorization of a\nmatrix, it is required\nthat the matrix is\nsymmetric and\npositive definite\n(Section 3.2.3).\nCovariance matrices\npossess this\nproperty .\nlike to construct the sample from a sampler that provides samples from\nthe multivariate standard normal N\n\u0000\n0, I\n\u0001\n.\nTo obtain samples from a multivariate normal N\n\u0000\n\u00b5, \u03a3\n\u0001\n, we can use\nthe properties of a linear transformation of a Gaussian random variable:\nIf x \u223c N\n\u0000\n0, I\n\u0001\n, then y = Ax + \u00b5, where AA\u22a4 = \u03a3 is Gaussian dis-\ntributed with mean \u00b5 and covariance matrix \u03a3. One convenient choice of\nA is to use the Cholesky decomposition (Section 4.3) of the covariance\nmatrix \u03a3 = AA\u22a4. The Cholesky decomposition has the benefit that A is\ntriangular, leading to effic"
  },
  {
    "vector_id": 795,
    "chunk_id": "p210_c4",
    "page_number": 210,
    "text": "rix \u03a3. One convenient choice of\nA is to use the Cholesky decomposition (Section 4.3) of the covariance\nmatrix \u03a3 = AA\u22a4. The Cholesky decomposition has the benefit that A is\ntriangular, leading to efficient computation.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 796,
    "chunk_id": "p211_c0",
    "page_number": 211,
    "text": "6.6 Conjugacy and the Exponential Family 205\n6.6 Conjugacy and the Exponential Family\nMany of the probability distributions \u201cwith names\u201d that we find in statis-\ntics textbooks were discovered to model particular types of phenomena.\nFor example, we have seen the Gaussian distribution in Section 6.5. The\ndistributions are also related to each other in complex ways (Leemis and\nMcQueston, 2008). For a beginner in the field, it can be overwhelming to\nfigure out which distribution to use. In addition, many of these distribu-\ntions were discovered at a time that statistics and computation were done \u201cComputers\u201d used to\nbe a job description.by pencil and paper. It is natural to ask what are meaningful concepts\nin the computing age (Efron and Hastie, 2016). In the previous section,\nwe saw that many"
  },
  {
    "vector_id": 797,
    "chunk_id": "p211_c1",
    "page_number": 211,
    "text": "Computers\u201d used to\nbe a job description.by pencil and paper. It is natural to ask what are meaningful concepts\nin the computing age (Efron and Hastie, 2016). In the previous section,\nwe saw that many of the operations required for inference can be conve-\nniently calculated when the distribution is Gaussian. It is worth recalling\nat this point the desiderata for manipulating probability distributions in\nthe machine learning context:\n1. There is some \u201cclosure property\u201d when applying the rules of probability ,\ne.g., Bayes\u2019 theorem. By closure, we mean that applying a particular\noperation returns an object of the same type.\n2. As we collect more data, we do not need more parameters to describe\nthe distribution.\n3. Since we are interested in learning from data, we want parameter es-\ntimation to"
  },
  {
    "vector_id": 798,
    "chunk_id": "p211_c2",
    "page_number": 211,
    "text": "an object of the same type.\n2. As we collect more data, we do not need more parameters to describe\nthe distribution.\n3. Since we are interested in learning from data, we want parameter es-\ntimation to behave nicely .\nIt turns out that the class of distributions called the exponential family exponential family\nprovides the right balance of generality while retaining favorable compu-\ntation and inference properties. Before we introduce the exponential fam-\nily , let us see three more members of \u201cnamed\u201d probability distributions,\nthe Bernoulli (Example 6.8), Binomial (Example 6.9), and Beta (Exam-\nple 6.10) distributions.\nExample 6.8\nThe Bernoulli distribution is a distribution for a single binary random Bernoulli\ndistributionvariable X with state x \u2208 {0, 1}. It is governed by a single contin"
  },
  {
    "vector_id": 799,
    "chunk_id": "p211_c3",
    "page_number": 211,
    "text": "-\nple 6.10) distributions.\nExample 6.8\nThe Bernoulli distribution is a distribution for a single binary random Bernoulli\ndistributionvariable X with state x \u2208 {0, 1}. It is governed by a single continuous pa-\nrameter \u00b5 \u2208 [0, 1] that represents the probability of X = 1. The Bernoulli\ndistribution Ber(\u00b5) is defined as\np(x |\u00b5) = \u00b5x(1 \u2212 \u00b5)1\u2212x , x \u2208 {0, 1}, (6.92)\nE[x] = \u00b5 , (6.93)\nV[x] = \u00b5(1 \u2212 \u00b5) , (6.94)\nwhere E[x] and V[x] are the mean and variance of the binary random\nvariable X.\nAn example where the Bernoulli distribution can be used is when we\nare interested in modeling the probability of \u201cheads\u201d when flipping a coin.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 800,
    "chunk_id": "p211_c4",
    "page_number": 211,
    "text": "ads\u201d when flipping a coin.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 801,
    "chunk_id": "p212_c0",
    "page_number": 212,
    "text": "206 Probability and Distributions\nFigure 6.10\nExamples of the\nBinomial\ndistribution for\n\u00b5 \u2208 {0.1, 0.4, 0.75}\nand N = 15.\n0.0 2.5 5.0 7.5 10.0 12.5 15.0\nNumber m of observations x = 1 in N = 15 experiments\n0.0\n0.1\n0.2\n0.3\np(m)\n\u00b5 = 0.1\n\u00b5 = 0.4\n\u00b5 = 0.75\nRemark. The rewriting above of the Bernoulli distribution, where we use\nBoolean variables as numerical 0 or 1 and express them in the exponents,\nis a trick that is often used in machine learning textbooks. Another oc-\ncurence of this is when expressing the Multinomial distribution. \u2662\nExample 6.9 (Binomial Distribution)\nThe Binomial distribution is a generalization of the Bernoulli distributionBinomial\ndistribution to a distribution over integers (illustrated in Figure 6.10). In particular,\nthe Binomial can be used to describe the probability o"
  },
  {
    "vector_id": 802,
    "chunk_id": "p212_c1",
    "page_number": 212,
    "text": "a generalization of the Bernoulli distributionBinomial\ndistribution to a distribution over integers (illustrated in Figure 6.10). In particular,\nthe Binomial can be used to describe the probability of observing m oc-\ncurrences of X = 1 in a set of N samples from a Bernoulli distribution\nwhere p(X = 1) = \u00b5 \u2208 [0, 1]. The Binomial distribution Bin (N, \u00b5) is\ndefined as\np(m |N, \u00b5) =\n \nN\nm\n!\n\u00b5m(1 \u2212 \u00b5)N\u2212m , (6.95)\nE[m] = N\u00b5 , (6.96)\nV[m] = N\u00b5(1 \u2212 \u00b5) , (6.97)\nwhere E[m] and V[m] are the mean and variance of m, respectively .\nAn example where the Binomial could be used is if we want to describe\nthe probability of observing m \u201cheads\u201d in N coin-flip experiments if the\nprobability for observing head in a single experiment is \u00b5.\nExample 6.10 (Beta Distribution)\nWe may wish to model a continuous random"
  },
  {
    "vector_id": 803,
    "chunk_id": "p212_c2",
    "page_number": 212,
    "text": "obability of observing m \u201cheads\u201d in N coin-flip experiments if the\nprobability for observing head in a single experiment is \u00b5.\nExample 6.10 (Beta Distribution)\nWe may wish to model a continuous random variable on a finite interval.\nThe Beta distribution is a distribution over a continuous random variableBeta distribution\n\u00b5 \u2208 [0, 1], which is often used to represent the probability for some binary\nevent (e.g., the parameter governing the Bernoulli distribution). The Beta\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 804,
    "chunk_id": "p213_c0",
    "page_number": 213,
    "text": "6.6 Conjugacy and the Exponential Family 207\ndistribution Beta(\u03b1, \u03b2) (illustrated in Figure 6.11) itself is governed by\ntwo parameters \u03b1 >0, \u03b2 >0 and is defined as\np(\u00b5 |\u03b1, \u03b2) = \u0393(\u03b1 + \u03b2)\n\u0393(\u03b1)\u0393(\u03b2)\u00b5\u03b1\u22121(1 \u2212 \u00b5)\u03b2\u22121 (6.98)\nE[\u00b5] = \u03b1\n\u03b1 + \u03b2 , V[\u00b5] = \u03b1\u03b2\n(\u03b1 + \u03b2)2(\u03b1 + \u03b2 + 1) (6.99)\nwhere \u0393(\u00b7) is the Gamma function defined as\n\u0393(t) :=\nZ \u221e\n0\nxt\u22121 exp(\u2212x)dx, t > 0 . (6.100)\n\u0393(t + 1) = t\u0393(t) . (6.101)\nNote that the fraction of Gamma functions in (6.98) normalizes the Beta\ndistribution.\nFigure 6.11\nExamples of the\nBeta distribution for\ndifferent values of \u03b1\nand \u03b2.\n0.0 0.2 0.4 0.6 0.8 1.0\n\u00b5\n0\n2\n4\n6\n8\n10p(\u00b5|\u03b1, \u03b2)\n\u03b1 = 0.5 = \u03b2\n\u03b1 = 1 = \u03b2\n\u03b1 = 2, \u03b2= 0.3\n\u03b1 = 4, \u03b2= 10\n\u03b1 = 5, \u03b2= 1\nIntuitively ,\u03b1 moves probability mass toward 1, whereas \u03b2 moves prob-\nability mass toward 0. There are some special cases (Murphy, 2012):\nF"
  },
  {
    "vector_id": 805,
    "chunk_id": "p213_c1",
    "page_number": 213,
    "text": "\u03b1 = 0.5 = \u03b2\n\u03b1 = 1 = \u03b2\n\u03b1 = 2, \u03b2= 0.3\n\u03b1 = 4, \u03b2= 10\n\u03b1 = 5, \u03b2= 1\nIntuitively ,\u03b1 moves probability mass toward 1, whereas \u03b2 moves prob-\nability mass toward 0. There are some special cases (Murphy, 2012):\nFor \u03b1 = 1 = \u03b2, we obtain the uniform distribution U[0, 1].\nFor \u03b1, \u03b2 <1, we get a bimodal distribution with spikes at 0 and 1.\nFor \u03b1, \u03b2 >1, the distribution is unimodal.\nFor \u03b1, \u03b2 >1 and \u03b1 = \u03b2, the distribution is unimodal, symmetric, and\ncentered in the interval [0, 1], i.e., the mode/mean is at 1\n2 .\nRemark. There is a whole zoo of distributions with names, and they are\nrelated in different ways to each other (Leemis and McQueston, 2008).\nIt is worth keeping in mind that each named distribution is created for a\nparticular reason, but may have other applications. Knowing the reason\nbehind the cr"
  },
  {
    "vector_id": 806,
    "chunk_id": "p213_c2",
    "page_number": 213,
    "text": "each other (Leemis and McQueston, 2008).\nIt is worth keeping in mind that each named distribution is created for a\nparticular reason, but may have other applications. Knowing the reason\nbehind the creation of a particular distribution often allows insight into\nhow to best use it. We introduced the preceding three distributions to be\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 807,
    "chunk_id": "p214_c0",
    "page_number": 214,
    "text": "208 Probability and Distributions\nable to illustrate the concepts of conjugacy (Section 6.6.1) and exponen-\ntial families (Section 6.6.3). \u2662\n6.6.1 Conjugacy\nAccording to Bayes\u2019 theorem (6.23), the posterior is proportional to the\nproduct of the prior and the likelihood. The specification of the prior can\nbe tricky for two reasons: First, the prior should encapsulate our knowl-\nedge about the problem before we see any data. This is often difficult to\ndescribe. Second, it is often not possible to compute the posterior distribu-\ntion analytically . However, there are some priors that are computationally\nconvenient: conjugate priors.conjugate prior\nDefinition 6.13 (Conjugate Prior). A prior is conjugate for the likelihoodconjugate\nfunction if the posterior is of the same form/type as the prior"
  },
  {
    "vector_id": 808,
    "chunk_id": "p214_c1",
    "page_number": 214,
    "text": "ionally\nconvenient: conjugate priors.conjugate prior\nDefinition 6.13 (Conjugate Prior). A prior is conjugate for the likelihoodconjugate\nfunction if the posterior is of the same form/type as the prior.\nConjugacy is particularly convenient because we can algebraically cal-\nculate our posterior distribution by updating the parameters of the prior\ndistribution.\nRemark. When considering the geometry of probability distributions, con-\njugate priors retain the same distance structure as the likelihood (Agarwal\nand Daum\u00b4e III, 2010). \u2662\nTo introduce a concrete example of conjugate priors, we describe in Ex-\nample 6.11 the Binomial distribution (defined on discrete random vari-\nables) and the Beta distribution (defined on continuous random vari-\nables).\nExample 6.11 (Beta-Binomial Conjugacy)\nConsid"
  },
  {
    "vector_id": 809,
    "chunk_id": "p214_c2",
    "page_number": 214,
    "text": "in Ex-\nample 6.11 the Binomial distribution (defined on discrete random vari-\nables) and the Beta distribution (defined on continuous random vari-\nables).\nExample 6.11 (Beta-Binomial Conjugacy)\nConsider a Binomial random variable x \u223c Bin(N, \u00b5) where\np(x |N, \u00b5) =\n \nN\nx\n!\n\u00b5x(1 \u2212 \u00b5)N\u2212x , x = 0, 1, . . . , N ,(6.102)\nis the probability of finding x times the outcome \u201cheads\u201d in N coin flips,\nwhere \u00b5 is the probability of a \u201chead\u201d. We place a Beta prior on the pa-\nrameter \u00b5, that is, \u00b5 \u223c Beta(\u03b1, \u03b2), where\np(\u00b5 |\u03b1, \u03b2) = \u0393(\u03b1 + \u03b2)\n\u0393(\u03b1)\u0393(\u03b2)\u00b5\u03b1\u22121(1 \u2212 \u00b5)\u03b2\u22121 . (6.103)\nIf we now observe some outcome x = h, that is, we see h heads in N coin\nflips, we compute the posterior distribution on \u00b5 as\np(\u00b5 |x = h, N, \u03b1, \u03b2) \u221d p(x |N, \u00b5)p(\u00b5 |\u03b1, \u03b2) (6.104a)\n\u221d \u00b5h(1 \u2212 \u00b5)(N\u2212h)\u00b5\u03b1\u22121(1 \u2212 \u00b5)\u03b2\u22121 (6.104b)\n= \u00b5h+\u03b1\u22121(1 \u2212 \u00b5)(N\u2212h)+\u03b2"
  },
  {
    "vector_id": 810,
    "chunk_id": "p214_c3",
    "page_number": 214,
    "text": "at is, we see h heads in N coin\nflips, we compute the posterior distribution on \u00b5 as\np(\u00b5 |x = h, N, \u03b1, \u03b2) \u221d p(x |N, \u00b5)p(\u00b5 |\u03b1, \u03b2) (6.104a)\n\u221d \u00b5h(1 \u2212 \u00b5)(N\u2212h)\u00b5\u03b1\u22121(1 \u2212 \u00b5)\u03b2\u22121 (6.104b)\n= \u00b5h+\u03b1\u22121(1 \u2212 \u00b5)(N\u2212h)+\u03b2\u22121 (6.104c)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 811,
    "chunk_id": "p215_c0",
    "page_number": 215,
    "text": "6.6 Conjugacy and the Exponential Family 209\nTable 6.2 Examples\nof conjugate priors\nfor common\nlikelihood functions.\nLikelihood Conjugate prior Posterior\nBernoulli Beta Beta\nBinomial Beta Beta\nGaussian Gaussian/inverse Gamma Gaussian/inverse Gamma\nGaussian Gaussian/inverse Wishart Gaussian/inverse Wishart\nMultinomial Dirichlet Dirichlet\n\u221d Beta(h + \u03b1, N\u2212 h + \u03b2) , (6.104d)\ni.e., the posterior distribution is a Beta distribution as the prior, i.e., the\nBeta prior is conjugate for the parameter \u00b5 in the Binomial likelihood\nfunction.\nIn the following example, we will derive a result that is similar to the\nBeta-Binomial conjugacy result. Here we will show that the Beta distribu-\ntion is a conjugate prior for the Bernoulli distribution.\nExample 6.12 (Beta-Bernoulli Conjugacy)\nLet x \u2208 {0, 1} be di"
  },
  {
    "vector_id": 812,
    "chunk_id": "p215_c1",
    "page_number": 215,
    "text": "to the\nBeta-Binomial conjugacy result. Here we will show that the Beta distribu-\ntion is a conjugate prior for the Bernoulli distribution.\nExample 6.12 (Beta-Bernoulli Conjugacy)\nLet x \u2208 {0, 1} be distributed according to the Bernoulli distribution with\nparameter \u03b8 \u2208 [0, 1], that is, p(x = 1 |\u03b8) = \u03b8. This can also be expressed\nas p(x |\u03b8) = \u03b8x(1 \u2212 \u03b8)1\u2212x. Let \u03b8 be distributed according to a Beta distri-\nbution with parameters \u03b1, \u03b2, that is, p(\u03b8 |\u03b1, \u03b2) \u221d \u03b8\u03b1\u22121(1 \u2212 \u03b8)\u03b2\u22121.\nMultiplying the Beta and the Bernoulli distributions, we get\np(\u03b8 |x, \u03b1, \u03b2) \u221d p(x |\u03b8)p(\u03b8 |\u03b1, \u03b2) (6.105a)\n= \u03b8x(1 \u2212 \u03b8)1\u2212x\u03b8\u03b1\u22121(1 \u2212 \u03b8)\u03b2\u22121 (6.105b)\n= \u03b8\u03b1+x\u22121(1 \u2212 \u03b8)\u03b2+(1\u2212x)\u22121 (6.105c)\n\u221d p(\u03b8 |\u03b1 + x, \u03b2+ (1 \u2212 x)) . (6.105d)\nThe last line is the Beta distribution with parameters (\u03b1 + x, \u03b2+ (1\u2212x)).\nTable 6.2 lists examples for conjugate p"
  },
  {
    "vector_id": 813,
    "chunk_id": "p215_c2",
    "page_number": 215,
    "text": "\u03b8)\u03b2\u22121 (6.105b)\n= \u03b8\u03b1+x\u22121(1 \u2212 \u03b8)\u03b2+(1\u2212x)\u22121 (6.105c)\n\u221d p(\u03b8 |\u03b1 + x, \u03b2+ (1 \u2212 x)) . (6.105d)\nThe last line is the Beta distribution with parameters (\u03b1 + x, \u03b2+ (1\u2212x)).\nTable 6.2 lists examples for conjugate priors for the parameters of some\nstandard likelihoods used in probabilistic modeling. Distributions such as The Gamma prior is\nconjugate for the\nprecision (inverse\nvariance) in the\nunivariate Gaussian\nlikelihood, and the\nWishart prior is\nconjugate for the\nprecision matrix\n(inverse covariance\nmatrix) in the\nmultivariate\nGaussian likelihood.\nMultinomial, inverse Gamma, inverse Wishart, and Dirichlet can be found\nin any statistical text, and are described in Bishop (2006), for example.\nThe Beta distribution is the conjugate prior for the parameter \u00b5 in both\nthe Binomial and the Bernoulli likeliho"
  },
  {
    "vector_id": 814,
    "chunk_id": "p215_c3",
    "page_number": 215,
    "text": "can be found\nin any statistical text, and are described in Bishop (2006), for example.\nThe Beta distribution is the conjugate prior for the parameter \u00b5 in both\nthe Binomial and the Bernoulli likelihood. For a Gaussian likelihood func-\ntion, we can place a conjugate Gaussian prior on the mean. The reason\nwhy the Gaussian likelihood appears twice in the table is that we need\nto distinguish the univariate from the multivariate case. In the univariate\n(scalar) case, the inverse Gamma is the conjugate prior for the variance.\nIn the multivariate case, we use a conjugate inverse Wishart distribution\nas a prior on the covariance matrix. The Dirichlet distribution is the conju-\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 815,
    "chunk_id": "p215_c4",
    "page_number": 215,
    "text": "as a prior on the covariance matrix. The Dirichlet distribution is the conju-\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 816,
    "chunk_id": "p216_c0",
    "page_number": 216,
    "text": "210 Probability and Distributions\ngate prior for the multinomial likelihood function. For further details, we\nrefer to Bishop (2006).\n6.6.2 Sufficient Statistics\nRecall that a statistic of a random variable is a deterministic function of\nthat random variable. For example, if x = [ x1, . . . , xN ]\u22a4 is a vector of\nunivariate Gaussian random variables, that is, xn \u223c N\n\u0000\n\u00b5, \u03c32\u0001\n, then the\nsample mean \u02c6\u00b5 = 1\nN (x1 + \u00b7 \u00b7\u00b7+ xN ) is a statistic. Sir Ronald Fisher dis-\ncovered the notion of sufficient statistics: the idea that there are statisticssufficient statistics\nthat will contain all available information that can be inferred from data\ncorresponding to the distribution under consideration. In other words, suf-\nficient statistics carry all the information needed to make inference about\nthe po"
  },
  {
    "vector_id": 817,
    "chunk_id": "p216_c1",
    "page_number": 216,
    "text": "nformation that can be inferred from data\ncorresponding to the distribution under consideration. In other words, suf-\nficient statistics carry all the information needed to make inference about\nthe population, that is, they are the statistics that are sufficient to repre-\nsent the distribution.\nFor a set of distributions parametrized by\u03b8, let X be a random variable\nwith distribution p(x |\u03b80) given an unknown \u03b80. A vector \u03d5(x) of statistics\nis called sufficient statistics for \u03b80 if they contain all possible informa-\ntion about \u03b80. To be more formal about \u201ccontain all possible information\u201d,\nthis means that the probability of x given \u03b8 can be factored into a part\nthat does not depend on \u03b8, and a part that depends on \u03b8 only via \u03d5(x).\nThe Fisher-Neyman factorization theorem formalizes this noti"
  },
  {
    "vector_id": 818,
    "chunk_id": "p216_c2",
    "page_number": 216,
    "text": "means that the probability of x given \u03b8 can be factored into a part\nthat does not depend on \u03b8, and a part that depends on \u03b8 only via \u03d5(x).\nThe Fisher-Neyman factorization theorem formalizes this notion, which\nwe state in Theorem 6.14 without proof.\nTheorem 6.14 (Fisher-Neyman). [Theorem 6.5 in Lehmann and Casella\n(1998)] Let X have probability density function p(x |\u03b8). Then the statisticsFisher-Neyman\ntheorem \u03d5(x) are sufficient for \u03b8 if and only if p(x |\u03b8) can be written in the form\np(x |\u03b8) = h(x)g\u03b8(\u03d5(x)) , (6.106)\nwhere h(x) is a distribution independent of \u03b8 and g\u03b8 captures all the depen-\ndence on \u03b8 via sufficient statistics \u03d5(x).\nIf p(x |\u03b8) does not depend on\u03b8, then \u03d5(x) is trivially a sufficient statistic\nfor any function \u03d5. The more interesting case is that p(x |\u03b8) is dependent\nonly"
  },
  {
    "vector_id": 819,
    "chunk_id": "p216_c3",
    "page_number": 216,
    "text": "dence on \u03b8 via sufficient statistics \u03d5(x).\nIf p(x |\u03b8) does not depend on\u03b8, then \u03d5(x) is trivially a sufficient statistic\nfor any function \u03d5. The more interesting case is that p(x |\u03b8) is dependent\nonly on \u03d5(x) and not x itself. In this case, \u03d5(x) is a sufficient statistic for\n\u03b8.\nIn machine learning, we consider a finite number of samples from a\ndistribution. One could imagine that for simple distributions (such as the\nBernoulli in Example 6.8) we only need a small number of samples to\nestimate the parameters of the distributions. We could also consider the\nopposite problem: If we have a set of data (a sample from an unknown\ndistribution), which distribution gives the best fit? A natural question to\nask is, as we observe more data, do we need more parameters \u03b8 to de-\nscribe the distribution?"
  },
  {
    "vector_id": 820,
    "chunk_id": "p216_c4",
    "page_number": 216,
    "text": "data (a sample from an unknown\ndistribution), which distribution gives the best fit? A natural question to\nask is, as we observe more data, do we need more parameters \u03b8 to de-\nscribe the distribution? It turns out that the answer is yes in general, and\nthis is studied in non-parametric statistics (Wasserman, 2007). A converse\nquestion is to consider which class of distributions have finite-dimensional\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 821,
    "chunk_id": "p217_c0",
    "page_number": 217,
    "text": "6.6 Conjugacy and the Exponential Family 211\nsufficient statistics, that is the number of parameters needed to describe\nthem does not increase arbitrarily . The answer is exponential family dis-\ntributions, described in the following section.\n6.6.3 Exponential Family\nThere are three possible levels of abstraction we can have when con-\nsidering distributions (of discrete or continuous random variables). At\nlevel one (the most concrete end of the spectrum), we have a particu-\nlar named distribution with fixed parameters, for example a univariate\nGaussian N\n\u0000\n0, 1\n\u0001\nwith zero mean and unit variance. In machine learning,\nwe often use the second level of abstraction, that is, we fix the paramet-\nric form (the univariate Gaussian) and infer the parameters from data. For\nexample, we assume a univ"
  },
  {
    "vector_id": 822,
    "chunk_id": "p217_c1",
    "page_number": 217,
    "text": "ce. In machine learning,\nwe often use the second level of abstraction, that is, we fix the paramet-\nric form (the univariate Gaussian) and infer the parameters from data. For\nexample, we assume a univariate GaussianN\n\u0000\n\u00b5, \u03c32\u0001\nwith unknown mean\n\u00b5 and unknown variance \u03c32, and use a maximum likelihood fit to deter-\nmine the best parameters (\u00b5, \u03c32). We will see an example of this when\nconsidering linear regression in Chapter 9. A third level of abstraction is\nto consider families of distributions, and in this book, we consider the ex-\nponential family . The univariate Gaussian is an example of a member of\nthe exponential family . Many of the widely used statistical models, includ-\ning all the \u201cnamed\u201d models in Table 6.2, are members of the exponential\nfamily . They can all be unified into one"
  },
  {
    "vector_id": 823,
    "chunk_id": "p217_c2",
    "page_number": 217,
    "text": "ember of\nthe exponential family . Many of the widely used statistical models, includ-\ning all the \u201cnamed\u201d models in Table 6.2, are members of the exponential\nfamily . They can all be unified into one concept (Brown, 1986).\nRemark. A brief historical anecdote: Like many concepts in mathemat-\nics and science, exponential families were independently discovered at\nthe same time by different researchers. In the years 1935\u20131936, Edwin\nPitman in Tasmania, Georges Darmois in Paris, and Bernard Koopman in\nNew York independently showed that the exponential families are the only\nfamilies that enjoy finite-dimensional sufficient statistics under repeated\nindependent sampling (Lehmann and Casella, 1998). \u2662\nAn exponential family is a family of probability distributions, parame- exponential family\nterize"
  },
  {
    "vector_id": 824,
    "chunk_id": "p217_c3",
    "page_number": 217,
    "text": "e-dimensional sufficient statistics under repeated\nindependent sampling (Lehmann and Casella, 1998). \u2662\nAn exponential family is a family of probability distributions, parame- exponential family\nterized by \u03b8 \u2208 RD, of the form\np(x|\u03b8) = h(x) exp (\u27e8\u03b8, \u03d5(x)\u27e9 \u2212A(\u03b8)) , (6.107)\nwhere \u03d5(x) is the vector of sufficient statistics. In general, any inner prod-\nuct (Section 3.2) can be used in (6.107), and for concreteness we will use\nthe standard dot product here (\u27e8\u03b8, \u03d5(x)\u27e9 = \u03b8\u22a4\u03d5(x)). Note that the form\nof the exponential family is essentially a particular expression of g\u03b8(\u03d5(x))\nin the Fisher-Neyman theorem (Theorem 6.14).\nThe factor h(x) can be absorbed into the dot product term by adding\nanother entry ( log h(x)) to the vector of sufficient statistics \u03d5(x), and\nconstraining the corresponding paramete"
  },
  {
    "vector_id": 825,
    "chunk_id": "p217_c4",
    "page_number": 217,
    "text": "m (Theorem 6.14).\nThe factor h(x) can be absorbed into the dot product term by adding\nanother entry ( log h(x)) to the vector of sufficient statistics \u03d5(x), and\nconstraining the corresponding parameter \u03b80 = 1. The term A(\u03b8) is the\nnormalization constant that ensures that the distribution sums up or inte-\ngrates to one and is called the log-partition function. A good intuitive no- log-partition\nfunctiontion of exponential families can be obtained by ignoring these two terms\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 826,
    "chunk_id": "p218_c0",
    "page_number": 218,
    "text": "212 Probability and Distributions\nand considering exponential families as distributions of the form\np(x|\u03b8) \u221d exp\n\u0000\n\u03b8\u22a4\u03d5(x)\n\u0001\n. (6.108)\nFor this form of parametrization, the parameters \u03b8 are called the naturalnatural parameters\nparameters. At first glance, it seems that exponential families are a mun-\ndane transformation by adding the exponential function to the result of a\ndot product. However, there are many implications that allow for conve-\nnient modeling and efficient computation based on the fact that we can\ncapture information about data in \u03d5(x).\nExample 6.13 (Gaussian as Exponential Family)\nConsider the univariate Gaussian distributionN\n\u0000\n\u00b5, \u03c32\u0001\n. Let \u03d5(x) =\n\u0014 x\nx2\n\u0015\n.\nThen by using the definition of the exponential family ,\np(x |\u03b8) \u221d exp(\u03b81x + \u03b82x2) . (6.109)\nSetting\n\u03b8 =\n\u0014 \u00b5\n\u03c32 , \u2212"
  },
  {
    "vector_id": 827,
    "chunk_id": "p218_c1",
    "page_number": 218,
    "text": "ly)\nConsider the univariate Gaussian distributionN\n\u0000\n\u00b5, \u03c32\u0001\n. Let \u03d5(x) =\n\u0014 x\nx2\n\u0015\n.\nThen by using the definition of the exponential family ,\np(x |\u03b8) \u221d exp(\u03b81x + \u03b82x2) . (6.109)\nSetting\n\u03b8 =\n\u0014 \u00b5\n\u03c32 , \u2212 1\n2\u03c32\n\u0015\u22a4\n(6.110)\nand substituting into (6.109), we obtain\np(x |\u03b8) \u221d exp\n\u0012\u00b5x\n\u03c32 \u2212 x2\n2\u03c32\n\u0013\n\u221d exp\n\u0012\n\u2212 1\n2\u03c32 (x \u2212 \u00b5)2\n\u0013\n. (6.111)\nTherefore, the univariate Gaussian distribution is a member of the expo-\nnential family with sufficient statistic \u03d5(x) =\n\u0014 x\nx2\n\u0015\n, and natural parame-\nters given by \u03b8 in (6.110).\nExample 6.14 (Bernoulli as Exponential Family)\nRecall the Bernoulli distribution from Example 6.8\np(x |\u00b5) = \u00b5x(1 \u2212 \u00b5)1\u2212x , x \u2208 {0, 1}. (6.112)\nThis can be written in exponential family form\np(x |\u00b5) = exp\n\u0002\nlog\n\u0000\n\u00b5x(1 \u2212 \u00b5)1\u2212x\u0001\u0003\n(6.113a)\n= exp [x log \u00b5 + (1 \u2212 x) log(1\u2212 \u00b5)] (6.113b)\n= exp [x log"
  },
  {
    "vector_id": 828,
    "chunk_id": "p218_c2",
    "page_number": 218,
    "text": "6.8\np(x |\u00b5) = \u00b5x(1 \u2212 \u00b5)1\u2212x , x \u2208 {0, 1}. (6.112)\nThis can be written in exponential family form\np(x |\u00b5) = exp\n\u0002\nlog\n\u0000\n\u00b5x(1 \u2212 \u00b5)1\u2212x\u0001\u0003\n(6.113a)\n= exp [x log \u00b5 + (1 \u2212 x) log(1\u2212 \u00b5)] (6.113b)\n= exp [x log \u00b5 \u2212 x log(1 \u2212 \u00b5) + log(1\u2212 \u00b5)] (6.113c)\n= exp\nh\nx log \u00b5\n1\u2212\u00b5 + log(1 \u2212 \u00b5)\ni\n. (6.113d)\nThe last line (6.113d) can be identified as being in exponential family\nform (6.107) by observing that\nh(x) = 1 (6.114)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 829,
    "chunk_id": "p219_c0",
    "page_number": 219,
    "text": "6.6 Conjugacy and the Exponential Family 213\n\u03b8 = log \u00b5\n1\u2212\u00b5 (6.115)\n\u03d5(x) = x (6.116)\nA(\u03b8) = \u2212log(1 \u2212 \u00b5) = log(1 + exp(\u03b8)). (6.117)\nThe relationship between \u03b8 and \u00b5 is invertible so that\n\u00b5 = 1\n1 + exp(\u2212\u03b8) . (6.118)\nThe relation (6.118) is used to obtain the right equality of (6.117).\nRemark. The relationship between the original Bernoulli parameter\u00b5 and\nthe natural parameter \u03b8 is known as the sigmoid or logistic function. Ob- sigmoid\nserve that \u00b5 \u2208 (0, 1) but \u03b8 \u2208 R, and therefore the sigmoid function\nsqueezes a real value into the range (0, 1). This property is useful in ma-\nchine learning, for example it is used in logistic regression (Bishop, 2006,\nsection 4.3.2), as well as as a nonlinear activation functions in neural net-\nworks (Goodfellow et al., 2016, chapter 6). \u2662\nIt is often not obv"
  },
  {
    "vector_id": 830,
    "chunk_id": "p219_c1",
    "page_number": 219,
    "text": "example it is used in logistic regression (Bishop, 2006,\nsection 4.3.2), as well as as a nonlinear activation functions in neural net-\nworks (Goodfellow et al., 2016, chapter 6). \u2662\nIt is often not obvious how to find the parametric form of the conjugate\ndistribution of a particular distribution (for example, those in Table 6.2).\nExponential families provide a convenient way to find conjugate pairs of\ndistributions. Consider the random variable X is a member of the expo-\nnential family (6.107):\np(x|\u03b8) = h(x) exp (\u27e8\u03b8, \u03d5(x)\u27e9 \u2212A(\u03b8)) . (6.119)\nEvery member of the exponential family has a conjugate prior (Brown,\n1986)\np(\u03b8 |\u03b3) = hc(\u03b8) exp\n\u0012\u001c\u0014\u03b31\n\u03b32\n\u0015\n,\n\u0014 \u03b8\n\u2212A(\u03b8)\n\u0015\u001d\n\u2212 Ac(\u03b3)\n\u0013\n, (6.120)\nwhere \u03b3 =\n\u0014\u03b31\n\u03b32\n\u0015\nhas dimension dim(\u03b8) + 1. The sufficient statistics of\nthe conjugate prior are\n\u0014 \u03b8\n\u2212A(\u03b8)\n\u0015\n. By"
  },
  {
    "vector_id": 831,
    "chunk_id": "p219_c2",
    "page_number": 219,
    "text": "prior (Brown,\n1986)\np(\u03b8 |\u03b3) = hc(\u03b8) exp\n\u0012\u001c\u0014\u03b31\n\u03b32\n\u0015\n,\n\u0014 \u03b8\n\u2212A(\u03b8)\n\u0015\u001d\n\u2212 Ac(\u03b3)\n\u0013\n, (6.120)\nwhere \u03b3 =\n\u0014\u03b31\n\u03b32\n\u0015\nhas dimension dim(\u03b8) + 1. The sufficient statistics of\nthe conjugate prior are\n\u0014 \u03b8\n\u2212A(\u03b8)\n\u0015\n. By using the knowledge of the general\nform of conjugate priors for exponential families, we can derive functional\nforms of conjugate priors corresponding to particular distributions.\nExample 6.15\nRecall the exponential family form of the Bernoulli distribution (6.113d)\np(x |\u00b5) = exp\n\u0014\nx log \u00b5\n1 \u2212 \u00b5 + log(1 \u2212 \u00b5)\n\u0015\n. (6.121)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 832,
    "chunk_id": "p219_c3",
    "page_number": 219,
    "text": "rsity Press (2020)."
  },
  {
    "vector_id": 833,
    "chunk_id": "p220_c0",
    "page_number": 220,
    "text": "214 Probability and Distributions\nThe canonical conjugate prior has the form\np(\u00b5 |\u03b1, \u03b2) = \u00b5\n1 \u2212 \u00b5 exp\n\u0014\n\u03b1 log \u00b5\n1 \u2212 \u00b5 + (\u03b2 + \u03b1) log(1\u2212 \u00b5) \u2212 Ac(\u03b3)\n\u0015\n,\n(6.122)\nwhere we defined \u03b3 := [ \u03b1, \u03b2+ \u03b1]\u22a4 and hc(\u00b5) := \u00b5/(1 \u2212 \u00b5). Equa-\ntion (6.122) then simplifies to\np(\u00b5 |\u03b1, \u03b2) = exp [(\u03b1 \u2212 1) log\u00b5 + (\u03b2 \u2212 1) log(1\u2212 \u00b5) \u2212 Ac(\u03b1, \u03b2)] .\n(6.123)\nPutting this in non-exponential family form yields\np(\u00b5 |\u03b1, \u03b2) \u221d \u00b5\u03b1\u22121(1 \u2212 \u00b5)\u03b2\u22121 , (6.124)\nwhich we identify as the Beta distribution (6.98). In example 6.12, we\nassumed that the Beta distribution is the conjugate prior of the Bernoulli\ndistribution and showed that it was indeed the conjugate prior. In this\nexample, we derived the form of the Beta distribution by looking at the\ncanonical conjugate prior of the Bernoulli distribution in exponential fam-\nily form.\nAs menti"
  },
  {
    "vector_id": 834,
    "chunk_id": "p220_c1",
    "page_number": 220,
    "text": "deed the conjugate prior. In this\nexample, we derived the form of the Beta distribution by looking at the\ncanonical conjugate prior of the Bernoulli distribution in exponential fam-\nily form.\nAs mentioned in the previous section, the main motivation for expo-\nnential families is that they have finite-dimensional sufficient statistics.\nAdditionally , conjugate distributions are easy to write down, and the con-\njugate distributions also come from an exponential family . From an infer-\nence perspective, maximum likelihood estimation behaves nicely because\nempirical estimates of sufficient statistics are optimal estimates of the pop-\nulation values of sufficient statistics (recall the mean and covariance of a\nGaussian). From an optimization perspective, the log-likelihood function\nis concave,"
  },
  {
    "vector_id": 835,
    "chunk_id": "p220_c2",
    "page_number": 220,
    "text": "ics are optimal estimates of the pop-\nulation values of sufficient statistics (recall the mean and covariance of a\nGaussian). From an optimization perspective, the log-likelihood function\nis concave, allowing for efficient optimization approaches to be applied\n(Chapter 7).\n6.7 Change of Variables/Inverse Transform\nIt may seem that there are very many known distributions, but in reality\nthe set of distributions for which we have names is quite limited. There-\nfore, it is often useful to understand how transformed random variables\nare distributed. For example, assuming that X is a random variable dis-\ntributed according to the univariate normal distribution N\n\u0000\n0, 1\n\u0001\n, what is\nthe distribution of X2? Another example, which is quite common in ma-\nchine learning, is, given that X1 and X2 are"
  },
  {
    "vector_id": 836,
    "chunk_id": "p220_c3",
    "page_number": 220,
    "text": "e dis-\ntributed according to the univariate normal distribution N\n\u0000\n0, 1\n\u0001\n, what is\nthe distribution of X2? Another example, which is quite common in ma-\nchine learning, is, given that X1 and X2 are univariate standard normal,\nwhat is the distribution of 1\n2 (X1 + X2)?\nOne option to work out the distribution of 1\n2 (X1 + X2) is to calculate\nthe mean and variance of X1 and X2 and then combine them. As we saw\nin Section 6.4.4, we can calculate the mean and variance of resulting ran-\ndom variables when we consider affine transformations of random vari-\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 837,
    "chunk_id": "p220_c4",
    "page_number": 220,
    "text": "ine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 838,
    "chunk_id": "p221_c0",
    "page_number": 221,
    "text": "6.7 Change of Variables/Inverse Transform 215\nables. However, we may not be able to obtain the functional form of the\ndistribution under transformations. Furthermore, we may be interested\nin nonlinear transformations of random variables for which closed-form\nexpressions are not readily available.\nRemark (Notation). In this section, we will be explicit about random vari-\nables and the values they take. Hence, recall that we use capital letters\nX, Yto denote random variables and small letters x, yto denote the val-\nues in the target spaceT that the random variables take. We will explicitly\nwrite pmfs of discrete random variables X as P(X = x). For continuous\nrandom variables X (Section 6.2.2), the pdf is written asf(x) and the cdf\nis written as FX(x). \u2662\nWe will look at two approaches for obt"
  },
  {
    "vector_id": 839,
    "chunk_id": "p221_c1",
    "page_number": 221,
    "text": "pmfs of discrete random variables X as P(X = x). For continuous\nrandom variables X (Section 6.2.2), the pdf is written asf(x) and the cdf\nis written as FX(x). \u2662\nWe will look at two approaches for obtaining distributions of transfor-\nmations of random variables: a direct approach using the definition of a\ncumulative distribution function and a change-of-variable approach that\nuses the chain rule of calculus (Section 5.2.2). The change-of-variable ap- Moment generating\nfunctions can also\nbe used to study\ntransformations of\nrandom\nvariables (Casella\nand Berger, 2002,\nchapter 2).\nproach is widely used because it provides a \u201crecipe\u201d for attempting to\ncompute the resulting distribution due to a transformation. We will ex-\nplain the techniques for univariate random variables, and will only brief"
  },
  {
    "vector_id": 840,
    "chunk_id": "p221_c2",
    "page_number": 221,
    "text": "used because it provides a \u201crecipe\u201d for attempting to\ncompute the resulting distribution due to a transformation. We will ex-\nplain the techniques for univariate random variables, and will only briefly\nprovide the results for the general case of multivariate random variables.\nTransformations of discrete random variables can be understood di-\nrectly . Suppose that there is a discrete random variableX with pmf P(X =\nx) (Section 6.2.1), and an invertible function U(x). Consider the trans-\nformed random variable Y := U(X), with pmf P(Y = y). Then\nP(Y = y) = P(U(X) = y) transformation of interest (6.125a)\n= P(X = U\u22121(y)) inverse (6.125b)\nwhere we can observe that x = U\u22121(y). Therefore, for discrete random\nvariables, transformations directly change the individual events (with the\nprobabilities"
  },
  {
    "vector_id": 841,
    "chunk_id": "p221_c3",
    "page_number": 221,
    "text": "(6.125a)\n= P(X = U\u22121(y)) inverse (6.125b)\nwhere we can observe that x = U\u22121(y). Therefore, for discrete random\nvariables, transformations directly change the individual events (with the\nprobabilities appropriately transformed).\n6.7.1 Distribution Function Technique\nThe distribution function technique goes back to first principles, and uses\nthe definition of a cdf FX(x) = P(X \u2a7d x) and the fact that its differential\nis the pdf f(x) (Wasserman, 2004, chapter 2). For a random variable X\nand a function U, we find the pdf of the random variable Y := U(X) by\n1. Finding the cdf:\nFY (y) = P(Y \u2a7d y) (6.126)\n2. Differentiating the cdf FY (y) to get the pdf f(y).\nf(y) = d\ndyFY (y) . (6.127)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 842,
    "chunk_id": "p221_c4",
    "page_number": 221,
    "text": "26)\n2. Differentiating the cdf FY (y) to get the pdf f(y).\nf(y) = d\ndyFY (y) . (6.127)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 843,
    "chunk_id": "p222_c0",
    "page_number": 222,
    "text": "216 Probability and Distributions\nWe also need to keep in mind that the domain of the random variable may\nhave changed due to the transformation by U.\nExample 6.16\nLet X be a continuous random variable with probability density function\non 0 \u2a7d x \u2a7d 1\nf(x) = 3x2 . (6.128)\nWe are interested in finding the pdf of Y = X2.\nThe function f is an increasing function ofx, and therefore the resulting\nvalue of y lies in the interval [0, 1]. We obtain\nFY (y) = P(Y \u2a7d y) definition of cdf (6.129a)\n= P(X2 \u2a7d y) transformation of interest (6.129b)\n= P(X \u2a7d y\n1\n2 ) inverse (6.129c)\n= FX(y\n1\n2 ) definition of cdf (6.129d)\n=\nZ y\n1\n2\n0\n3t2dt cdf as a definite integral (6.129e)\n=\n\u0002\nt3\u0003t=y\n1\n2\nt=0 result of integration (6.129f)\n= y\n3\n2 , 0 \u2a7d y \u2a7d 1 . (6.129g)\nTherefore, the cdf of Y is\nFY (y) = y\n3\n2 (6.130)\nfor 0 \u2a7d"
  },
  {
    "vector_id": 844,
    "chunk_id": "p222_c1",
    "page_number": 222,
    "text": "6.129d)\n=\nZ y\n1\n2\n0\n3t2dt cdf as a definite integral (6.129e)\n=\n\u0002\nt3\u0003t=y\n1\n2\nt=0 result of integration (6.129f)\n= y\n3\n2 , 0 \u2a7d y \u2a7d 1 . (6.129g)\nTherefore, the cdf of Y is\nFY (y) = y\n3\n2 (6.130)\nfor 0 \u2a7d y \u2a7d 1. To obtain the pdf, we differentiate the cdf\nf(y) = d\ndyFY (y) = 3\n2y\n1\n2 (6.131)\nfor 0 \u2a7d y \u2a7d 1.\nIn Example 6.16, we considered a strictly monotonically increasing func-\ntion f(x) = 3x2. This means that we could compute an inverse function.Functions that have\ninverses are called\nbijective functions\n(Section 2.7).\nIn general, we require that the function of interest y = U(x) has an in-\nverse x = U\u22121(y). A useful result can be obtained by considering the cu-\nmulative distribution function FX(x) of a random variable X, and using\nit as the transformation U(x). This leads to the following th"
  },
  {
    "vector_id": 845,
    "chunk_id": "p222_c2",
    "page_number": 222,
    "text": "x = U\u22121(y). A useful result can be obtained by considering the cu-\nmulative distribution function FX(x) of a random variable X, and using\nit as the transformation U(x). This leads to the following theorem.\nTheorem 6.15. [Theorem 2.1.10 in Casella and Berger (2002)] Let X be a\ncontinuous random variable with a strictly monotonic cumulative distribu-\ntion function FX(x). Then the random variable Y defined as\nY := FX(X) (6.132)\nhas a uniform distribution.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 846,
    "chunk_id": "p223_c0",
    "page_number": 223,
    "text": "6.7 Change of Variables/Inverse Transform 217\nTheorem 6.15 is known as the probability integral transform , and it is probability integral\ntransformused to derive algorithms for sampling from distributions by transforming\nthe result of sampling from a uniform random variable (Bishop, 2006).\nThe algorithm works by first generating a sample from a uniform distribu-\ntion, then transforming it by the inverse cdf (assuming this is available)\nto obtain a sample from the desired distribution. The probability integral\ntransform is also used for hypothesis testing whether a sample comes from\na particular distribution (Lehmann and Romano, 2005). The idea that the\noutput of a cdf gives a uniform distribution also forms the basis of copu-\nlas (Nelsen, 2006).\n6.7.2 Change of Variables\nThe distribution"
  },
  {
    "vector_id": 847,
    "chunk_id": "p223_c1",
    "page_number": 223,
    "text": "ar distribution (Lehmann and Romano, 2005). The idea that the\noutput of a cdf gives a uniform distribution also forms the basis of copu-\nlas (Nelsen, 2006).\n6.7.2 Change of Variables\nThe distribution function technique in Section 6.7.1 is derived from first\nprinciples, based on the definitions of cdfs and using properties of in-\nverses, differentiation, and integration. This argument from first principles\nrelies on two facts:\n1. We can transform the cdf of Y into an expression that is a cdf of X.\n2. We can differentiate the cdf to obtain the pdf.\nLet us break down the reasoning step by step, with the goal of understand-\ning the more general change-of-variables approach in Theorem 6.16. Change of variables\nin probability relies\non the\nchange-of-variables\nmethod in\ncalculus (Tandra,\n2014).\nR"
  },
  {
    "vector_id": 848,
    "chunk_id": "p223_c2",
    "page_number": 223,
    "text": "ith the goal of understand-\ning the more general change-of-variables approach in Theorem 6.16. Change of variables\nin probability relies\non the\nchange-of-variables\nmethod in\ncalculus (Tandra,\n2014).\nRemark. The name \u201cchange of variables\u201d comes from the idea of chang-\ning the variable of integration when faced with a difficult integral. For\nunivariate functions, we use the substitution rule of integration,\nZ\nf(g(x))g\u2032(x)dx =\nZ\nf(u)du , where u = g(x) . (6.133)\nThe derivation of this rule is based on the chain rule of calculus (5.32) and\nby applying twice the fundamental theorem of calculus. The fundamental\ntheorem of calculus formalizes the fact that integration and differentiation\nare somehow \u201cinverses\u201d of each other. An intuitive understanding of the\nrule can be obtained by thinking (loos"
  },
  {
    "vector_id": 849,
    "chunk_id": "p223_c3",
    "page_number": 223,
    "text": "fundamental\ntheorem of calculus formalizes the fact that integration and differentiation\nare somehow \u201cinverses\u201d of each other. An intuitive understanding of the\nrule can be obtained by thinking (loosely) about small changes (differen-\ntials) to the equation u = g(x), that is by considering \u2206u = g\u2032(x)\u2206x as a\ndifferential of u = g(x). By substituting u = g(x), the argument inside the\nintegral on the right-hand side of (6.133) becomesf(g(x)). By pretending\nthat the term du can be approximated by du \u2248 \u2206u = g\u2032(x)\u2206x, and that\ndx \u2248 \u2206x, we obtain (6.133). \u2662\nConsider a univariate random variable X, and an invertible function\nU, which gives us another random variable Y = U(X). We assume that\nrandom variable X has states x \u2208 [a, b]. By the definition of the cdf, we\nhave\nFY (y) = P(Y \u2a7d y) . (6.134)\n\u00a9"
  },
  {
    "vector_id": 850,
    "chunk_id": "p223_c4",
    "page_number": 223,
    "text": "d an invertible function\nU, which gives us another random variable Y = U(X). We assume that\nrandom variable X has states x \u2208 [a, b]. By the definition of the cdf, we\nhave\nFY (y) = P(Y \u2a7d y) . (6.134)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 851,
    "chunk_id": "p224_c0",
    "page_number": 224,
    "text": "218 Probability and Distributions\nWe are interested in a function U of the random variable\nP(Y \u2a7d y) = P(U(X) \u2a7d y) , (6.135)\nwhere we assume that the function U is invertible. An invertible function\non an interval is either strictly increasing or strictly decreasing. In the case\nthat U is strictly increasing, then its inverse U\u22121 is also strictly increasing.\nBy applying the inverseU\u22121 to the arguments ofP(U(X) \u2a7d y), we obtain\nP(U(X) \u2a7d y) = P(U\u22121(U(X)) \u2a7d U\u22121(y)) = P(X \u2a7d U\u22121(y)) .\n(6.136)\nThe right-most term in (6.136) is an expression of the cdf ofX. Recall the\ndefinition of the cdf in terms of the pdf\nP(X \u2a7d U\u22121(y)) =\nZ U\u22121(y)\na\nf(x)dx . (6.137)\nNow we have an expression of the cdf of Y in terms of x:\nFY (y) =\nZ U\u22121(y)\na\nf(x)dx . (6.138)\nTo obtain the pdf, we differentiate (6.138) with respe"
  },
  {
    "vector_id": 852,
    "chunk_id": "p224_c1",
    "page_number": 224,
    "text": "the pdf\nP(X \u2a7d U\u22121(y)) =\nZ U\u22121(y)\na\nf(x)dx . (6.137)\nNow we have an expression of the cdf of Y in terms of x:\nFY (y) =\nZ U\u22121(y)\na\nf(x)dx . (6.138)\nTo obtain the pdf, we differentiate (6.138) with respect to y:\nf(y) = d\ndyFy(y) = d\ndy\nZ U\u22121(y)\na\nf(x)dx . (6.139)\nNote that the integral on the right-hand side is with respect to x, but we\nneed an integral with respect to y because we are differentiating with\nrespect to y. In particular, we use (6.133) to get the substitution\nZ\nf(U\u22121(y))U\u22121\u2032\n(y)dy =\nZ\nf(x)dx where x = U\u22121(y) . (6.140)\nUsing (6.140) on the right-hand side of (6.139) gives us\nf(y) = d\ndy\nZ U\u22121(y)\na\nfx(U\u22121(y))U\u22121\u2032\n(y)dy . (6.141)\nWe then recall that differentiation is a linear operator and we use the\nsubscript x to remind ourselves that fx(U\u22121(y)) is a function of x and not\ny. Invo"
  },
  {
    "vector_id": 853,
    "chunk_id": "p224_c2",
    "page_number": 224,
    "text": "dy\nZ U\u22121(y)\na\nfx(U\u22121(y))U\u22121\u2032\n(y)dy . (6.141)\nWe then recall that differentiation is a linear operator and we use the\nsubscript x to remind ourselves that fx(U\u22121(y)) is a function of x and not\ny. Invoking the fundamental theorem of calculus again gives us\nf(y) = fx(U\u22121(y)) \u00b7\n\u0012 d\ndy U\u22121(y)\n\u0013\n. (6.142)\nRecall that we assumed thatU is a strictly increasing function. For decreas-\ning functions, it turns out that we have a negative sign when we follow\nthe same derivation. We introduce the absolute value of the differential to\nhave the same expression for both increasing and decreasing U:\nf(y) = fx(U\u22121(y)) \u00b7\n\f\f\f\f\nd\ndy U\u22121(y)\n\f\f\f\f . (6.143)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 854,
    "chunk_id": "p224_c3",
    "page_number": 224,
    "text": "U\u22121(y)) \u00b7\n\f\f\f\f\nd\ndy U\u22121(y)\n\f\f\f\f . (6.143)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 855,
    "chunk_id": "p225_c0",
    "page_number": 225,
    "text": "6.7 Change of Variables/Inverse Transform 219\nThis is called the change-of-variable technique . The term\n\f\f\f d\ndy U\u22121(y)\n\f\f\f in change-of-variable\ntechnique(6.143) measures how much a unit volume changes when applying U\n(see also the definition of the Jacobian in Section 5.3).\nRemark. In comparison to the discrete case in (6.125b), we have an addi-\ntional factor\n\f\f\f d\ndy U\u22121(y)\n\f\f\f. The continuous case requires more care because\nP(Y = y) = 0 for all y. The probability density function f(y) does not\nhave a description as a probability of an event involving y. \u2662\nSo far in this section, we have been studying univariate change of vari-\nables. The case for multivariate random variables is analogous, but com-\nplicated by fact that the absolute value cannot be used for multivariate\nfunctions. Ins"
  },
  {
    "vector_id": 856,
    "chunk_id": "p225_c1",
    "page_number": 225,
    "text": "een studying univariate change of vari-\nables. The case for multivariate random variables is analogous, but com-\nplicated by fact that the absolute value cannot be used for multivariate\nfunctions. Instead, we use the determinant of the Jacobian matrix. Recall\nfrom (5.58) that the Jacobian is a matrix of partial derivatives, and that\nthe existence of a nonzero determinant shows that we can invert the Ja-\ncobian. Recall the discussion in Section 4.1 that the determinant arises\nbecause our differentials (cubes of volume) are transformed into paral-\nlelepipeds by the Jacobian. Let us summarize preceding the discussion in\nthe following theorem, which gives us a recipe for multivariate change of\nvariables.\nTheorem 6.16. [Theorem 17.2 in Billingsley (1995)] Let f(x) be the value\nof the probabilit"
  },
  {
    "vector_id": 857,
    "chunk_id": "p225_c2",
    "page_number": 225,
    "text": "ceding the discussion in\nthe following theorem, which gives us a recipe for multivariate change of\nvariables.\nTheorem 6.16. [Theorem 17.2 in Billingsley (1995)] Let f(x) be the value\nof the probability density of the multivariate continuous random variableX.\nIf the vector-valued function y = U(x) is differentiable and invertible for\nall values within the domain of x, then for corresponding values of y, the\nprobability density of Y = U(X) is given by\nf(y) = fx(U\u22121(y)) \u00b7\n\f\f\f\fdet\n\u0012 \u2202\n\u2202yU\u22121(y)\n\u0013\f\f\f\f. (6.144)\nThe theorem looks intimidating at first glance, but the key point is that\na change of variable of a multivariate random variable follows the pro-\ncedure of the univariate change of variable. First we need to work out\nthe inverse transform, and substitute that into the density of x. Then we"
  },
  {
    "vector_id": 858,
    "chunk_id": "p225_c3",
    "page_number": 225,
    "text": "able of a multivariate random variable follows the pro-\ncedure of the univariate change of variable. First we need to work out\nthe inverse transform, and substitute that into the density of x. Then we\ncalculate the determinant of the Jacobian and multiply the result. The\nfollowing example illustrates the case of a bivariate random variable.\nExample 6.17\nConsider a bivariate random variable X with states x =\n\u0014x1\nx2\n\u0015\nand proba-\nbility density function\nf\n\u0012\u0014x1\nx2\n\u0015\u0013\n= 1\n2\u03c0 exp\n \n\u22121\n2\n\u0014x1\nx2\n\u0015\u22a4 \u0014x1\nx2\n\u0015!\n. (6.145)\nWe use the change-of-variable technique from Theorem 6.16 to derive the\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 859,
    "chunk_id": "p225_c4",
    "page_number": 225,
    "text": "Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 860,
    "chunk_id": "p226_c0",
    "page_number": 226,
    "text": "220 Probability and Distributions\neffect of a linear transformation (Section 2.7) of the random variable.\nConsider a matrix A \u2208 R2\u00d72 defined as\nA =\n\u0014a b\nc d\n\u0015\n. (6.146)\nWe are interested in finding the probability density function of the trans-\nformed bivariate random variable Y with states y = Ax.\nRecall that for change of variables we require the inverse transformation\nof x as a function of y. Since we consider linear transformations, the\ninverse transformation is given by the matrix inverse (see Section 2.2.2).\nFor 2 \u00d7 2 matrices, we can explicitly write out the formula, given by\n\u0014x1\nx2\n\u0015\n= A\u22121\n\u0014y1\ny2\n\u0015\n= 1\nad \u2212 bc\n\u0014 d \u2212b\n\u2212c a\n\u0015\u0014y1\ny2\n\u0015\n. (6.147)\nObserve that ad \u2212 bc is the determinant (Section 4.1) of A. The corre-\nsponding probability density function is given by\nf(x) = f(A\u22121y) = 1\n2\u03c0"
  },
  {
    "vector_id": 861,
    "chunk_id": "p226_c1",
    "page_number": 226,
    "text": "A\u22121\n\u0014y1\ny2\n\u0015\n= 1\nad \u2212 bc\n\u0014 d \u2212b\n\u2212c a\n\u0015\u0014y1\ny2\n\u0015\n. (6.147)\nObserve that ad \u2212 bc is the determinant (Section 4.1) of A. The corre-\nsponding probability density function is given by\nf(x) = f(A\u22121y) = 1\n2\u03c0 exp\n\u0010\n\u22121\n2 y\u22a4A\u2212\u22a4A\u22121y\n\u0011\n. (6.148)\nThe partial derivative of a matrix times a vector with respect to the vector\nis the matrix itself (Section 5.5), and therefore\n\u2202\n\u2202yA\u22121y = A\u22121 . (6.149)\nRecall from Section 4.1 that the determinant of the inverse is the inverse\nof the determinant so that the determinant of the Jacobian matrix is\ndet\n\u0012 \u2202\n\u2202yA\u22121y\n\u0013\n= 1\nad \u2212 bc . (6.150)\nWe are now able to apply the change-of-variable formula from Theo-\nrem 6.16 by multiplying (6.148) with (6.150), which yields\nf(y) = f(x)\n\f\f\f\fdet\n\u0012 \u2202\n\u2202yA\u22121y\n\u0013\f\f\f\f (6.151a)\n= 1\n2\u03c0 exp\n\u0010\n\u22121\n2 y\u22a4A\u2212\u22a4A\u22121y\n\u0011\n|ad \u2212 bc|\u22121. (6.151b)\nWhile E"
  },
  {
    "vector_id": 862,
    "chunk_id": "p226_c2",
    "page_number": 226,
    "text": "ange-of-variable formula from Theo-\nrem 6.16 by multiplying (6.148) with (6.150), which yields\nf(y) = f(x)\n\f\f\f\fdet\n\u0012 \u2202\n\u2202yA\u22121y\n\u0013\f\f\f\f (6.151a)\n= 1\n2\u03c0 exp\n\u0010\n\u22121\n2 y\u22a4A\u2212\u22a4A\u22121y\n\u0011\n|ad \u2212 bc|\u22121. (6.151b)\nWhile Example 6.17 is based on a bivariate random variable, which al-\nlows us to easily compute the matrix inverse, the preceding relation holds\nfor higher dimensions.\nRemark. We saw in Section 6.5 that the densityf(x) in (6.148) is actually\nthe standard Gaussian distribution, and the transformed density f(y) is a\nbivariate Gaussian with covariance \u03a3 = AA\u22a4. \u2662\nWe will use the ideas in this chapter to describe probabilistic modeling\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 863,
    "chunk_id": "p226_c3",
    "page_number": 226,
    "text": "ribe probabilistic modeling\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 864,
    "chunk_id": "p227_c0",
    "page_number": 227,
    "text": "6.8 Further Reading 221\nin Section 8.4, as well as introduce a graphical language in Section 8.5. We\nwill see direct machine learning applications of these ideas in Chapters 9\nand 11.\n6.8 Further Reading\nThis chapter is rather terse at times. Grinstead and Snell (1997) and\nWalpole et al. (2011) provide more relaxed presentations that are suit-\nable for self-study . Readers interested in more philosophical aspects of\nprobability should consider Hacking (2001), whereas an approach that\nis more related to software engineering is presented by Downey (2014).\nAn overview of exponential families can be found in Barndorff-Nielsen\n(2014). We will see more about how to use probability distributions to\nmodel machine learning tasks in Chapter 8. Ironically , the recent surge\nin interest in neural netw"
  },
  {
    "vector_id": 865,
    "chunk_id": "p227_c1",
    "page_number": 227,
    "text": "be found in Barndorff-Nielsen\n(2014). We will see more about how to use probability distributions to\nmodel machine learning tasks in Chapter 8. Ironically , the recent surge\nin interest in neural networks has resulted in a broader appreciation of\nprobabilistic models. For example, the idea of normalizing flows (Jimenez\nRezende and Mohamed, 2015) relies on change of variables for transform-\ning random variables. An overview of methods for variational inference as\napplied to neural networks is described in chapters 16 to 20 of the book\nby Goodfellow et al. (2016).\nWe side stepped a large part of the difficulty in continuous random vari-\nables by avoiding measure theoretic questions (Billingsley, 1995; Pollard,\n2002), and by assuming without construction that we have real numbers,\nand ways of"
  },
  {
    "vector_id": 866,
    "chunk_id": "p227_c2",
    "page_number": 227,
    "text": "the difficulty in continuous random vari-\nables by avoiding measure theoretic questions (Billingsley, 1995; Pollard,\n2002), and by assuming without construction that we have real numbers,\nand ways of defining sets on real numbers as well as their appropriate fre-\nquency of occurrence. These details do matter, for example, in the specifi-\ncation of conditional probability p(y |x) for continuous random variables\nx, y(Proschan and Presnell, 1998). The lazy notation hides the fact that\nwe want to specify that X = x (which is a set of measure zero). Fur-\nthermore, we are interested in the probability density function of y. A\nmore precise notation would have to say Ey[f(y) |\u03c3(x)], where we take\nthe expectation over y of a test function f conditioned on the \u03c3-algebra of\nx. A more technical audie"
  },
  {
    "vector_id": 867,
    "chunk_id": "p227_c3",
    "page_number": 227,
    "text": "ity density function of y. A\nmore precise notation would have to say Ey[f(y) |\u03c3(x)], where we take\nthe expectation over y of a test function f conditioned on the \u03c3-algebra of\nx. A more technical audience interested in the details of probability the-\nory have many options (Jaynes, 2003; MacKay, 2003; Jacod and Protter,\n2004; Grimmett and Welsh, 2014), including some very technical discus-\nsions (Shiryayev, 1984; Lehmann and Casella, 1998; Dudley, 2002; Bickel\nand Doksum, 2006; C \u00b8inlar, 2011). An alternative way to approach proba-\nbility is to start with the concept of expectation, and \u201cwork backward\u201d to\nderive the necessary properties of a probability space (Whittle, 2000). As\nmachine learning allows us to model more intricate distributions on ever\nmore complex types of data, a developer o"
  },
  {
    "vector_id": 868,
    "chunk_id": "p227_c4",
    "page_number": 227,
    "text": "ckward\u201d to\nderive the necessary properties of a probability space (Whittle, 2000). As\nmachine learning allows us to model more intricate distributions on ever\nmore complex types of data, a developer of probabilistic machine learn-\ning models would have to understand these more technical aspects. Ma-\nchine learning texts with a probabilistic modeling focus include the books\nby MacKay (2003); Bishop (2006); Rasmussen and Williams (2006); Bar-\nber (2012); Murphy (2012).\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 869,
    "chunk_id": "p228_c0",
    "page_number": 228,
    "text": "222 Probability and Distributions\nExercises\n6.1 Consider the following bivariate distribution p(x, y) of two discrete random\nvariables X and Y .\nX\nx1 x2 x3 x4 x5\nY\ny3\ny2\ny1 0.010.020.030.1 0.1\n0.050.1 0.050.070.2\n0.1 0.050.030.050.04\nCompute:\na. The marginal distributions p(x) and p(y).\nb. The conditional distributions p(x|Y = y1) and p(y|X = x3).\n6.2 Consider a mixture of two Gaussian distributions (illustrated in Figure 6.4),\n0.4 N\n\u0012\u0014\n10\n2\n\u0015\n,\n\u0014\n1 0\n0 1\n\u0015\u0013\n+ 0.6 N\n\u0012\u0014\n0\n0\n\u0015\n,\n\u0014\n8.4 2 .0\n2.0 1 .7\n\u0015\u0013\n.\na. Compute the marginal distributions for each dimension.\nb. Compute the mean, mode and median for each marginal distribution.\nc. Compute the mean and mode for the two-dimensional distribution.\n6.3 You have written a computer program that sometimes compiles and some-\ntimes not (code does not"
  },
  {
    "vector_id": 870,
    "chunk_id": "p228_c1",
    "page_number": 228,
    "text": "n for each marginal distribution.\nc. Compute the mean and mode for the two-dimensional distribution.\n6.3 You have written a computer program that sometimes compiles and some-\ntimes not (code does not change). You decide to model the apparent stochas-\nticity (success vs. no success)x of the compiler using a Bernoulli distribution\nwith parameter \u00b5:\np(x |\u00b5) = \u00b5x(1 \u2212 \u00b5)1\u2212x , x \u2208 {0, 1}.\nChoose a conjugate prior for the Bernoulli likelihood and compute the pos-\nterior distribution p(\u00b5 |x1, . . . , xN ).\n6.4 There are two bags. The first bag contains four mangos and two apples; the\nsecond bag contains four mangos and four apples.\nWe also have a biased coin, which shows \u201cheads\u201d with probability 0.6 and\n\u201ctails\u201d with probability 0.4. If the coin shows \u201cheads\u201d. we pick a fruit at\nrandom from bag 1;"
  },
  {
    "vector_id": 871,
    "chunk_id": "p228_c2",
    "page_number": 228,
    "text": "ns four mangos and four apples.\nWe also have a biased coin, which shows \u201cheads\u201d with probability 0.6 and\n\u201ctails\u201d with probability 0.4. If the coin shows \u201cheads\u201d. we pick a fruit at\nrandom from bag 1; otherwise we pick a fruit at random from bag 2.\nYour friend flips the coin (you cannot see the result), picks a fruit at random\nfrom the corresponding bag, and presents you a mango.\nWhat is the probability that the mango was picked from bag 2?\nHint: Use Bayes\u2019 theorem.\n6.5 Consider the time-series model\nxt+1 = Axt + w , w \u223c N\n\u0000\n0, Q\n\u0001\nyt = Cxt + v , v \u223c N\n\u0000\n0, R\n\u0001\n,\nwhere w, v are i.i.d. Gaussian noise variables. Further, assume that p(x0) =\nN\n\u0000\n\u00b50, \u03a30\n\u0001.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 872,
    "chunk_id": "p228_c3",
    "page_number": 228,
    "text": "noise variables. Further, assume that p(x0) =\nN\n\u0000\n\u00b50, \u03a30\n\u0001.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 873,
    "chunk_id": "p229_c0",
    "page_number": 229,
    "text": "Exercises 223\na. What is the form of p(x0, x1, . . . ,xT )? Justify your answer (you do not\nhave to explicitly compute the joint distribution).\nb. Assume that p(xt |y1, . . . ,yt) = N\n\u0000\n\u00b5t, \u03a3t\n\u0001.\n1. Compute p(xt+1 |y1, . . . ,yt).\n2. Compute p(xt+1, yt+1 |y1, . . . ,yt).\n3. At time t+1, we observe the valueyt+1 = \u02c6y. Compute the conditional\ndistribution p(xt+1 |y1, . . . ,yt+1).\n6.6 Prove the relationship in (6.44), which relates the standard definition of the\nvariance to the raw-score expression for the variance.\n6.7 Prove the relationship in (6.45), which relates the pairwise difference be-\ntween examples in a dataset with the raw-score expression for the variance.\n6.8 Express the Bernoulli distribution in the natural parameter form of the ex-\nponential family , see (6.107).\n6.9 Express"
  },
  {
    "vector_id": 874,
    "chunk_id": "p229_c1",
    "page_number": 229,
    "text": "tween examples in a dataset with the raw-score expression for the variance.\n6.8 Express the Bernoulli distribution in the natural parameter form of the ex-\nponential family , see (6.107).\n6.9 Express the Binomial distribution as an exponential family distribution. Also\nexpress the Beta distribution is an exponential family distribution. Show that\nthe product of the Beta and the Binomial distribution is also a member of\nthe exponential family .\n6.10 Derive the relationship in Section 6.5.2 in two ways:\na. By completing the square\nb. By expressing the Gaussian in its exponential family form\nThe product of two Gaussians N\n\u0000\nx|a, A\n\u0001\nN\n\u0000\nx|b, B\n\u0001 is an unnormalized\nGaussian distribution c N\n\u0000\nx|c, C\n\u0001 with\nC = (A\u22121 + B\u22121)\u22121\nc = C(A\u22121a + B\u22121b)\nc = (2\u03c0)\u2212D\n2 |A + B |\u22121\n2 exp\n\u0000\n\u2212 1\n2 (a \u2212 b)\u22a4(A +"
  },
  {
    "vector_id": 875,
    "chunk_id": "p229_c2",
    "page_number": 229,
    "text": "product of two Gaussians N\n\u0000\nx|a, A\n\u0001\nN\n\u0000\nx|b, B\n\u0001 is an unnormalized\nGaussian distribution c N\n\u0000\nx|c, C\n\u0001 with\nC = (A\u22121 + B\u22121)\u22121\nc = C(A\u22121a + B\u22121b)\nc = (2\u03c0)\u2212D\n2 |A + B |\u22121\n2 exp\n\u0000\n\u2212 1\n2 (a \u2212 b)\u22a4(A + B)\u22121(a \u2212 b)\n\u0001\n.\nNote that the normalizing constantc itself can be considered a (normalized)\nGaussian distribution either ina or in b with an \u201cinflated\u201d covariance matrix\nA + B, i.e., c = N\n\u0000\na|b, A + B\n\u0001\n= N\n\u0000\nb|a, A + B\n\u0001.\n6.11 Iterated Expectations.\nConsider two random variablesx, y with joint distributionp(x, y). Show that\nEX[x] = EY\n\u0002EX[x |y]\n\u0003\n.\nHere, EX[x |y] denotes the expected value of x under the conditional distri-\nbution p(x |y).\n6.12 Manipulation of Gaussian Random Variables.\nConsider a Gaussian random variable x \u223c N\n\u0000\nx|\u00b5x, \u03a3x\n\u0001, where x \u2208 RD.\nFurthermore, we have\ny = Ax + b + w"
  },
  {
    "vector_id": 876,
    "chunk_id": "p229_c3",
    "page_number": 229,
    "text": "under the conditional distri-\nbution p(x |y).\n6.12 Manipulation of Gaussian Random Variables.\nConsider a Gaussian random variable x \u223c N\n\u0000\nx|\u00b5x, \u03a3x\n\u0001, where x \u2208 RD.\nFurthermore, we have\ny = Ax + b + w ,\nwhere y \u2208 RE, A \u2208 RE\u00d7D, b \u2208 RE, and w \u223c N\n\u0000\nw |0, Q\n\u0001 is indepen-\ndent Gaussian noise. \u201cIndependent\u201d implies that x and w are independent\nrandom variables and that Q is diagonal.\na. Write down the likelihood p(y |x).\nb. The distribution p(y) =\nR\np(y |x)p(x)dx is Gaussian. Compute the mean\n\u00b5y and the covariance \u03a3y. Derive your result in detail.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 877,
    "chunk_id": "p229_c4",
    "page_number": 229,
    "text": "lished by Cambridge University Press (2020)."
  },
  {
    "vector_id": 878,
    "chunk_id": "p230_c0",
    "page_number": 230,
    "text": "224 Probability and Distributions\nc. The random variable y is being transformed according to the measure-\nment mapping\nz = Cy + v ,\nwhere z \u2208 RF , C \u2208 RF\u00d7E, and v \u223c N\n\u0000\nv |0, R\n\u0001 is independent Gaus-\nsian (measurement) noise.\nWrite down p(z |y).\nCompute p(z), i.e., the mean \u00b5z and the covariance \u03a3z. Derive your\nresult in detail.\nd. Now, a value \u02c6y is measured. Compute the posterior distribution p(x| \u02c6y).\nHint for solution: This posterior is also Gaussian, i.e., we need to de-\ntermine only its mean and covariance matrix. Start by explicitly com-\nputing the joint Gaussian p(x, y). This also requires us to compute the\ncross-covariances Covx,y[x, y] and Covy,x[y, x]. Then apply the rules\nfor Gaussian conditioning.\n6.13 Probability Integral Transformation\nGiven a continuous random variable X, w"
  },
  {
    "vector_id": 879,
    "chunk_id": "p230_c1",
    "page_number": 230,
    "text": "ires us to compute the\ncross-covariances Covx,y[x, y] and Covy,x[y, x]. Then apply the rules\nfor Gaussian conditioning.\n6.13 Probability Integral Transformation\nGiven a continuous random variable X, with cdf FX(x), show that the ran-\ndom variable Y := FX(X) is uniformly distributed (Theorem 6.15).\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 880,
    "chunk_id": "p231_c0",
    "page_number": 231,
    "text": "7\nContinuous Optimization\nSince machine learning algorithms are implemented on a computer, the\nmathematical formulations are expressed as numerical optimization meth-\nods. This chapter describes the basic numerical methods for training ma-\nchine learning models. Training a machine learning model often boils\ndown to finding a good set of parameters. The notion of \u201cgood\u201d is de-\ntermined by the objective function or the probabilistic model, which we\nwill see examples of in the second part of this book. Given an objective\nfunction, finding the best value is done using optimization algorithms. Since we consider\ndata and models in\nRD, the\noptimization\nproblems we face\nare continuous\noptimization\nproblems, as\nopposed to\ncombinatorial\noptimization\nproblems for\ndiscrete variables.\nThis chapter cove"
  },
  {
    "vector_id": 881,
    "chunk_id": "p231_c1",
    "page_number": 231,
    "text": "e we consider\ndata and models in\nRD, the\noptimization\nproblems we face\nare continuous\noptimization\nproblems, as\nopposed to\ncombinatorial\noptimization\nproblems for\ndiscrete variables.\nThis chapter covers two main branches of continuous optimization (Fig-\nure 7.1): unconstrained and constrained optimization. We will assume in\nthis chapter that our objective function is differentiable (see Chapter 5),\nhence we have access to a gradient at each location in the space to help us\nfind the optimum value. By convention, most objective functions in ma-\nchine learning are intended to be minimized, that is, the best value is the\nminimum value. Intuitively finding the best value is like finding the val-\nleys of the objective function, and the gradients point us uphill. The idea is\nto move downhill (opp"
  },
  {
    "vector_id": 882,
    "chunk_id": "p231_c2",
    "page_number": 231,
    "text": "s, the best value is the\nminimum value. Intuitively finding the best value is like finding the val-\nleys of the objective function, and the gradients point us uphill. The idea is\nto move downhill (opposite to the gradient) and hope to find the deepest\npoint. For unconstrained optimization, this is the only concept we need,\nbut there are several design choices, which we discuss in Section 7.1. For\nconstrained optimization, we need to introduce other concepts to man-\nage the constraints (Section 7.2). We will also introduce a special class\nof problems (convex optimization problems in Section 7.3) where we can\nmake statements about reaching the global optimum.\nConsider the function in Figure 7.2. The function has aglobal minimum global minimum\naround x = \u22124.5, with a function value of approxi"
  },
  {
    "vector_id": 883,
    "chunk_id": "p231_c3",
    "page_number": 231,
    "text": ") where we can\nmake statements about reaching the global optimum.\nConsider the function in Figure 7.2. The function has aglobal minimum global minimum\naround x = \u22124.5, with a function value of approximately \u221247. Since\nthe function is \u201csmooth,\u201d the gradients can be used to help find the min-\nimum by indicating whether we should take a step to the right or left.\nThis assumes that we are in the correct bowl, as there exists another local local minimum\nminimum around x = 0.7. Recall that we can solve for all the stationary\npoints of a function by calculating its derivative and setting it to zero. For Stationary points\nare the real roots of\nthe derivative, that\nis, points that have\nzero gradient.\n\u2113(x) = x4 + 7x3 + 5x2 \u2212 17x + 3, (7.1)\nwe obtain the corresponding gradient as\nd\u2113(x)\ndx = 4x3 + 21x"
  },
  {
    "vector_id": 884,
    "chunk_id": "p231_c4",
    "page_number": 231,
    "text": "For Stationary points\nare the real roots of\nthe derivative, that\nis, points that have\nzero gradient.\n\u2113(x) = x4 + 7x3 + 5x2 \u2212 17x + 3, (7.1)\nwe obtain the corresponding gradient as\nd\u2113(x)\ndx = 4x3 + 21x2 + 10x \u2212 17 . (7.2)\n225\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 885,
    "chunk_id": "p232_c0",
    "page_number": 232,
    "text": "226 Continuous Optimization\nFigure 7.1 A mind\nmap of the concepts\nrelated to\noptimization, as\npresented in this\nchapter. There are\ntwo main ideas:\ngradient descent\nand convex\noptimization.\nContinuousoptimization\nUnconstrainedoptimization\nConstrainedoptimization\nGradient descent\nStepsize\nMomentum\nStochasticgradientdescent\nLagrangemultipliers\nConvex optimization& duality\nConvex\nConvex conjugate\nLinearprogramming\nQuadraticprogramming\nChapter 10Dimension reduc.\nChapter 11Density estimation\nChapter 12Classification\nSince this is a cubic equation, it has in general three solutions when set to\nzero. In the example, two of them are minimums and one is a maximum\n(around x = \u22121.4). To check whether a stationary point is a minimum\nor maximum, we need to take the derivative a second time and check\nwhe"
  },
  {
    "vector_id": 886,
    "chunk_id": "p232_c1",
    "page_number": 232,
    "text": "In the example, two of them are minimums and one is a maximum\n(around x = \u22121.4). To check whether a stationary point is a minimum\nor maximum, we need to take the derivative a second time and check\nwhether the second derivative is positive or negative at the stationary\npoint. In our case, the second derivative is\nd2\u2113(x)\ndx2 = 12x2 + 42x + 10. (7.3)\nBy substituting our visually estimated values of x = \u22124.5, \u22121.4, 0.7, we\nwill observe that as expected the middle point is a maximum\n\u0010\nd2\u2113(x)\ndx2 < 0\n\u0011\nand the other two stationary points are minimums.\nNote that we have avoided analytically solving for values of x in the\nprevious discussion, although for low-order polynomials such as the pre-\nceding we could do so. In general, we are unable to find analytic solu-\ntions, and hence we need to start"
  },
  {
    "vector_id": 887,
    "chunk_id": "p232_c2",
    "page_number": 232,
    "text": "or values of x in the\nprevious discussion, although for low-order polynomials such as the pre-\nceding we could do so. In general, we are unable to find analytic solu-\ntions, and hence we need to start at some value, say x0 = \u22126, and follow\nthe negative gradient. The negative gradient indicates that we should go\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 888,
    "chunk_id": "p233_c0",
    "page_number": 233,
    "text": "7.1 Optimization Using Gradient Descent 227\nFigure 7.2 Example\nobjective function.\nNegative gradients\nare indicated by\narrows, and the\nglobal minimum is\nindicated by the\ndashed blue line.\n\u22126 \u22125 \u22124 \u22123 \u22122 \u22121 0 1 2\nValue of parameter\n\u221260\n\u221240\n\u221220\n0\n20\n40\n60\nObjective\nx4 + 7x3 + 5x2 \u2212 17x + 3\nright, but not how far (this is called the step-size). Furthermore, if we According to the\nAbel\u2013Ruffini\ntheorem, there is in\ngeneral no algebraic\nsolution for\npolynomials of\ndegree 5 or more\n(Abel, 1826).\nhad started at the right side (e.g., x0 = 0) the negative gradient would\nhave led us to the wrong minimum. Figure 7.2 illustrates the fact that for\nx >\u22121, the negative gradient points toward the minimum on the right of\nthe figure, which has a larger objective value.\nIn Section 7.3, we will learn about a c"
  },
  {
    "vector_id": 889,
    "chunk_id": "p233_c1",
    "page_number": 233,
    "text": "Figure 7.2 illustrates the fact that for\nx >\u22121, the negative gradient points toward the minimum on the right of\nthe figure, which has a larger objective value.\nIn Section 7.3, we will learn about a class of functions, called convex\nfunctions, that do not exhibit this tricky dependency on the starting point\nof the optimization algorithm. For convex functions, all local minimums\nare global minimum. It turns out that many machine learning objective For convex functions\nall local minima are\nglobal minimum.\nfunctions are designed such that they are convex, and we will see an ex-\nample in Chapter 12.\nThe discussion in this chapter so far was about a one-dimensional func-\ntion, where we are able to visualize the ideas of gradients, descent direc-\ntions, and optimal values. In the rest of this ch"
  },
  {
    "vector_id": 890,
    "chunk_id": "p233_c2",
    "page_number": 233,
    "text": "2.\nThe discussion in this chapter so far was about a one-dimensional func-\ntion, where we are able to visualize the ideas of gradients, descent direc-\ntions, and optimal values. In the rest of this chapter we develop the same\nideas in high dimensions. Unfortunately , we can only visualize the con-\ncepts in one dimension, but some concepts do not generalize directly to\nhigher dimensions, therefore some care needs to be taken when reading.\n7.1 Optimization Using Gradient Descent\nWe now consider the problem of solving for the minimum of a real-valued\nfunction\nmin\nx\nf(x) , (7.4)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 891,
    "chunk_id": "p233_c3",
    "page_number": 233,
    "text": "roth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 892,
    "chunk_id": "p234_c0",
    "page_number": 234,
    "text": "228 Continuous Optimization\nwhere f : Rd \u2192 R is an objective function that captures the machine\nlearning problem at hand. We assume that our functionf is differentiable,\nand we are unable to analytically find a solution in closed form.\nGradient descent is a first-order optimization algorithm. To find a local\nminimum of a function using gradient descent, one takes steps propor-\ntional to the negative of the gradient of the function at the current point.\nRecall from Section 5.1 that the gradient points in the direction of theWe use the\nconvention of row\nvectors for\ngradients.\nsteepest ascent. Another useful intuition is to consider the set of lines\nwhere the function is at a certain value (f(x) = c for some value c \u2208 R),\nwhich are known as the contour lines. The gradient points in a directio"
  },
  {
    "vector_id": 893,
    "chunk_id": "p234_c1",
    "page_number": 234,
    "text": "other useful intuition is to consider the set of lines\nwhere the function is at a certain value (f(x) = c for some value c \u2208 R),\nwhich are known as the contour lines. The gradient points in a direction\nthat is orthogonal to the contour lines of the function we wish to optimize.\nLet us consider multivariate functions. Imagine a surface (described by\nthe function f(x)) with a ball starting at a particular location x0. When\nthe ball is released, it will move downhill in the direction of steepest de-\nscent. Gradient descent exploits the fact thatf(x0) decreases fastest if one\nmoves from x0 in the direction of the negative gradient \u2212((\u2207f)(x0))\u22a4 of\nf at x0. We assume in this book that the functions are differentiable, and\nrefer the reader to more general settings in Section 7.4. Then, if\nx1 = x0"
  },
  {
    "vector_id": 894,
    "chunk_id": "p234_c2",
    "page_number": 234,
    "text": "direction of the negative gradient \u2212((\u2207f)(x0))\u22a4 of\nf at x0. We assume in this book that the functions are differentiable, and\nrefer the reader to more general settings in Section 7.4. Then, if\nx1 = x0 \u2212 \u03b3((\u2207f)(x0))\u22a4 (7.5)\nfor a small step-size \u03b3 \u2a7e 0, then f(x1) \u2a7d f(x0). Note that we use the\ntranspose for the gradient since otherwise the dimensions will not work\nout.\nThis observation allows us to define a simple gradient descent algo-\nrithm: If we want to find a local optimum f(x\u2217) of a function f : Rn \u2192\nR, x 7\u2192 f(x), we start with an initial guessx0 of the parameters we wish\nto optimize and then iterate according to\nxi+1 = xi \u2212 \u03b3i((\u2207f)(xi))\u22a4 . (7.6)\nFor suitable step-size \u03b3i, the sequence f(x0) \u2a7e f(x1) \u2a7e . . .converges to\na local minimum.\nExample 7.1\nConsider a quadratic function in two di"
  },
  {
    "vector_id": 895,
    "chunk_id": "p234_c3",
    "page_number": 234,
    "text": "en iterate according to\nxi+1 = xi \u2212 \u03b3i((\u2207f)(xi))\u22a4 . (7.6)\nFor suitable step-size \u03b3i, the sequence f(x0) \u2a7e f(x1) \u2a7e . . .converges to\na local minimum.\nExample 7.1\nConsider a quadratic function in two dimensions\nf\n\u0012\u0014x1\nx2\n\u0015\u0013\n= 1\n2\n\u0014x1\nx2\n\u0015\u22a4 \u00142 1\n1 20\n\u0015\u0014x1\nx2\n\u0015\n\u2212\n\u00145\n3\n\u0015\u22a4 \u0014x1\nx2\n\u0015\n(7.7)\nwith gradient\n\u2207f\n\u0012\u0014x1\nx2\n\u0015\u0013\n=\n\u0014x1\nx2\n\u0015\u22a4 \u00142 1\n1 20\n\u0015\n\u2212\n\u00145\n3\n\u0015\u22a4\n. (7.8)\nStarting at the initial location x0 = [\u22123, \u22121]\u22a4, we iteratively apply (7.6)\nto obtain a sequence of estimates that converge to the minimum value\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 896,
    "chunk_id": "p235_c0",
    "page_number": 235,
    "text": "7.1 Optimization Using Gradient Descent 229\nFigure 7.3 Gradient\ndescent on a\ntwo-dimensional\nquadratic surface\n(shown as a\nheatmap). See\nExample 7.1 for a\ndescription.\n\u22124 \u22122 0 2 4\nx1\n\u22122\n\u22121\n0\n1\n2\nx2\n0.0\n10.0\n20.0\n30.0\n40.0\n40.0\n50.0\n50.0\n60.070.0\n80.0\n\u221215\n0\n15\n30\n45\n60\n75\n90\n(illustrated in Figure 7.3). We can see (both from the figure and by plug-\nging x0 into (7.8) with \u03b3 = 0.085) that the negative gradient at x0 points\nnorth and east, leading to x1 = [\u22121.98, 1.21]\u22a4. Repeating that argument\ngives us x2 = [\u22121.32, \u22120.42]\u22a4, and so on.\nRemark. Gradient descent can be relatively slow close to the minimum:\nIts asymptotic rate of convergence is inferior to many other methods. Us-\ning the ball rolling down the hill analogy , when the surface is a long, thin\nvalley , the problem is poorly conditio"
  },
  {
    "vector_id": 897,
    "chunk_id": "p235_c1",
    "page_number": 235,
    "text": "minimum:\nIts asymptotic rate of convergence is inferior to many other methods. Us-\ning the ball rolling down the hill analogy , when the surface is a long, thin\nvalley , the problem is poorly conditioned (Trefethen and Bau III, 1997).\nFor poorly conditioned convex problems, gradient descent increasingly\n\u201czigzags\u201d as the gradients point nearly orthogonally to the shortest di-\nrection to a minimum point; see Figure 7.3. \u2662\n7.1.1 Step-size\nAs mentioned earlier, choosing a good step-size is important in gradient\ndescent. If the step-size is too small, gradient descent can be slow. If the The step-size is also\ncalled the learning\nrate.\nstep-size is chosen too large, gradient descent can overshoot, fail to con-\nverge, or even diverge. We will discuss the use of momentum in the next\nsection. It is"
  },
  {
    "vector_id": 898,
    "chunk_id": "p235_c2",
    "page_number": 235,
    "text": "ize is also\ncalled the learning\nrate.\nstep-size is chosen too large, gradient descent can overshoot, fail to con-\nverge, or even diverge. We will discuss the use of momentum in the next\nsection. It is a method that smoothes out erratic behavior of gradient up-\ndates and dampens oscillations.\nAdaptive gradient methods rescale the step-size at each iteration, de-\npending on local properties of the function. There are two simple heuris-\ntics (Toussaint, 2012):\nWhen the function value increases after a gradient step, the step-size\nwas too large. Undo the step and decrease the step-size.\nWhen the function value decreases the step could have been larger. Try\nto increase the step-size.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 899,
    "chunk_id": "p235_c3",
    "page_number": 235,
    "text": "unction value decreases the step could have been larger. Try\nto increase the step-size.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 900,
    "chunk_id": "p236_c0",
    "page_number": 236,
    "text": "230 Continuous Optimization\nAlthough the \u201cundo\u201d step seems to be a waste of resources, using this\nheuristic guarantees monotonic convergence.\nExample 7.2 (Solving a Linear Equation System)\nWhen we solve linear equations of the form Ax = b, in practice we solve\nAx\u2212b = 0 approximately by finding x\u2217 that minimizes the squared error\n\u2225Ax \u2212 b\u22252 = (Ax \u2212 b)\u22a4(Ax \u2212 b) (7.9)\nif we use the Euclidean norm. The gradient of (7.9) with respect to x is\n\u2207x = 2(Ax \u2212 b)\u22a4A. (7.10)\nWe can use this gradient directly in a gradient descent algorithm. How-\never, for this particular special case, it turns out that there is an analytic\nsolution, which can be found by setting the gradient to zero. We will see\nmore on solving squared error problems in Chapter 9.\nRemark. When applied to the solution of linear systems of"
  },
  {
    "vector_id": 901,
    "chunk_id": "p236_c1",
    "page_number": 236,
    "text": "is an analytic\nsolution, which can be found by setting the gradient to zero. We will see\nmore on solving squared error problems in Chapter 9.\nRemark. When applied to the solution of linear systems of equationsAx =\nb, gradient descent may converge slowly . The speed of convergence of gra-\ndient descent is dependent on the condition number \u03ba = \u03c3(A)max\n\u03c3(A)min\n, whichcondition number\nis the ratio of the maximum to the minimum singular value (Section 4.5)\nof A. The condition number essentially measures the ratio of the most\ncurved direction versus the least curved direction, which corresponds to\nour imagery that poorly conditioned problems are long, thin valleys: They\nare very curved in one direction, but very flat in the other. Instead of di-\nrectly solving Ax = b, one could instead solve P\u2212"
  },
  {
    "vector_id": 902,
    "chunk_id": "p236_c2",
    "page_number": 236,
    "text": "our imagery that poorly conditioned problems are long, thin valleys: They\nare very curved in one direction, but very flat in the other. Instead of di-\nrectly solving Ax = b, one could instead solve P\u22121(Ax \u2212 b) = 0, where\nP is called the preconditioner. The goal is to design P\u22121 such that P\u22121Apreconditioner\nhas a better condition number, but at the same time P\u22121 is easy to com-\npute. For further information on gradient descent, preconditioning, and\nconvergence we refer to Boyd and Vandenberghe (2004, chapter 9). \u2662\n7.1.2 Gradient Descent With Momentum\nAs illustrated in Figure 7.3, the convergence of gradient descent may be\nvery slow if the curvature of the optimization surface is such that there\nare regions that are poorly scaled. The curvature is such that the gradient\ndescent steps hops be"
  },
  {
    "vector_id": 903,
    "chunk_id": "p236_c3",
    "page_number": 236,
    "text": "e of gradient descent may be\nvery slow if the curvature of the optimization surface is such that there\nare regions that are poorly scaled. The curvature is such that the gradient\ndescent steps hops between the walls of the valley and approaches the\noptimum in small steps. The proposed tweak to improve convergence is\nto give gradient descent some memory .Goh (2017) wrote\nan intuitive blog\npost on gradient\ndescent with\nmomentum.\nGradient descent with momentum (Rumelhart et al., 1986) is a method\nthat introduces an additional term to remember what happened in the\nprevious iteration. This memory dampens oscillations and smoothes out\nthe gradient updates. Continuing the ball analogy , the momentum term\nemulates the phenomenon of a heavy ball that is reluctant to change di-\nrections. The idea is"
  },
  {
    "vector_id": 904,
    "chunk_id": "p236_c4",
    "page_number": 236,
    "text": "ampens oscillations and smoothes out\nthe gradient updates. Continuing the ball analogy , the momentum term\nemulates the phenomenon of a heavy ball that is reluctant to change di-\nrections. The idea is to have a gradient update with memory to implement\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 905,
    "chunk_id": "p237_c0",
    "page_number": 237,
    "text": "7.1 Optimization Using Gradient Descent 231\na moving average. The momentum-based method remembers the update\n\u2206xi at each iteration i and determines the next update as a linear combi-\nnation of the current and previous gradients\nxi+1 = xi \u2212 \u03b3i((\u2207f)(xi))\u22a4 + \u03b1\u2206xi (7.11)\n\u2206xi = xi \u2212 xi\u22121 = \u03b1\u2206xi\u22121 \u2212 \u03b3i\u22121((\u2207f)(xi\u22121))\u22a4 , (7.12)\nwhere \u03b1 \u2208 [0, 1]. Sometimes we will only know the gradient approxi-\nmately . In such cases, the momentum term is useful since it averages out\ndifferent noisy estimates of the gradient. One particularly useful way to\nobtain an approximate gradient is by using a stochastic approximation,\nwhich we discuss next.\n7.1.3 Stochastic Gradient Descent\nComputing the gradient can be very time consuming. However, often it is\npossible to find a \u201ccheap\u201d approximation of the gradient. Appr"
  },
  {
    "vector_id": 906,
    "chunk_id": "p237_c1",
    "page_number": 237,
    "text": "imation,\nwhich we discuss next.\n7.1.3 Stochastic Gradient Descent\nComputing the gradient can be very time consuming. However, often it is\npossible to find a \u201ccheap\u201d approximation of the gradient. Approximating\nthe gradient is still useful as long as it points in roughly the same direction\nas the true gradient. stochastic gradient\ndescentStochastic gradient descent (often shortened as SGD) is a stochastic ap-\nproximation of the gradient descent method for minimizing an objective\nfunction that is written as a sum of differentiable functions. The word\nstochastic here refers to the fact that we acknowledge that we do not\nknow the gradient precisely , but instead only know a noisy approxima-\ntion to it. By constraining the probability distribution of the approximate\ngradients, we can still theo"
  },
  {
    "vector_id": 907,
    "chunk_id": "p237_c2",
    "page_number": 237,
    "text": "knowledge that we do not\nknow the gradient precisely , but instead only know a noisy approxima-\ntion to it. By constraining the probability distribution of the approximate\ngradients, we can still theoretically guarantee that SGD will converge.\nIn machine learning, given n = 1, . . . , Ndata points, we often consider\nobjective functions that are the sum of the losses Ln incurred by each\nexample n. In mathematical notation, we have the form\nL(\u03b8) =\nNX\nn=1\nLn(\u03b8) , (7.13)\nwhere \u03b8 is the vector of parameters of interest, i.e., we want to find\u03b8 that\nminimizes L. An example from regression (Chapter 9) is the negative log-\nlikelihood, which is expressed as a sum over log-likelihoods of individual\nexamples so that\nL(\u03b8) = \u2212\nNX\nn=1\nlog p(yn|xn, \u03b8) , (7.14)\nwhere xn \u2208 RD are the training inputs, yn are"
  },
  {
    "vector_id": 908,
    "chunk_id": "p237_c3",
    "page_number": 237,
    "text": "is the negative log-\nlikelihood, which is expressed as a sum over log-likelihoods of individual\nexamples so that\nL(\u03b8) = \u2212\nNX\nn=1\nlog p(yn|xn, \u03b8) , (7.14)\nwhere xn \u2208 RD are the training inputs, yn are the training targets, and \u03b8\nare the parameters of the regression model.\nStandard gradient descent, as introduced previously , is a \u201cbatch\u201d opti-\nmization method, i.e., optimization is performed using the full training set\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 909,
    "chunk_id": "p238_c0",
    "page_number": 238,
    "text": "232 Continuous Optimization\nby updating the vector of parameters according to\n\u03b8i+1 = \u03b8i \u2212 \u03b3i(\u2207L(\u03b8i))\u22a4 = \u03b8i \u2212 \u03b3i\nNX\nn=1\n(\u2207Ln(\u03b8i))\u22a4 (7.15)\nfor a suitable step-size parameter \u03b3i. Evaluating the sum gradient may re-\nquire expensive evaluations of the gradients from all individual functions\nLn. When the training set is enormous and/or no simple formulas exist,\nevaluating the sums of gradients becomes very expensive.\nConsider the termPN\nn=1(\u2207Ln(\u03b8i)) in (7.15). We can reduce the amount\nof computation by taking a sum over a smaller set of Ln. In contrast to\nbatch gradient descent, which uses all Ln for n = 1, . . . , N, we randomly\nchoose a subset of Ln for mini-batch gradient descent. In the extreme\ncase, we randomly select only a single Ln to estimate the gradient. The\nkey insight about why taki"
  },
  {
    "vector_id": 910,
    "chunk_id": "p238_c1",
    "page_number": 238,
    "text": "r n = 1, . . . , N, we randomly\nchoose a subset of Ln for mini-batch gradient descent. In the extreme\ncase, we randomly select only a single Ln to estimate the gradient. The\nkey insight about why taking a subset of data is sensible is to realize that\nfor gradient descent to converge, we only require that the gradient is an\nunbiased estimate of the true gradient. In fact the term PN\nn=1(\u2207Ln(\u03b8i))\nin (7.15) is an empirical estimate of the expected value (Section 6.4.1) of\nthe gradient. Therefore, any other unbiased empirical estimate of the ex-\npected value, for example using any subsample of the data, would suffice\nfor convergence of gradient descent.\nRemark. When the learning rate decreases at an appropriate rate, and sub-\nject to relatively mild assumptions, stochastic gradient descent con"
  },
  {
    "vector_id": 911,
    "chunk_id": "p238_c2",
    "page_number": 238,
    "text": "data, would suffice\nfor convergence of gradient descent.\nRemark. When the learning rate decreases at an appropriate rate, and sub-\nject to relatively mild assumptions, stochastic gradient descent converges\nalmost surely to local minimum (Bottou, 1998). \u2662\nWhy should one consider using an approximate gradient? A major rea-\nson is practical implementation constraints, such as the size of central\nprocessing unit (CPU)/graphics processing unit (GPU) memory or limits\non computational time. We can think of the size of the subset used to esti-\nmate the gradient in the same way that we thought of the size of a sample\nwhen estimating empirical means (Section 6.4.1). Large mini-batch sizes\nwill provide accurate estimates of the gradient, reducing the variance in\nthe parameter update. Furthermore, la"
  },
  {
    "vector_id": 912,
    "chunk_id": "p238_c3",
    "page_number": 238,
    "text": "size of a sample\nwhen estimating empirical means (Section 6.4.1). Large mini-batch sizes\nwill provide accurate estimates of the gradient, reducing the variance in\nthe parameter update. Furthermore, large mini-batches take advantage of\nhighly optimized matrix operations in vectorized implementations of the\ncost and gradient. The reduction in variance leads to more stable conver-\ngence, but each gradient calculation will be more expensive.\nIn contrast, small mini-batches are quick to estimate. If we keep the\nmini-batch size small, the noise in our gradient estimate will allow us to\nget out of some bad local optima, which we may otherwise get stuck in.\nIn machine learning, optimization methods are used for training by min-\nimizing an objective function on the training data, but the overall go"
  },
  {
    "vector_id": 913,
    "chunk_id": "p238_c4",
    "page_number": 238,
    "text": "me bad local optima, which we may otherwise get stuck in.\nIn machine learning, optimization methods are used for training by min-\nimizing an objective function on the training data, but the overall goal\nis to improve generalization performance (Chapter 8). Since the goal in\nmachine learning does not necessarily need a precise estimate of the min-\nimum of the objective function, approximate gradients using mini-batch\napproaches have been widely used. Stochastic gradient descent is very\neffective in large-scale machine learning problems (Bottou et al., 2018),\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 914,
    "chunk_id": "p238_c5",
    "page_number": 238,
    "text": "or Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 915,
    "chunk_id": "p239_c0",
    "page_number": 239,
    "text": "7.2 Constrained Optimization and Lagrange Multipliers 233\nFigure 7.4\nIllustration of\nconstrained\noptimization. The\nunconstrained\nproblem (indicated\nby the contour\nlines) has a\nminimum on the\nright side (indicated\nby the circle). The\nbox constraints\n(\u22121 \u2a7d x \u2a7d 1 and\n\u22121 \u2a7d y \u2a7d 1) require\nthat the optimal\nsolution is within\nthe box, resulting in\nan optimal value\nindicated by the\nstar.\n\u22123 \u22122 \u22121 0 1 2 3\nx1\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\nx2\nsuch as training deep neural networks on millions of images (Dean et al.,\n2012), topic models (Hoffman et al., 2013), reinforcement learning (Mnih\net al., 2015), or training of large-scale Gaussian process models (Hensman\net al., 2013; Gal et al., 2014).\n7.2 Constrained Optimization and Lagrange Multipliers\nIn the previous section, we considered the problem of solving for th"
  },
  {
    "vector_id": 916,
    "chunk_id": "p239_c1",
    "page_number": 239,
    "text": "large-scale Gaussian process models (Hensman\net al., 2013; Gal et al., 2014).\n7.2 Constrained Optimization and Lagrange Multipliers\nIn the previous section, we considered the problem of solving for the min-\nimum of a function\nmin\nx\nf(x) , (7.16)\nwhere f : RD \u2192 R.\nIn this section, we have additional constraints. That is, for real-valued\nfunctions gi : RD \u2192 R for i = 1 , . . . , m, we consider the constrained\noptimization problem (see Figure 7.4 for an illustration)\nmin\nx\nf(x) (7.17)\nsubject to gi(x) \u2a7d 0 for all i = 1, . . . , m .\nIt is worth pointing out that the functions f and gi could be non-convex\nin general, and we will consider the convex case in the next section.\nOne obvious, but not very practical, way of converting the constrained\nproblem (7.17) into an unconstrained one is to use"
  },
  {
    "vector_id": 917,
    "chunk_id": "p239_c2",
    "page_number": 239,
    "text": "-convex\nin general, and we will consider the convex case in the next section.\nOne obvious, but not very practical, way of converting the constrained\nproblem (7.17) into an unconstrained one is to use an indicator function\nJ(x) = f(x) +\nmX\ni=1\n1(gi(x)) , (7.18)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 918,
    "chunk_id": "p240_c0",
    "page_number": 240,
    "text": "234 Continuous Optimization\nwhere 1(z) is an infinite step function\n1(z) =\n(\n0 if z \u2a7d 0\n\u221e otherwise . (7.19)\nThis gives infinite penalty if the constraint is not satisfied, and hence\nwould provide the same solution. However, this infinite step function is\nequally difficult to optimize. We can overcome this difficulty by introduc-\ning Lagrange multipliers. The idea of Lagrange multipliers is to replace theLagrange multiplier\nstep function with a linear function.\nWe associate to problem (7.17) the Lagrangian by introducing the La-Lagrangian\ngrange multipliers \u03bbi \u2a7e 0 corresponding to each inequality constraint re-\nspectively (Boyd and Vandenberghe, 2004, chapter 4) so that\nL(x, \u03bb) = f(x) +\nmX\ni=1\n\u03bbigi(x) (7.20a)\n= f(x) + \u03bb\u22a4g(x) , (7.20b)\nwhere in the last line we have concatenated all constra"
  },
  {
    "vector_id": 919,
    "chunk_id": "p240_c1",
    "page_number": 240,
    "text": "ity constraint re-\nspectively (Boyd and Vandenberghe, 2004, chapter 4) so that\nL(x, \u03bb) = f(x) +\nmX\ni=1\n\u03bbigi(x) (7.20a)\n= f(x) + \u03bb\u22a4g(x) , (7.20b)\nwhere in the last line we have concatenated all constraints gi(x) into a\nvector g(x), and all the Lagrange multipliers into a vector \u03bb \u2208 Rm.\nWe now introduce the idea of Lagrangian duality . In general, duality\nin optimization is the idea of converting an optimization problem in one\nset of variables x (called the primal variables), into another optimization\nproblem in a different set of variables \u03bb (called the dual variables). We\nintroduce two different approaches to duality: In this section, we discuss\nLagrangian duality; in Section 7.3.3, we discuss Legendre-Fenchel duality .\nDefinition 7.1. The problem in (7.17)\nmin\nx\nf(x) (7.21)\nsubject to gi("
  },
  {
    "vector_id": 920,
    "chunk_id": "p240_c2",
    "page_number": 240,
    "text": "nt approaches to duality: In this section, we discuss\nLagrangian duality; in Section 7.3.3, we discuss Legendre-Fenchel duality .\nDefinition 7.1. The problem in (7.17)\nmin\nx\nf(x) (7.21)\nsubject to gi(x) \u2a7d 0 for all i = 1, . . . , m\nis known as the primal problem, corresponding to the primal variables x.primal problem\nThe associated Lagrangian dual problem is given byLagrangian dual\nproblem\nmax\n\u03bb\u2208Rm\nD(\u03bb)\nsubject to \u03bb \u2a7e 0 ,\n(7.22)\nwhere \u03bb are the dual variables and D(\u03bb) = minx\u2208Rd L(x, \u03bb).\nRemark. In the discussion of Definition 7.1, we use two concepts that are\nalso of independent interest (Boyd and Vandenberghe, 2004).\nFirst is the minimax inequality, which says that for any function withminimax inequality\ntwo arguments \u03c6(x, y), the maximin is less than the minimax, i.e.,\nmax\ny\nmin\nx\n\u03c6(x, y"
  },
  {
    "vector_id": 921,
    "chunk_id": "p240_c3",
    "page_number": 240,
    "text": "and Vandenberghe, 2004).\nFirst is the minimax inequality, which says that for any function withminimax inequality\ntwo arguments \u03c6(x, y), the maximin is less than the minimax, i.e.,\nmax\ny\nmin\nx\n\u03c6(x, y) \u2a7d min\nx\nmax\ny\n\u03c6(x, y) . (7.23)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 922,
    "chunk_id": "p241_c0",
    "page_number": 241,
    "text": "7.2 Constrained Optimization and Lagrange Multipliers 235\nThis inequality can be proved by considering the inequality\nFor all x, y min\nx\n\u03c6(x, y) \u2a7d max\ny\n\u03c6(x, y) . (7.24)\nNote that taking the maximum overy of the left-hand side of (7.24) main-\ntains the inequality since the inequality is true for all y. Similarly , we can\ntake the minimum overx of the right-hand side of (7.24) to obtain (7.23).\nThe second concept is weak duality , which uses (7.23) to show that weak duality\nprimal values are always greater than or equal to dual values. This is de-\nscribed in more detail in (7.27). \u2662\nRecall that the difference between J(x) in (7.18) and the Lagrangian\nin (7.20b) is that we have relaxed the indicator function to a linear func-\ntion. Therefore, when \u03bb \u2a7e 0, the Lagrangian L(x, \u03bb) is a lower bou"
  },
  {
    "vector_id": 923,
    "chunk_id": "p241_c1",
    "page_number": 241,
    "text": "the difference between J(x) in (7.18) and the Lagrangian\nin (7.20b) is that we have relaxed the indicator function to a linear func-\ntion. Therefore, when \u03bb \u2a7e 0, the Lagrangian L(x, \u03bb) is a lower bound of\nJ(x). Hence, the maximum of L(x, \u03bb) with respect to \u03bb is\nJ(x) = max\n\u03bb\u2a7e0\nL(x, \u03bb) . (7.25)\nRecall that the original problem was minimizing J(x),\nmin\nx\u2208Rd\nmax\n\u03bb\u2a7e0\nL(x, \u03bb) . (7.26)\nBy the minimax inequality (7.23), it follows that swapping the order of\nthe minimum and maximum results in a smaller value, i.e.,\nmin\nx\u2208Rd\nmax\n\u03bb\u2a7e0\nL(x, \u03bb) \u2a7e max\n\u03bb\u2a7e0\nmin\nx\u2208Rd\nL(x, \u03bb) . (7.27)\nThis is also known as weak duality. Note that the inner part of the right- weak duality\nhand side is the dual objective function D(\u03bb) and the definition follows.\nIn contrast to the original optimization problem, which has cons"
  },
  {
    "vector_id": 924,
    "chunk_id": "p241_c2",
    "page_number": 241,
    "text": "duality. Note that the inner part of the right- weak duality\nhand side is the dual objective function D(\u03bb) and the definition follows.\nIn contrast to the original optimization problem, which has constraints,\nminx\u2208Rd L(x, \u03bb) is an unconstrained optimization problem for a given\nvalue of \u03bb. If solving minx\u2208Rd L(x, \u03bb) is easy , then the overall problem is\neasy to solve. We can see this by observing from (7.20b) that L(x, \u03bb) is\naffine with respect to \u03bb. Therefore minx\u2208Rd L(x, \u03bb) is a pointwise min-\nimum of affine functions of \u03bb, and hence D(\u03bb) is concave even though\nf(\u00b7) and gi(\u00b7) may be nonconvex. The outer problem, maximization over\n\u03bb, is the maximum of a concave function and can be efficiently computed.\nAssuming f(\u00b7) and gi(\u00b7) are differentiable, we find the Lagrange dual\nproblem by differe"
  },
  {
    "vector_id": 925,
    "chunk_id": "p241_c3",
    "page_number": 241,
    "text": ". The outer problem, maximization over\n\u03bb, is the maximum of a concave function and can be efficiently computed.\nAssuming f(\u00b7) and gi(\u00b7) are differentiable, we find the Lagrange dual\nproblem by differentiating the Lagrangian with respect to x, setting the\ndifferential to zero, and solving for the optimal value. We will discuss two\nconcrete examples in Sections 7.3.1 and 7.3.2, where f(\u00b7) and gi(\u00b7) are\nconvex.\nRemark (Equality Constraints). Consider (7.17) with additional equality\nconstraints\nmin\nx\nf(x)\nsubject to gi(x) \u2a7d 0 for all i = 1, . . . , m\nhj(x) = 0 for all j = 1, . . . , n .\n(7.28)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 926,
    "chunk_id": "p241_c4",
    "page_number": 241,
    "text": "24 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 927,
    "chunk_id": "p242_c0",
    "page_number": 242,
    "text": "236 Continuous Optimization\nWe can model equality constraints by replacing them with two inequality\nconstraints. That is for each equality constrainthj(x) = 0 we equivalently\nreplace it by two constraints hj(x) \u2a7d 0 and hj(x) \u2a7e 0. It turns out that\nthe resulting Lagrange multipliers are then unconstrained.\nTherefore, we constrain the Lagrange multipliers corresponding to the\ninequality constraints in (7.28) to be non-negative, and leave the La-\ngrange multipliers corresponding to the equality constraints unconstrained.\n\u2662\n7.3 Convex Optimization\nWe focus our attention of a particularly useful class of optimization prob-\nlems, where we can guarantee global optimality . When f(\u00b7) is a convex\nfunction, and when the constraints involvingg(\u00b7) and h(\u00b7) are convex sets,\nthis is called a convex opti"
  },
  {
    "vector_id": 928,
    "chunk_id": "p242_c1",
    "page_number": 242,
    "text": "ass of optimization prob-\nlems, where we can guarantee global optimality . When f(\u00b7) is a convex\nfunction, and when the constraints involvingg(\u00b7) and h(\u00b7) are convex sets,\nthis is called a convex optimization problem. In this setting, we have strongconvex optimization\nproblem\nstrong duality\nduality: The optimal solution of the dual problem is the same as the opti-\nmal solution of the primal problem. The distinction between convex func-\ntions and convex sets are often not strictly presented in machine learning\nliterature, but one can often infer the implied meaning from context.\nDefinition 7.2. A set C is a convex set if for any x, y\u2208 Cand for any scalarconvex set\n\u03b8 with 0 \u2a7d \u03b8 \u2a7d 1, we have\n\u03b8x + (1 \u2212 \u03b8)y \u2208 C. (7.29)\nFigure 7.5 Example\nof a convex set.\n Convex sets are sets such that a straig"
  },
  {
    "vector_id": 929,
    "chunk_id": "p242_c2",
    "page_number": 242,
    "text": "A set C is a convex set if for any x, y\u2208 Cand for any scalarconvex set\n\u03b8 with 0 \u2a7d \u03b8 \u2a7d 1, we have\n\u03b8x + (1 \u2212 \u03b8)y \u2208 C. (7.29)\nFigure 7.5 Example\nof a convex set.\n Convex sets are sets such that a straight line connecting any two ele-\nments of the set lie inside the set. Figures 7.5 and 7.6 illustrate convex\nand nonconvex sets, respectively .\nFigure 7.6 Example\nof a nonconvex set.\nConvex functions are functions such that a straight line between any\ntwo points of the function lie above the function. Figure 7.2 shows a non-\nconvex function, and Figure 7.3 shows a convex function. Another convex\nfunction is shown in Figure 7.7.\nDefinition 7.3. Let function f : RD \u2192 R be a function whose domain is a\nconvex set. The function f is a convex function if for all x, y in the domain\nconvex function\nof f"
  },
  {
    "vector_id": 930,
    "chunk_id": "p242_c3",
    "page_number": 242,
    "text": "ction is shown in Figure 7.7.\nDefinition 7.3. Let function f : RD \u2192 R be a function whose domain is a\nconvex set. The function f is a convex function if for all x, y in the domain\nconvex function\nof f, and for any scalar \u03b8 with 0 \u2a7d \u03b8 \u2a7d 1, we have\nf(\u03b8x + (1 \u2212 \u03b8)y) \u2a7d \u03b8f(x) + (1\u2212 \u03b8)f(y) . (7.30)\nRemark. A concave function is the negative of a convex function. \u2662\nconcave function\nThe constraints involving g(\u00b7) and h(\u00b7) in (7.28) truncate functions at a\nscalar value, resulting in sets. Another relation between convex functions\nand convex sets is to consider the set obtained by \u201cfilling in\u201d a convex\nfunction. A convex function is a bowl-like object, and we imagine pouring\nwater into it to fill it up. This resulting filled-in set, called the epigraph ofepigraph\nthe convex function, is a convex set"
  },
  {
    "vector_id": 931,
    "chunk_id": "p242_c4",
    "page_number": 242,
    "text": "function. A convex function is a bowl-like object, and we imagine pouring\nwater into it to fill it up. This resulting filled-in set, called the epigraph ofepigraph\nthe convex function, is a convex set.\nIf a function f : Rn \u2192 R is differentiable, we can specify convexity in\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 932,
    "chunk_id": "p243_c0",
    "page_number": 243,
    "text": "7.3 Convex Optimization 237\nFigure 7.7 Example\nof a convex\nfunction.\n\u22123 \u22122 \u22121 0 1 2 3\nx\n0\n10\n20\n30\n40y\ny = 3x2 \u2212 5x + 2\nterms of its gradient \u2207xf(x) (Section 5.2). A function f(x) is convex if\nand only if for any two points x, y it holds that\nf(y) \u2a7e f(x) + \u2207xf(x)\u22a4(y \u2212 x) . (7.31)\nIf we further know that a function f(x) is twice differentiable, that is, the\nHessian (5.147) exists for all values in the domain of x, then the function\nf(x) is convex if and only if \u22072\nxf(x) is positive semidefinite (Boyd and\nVandenberghe, 2004).\nExample 7.3\nThe negative entropy f(x) = x log2 x is convex for x >0. A visualization\nof the function is shown in Figure 7.8, and we can see that the function is\nconvex. To illustrate the previous definitions of convexity , let us check the\ncalculations for two points x"
  },
  {
    "vector_id": 933,
    "chunk_id": "p243_c1",
    "page_number": 243,
    "text": "visualization\nof the function is shown in Figure 7.8, and we can see that the function is\nconvex. To illustrate the previous definitions of convexity , let us check the\ncalculations for two points x = 2 and x = 4. Note that to prove convexity\nof f(x) we would need to check for all points x \u2208 R.\nRecall Definition 7.3. Consider a point midway between the two points\n(that is \u03b8 = 0.5); then the left-hand side is f(0.5 \u00b7 2 + 0.5 \u00b7 4) = 3 log2 3 \u2248\n4.75. The right-hand side is 0.5(2 log2 2) + 0.5(4 log2 4) = 1 + 4 = 5. And\ntherefore the definition is satisfied.\nSince f(x) is differentiable, we can alternatively use (7.31). Calculating\nthe derivative of f(x), we obtain\n\u2207x(x log2 x) = 1 \u00b7 log2 x + x \u00b7 1\nx loge 2 = log2 x + 1\nloge 2 . (7.32)\nUsing the same two test points x = 2 and x = 4 , the left"
  },
  {
    "vector_id": 934,
    "chunk_id": "p243_c2",
    "page_number": 243,
    "text": "lternatively use (7.31). Calculating\nthe derivative of f(x), we obtain\n\u2207x(x log2 x) = 1 \u00b7 log2 x + x \u00b7 1\nx loge 2 = log2 x + 1\nloge 2 . (7.32)\nUsing the same two test points x = 2 and x = 4 , the left-hand side of\n(7.31) is given by f(4) = 8. The right-hand side is\nf(x) + \u2207\u22a4\nx (y \u2212 x) = f(2) + \u2207f(2) \u00b7 (4 \u2212 2) (7.33a)\n= 2 + (1 + 1\nloge 2) \u00b7 2 \u2248 6.9 . (7.33b)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 935,
    "chunk_id": "p244_c0",
    "page_number": 244,
    "text": "238 Continuous Optimization\nFigure 7.8 The\nnegative entropy\nfunction (which is\nconvex) and its\ntangent at x = 2.\n0 1 2 3 4 5\nx\n0\n5\n10f(x)\nx log2 x\ntangent at x = 2\nWe can check that a function or set is convex from first principles by\nrecalling the definitions. In practice, we often rely on operations that pre-\nserve convexity to check that a particular function or set is convex. Al-\nthough the details are vastly different, this is again the idea of closure\nthat we introduced in Chapter 2 for vector spaces.\nExample 7.4\nA nonnegative weighted sum of convex functions is convex. Observe that\nif f is a convex function, and \u03b1 \u2a7e 0 is a nonnegative scalar, then the\nfunction \u03b1f is convex. We can see this by multiplying\u03b1 to both sides of the\nequation in Definition 7.3, and recalling that multiplyin"
  },
  {
    "vector_id": 936,
    "chunk_id": "p244_c1",
    "page_number": 244,
    "text": "is a convex function, and \u03b1 \u2a7e 0 is a nonnegative scalar, then the\nfunction \u03b1f is convex. We can see this by multiplying\u03b1 to both sides of the\nequation in Definition 7.3, and recalling that multiplying a nonnegative\nnumber does not change the inequality .\nIf f1 and f2 are convex functions, then we have by the definition\nf1(\u03b8x + (1 \u2212 \u03b8)y) \u2a7d \u03b8f1(x) + (1\u2212 \u03b8)f1(y) (7.34)\nf2(\u03b8x + (1 \u2212 \u03b8)y) \u2a7d \u03b8f2(x) + (1\u2212 \u03b8)f2(y) . (7.35)\nSumming up both sides gives us\nf1(\u03b8x + (1 \u2212 \u03b8)y) + f2(\u03b8x + (1 \u2212 \u03b8)y)\n\u2a7d \u03b8f1(x) + (1\u2212 \u03b8)f1(y) + \u03b8f2(x) + (1\u2212 \u03b8)f2(y) , (7.36)\nwhere the right-hand side can be rearranged to\n\u03b8(f1(x) + f2(x)) + (1\u2212 \u03b8)(f1(y) + f2(y)) , (7.37)\ncompleting the proof that the sum of convex functions is convex.\nCombining the preceding two facts, we see that \u03b1f1(x) + \u03b2f2(x) is\nconvex for \u03b1, \u03b2\u2a7e 0. This clo"
  },
  {
    "vector_id": 937,
    "chunk_id": "p244_c2",
    "page_number": 244,
    "text": "f2(x)) + (1\u2212 \u03b8)(f1(y) + f2(y)) , (7.37)\ncompleting the proof that the sum of convex functions is convex.\nCombining the preceding two facts, we see that \u03b1f1(x) + \u03b2f2(x) is\nconvex for \u03b1, \u03b2\u2a7e 0. This closure property can be extended using a sim-\nilar argument for nonnegative weighted sums of more than two convex\nfunctions.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 938,
    "chunk_id": "p245_c0",
    "page_number": 245,
    "text": "7.3 Convex Optimization 239\nRemark. The inequality in (7.30) is sometimes called Jensen\u2019s inequality. Jensen\u2019s inequality\nIn fact, a whole class of inequalities for taking nonnegative weighted sums\nof convex functions are all called Jensen\u2019s inequality . \u2662\nIn summary , a constrained optimization problem is called aconvex opti- convex optimization\nproblemmization problem if\nmin\nx\nf(x)\nsubject to gi(x) \u2a7d 0 for all i = 1, . . . , m\nhj(x) = 0 for all j = 1, . . . , n ,\n(7.38)\nwhere all functions f(x) and gi(x) are convex functions, and all hj(x) =\n0 are convex sets. In the following, we will describe two classes of convex\noptimization problems that are widely used and well understood.\n7.3.1 Linear Programming\nConsider the special case when all the preceding functions are linear, i.e.,\nmin\nx\u2208Rd"
  },
  {
    "vector_id": 939,
    "chunk_id": "p245_c1",
    "page_number": 245,
    "text": "ibe two classes of convex\noptimization problems that are widely used and well understood.\n7.3.1 Linear Programming\nConsider the special case when all the preceding functions are linear, i.e.,\nmin\nx\u2208Rd\nc\u22a4x (7.39)\nsubject to Ax \u2a7d b,\nwhere A \u2208 Rm\u00d7d and b \u2208 Rm. This is known as a linear program. It has d linear program\nLinear programs are\none of the most\nwidely used\napproaches in\nindustry .\nvariables and m linear constraints. The Lagrangian is given by\nL(x, \u03bb) = c\u22a4x + \u03bb\u22a4(Ax \u2212 b) , (7.40)\nwhere \u03bb \u2208 Rm is the vector of non-negative Lagrange multipliers. Rear-\nranging the terms corresponding to x yields\nL(x, \u03bb) = (c + A\u22a4\u03bb)\u22a4x \u2212 \u03bb\u22a4b. (7.41)\nTaking the derivative of L(x, \u03bb) with respect to x and setting it to zero\ngives us\nc + A\u22a4\u03bb = 0 . (7.42)\nTherefore, the dual Lagrangian is D(\u03bb) = \u2212\u03bb\u22a4b. Recall we"
  },
  {
    "vector_id": 940,
    "chunk_id": "p245_c2",
    "page_number": 245,
    "text": "lds\nL(x, \u03bb) = (c + A\u22a4\u03bb)\u22a4x \u2212 \u03bb\u22a4b. (7.41)\nTaking the derivative of L(x, \u03bb) with respect to x and setting it to zero\ngives us\nc + A\u22a4\u03bb = 0 . (7.42)\nTherefore, the dual Lagrangian is D(\u03bb) = \u2212\u03bb\u22a4b. Recall we would like\nto maximize D(\u03bb). In addition to the constraint due to the derivative of\nL(x, \u03bb) being zero, we also have the fact that \u03bb \u2a7e 0, resulting in the\nfollowing dual optimization problem It is convention to\nminimize the primal\nand maximize the\ndual.\nmax\n\u03bb\u2208Rm\n\u2212 b\u22a4\u03bb (7.43)\nsubject to c + A\u22a4\u03bb = 0\n\u03bb \u2a7e 0 .\nThis is also a linear program, but with m variables. We have the choice\nof solving the primal (7.39) or the dual (7.43) program depending on\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 941,
    "chunk_id": "p245_c3",
    "page_number": 245,
    "text": "l (7.39) or the dual (7.43) program depending on\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 942,
    "chunk_id": "p246_c0",
    "page_number": 246,
    "text": "240 Continuous Optimization\nwhether m or d is larger. Recall thatd is the number of variables and m is\nthe number of constraints in the primal linear program.\nExample 7.5 (Linear Program)\nConsider the linear program\nmin\nx\u2208R2\n\u2212\n\u00145\n3\n\u0015\u22a4 \u0014x1\nx2\n\u0015\nsubject to\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n2 2\n2 \u22124\n\u22122 1\n0 \u22121\n0 1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u0014x1\nx2\n\u0015\n\u2a7d\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n33\n8\n5\n\u22121\n8\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(7.44)\nwith two variables. This program is also shown in Figure 7.9. The objective\nfunction is linear, resulting in linear contour lines. The constraint set in\nstandard form is translated into the legend. The optimal value must lie in\nthe shaded (feasible) region, and is indicated by the star.\nFigure 7.9\nIllustration of a\nlinear program. The\nunconstrained\nproblem (indicated\nby the contour\nlines) has a\nminimum on the\nright side. The\noptimal value given\nthe constra"
  },
  {
    "vector_id": 943,
    "chunk_id": "p246_c1",
    "page_number": 246,
    "text": "d is indicated by the star.\nFigure 7.9\nIllustration of a\nlinear program. The\nunconstrained\nproblem (indicated\nby the contour\nlines) has a\nminimum on the\nright side. The\noptimal value given\nthe constraints are\nshown by the star.\n0 2 4 6 8 10 12 14 16\nx1\n0\n2\n4\n6\n8\n10x2\n2x2 \u2264 33 \u2212 2x1\n4x2 \u2265 2x1 \u2212 8\nx2 \u2264 2x1 \u2212 5\nx2 \u2265 1\nx2 \u2264 8\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 944,
    "chunk_id": "p247_c0",
    "page_number": 247,
    "text": "7.3 Convex Optimization 241\n7.3.2 Quadratic Programming\nConsider the case of a convex quadratic objective function, where the con-\nstraints are affine, i.e.,\nmin\nx\u2208Rd\n1\n2x\u22a4Qx + c\u22a4x (7.45)\nsubject to Ax \u2a7d b,\nwhere A \u2208 Rm\u00d7d, b \u2208 Rm, and c \u2208 Rd. The square symmetric matrixQ \u2208\nRd\u00d7d is positive definite, and therefore the objective function is convex.\nThis is known as a quadratic program. Observe that it has d variables and\nm linear constraints.\nExample 7.6 (Quadratic Program)\nConsider the quadratic program\nmin\nx\u2208R2\n1\n2\n\u0014x1\nx2\n\u0015\u22a4 \u00142 1\n1 4\n\u0015\u0014x1\nx2\n\u0015\n+\n\u00145\n3\n\u0015\u22a4 \u0014x1\nx2\n\u0015\n(7.46)\nsubject to\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 0\n\u22121 0\n0 1\n0 \u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u0014x1\nx2\n\u0015\n\u2a7d\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n1\n1\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fb (7.47)\nof two variables. The program is also illustrated in Figure 7.4. The objec-\ntive function is quadratic with a positive semidefinite matrix Q, res"
  },
  {
    "vector_id": 945,
    "chunk_id": "p247_c1",
    "page_number": 247,
    "text": "0\n0 1\n0 \u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u0014x1\nx2\n\u0015\n\u2a7d\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n1\n1\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fb (7.47)\nof two variables. The program is also illustrated in Figure 7.4. The objec-\ntive function is quadratic with a positive semidefinite matrix Q, resulting\nin elliptical contour lines. The optimal value must lie in the shaded (feasi-\nble) region, and is indicated by the star.\nThe Lagrangian is given by\nL(x, \u03bb) = 1\n2x\u22a4Qx + c\u22a4x + \u03bb\u22a4(Ax \u2212 b) (7.48a)\n= 1\n2x\u22a4Qx + (c + A\u22a4\u03bb)\u22a4x \u2212 \u03bb\u22a4b, (7.48b)\nwhere again we have rearranged the terms. Taking the derivative ofL(x, \u03bb)\nwith respect to x and setting it to zero gives\nQx + (c + A\u22a4\u03bb) = 0 . (7.49)\nSince Q is positive definite and therefore invertible, we get\nx = \u2212Q\u22121(c + A\u22a4\u03bb) . (7.50)\nSubstituting (7.50) into the primal Lagrangian L(x, \u03bb), we get the dual\nLagrangian\nD(\u03bb) = \u22121\n2(c + A\u22a4\u03bb)\u22a4Q\u22121(c + A\u22a4\u03bb) \u2212 \u03bb\u22a4b."
  },
  {
    "vector_id": 946,
    "chunk_id": "p247_c2",
    "page_number": 247,
    "text": "ositive definite and therefore invertible, we get\nx = \u2212Q\u22121(c + A\u22a4\u03bb) . (7.50)\nSubstituting (7.50) into the primal Lagrangian L(x, \u03bb), we get the dual\nLagrangian\nD(\u03bb) = \u22121\n2(c + A\u22a4\u03bb)\u22a4Q\u22121(c + A\u22a4\u03bb) \u2212 \u03bb\u22a4b. (7.51)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 947,
    "chunk_id": "p248_c0",
    "page_number": 248,
    "text": "242 Continuous Optimization\nTherefore, the dual optimization problem is given by\nmax\n\u03bb\u2208Rm\n\u2212 1\n2(c + A\u22a4\u03bb)\u22a4Q\u22121(c + A\u22a4\u03bb) \u2212 \u03bb\u22a4b\nsubject to \u03bb \u2a7e 0 .\n(7.52)\nWe will see an application of quadratic programming in machine learning\nin Chapter 12.\n7.3.3 Legendre\u2013Fenchel Transform and Convex Conjugate\nLet us revisit the idea of duality from Section 7.2, without considering\nconstraints. One useful fact about a convex set is that it can be equiva-\nlently described by its supporting hyperplanes. A hyperplane is called a\nsupporting hyperplane of a convex set if it intersects the convex set, andsupporting\nhyperplane the convex set is contained on just one side of it. Recall that we can fill up\na convex function to obtain the epigraph, which is a convex set. Therefore,\nwe can also describe convex functions"
  },
  {
    "vector_id": 948,
    "chunk_id": "p248_c1",
    "page_number": 248,
    "text": "rplane the convex set is contained on just one side of it. Recall that we can fill up\na convex function to obtain the epigraph, which is a convex set. Therefore,\nwe can also describe convex functions in terms of their supporting hyper-\nplanes. Furthermore, observe that the supporting hyperplane just touches\nthe convex function, and is in fact the tangent to the function at that point.\nAnd recall that the tangent of a function f(x) at a given point x0 is the\nevaluation of the gradient of that function at that point df(x)\ndx\n\f\f\f\nx=x0\n. In\nsummary , because convex sets can be equivalently described by their sup-\nporting hyperplanes, convex functions can be equivalently described by a\nfunction of their gradient. TheLegendre transform formalizes this concept.Legendre transform\nPhysics students"
  },
  {
    "vector_id": 949,
    "chunk_id": "p248_c2",
    "page_number": 248,
    "text": "ed by their sup-\nporting hyperplanes, convex functions can be equivalently described by a\nfunction of their gradient. TheLegendre transform formalizes this concept.Legendre transform\nPhysics students are\noften introduced to\nthe Legendre\ntransform as\nrelating the\nLagrangian and the\nHamiltonian in\nclassical mechanics.\nWe begin with the most general definition, which unfortunately has a\ncounter-intuitive form, and look at special cases to relate the definition to\nthe intuition described in the preceding paragraph. The Legendre-Fenchel\nLegendre-Fenchel\ntransform\ntransform is a transformation (in the sense of a Fourier transform) from\na convex differentiable function f(x) to a function that depends on the\ntangents s(x) = \u2207xf(x). It is worth stressing that this is a transformation\nof the functio"
  },
  {
    "vector_id": 950,
    "chunk_id": "p248_c3",
    "page_number": 248,
    "text": "he sense of a Fourier transform) from\na convex differentiable function f(x) to a function that depends on the\ntangents s(x) = \u2207xf(x). It is worth stressing that this is a transformation\nof the function f(\u00b7) and not the variable x or the function evaluated at x.\nThe Legendre-Fenchel transform is also known as theconvex conjugate (forconvex conjugate\nreasons we will see soon) and is closely related to duality (Hiriart-Urruty\nand Lemar\u00b4echal, 2001, chapter 5).\nDefinition 7.4. The convex conjugate of a function f : RD \u2192 R is aconvex conjugate\nfunction f\u2217 defined by\nf\u2217(s) = sup\nx\u2208RD\n(\u27e8s, x\u27e9 \u2212f(x)) . (7.53)\nNote that the preceding convex conjugate definition does not need the\nfunction f to be convex nor differentiable. In Definition 7.4, we have used\na general inner product (Section 3.2) but in"
  },
  {
    "vector_id": 951,
    "chunk_id": "p248_c4",
    "page_number": 248,
    "text": ". (7.53)\nNote that the preceding convex conjugate definition does not need the\nfunction f to be convex nor differentiable. In Definition 7.4, we have used\na general inner product (Section 3.2) but in the rest of this section we\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 952,
    "chunk_id": "p249_c0",
    "page_number": 249,
    "text": "7.3 Convex Optimization 243\nwill consider the standard dot product between finite-dimensional vectors\n(\u27e8s, x\u27e9 = s\u22a4x) to avoid too many technical details.\nTo understand Definition 7.4 in a geometric fashion, consider a nice This derivation is\neasiest to\nunderstand by\ndrawing the\nreasoning as it\nprogresses.\nsimple one-dimensional convex and differentiable function, for example\nf(x) = x2. Note that since we are looking at a one-dimensional problem,\nhyperplanes reduce to a line. Consider a liney = sx+c. Recall that we are\nable to describe convex functions by their supporting hyperplanes, so let\nus try to describe this function f(x) by its supporting lines. Fix the gradi-\nent of the line s \u2208 R and for each point (x0, f(x0)) on the graph of f, find\nthe minimum value of c such that the line still"
  },
  {
    "vector_id": 953,
    "chunk_id": "p249_c1",
    "page_number": 249,
    "text": "try to describe this function f(x) by its supporting lines. Fix the gradi-\nent of the line s \u2208 R and for each point (x0, f(x0)) on the graph of f, find\nthe minimum value of c such that the line still intersects (x0, f(x0)). Note\nthat the minimum value of c is the place where a line with slope s \u201cjust\ntouches\u201d the function f(x) = x2. The line passing through (x0, f(x0))\nwith gradient s is given by\ny \u2212 f(x0) = s(x \u2212 x0) . (7.54)\nThe y-intercept of this line is \u2212sx0 + f(x0). The minimum of c for which\ny = sx + c intersects with the graph of f is therefore\ninf\nx0\n\u2212sx0 + f(x0) . (7.55)\nThe preceding convex conjugate is by convention defined to be the nega-\ntive of this. The reasoning in this paragraph did not rely on the fact that\nwe chose a one-dimensional convex and differentiable function,"
  },
  {
    "vector_id": 954,
    "chunk_id": "p249_c2",
    "page_number": 249,
    "text": "ng convex conjugate is by convention defined to be the nega-\ntive of this. The reasoning in this paragraph did not rely on the fact that\nwe chose a one-dimensional convex and differentiable function, and holds\nfor f : RD \u2192 R, which are nonconvex and non-differentiable. The classical\nLegendre transform\nis defined on convex\ndifferentiable\nfunctions in RD.\nRemark. Convex differentiable functions such as the examplef(x) = x2 is\na nice special case, where there is no need for the supremum, and there is\na one-to-one correspondence between a function and its Legendre trans-\nform. Let us derive this from first principles. For a convex differentiable\nfunction, we know that at x0 the tangent touches f(x0) so that\nf(x0) = sx0 + c . (7.56)\nRecall that we want to describe the convex function f(x) in te"
  },
  {
    "vector_id": 955,
    "chunk_id": "p249_c3",
    "page_number": 249,
    "text": "rom first principles. For a convex differentiable\nfunction, we know that at x0 the tangent touches f(x0) so that\nf(x0) = sx0 + c . (7.56)\nRecall that we want to describe the convex function f(x) in terms of its\ngradient \u2207xf(x), and that s = \u2207xf(x0). We rearrange to get an expres-\nsion for \u2212c to obtain\n\u2212 c = sx0 \u2212 f(x0) . (7.57)\nNote that \u2212c changes with x0 and therefore with s, which is why we can\nthink of it as a function of s, which we call\nf\u2217(s) := sx0 \u2212 f(x0) . (7.58)\nComparing (7.58) with Definition 7.4, we see that (7.58) is a special case\n(without the supremum). \u2662\nThe conjugate function has nice properties; for example, for convex\nfunctions, applying the Legendre transform again gets us back to the orig-\ninal function. In the same way that the slope off(x) is s, the slope off\u2217(s)\n\u00a92"
  },
  {
    "vector_id": 956,
    "chunk_id": "p249_c4",
    "page_number": 249,
    "text": "has nice properties; for example, for convex\nfunctions, applying the Legendre transform again gets us back to the orig-\ninal function. In the same way that the slope off(x) is s, the slope off\u2217(s)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 957,
    "chunk_id": "p250_c0",
    "page_number": 250,
    "text": "244 Continuous Optimization\nis x. The following two examples show common uses of convex conjugates\nin machine learning.\nExample 7.7 (Convex Conjugates)\nTo illustrate the application of convex conjugates, consider the quadratic\nfunction\nf(y) = \u03bb\n2 y\u22a4K\u22121y (7.59)\nbased on a positive definite matrix K \u2208 Rn\u00d7n. We denote the primal\nvariable to be y \u2208 Rn and the dual variable to be \u03b1 \u2208 Rn.\nApplying Definition 7.4, we obtain the function\nf\u2217(\u03b1) = sup\ny\u2208Rn\n\u27e8y, \u03b1\u27e9 \u2212\u03bb\n2 y\u22a4K\u22121y . (7.60)\nSince the function is differentiable, we can find the maximum by taking\nthe derivative and with respect to y setting it to zero.\n\u2202\n\u0002\n\u27e8y, \u03b1\u27e9 \u2212\u03bb\n2 y\u22a4K\u22121y\n\u0003\n\u2202y = (\u03b1 \u2212 \u03bbK\u22121y)\u22a4 (7.61)\nand hence when the gradient is zero we have y = 1\n\u03bb K\u03b1. Substituting\ninto (7.60) yields\nf\u2217(\u03b1) = 1\n\u03bb\u03b1\u22a4K\u03b1 \u2212 \u03bb\n2\n\u00121\n\u03bbK\u03b1\n\u0013\u22a4\nK\u22121\n\u00121\n\u03bbK\u03b1\n\u0013\n= 1\n2\u03bb\u03b1\u22a4K"
  },
  {
    "vector_id": 958,
    "chunk_id": "p250_c1",
    "page_number": 250,
    "text": "o zero.\n\u2202\n\u0002\n\u27e8y, \u03b1\u27e9 \u2212\u03bb\n2 y\u22a4K\u22121y\n\u0003\n\u2202y = (\u03b1 \u2212 \u03bbK\u22121y)\u22a4 (7.61)\nand hence when the gradient is zero we have y = 1\n\u03bb K\u03b1. Substituting\ninto (7.60) yields\nf\u2217(\u03b1) = 1\n\u03bb\u03b1\u22a4K\u03b1 \u2212 \u03bb\n2\n\u00121\n\u03bbK\u03b1\n\u0013\u22a4\nK\u22121\n\u00121\n\u03bbK\u03b1\n\u0013\n= 1\n2\u03bb\u03b1\u22a4K\u03b1 .\n(7.62)\nExample 7.8\nIn machine learning, we often use sums of functions; for example, the ob-\njective function of the training set includes a sum of the losses for each ex-\nample in the training set. In the following, we derive the convex conjugate\nof a sum of losses \u2113(t), where \u2113 : R \u2192 R. This also illustrates the appli-\ncation of the convex conjugate to the vector case. Let L(t) = Pn\ni=1 \u2113i(ti).\nThen,\nL\u2217(z) = sup\nt\u2208Rn\n\u27e8z, t\u27e9 \u2212\nnX\ni=1\n\u2113i(ti) (7.63a)\n= sup\nt\u2208Rn\nnX\ni=1\nziti \u2212 \u2113i(ti) definition of dot product (7.63b)\n=\nnX\ni=1\nsup\nt\u2208Rn\nziti \u2212 \u2113i(ti) (7.63c)\nDraft (2024-01-15) of \u201cMathematics f"
  },
  {
    "vector_id": 959,
    "chunk_id": "p250_c2",
    "page_number": 250,
    "text": "i).\nThen,\nL\u2217(z) = sup\nt\u2208Rn\n\u27e8z, t\u27e9 \u2212\nnX\ni=1\n\u2113i(ti) (7.63a)\n= sup\nt\u2208Rn\nnX\ni=1\nziti \u2212 \u2113i(ti) definition of dot product (7.63b)\n=\nnX\ni=1\nsup\nt\u2208Rn\nziti \u2212 \u2113i(ti) (7.63c)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 960,
    "chunk_id": "p251_c0",
    "page_number": 251,
    "text": "7.3 Convex Optimization 245\n=\nnX\ni=1\n\u2113\u2217\ni (zi) . definition of conjugate (7.63d)\nRecall that in Section 7.2 we derived a dual optimization problem using\nLagrange multipliers. Furthermore, for convex optimization problems we\nhave strong duality , that is the solutions of the primal and dual problem\nmatch. The Legendre-Fenchel transform described here also can be used\nto derive a dual optimization problem. Furthermore, when the function\nis convex and differentiable, the supremum is unique. To further investi-\ngate the relation between these two approaches, let us consider a linear\nequality constrained convex optimization problem.\nExample 7.9\nLet f(y) and g(x) be convex functions, andA a real matrix of appropriate\ndimensions such that Ax = y. Then\nmin\nx\nf(Ax) + g(x) = min\nAx=y\nf(y) + g(x). (7"
  },
  {
    "vector_id": 961,
    "chunk_id": "p251_c1",
    "page_number": 251,
    "text": "rained convex optimization problem.\nExample 7.9\nLet f(y) and g(x) be convex functions, andA a real matrix of appropriate\ndimensions such that Ax = y. Then\nmin\nx\nf(Ax) + g(x) = min\nAx=y\nf(y) + g(x). (7.64)\nBy introducing the Lagrange multiplier u for the constraints Ax = y,\nmin\nAx=y\nf(y) + g(x) = min\nx,y\nmax\nu\nf(y) + g(x) + (Ax \u2212 y)\u22a4u (7.65a)\n= max\nu\nmin\nx,y\nf(y) + g(x) + (Ax \u2212 y)\u22a4u, (7.65b)\nwhere the last step of swapping max and min is due to the fact that f(y)\nand g(x) are convex functions. By splitting up the dot product term and\ncollecting x and y,\nmax\nu\nmin\nx,y\nf(y) + g(x) + (Ax \u2212 y)\u22a4u (7.66a)\n= max\nu\n\u0014\nmin\ny\n\u2212y\u22a4u + f(y)\n\u0015\n+\nh\nmin\nx\n(Ax)\u22a4u + g(x)\ni\n(7.66b)\n= max\nu\n\u0014\nmin\ny\n\u2212y\u22a4u + f(y)\n\u0015\n+\nh\nmin\nx\nx\u22a4A\u22a4u + g(x)\ni\n(7.66c)\nRecall the convex conjugate (Definition 7.4) and the fact that dot"
  },
  {
    "vector_id": 962,
    "chunk_id": "p251_c2",
    "page_number": 251,
    "text": ".66a)\n= max\nu\n\u0014\nmin\ny\n\u2212y\u22a4u + f(y)\n\u0015\n+\nh\nmin\nx\n(Ax)\u22a4u + g(x)\ni\n(7.66b)\n= max\nu\n\u0014\nmin\ny\n\u2212y\u22a4u + f(y)\n\u0015\n+\nh\nmin\nx\nx\u22a4A\u22a4u + g(x)\ni\n(7.66c)\nRecall the convex conjugate (Definition 7.4) and the fact that dot prod- For general inner\nproducts, A\u22a4 is\nreplaced by the\nadjoint A\u2217.\nucts are symmetric,\nmax\nu\n\u0014\nmin\ny\n\u2212y\u22a4u + f(y)\n\u0015\n+\nh\nmin\nx\nx\u22a4A\u22a4u + g(x)\ni\n(7.67a)\n= max\nu\n\u2212f\u2217(u) \u2212 g\u2217(\u2212A\u22a4u) . (7.67b)\nTherefore, we have shown that\nmin\nx\nf(Ax) + g(x) = max\nu\n\u2212f\u2217(u) \u2212 g\u2217(\u2212A\u22a4u) . (7.68)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 963,
    "chunk_id": "p252_c0",
    "page_number": 252,
    "text": "246 Continuous Optimization\nThe Legendre-Fenchel conjugate turns out to be quite useful for ma-\nchine learning problems that can be expressed as convex optimization\nproblems. In particular, for convex loss functions that apply independently\nto each example, the conjugate loss is a convenient way to derive a dual\nproblem.\n7.4 Further Reading\nContinuous optimization is an active area of research, and we do not try\nto provide a comprehensive account of recent advances.\nFrom a gradient descent perspective, there are two major weaknesses\nwhich each have their own set of literature. The first challenge is the fact\nthat gradient descent is a first-order algorithm, and does not use infor-\nmation about the curvature of the surface. When there are long valleys,\nthe gradient points perpendicularly to"
  },
  {
    "vector_id": 964,
    "chunk_id": "p252_c1",
    "page_number": 252,
    "text": "nge is the fact\nthat gradient descent is a first-order algorithm, and does not use infor-\nmation about the curvature of the surface. When there are long valleys,\nthe gradient points perpendicularly to the direction of interest. The idea\nof momentum can be generalized to a general class of acceleration meth-\nods (Nesterov, 2018). Conjugate gradient methods avoid the issues faced\nby gradient descent by taking previous directions into account (Shewchuk,\n1994). Second-order methods such as Newton methods use the Hessian to\nprovide information about the curvature. Many of the choices for choos-\ning step-sizes and ideas like momentum arise by considering the curvature\nof the objective function (Goh, 2017; Bottou et al., 2018). Quasi-Newton\nmethods such as L-BFGS try to use cheaper computational"
  },
  {
    "vector_id": 965,
    "chunk_id": "p252_c2",
    "page_number": 252,
    "text": "step-sizes and ideas like momentum arise by considering the curvature\nof the objective function (Goh, 2017; Bottou et al., 2018). Quasi-Newton\nmethods such as L-BFGS try to use cheaper computational methods to ap-\nproximate the Hessian (Nocedal and Wright, 2006). Recently there has\nbeen interest in other metrics for computing descent directions, result-\ning in approaches such as mirror descent (Beck and Teboulle, 2003) and\nnatural gradient (Toussaint, 2012).\nThe second challenge is to handle non-differentiable functions. Gradi-\nent methods are not well defined when there are kinks in the function.\nIn these cases, subgradient methods can be used (Shor, 1985). For fur-\nther information and algorithms for optimizing non-differentiable func-\ntions, we refer to the book by Bertsekas (1999). Th"
  },
  {
    "vector_id": 966,
    "chunk_id": "p252_c3",
    "page_number": 252,
    "text": "tion.\nIn these cases, subgradient methods can be used (Shor, 1985). For fur-\nther information and algorithms for optimizing non-differentiable func-\ntions, we refer to the book by Bertsekas (1999). There is a vast amount\nof literature on different approaches for numerically solving continuous\noptimization problems, including algorithms for constrained optimization\nproblems. Good starting points to appreciate this literature are the books\nby Luenberger (1969) and Bonnans et al. (2006). A recent survey of con-\ntinuous optimization is provided by Bubeck (2015).Hugo Gonc \u00b8alves\u2019\nblog is also a good\nresource for an\neasier introduction\nto Legendre\u2013Fenchel\ntransforms:\nhttps://tinyurl.\ncom/ydaal7hj\nModern applications of machine learning often mean that the size of\ndatasets prohibit the use of bat"
  },
  {
    "vector_id": 967,
    "chunk_id": "p252_c4",
    "page_number": 252,
    "text": "d\nresource for an\neasier introduction\nto Legendre\u2013Fenchel\ntransforms:\nhttps://tinyurl.\ncom/ydaal7hj\nModern applications of machine learning often mean that the size of\ndatasets prohibit the use of batch gradient descent, and hence stochastic\ngradient descent is the current workhorse of large-scale machine learning\nmethods. Recent surveys of the literature include Hazan (2015) and Bot-\ntou et al. (2018).\nFor duality and convex optimization, the book by Boyd and Vanden-\nberghe (2004) includes lectures and slides online. A more mathematical\ntreatment is provided by Bertsekas (2009), and recent book by one of\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 968,
    "chunk_id": "p252_c5",
    "page_number": 252,
    "text": "ok by one of\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 969,
    "chunk_id": "p253_c0",
    "page_number": 253,
    "text": "Exercises 247\nthe key researchers in the area of optimization is Nesterov (2018). Con-\nvex optimization is based upon convex analysis, and the reader interested\nin more foundational results about convex functions is referred to Rock-\nafellar (1970), Hiriart-Urruty and Lemar \u00b4echal (2001), and Borwein and\nLewis (2006). Legendre\u2013Fenchel transforms are also covered in the afore-\nmentioned books on convex analysis, but a more beginner-friendly pre-\nsentation is available at Zia et al. (2009). The role of Legendre\u2013Fenchel\ntransforms in the analysis of convex optimization algorithms is surveyed\nin Polyak (2016).\nExercises\n7.1 Consider the univariate function\nf(x) = x3 + 6x2 \u2212 3x \u2212 5.\nFind its stationary points and indicate whether they are maximum, mini-\nmum, or saddle points.\n7.2 Consider the u"
  },
  {
    "vector_id": 970,
    "chunk_id": "p253_c1",
    "page_number": 253,
    "text": "olyak (2016).\nExercises\n7.1 Consider the univariate function\nf(x) = x3 + 6x2 \u2212 3x \u2212 5.\nFind its stationary points and indicate whether they are maximum, mini-\nmum, or saddle points.\n7.2 Consider the update equation for stochastic gradient descent (Equation (7.15)).\nWrite down the update when we use a mini-batch size of one.\n7.3 Consider whether the following statements are true or false:\na. The intersection of any two convex sets is convex.\nb. The union of any two convex sets is convex.\nc. The difference of a convex set A from another convex set B is convex.\n7.4 Consider whether the following statements are true or false:\na. The sum of any two convex functions is convex.\nb. The difference of any two convex functions is convex.\nc. The product of any two convex functions is convex.\nd. The ma"
  },
  {
    "vector_id": 971,
    "chunk_id": "p253_c2",
    "page_number": 253,
    "text": "statements are true or false:\na. The sum of any two convex functions is convex.\nb. The difference of any two convex functions is convex.\nc. The product of any two convex functions is convex.\nd. The maximum of any two convex functions is convex.\n7.5 Express the following optimization problem as a standard linear program in\nmatrix notation\nmax\nx\u2208R2, \u03be\u2208R\np\u22a4x + \u03be\nsubject to the constraints that \u03be \u2a7e 0, x0 \u2a7d 0 and x1 \u2a7d 3.\n7.6 Consider the linear program illustrated in Figure 7.9,\nmin\nx\u2208R2\n\u2212\n\u0014\n5\n3\n\u0015\u22a4 \u0014\nx1\nx2\n\u0015\nsubject to\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n2 2\n2 \u22124\n\u22122 1\n0 \u22121\n0 1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u0014\nx1\nx2\n\u0015\n\u2a7d\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n33\n8\n5\n\u22121\n8\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nDerive the dual linear program using Lagrange duality .\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 972,
    "chunk_id": "p253_c3",
    "page_number": 253,
    "text": "erive the dual linear program using Lagrange duality .\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 973,
    "chunk_id": "p254_c0",
    "page_number": 254,
    "text": "248 Continuous Optimization\n7.7 Consider the quadratic program illustrated in Figure 7.4,\nmin\nx\u2208R2\n1\n2\n\u0014\nx1\nx2\n\u0015\u22a4 \u0014\n2 1\n1 4\n\u0015\u0014\nx1\nx2\n\u0015\n+\n\u0014\n5\n3\n\u0015\u22a4 \u0014\nx1\nx2\n\u0015\nsubject to\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1 0\n\u22121 0\n0 1\n0 \u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fb\n\u0014\nx1\nx2\n\u0015\n\u2a7d\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n1\n1\n1\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fb\nDerive the dual quadratic program using Lagrange duality .\n7.8 Consider the following convex optimization problem\nmin\nw\u2208RD\n1\n2w\u22a4w\nsubject to w\u22a4x \u2a7e 1 .\nDerive the Lagrangian dual by introducing the Lagrange multiplier \u03bb.\n7.9 Consider the negative entropy of x \u2208 RD,\nf(x) =\nDX\nd=1\nxd log xd .\nDerive the convex conjugate function f\u2217(s), by assuming the standard dot\nproduct.\nHint: Take the gradient of an appropriate function and set the gradient to zero.\n7.10 Consider the function\nf(x) = 1\n2x\u22a4Ax + b\u22a4x + c ,\nwhere A is strictly positive definite, which means that it is i"
  },
  {
    "vector_id": 974,
    "chunk_id": "p254_c1",
    "page_number": 254,
    "text": ".\nHint: Take the gradient of an appropriate function and set the gradient to zero.\n7.10 Consider the function\nf(x) = 1\n2x\u22a4Ax + b\u22a4x + c ,\nwhere A is strictly positive definite, which means that it is invertible. Derive\nthe convex conjugate of f(x).\nHint: Take the gradient of an appropriate function and set the gradient to zero.\n7.11 The hinge loss (which is the loss used by the support vector machine) is\ngiven by\nL(\u03b1) = max{0, 1 \u2212 \u03b1},\nIf we are interested in applying gradient methods such as L-BFGS, and do\nnot want to resort to subgradient methods, we need to smooth the kink in\nthe hinge loss. Compute the convex conjugate of the hinge lossL\u2217(\u03b2) where\n\u03b2 is the dual variable. Add a \u21132 proximal term, and compute the conjugate\nof the resulting function\nL\u2217(\u03b2) + \u03b3\n2 \u03b22 ,\nwhere \u03b3 is a given hyperp"
  },
  {
    "vector_id": 975,
    "chunk_id": "p254_c2",
    "page_number": 254,
    "text": "Compute the convex conjugate of the hinge lossL\u2217(\u03b2) where\n\u03b2 is the dual variable. Add a \u21132 proximal term, and compute the conjugate\nof the resulting function\nL\u2217(\u03b2) + \u03b3\n2 \u03b22 ,\nwhere \u03b3 is a given hyperparameter.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 976,
    "chunk_id": "p255_c0",
    "page_number": 255,
    "text": "Part II\nCentral Machine Learning Problems\n249\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 977,
    "chunk_id": "p257_c0",
    "page_number": 257,
    "text": "8\nWhen Models Meet Data\nIn the first part of the book, we introduced the mathematics that form\nthe foundations of many machine learning methods. The hope is that a\nreader would be able to learn the rudimentary forms of the language of\nmathematics from the first part, which we will now use to describe and\ndiscuss machine learning. The second part of the book introduces four\npillars of machine learning:\nRegression (Chapter 9)\nDimensionality reduction (Chapter 10)\nDensity estimation (Chapter 11)\nClassification (Chapter 12)\nThe main aim of this part of the book is to illustrate how the mathematical\nconcepts introduced in the first part of the book can be used to design\nmachine learning algorithms that can be used to solve tasks within the\nremit of the four pillars. We do not intend to introduc"
  },
  {
    "vector_id": 978,
    "chunk_id": "p257_c1",
    "page_number": 257,
    "text": "l\nconcepts introduced in the first part of the book can be used to design\nmachine learning algorithms that can be used to solve tasks within the\nremit of the four pillars. We do not intend to introduce advanced machine\nlearning concepts, but instead to provide a set of practical methods that\nallow the reader to apply the knowledge they gained from the first part\nof the book. It also provides a gateway to the wider machine learning\nliterature for readers already familiar with the mathematics.\n8.1 Data, Models, and Learning\nIt is worth at this point, to pause and consider the problem that a ma-\nchine learning algorithm is designed to solve. As discussed in Chapter 1,\nthere are three major components of a machine learning system: data,\nmodels, and learning. The main question of machine learni"
  },
  {
    "vector_id": 979,
    "chunk_id": "p257_c2",
    "page_number": 257,
    "text": "chine learning algorithm is designed to solve. As discussed in Chapter 1,\nthere are three major components of a machine learning system: data,\nmodels, and learning. The main question of machine learning is \u201cWhat do\nwe mean by good models?\u201d. The word model has many subtleties, and we model\nwill revisit it multiple times in this chapter. It is also not entirely obvious\nhow to objectively define the word \u201cgood\u201d. One of the guiding principles\nof machine learning is that good models should perform well on unseen\ndata. This requires us to define some performance metrics, such as accu-\nracy or distance from ground truth, as well as figuring out ways to do well\nunder these performance metrics. This chapter covers a few necessary bits\nand pieces of mathematical and statistical language that are com"
  },
  {
    "vector_id": 980,
    "chunk_id": "p257_c3",
    "page_number": 257,
    "text": "ce from ground truth, as well as figuring out ways to do well\nunder these performance metrics. This chapter covers a few necessary bits\nand pieces of mathematical and statistical language that are commonly\n251\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 981,
    "chunk_id": "p258_c0",
    "page_number": 258,
    "text": "252 When Models Meet Data\nTable 8.1 Example\ndata from a\nfictitious human\nresource database\nthat is not in a\nnumerical format.\nName Gender Degree Postcode Age Annual salary\nAditya M MSc W21BG 36 89563\nBob M PhD EC1A1BA 47 123543\nChlo\u00b4e F BEcon SW1A1BH 26 23989\nDaisuke M BSc SE207AT 68 138769\nElisabeth F MBA SE10AA 33 113888\nused to talk about machine learning models. By doing so, we briefly out-\nline the current best practices for training a model such that the resulting\npredictor does well on data that we have not yet seen.\nAs mentioned in Chapter 1, there are two different senses in which we\nuse the phrase \u201cmachine learning algorithm\u201d: training and prediction. We\nwill describe these ideas in this chapter, as well as the idea of selecting\namong different models. We will introduce the frame"
  },
  {
    "vector_id": 982,
    "chunk_id": "p258_c1",
    "page_number": 258,
    "text": "use the phrase \u201cmachine learning algorithm\u201d: training and prediction. We\nwill describe these ideas in this chapter, as well as the idea of selecting\namong different models. We will introduce the framework of empirical\nrisk minimization in Section 8.2, the principle of maximum likelihood in\nSection 8.3, and the idea of probabilistic models in Section 8.4. We briefly\noutline a graphical language for specifying probabilistic models in Sec-\ntion 8.5 and finally discuss model selection in Section 8.6. The rest of this\nsection expands upon the three main components of machine learning:\ndata, models and learning.\n8.1.1 Data as Vectors\nWe assume that our data can be read by a computer, and represented ade-\nquately in a numerical format. Data is assumed to be tabular (Figure 8.1),\nwhere we think of"
  },
  {
    "vector_id": 983,
    "chunk_id": "p258_c2",
    "page_number": 258,
    "text": "and learning.\n8.1.1 Data as Vectors\nWe assume that our data can be read by a computer, and represented ade-\nquately in a numerical format. Data is assumed to be tabular (Figure 8.1),\nwhere we think of each row of the table as representing a particular in-\nstance or example, and each column to be a particular feature. In recentData is assumed to\nbe in a tidy\nformat (Wickham,\n2014; Codd, 1990).\nyears, machine learning has been applied to many types of data that do not\nobviously come in the tabular numerical format, for example genomic se-\nquences, text and image contents of a webpage, and social media graphs.\nWe do not discuss the important and challenging aspects of identifying\ngood features. Many of these aspects depend on domain expertise and re-\nquire careful engineering, and, in recent"
  },
  {
    "vector_id": 984,
    "chunk_id": "p258_c3",
    "page_number": 258,
    "text": "media graphs.\nWe do not discuss the important and challenging aspects of identifying\ngood features. Many of these aspects depend on domain expertise and re-\nquire careful engineering, and, in recent years, they have been put under\nthe umbrella of data science (Stray, 2016; Adhikari and DeNero, 2018).\nEven when we have data in tabular format, there are still choices to be\nmade to obtain a numerical representation. For example, in Table 8.1, the\ngender column (a categorical variable) may be converted into numbers 0\nrepresenting \u201cMale\u201d and 1 representing \u201cFemale\u201d. Alternatively , the gen-\nder could be represented by numbers \u22121, +1, respectively (as shown in\nTable 8.2). Furthermore, it is often important to use domain knowledge\nwhen constructing the representation, such as knowing that univer"
  },
  {
    "vector_id": 985,
    "chunk_id": "p258_c4",
    "page_number": 258,
    "text": "uld be represented by numbers \u22121, +1, respectively (as shown in\nTable 8.2). Furthermore, it is often important to use domain knowledge\nwhen constructing the representation, such as knowing that university\ndegrees progress from bachelor\u2019s to master\u2019s to PhD or realizing that the\npostcode provided is not just a string of characters but actually encodes\nan area in London. In Table 8.2, we converted the data from Table 8.1\nto a numerical format, and each postcode is represented as two numbers,\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 986,
    "chunk_id": "p259_c0",
    "page_number": 259,
    "text": "8.1 Data, Models, and Learning 253\nTable 8.2 Example\ndata from a\nfictitious human\nresource database\n(see Table 8.1),\nconverted to a\nnumerical format.\nGender ID Degree Latitude Longitude Age Annual Salary\n(in degrees) (in degrees) (in thousands)\n-1 2 51.5073 0.1290 36 89.563\n-1 3 51.5074 0.1275 47 123.543\n+1 1 51.5071 0.1278 26 23.989\n-1 1 51.5075 0.1281 68 138.769\n+1 2 51.5074 0.1278 33 113.888\na latitude and longitude. Even numerical data that could potentially be\ndirectly read into a machine learning algorithm should be carefully con-\nsidered for units, scaling, and constraints. Without additional information,\none should shift and scale all columns of the dataset such that they have\nan empirical mean of 0 and an empirical variance of 1. For the purposes\nof this book, we assume that a dom"
  },
  {
    "vector_id": 987,
    "chunk_id": "p259_c1",
    "page_number": 259,
    "text": "tional information,\none should shift and scale all columns of the dataset such that they have\nan empirical mean of 0 and an empirical variance of 1. For the purposes\nof this book, we assume that a domain expert already converted data ap-\npropriately , i.e., each inputxn is a D-dimensional vector of real numbers,\nwhich are calledfeatures, attributes, or covariates. We consider a dataset to feature\nattribute\ncovariate\nbe of the form as illustrated by Table 8.2. Observe that we have dropped\nthe Name column of Table 8.1 in the new numerical representation. There\nare two main reasons why this is desirable: (1) we do not expect the iden-\ntifier (the Name) to be informative for a machine learning task; and (2)\nwe may wish to anonymize the data to help protect the privacy of the\nemployees.\nIn this"
  },
  {
    "vector_id": 988,
    "chunk_id": "p259_c2",
    "page_number": 259,
    "text": "sirable: (1) we do not expect the iden-\ntifier (the Name) to be informative for a machine learning task; and (2)\nwe may wish to anonymize the data to help protect the privacy of the\nemployees.\nIn this part of the book, we will use N to denote the number of exam-\nples in a dataset and index the examples with lowercase n = 1, . . . , N.\nWe assume that we are given a set of numerical data, represented as an\narray of vectors (Table 8.2). Each row is a particular individual xn, often\nreferred to as an example or data point in machine learning. The subscript example\ndata pointn refers to the fact that this is the nth example out of a total of N exam-\nples in the dataset. Each column represents a particular feature of interest\nabout the example, and we index the features asd = 1, . . . , D. Recal"
  },
  {
    "vector_id": 989,
    "chunk_id": "p259_c3",
    "page_number": 259,
    "text": "at this is the nth example out of a total of N exam-\nples in the dataset. Each column represents a particular feature of interest\nabout the example, and we index the features asd = 1, . . . , D. Recall that\ndata is represented as vectors, which means that each example (each data\npoint) is a D-dimensional vector. The orientation of the table originates\nfrom the database community , but for some machine learning algorithms\n(e.g., in Chapter 10) it is more convenient to represent examples as col-\numn vectors.\nLet us consider the problem of predicting annual salary from age, based\non the data in Table 8.2. This is called a supervised learning problem\nwhere we have a label yn (the salary) associated with each example xn label\n(the age). The label yn has various other names, including target, re"
  },
  {
    "vector_id": 990,
    "chunk_id": "p259_c4",
    "page_number": 259,
    "text": "able 8.2. This is called a supervised learning problem\nwhere we have a label yn (the salary) associated with each example xn label\n(the age). The label yn has various other names, including target, re-\nsponse variable, and annotation. A dataset is written as a set of example-\nlabel pairs {(x1, y1), . . . ,(xn, yn), . . . ,(xN , yN )}. The table of examples\n{x1, . . . ,xN } is often concatenated, and written as X \u2208 RN\u00d7D. Fig-\nure 8.1 illustrates the dataset consisting of the two rightmost columns\nof Table 8.2, where x = age and y = salary.\nWe use the concepts introduced in the first part of the book to formalize\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 991,
    "chunk_id": "p259_c5",
    "page_number": 259,
    "text": "book to formalize\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 992,
    "chunk_id": "p260_c0",
    "page_number": 260,
    "text": "254 When Models Meet Data\nFigure 8.1 Toy data\nfor linear regression.\nTraining data in\n(xn, yn) pairs from\nthe rightmost two\ncolumns of\nTable 8.2. We are\ninterested in the\nsalary of a person\naged sixty (x = 60)\nillustrated as a\nvertical dashed red\nline, which is not\npart of the training\ndata.\n0 10 20 30 40 50 60 70 80\nx\n0\n25\n50\n75\n100\n125\n150y\n?\nthe machine learning problems such as that in the previous paragraph.\nRepresenting data as vectors xn allows us to use concepts from linear al-\ngebra (introduced in Chapter 2). In many machine learning algorithms,\nwe need to additionally be able to compare two vectors. As we will see in\nChapters 9 and 12, computing the similarity or distance between two ex-\namples allows us to formalize the intuition that examples with similar fea-\ntures should have"
  },
  {
    "vector_id": 993,
    "chunk_id": "p260_c1",
    "page_number": 260,
    "text": "are two vectors. As we will see in\nChapters 9 and 12, computing the similarity or distance between two ex-\namples allows us to formalize the intuition that examples with similar fea-\ntures should have similar labels. The comparison of two vectors requires\nthat we construct a geometry (explained in Chapter 3) and allows us to\noptimize the resulting learning problem using techniques from Chapter 7.\nSince we have vector representations of data, we can manipulate data to\nfind potentially better representations of it. We will discuss finding good\nrepresentations in two ways: finding lower-dimensional approximations\nof the original feature vector, and using nonlinear higher-dimensional\ncombinations of the original feature vector. In Chapter 10, we will see an\nexample of finding a low-dimensional"
  },
  {
    "vector_id": 994,
    "chunk_id": "p260_c2",
    "page_number": 260,
    "text": "al approximations\nof the original feature vector, and using nonlinear higher-dimensional\ncombinations of the original feature vector. In Chapter 10, we will see an\nexample of finding a low-dimensional approximation of the original data\nspace by finding the principal components. Finding principal components\nis closely related to concepts of eigenvalue and singular value decomposi-\ntion as introduced in Chapter 4. For the high-dimensional representation,\nwe will see an explicit feature map \u03d5(\u00b7) that allows us to represent in-feature map\nputs xn using a higher-dimensional representation \u03d5(xn). The main mo-\ntivation for higher-dimensional representations is that we can construct\nnew features as non-linear combinations of the original features, which in\nturn may make the learning problem easier"
  },
  {
    "vector_id": 995,
    "chunk_id": "p260_c3",
    "page_number": 260,
    "text": "e main mo-\ntivation for higher-dimensional representations is that we can construct\nnew features as non-linear combinations of the original features, which in\nturn may make the learning problem easier. We will discuss the feature\nmap in Section 9.2 and show how this feature map leads to a kernel inkernel\nSection 12.4. In recent years, deep learning methods (Goodfellow et al.,\n2016) have shown promise in using the data itself to learn new good fea-\ntures and have been very successful in areas, such as computer vision,\nspeech recognition, and natural language processing. We will not cover\nneural networks in this part of the book, but the reader is referred to\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 996,
    "chunk_id": "p260_c4",
    "page_number": 260,
    "text": "networks in this part of the book, but the reader is referred to\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 997,
    "chunk_id": "p261_c0",
    "page_number": 261,
    "text": "8.1 Data, Models, and Learning 255\nFigure 8.2 Example\nfunction (black solid\ndiagonal line) and\nits prediction at\nx = 60, i.e.,\nf(60) = 100.\n0 10 20 30 40 50 60 70 80\nx\n0\n25\n50\n75\n100\n125\n150y\nSection 5.6 for the mathematical description of backpropagation, a key\nconcept for training neural networks.\n8.1.2 Models as Functions\nOnce we have data in an appropriate vector representation, we can get to\nthe business of constructing a predictive function (known as a predictor). predictor\nIn Chapter 1, we did not yet have the language to be precise about models.\nUsing the concepts from the first part of the book, we can now introduce\nwhat \u201cmodel\u201d means. We present two major approaches in this book: a\npredictor as a function, and a predictor as a probabilistic model. We de-\nscribe the former here an"
  },
  {
    "vector_id": 998,
    "chunk_id": "p261_c1",
    "page_number": 261,
    "text": "f the book, we can now introduce\nwhat \u201cmodel\u201d means. We present two major approaches in this book: a\npredictor as a function, and a predictor as a probabilistic model. We de-\nscribe the former here and the latter in the next subsection.\nA predictor is a function that, when given a particular input example\n(in our case, a vector of features), produces an output. For now, consider\nthe output to be a single number, i.e., a real-valued scalar output. This can\nbe written as\nf : RD \u2192 R, (8.1)\nwhere the input vectorx is D-dimensional (has D features), and the func-\ntion f then applied to it (written as f(x)) returns a real number. Fig-\nure 8.2 illustrates a possible function that can be used to compute the\nvalue of the prediction for input values x.\nIn this book, we do not consider the general ca"
  },
  {
    "vector_id": 999,
    "chunk_id": "p261_c2",
    "page_number": 261,
    "text": "as f(x)) returns a real number. Fig-\nure 8.2 illustrates a possible function that can be used to compute the\nvalue of the prediction for input values x.\nIn this book, we do not consider the general case of all functions, which\nwould involve the need for functional analysis. Instead, we consider the\nspecial case of linear functions\nf(x) = \u03b8\u22a4x + \u03b80 (8.2)\nfor unknown \u03b8 and \u03b80. This restriction means that the contents of Chap-\nters 2 and 3 suffice for precisely stating the notion of a predictor for\nthe non-probabilistic (in contrast to the probabilistic view described next)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1000,
    "chunk_id": "p261_c3",
    "page_number": 261,
    "text": "A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1001,
    "chunk_id": "p262_c0",
    "page_number": 262,
    "text": "256 When Models Meet Data\nFigure 8.3 Example\nfunction (black solid\ndiagonal line) and\nits predictive\nuncertainty at\nx = 60 (drawn as a\nGaussian).\n0 10 20 30 40 50 60 70 80\nx\n0\n25\n50\n75\n100\n125\n150y\nview of machine learning. Linear functions strike a good balance between\nthe generality of the problems that can be solved and the amount of back-\nground mathematics that is needed.\n8.1.3 Models as Probability Distributions\nWe often consider data to be noisy observations of some true underlying\neffect, and hope that by applying machine learning we can identify the\nsignal from the noise. This requires us to have a language for quantify-\ning the effect of noise. We often would also like to have predictors that\nexpress some sort of uncertainty , e.g., to quantify the confidence we have\nabout the va"
  },
  {
    "vector_id": 1002,
    "chunk_id": "p262_c1",
    "page_number": 262,
    "text": "s us to have a language for quantify-\ning the effect of noise. We often would also like to have predictors that\nexpress some sort of uncertainty , e.g., to quantify the confidence we have\nabout the value of the prediction for a particular test data point. As we\nhave seen in Chapter 6, probability theory provides a language for quan-\ntifying uncertainty . Figure 8.3 illustrates the predictive uncertainty of the\nfunction as a Gaussian distribution.\nInstead of considering a predictor as a single function, we could con-\nsider predictors to be probabilistic models, i.e., models describing the dis-\ntribution of possible functions. We limit ourselves in this book to the spe-\ncial case of distributions with finite-dimensional parameters, which allows\nus to describe probabilistic models without nee"
  },
  {
    "vector_id": 1003,
    "chunk_id": "p262_c2",
    "page_number": 262,
    "text": "tribution of possible functions. We limit ourselves in this book to the spe-\ncial case of distributions with finite-dimensional parameters, which allows\nus to describe probabilistic models without needing stochastic processes\nand random measures. For this special case, we can think about prob-\nabilistic models as multivariate probability distributions, which already\nallow for a rich class of models.\nWe will introduce how to use concepts from probability (Chapter 6) to\ndefine machine learning models in Section 8.4, and introduce a graphical\nlanguage for describing probabilistic models in a compact way in Sec-\ntion 8.5.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1004,
    "chunk_id": "p262_c3",
    "page_number": 262,
    "text": "act way in Sec-\ntion 8.5.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1005,
    "chunk_id": "p263_c0",
    "page_number": 263,
    "text": "8.1 Data, Models, and Learning 257\n8.1.4 Learning is Finding Parameters\nThe goal of learning is to find a model and its corresponding parame-\nters such that the resulting predictor will perform well on unseen data.\nThere are conceptually three distinct algorithmic phases when discussing\nmachine learning algorithms:\n1. Prediction or inference\n2. Training or parameter estimation\n3. Hyperparameter tuning or model selection\nThe prediction phase is when we use a trained predictor on previously un-\nseen test data. In other words, the parameters and model choice is already\nfixed and the predictor is applied to new vectors representing new input\ndata points. As outlined in Chapter 1 and the previous subsection, we will\nconsider two schools of machine learning in this book, corresponding to\nwhether"
  },
  {
    "vector_id": 1006,
    "chunk_id": "p263_c1",
    "page_number": 263,
    "text": "applied to new vectors representing new input\ndata points. As outlined in Chapter 1 and the previous subsection, we will\nconsider two schools of machine learning in this book, corresponding to\nwhether the predictor is a function or a probabilistic model. When we\nhave a probabilistic model (discussed further in Section 8.4) the predic-\ntion phase is called inference.\nRemark. Unfortunately , there is no agreed upon naming for the different\nalgorithmic phases. The word \u201cinference\u201d is sometimes also used to mean\nparameter estimation of a probabilistic model, and less often may be also\nused to mean prediction for non-probabilistic models. \u2662\nThe training or parameter estimation phase is when we adjust our pre-\ndictive model based on training data. We would like to find good predic-\ntors given tr"
  },
  {
    "vector_id": 1007,
    "chunk_id": "p263_c2",
    "page_number": 263,
    "text": "prediction for non-probabilistic models. \u2662\nThe training or parameter estimation phase is when we adjust our pre-\ndictive model based on training data. We would like to find good predic-\ntors given training data, and there are two main strategies for doing so:\nfinding the best predictor based on some measure of quality (sometimes\ncalled finding a point estimate), or using Bayesian inference. Finding a\npoint estimate can be applied to both types of predictors, but Bayesian\ninference requires probabilistic models.\nFor the non-probabilistic model, we follow the principle ofempirical risk empirical risk\nminimizationminimization, which we describe in Section 8.2. Empirical risk minimiza-\ntion directly provides an optimization problem for finding good parame-\nters. With a statistical model, the"
  },
  {
    "vector_id": 1008,
    "chunk_id": "p263_c3",
    "page_number": 263,
    "text": "l risk\nminimizationminimization, which we describe in Section 8.2. Empirical risk minimiza-\ntion directly provides an optimization problem for finding good parame-\nters. With a statistical model, the principle of maximum likelihood is used maximum likelihood\nto find a good set of parameters (Section 8.3). We can additionally model\nthe uncertainty of parameters using a probabilistic model, which we will\nlook at in more detail in Section 8.4.\nWe use numerical methods to find good parameters that \u201cfit\u201d the data,\nand most training methods can be thought of as hill-climbing approaches\nto find the maximum of an objective, for example the maximum of a likeli-\nhood. To apply hill-climbing approaches we use the gradients described in The convention in\noptimization is to\nminimize objectives.\nHence,"
  },
  {
    "vector_id": 1009,
    "chunk_id": "p263_c4",
    "page_number": 263,
    "text": "aximum of an objective, for example the maximum of a likeli-\nhood. To apply hill-climbing approaches we use the gradients described in The convention in\noptimization is to\nminimize objectives.\nHence, there is often\nan extra minus sign\nin machine learning\nobjectives.\nChapter 5 and implement numerical optimization approaches from Chap-\nter 7.\nAs mentioned in Chapter 1, we are interested in learning a model based\non data such that it performs well on future data. It is not enough for\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1010,
    "chunk_id": "p264_c0",
    "page_number": 264,
    "text": "258 When Models Meet Data\nthe model to only fit the training data well, the predictor needs to per-\nform well on unseen data. We simulate the behavior of our predictor on\nfuture unseen data using cross-validation (Section 8.2.4). As we will seecross-validation\nin this chapter, to achieve the goal of performing well on unseen data,\nwe will need to balance between fitting well on training data and finding\n\u201csimple\u201d explanations of the phenomenon. This trade-off is achieved us-\ning regularization (Section 8.2.3) or by adding a prior (Section 8.3.2). In\nphilosophy , this is considered to be neither induction nor deduction, but\nis called abduction. According to the Stanford Encyclopedia of Philosophy ,abduction\nabduction is the process of inference to the best explanation (Douven,\n2017).A good m"
  },
  {
    "vector_id": 1011,
    "chunk_id": "p264_c1",
    "page_number": 264,
    "text": "induction nor deduction, but\nis called abduction. According to the Stanford Encyclopedia of Philosophy ,abduction\nabduction is the process of inference to the best explanation (Douven,\n2017).A good movie title is\n\u201cAI abduction\u201d. We often need to make high-level modeling decisions about the struc-\nture of the predictor, such as the number of components to use or the\nclass of probability distributions to consider. The choice of the number of\ncomponents is an example of a hyperparameter, and this choice can af-hyperparameter\nfect the performance of the model significantly . The problem of choosing\namong different models is called model selection , which we describe inmodel selection\nSection 8.6. For non-probabilistic models, model selection is often done\nusing nested cross-validation, which"
  },
  {
    "vector_id": 1012,
    "chunk_id": "p264_c2",
    "page_number": 264,
    "text": "ng\namong different models is called model selection , which we describe inmodel selection\nSection 8.6. For non-probabilistic models, model selection is often done\nusing nested cross-validation, which is described in Section 8.6.1. We alsonested\ncross-validation use model selection to choose hyperparameters of our model.\nRemark. The distinction between parameters and hyperparameters is some-\nwhat arbitrary , and is mostly driven by the distinction between what can\nbe numerically optimized versus what needs to use search techniques.\nAnother way to consider the distinction is to consider parameters as the\nexplicit parameters of a probabilistic model, and to consider hyperparam-\neters (higher-level parameters) as parameters that control the distribution\nof these explicit parameters. \u2662\nIn the f"
  },
  {
    "vector_id": 1013,
    "chunk_id": "p264_c3",
    "page_number": 264,
    "text": "rs as the\nexplicit parameters of a probabilistic model, and to consider hyperparam-\neters (higher-level parameters) as parameters that control the distribution\nof these explicit parameters. \u2662\nIn the following sections, we will look at three flavors of machine learn-\ning: empirical risk minimization (Section 8.2), the principle of maximum\nlikelihood (Section 8.3), and probabilistic modeling (Section 8.4).\n8.2 Empirical Risk Minimization\nAfter having all the mathematics under our belt, we are now in a posi-\ntion to introduce what it means to learn. The \u201clearning\u201d part of machine\nlearning boils down to estimating parameters based on training data.\nIn this section, we consider the case of a predictor that is a function,\nand consider the case of probabilistic models in Section 8.3. We describe"
  },
  {
    "vector_id": 1014,
    "chunk_id": "p264_c4",
    "page_number": 264,
    "text": "own to estimating parameters based on training data.\nIn this section, we consider the case of a predictor that is a function,\nand consider the case of probabilistic models in Section 8.3. We describe\nthe idea of empirical risk minimization, which was originally popularized\nby the proposal of the support vector machine (described in Chapter 12).\nHowever, its general principles are widely applicable and allow us to ask\nthe question of what is learning without explicitly constructing probabilis-\ntic models. There are four main design choices, which we will cover in\ndetail in the following subsections:\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1015,
    "chunk_id": "p264_c5",
    "page_number": 264,
    "text": "ions:\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1016,
    "chunk_id": "p265_c0",
    "page_number": 265,
    "text": "8.2 Empirical Risk Minimization 259\nSection 8.2.1 What is the set of functions we allow the predictor to take?\nSection 8.2.2 How do we measure how well the predictor performs on\nthe training data?\nSection 8.2.3 How do we construct predictors from only training data\nthat performs well on unseen test data?\nSection 8.2.4 What is the procedure for searching over the space of mod-\nels?\n8.2.1 Hypothesis Class of Functions\nAssume we are given N examples xn \u2208 RD and corresponding scalar la-\nbels yn \u2208 R. We consider the supervised learning setting, where we obtain\npairs (x1, y1), . . . ,(xN , yN ). Given this data, we would like to estimate a\npredictor f(\u00b7, \u03b8) : RD \u2192 R, parametrized by \u03b8. We hope to be able to find\na good parameter \u03b8\u2217 such that we fit the data well, that is,\nf(xn, \u03b8\u2217) \u2248 yn for all"
  },
  {
    "vector_id": 1017,
    "chunk_id": "p265_c1",
    "page_number": 265,
    "text": "en this data, we would like to estimate a\npredictor f(\u00b7, \u03b8) : RD \u2192 R, parametrized by \u03b8. We hope to be able to find\na good parameter \u03b8\u2217 such that we fit the data well, that is,\nf(xn, \u03b8\u2217) \u2248 yn for all n = 1, . . . , N . (8.3)\nIn this section, we use the notation\u02c6yn = f(xn, \u03b8\u2217) to represent the output\nof the predictor.\nRemark. For ease of presentation, we will describe empirical risk mini-\nmization in terms of supervised learning (where we have labels). This\nsimplifies the definition of the hypothesis class and the loss function. It\nis also common in machine learning to choose a parametrized class of\nfunctions, for example affine functions. \u2662\nExample 8.1\nWe introduce the problem of ordinary least-squares regression to illustrate\nempirical risk minimization. A more comprehensive account of re"
  },
  {
    "vector_id": 1018,
    "chunk_id": "p265_c2",
    "page_number": 265,
    "text": "ss of\nfunctions, for example affine functions. \u2662\nExample 8.1\nWe introduce the problem of ordinary least-squares regression to illustrate\nempirical risk minimization. A more comprehensive account of regression\nis given in Chapter 9. When the label yn is real-valued, a popular choice\nof function class for predictors is the set of affine functions. We choose a Affine functions are\noften referred to as\nlinear functions in\nmachine learning.\nmore compact notation for an affine function by concatenating an addi-\ntional unit feature x(0) = 1 to xn, i.e., xn = [1, x(1)\nn , x(2)\nn , . . . , x(D)\nn ]\u22a4. The\nparameter vector is correspondingly \u03b8 = [\u03b80, \u03b81, \u03b82, . . . , \u03b8D]\u22a4, allowing us\nto write the predictor as a linear function\nf(xn, \u03b8) = \u03b8\u22a4xn . (8.4)\nThis linear predictor is equivalent to the affine"
  },
  {
    "vector_id": 1019,
    "chunk_id": "p265_c3",
    "page_number": 265,
    "text": "he\nparameter vector is correspondingly \u03b8 = [\u03b80, \u03b81, \u03b82, . . . , \u03b8D]\u22a4, allowing us\nto write the predictor as a linear function\nf(xn, \u03b8) = \u03b8\u22a4xn . (8.4)\nThis linear predictor is equivalent to the affine model\nf(xn, \u03b8) = \u03b80 +\nDX\nd=1\n\u03b8dx(d)\nn . (8.5)\nThe predictor takes the vector of features representing a single example\nxn as input and produces a real-valued output, i.e., f : RD+1 \u2192 R. The\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1020,
    "chunk_id": "p266_c0",
    "page_number": 266,
    "text": "260 When Models Meet Data\nprevious figures in this chapter had a straight line as a predictor, which\nmeans that we have assumed an affine function.\nInstead of a linear function, we may wish to consider non-linear func-\ntions as predictors. Recent advances in neural networks allow for efficient\ncomputation of more complex non-linear function classes.\nGiven the class of functions, we want to search for a good predictor.\nWe now move on to the second ingredient of empirical risk minimization:\nhow to measure how well the predictor fits the training data.\n8.2.2 Loss Function for Training\nConsider the label yn for a particular example; and the corresponding pre-\ndiction \u02c6yn that we make based on xn. To define what it means to fit the\ndata well, we need to specify aloss function \u2113(yn, \u02c6yn) that ta"
  },
  {
    "vector_id": 1021,
    "chunk_id": "p266_c1",
    "page_number": 266,
    "text": "e label yn for a particular example; and the corresponding pre-\ndiction \u02c6yn that we make based on xn. To define what it means to fit the\ndata well, we need to specify aloss function \u2113(yn, \u02c6yn) that takes the groundloss function\ntruth label and the prediction as input and produces a non-negative num-\nber (referred to as the loss) representing how much error we have made\non this particular prediction. Our goal for finding a good parameter vectorThe expression\n\u201cerror\u201d is often used\nto mean loss.\n\u03b8\u2217 is to minimize the average loss on the set of N training examples.\nOne assumption that is commonly made in machine learning is that\nthe set of examples (x1, y1), . . . ,(xN , yN ) is independent and identicallyindependent and\nidentically\ndistributed\ndistributed. The word independent (Section 6.4.5)"
  },
  {
    "vector_id": 1022,
    "chunk_id": "p266_c2",
    "page_number": 266,
    "text": "made in machine learning is that\nthe set of examples (x1, y1), . . . ,(xN , yN ) is independent and identicallyindependent and\nidentically\ndistributed\ndistributed. The word independent (Section 6.4.5) means that two data\npoints (xi, yi) and (xj, yj) do not statistically depend on each other, mean-\ning that the empirical mean is a good estimate of the population mean\n(Section 6.4.1). This implies that we can use the empirical mean of the\nloss on the training data. For a giventraining set {(x1, y1), . . . ,(xN , yN )},training set\nwe introduce the notation of an example matrix X := [x1, . . . ,xN ]\u22a4 \u2208\nRN\u00d7D and a label vector y := [ y1, . . . , yN ]\u22a4 \u2208 RN . Using this matrix\nnotation the average loss is given by\nRemp(f, X, y) = 1\nN\nNX\nn=1\n\u2113(yn, \u02c6yn) , (8.6)\nwhere \u02c6yn = f(xn, \u03b8). Equation (8.6"
  },
  {
    "vector_id": 1023,
    "chunk_id": "p266_c3",
    "page_number": 266,
    "text": "N ]\u22a4 \u2208\nRN\u00d7D and a label vector y := [ y1, . . . , yN ]\u22a4 \u2208 RN . Using this matrix\nnotation the average loss is given by\nRemp(f, X, y) = 1\nN\nNX\nn=1\n\u2113(yn, \u02c6yn) , (8.6)\nwhere \u02c6yn = f(xn, \u03b8). Equation (8.6) is called the empirical risk and de-empirical risk\npends on three arguments, the predictorf and the dataX, y. This general\nstrategy for learning is called empirical risk minimization.empirical risk\nminimization\nExample 8.2 (Least-Squares Loss)\nContinuing the example of least-squares regression, we specify that we\nmeasure the cost of making an error during training using the squared\nloss \u2113(yn, \u02c6yn) = (yn \u2212 \u02c6yn)2. We wish to minimize the empirical risk (8.6),\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1024,
    "chunk_id": "p266_c4",
    "page_number": 266,
    "text": "n) = (yn \u2212 \u02c6yn)2. We wish to minimize the empirical risk (8.6),\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1025,
    "chunk_id": "p267_c0",
    "page_number": 267,
    "text": "8.2 Empirical Risk Minimization 261\nwhich is the average of the losses over the data\nmin\n\u03b8\u2208RD\n1\nN\nNX\nn=1\n(yn \u2212 f(xn, \u03b8))2, (8.7)\nwhere we substituted the predictor \u02c6yn = f(xn, \u03b8). By using our choice of\na linear predictor f(xn, \u03b8) = \u03b8\u22a4xn, we obtain the optimization problem\nmin\n\u03b8\u2208RD\n1\nN\nNX\nn=1\n(yn \u2212 \u03b8\u22a4xn)2 . (8.8)\nThis equation can be equivalently expressed in matrix form\nmin\n\u03b8\u2208RD\n1\nN \u2225y \u2212 X\u03b8\u2225\n2\n. (8.9)\nThis is known as the least-squares problem. There exists a closed-form an- least-squares\nproblemalytic solution for this by solving the normal equations, which we will\ndiscuss in Section 9.2.\nWe are not interested in a predictor that only performs well on the\ntraining data. Instead, we seek a predictor that performs well (has low\nrisk) on unseen test data. More formally , we are interested i"
  },
  {
    "vector_id": 1026,
    "chunk_id": "p267_c1",
    "page_number": 267,
    "text": "are not interested in a predictor that only performs well on the\ntraining data. Instead, we seek a predictor that performs well (has low\nrisk) on unseen test data. More formally , we are interested in finding a\npredictor f (with parameters fixed) that minimizes the expected risk expected risk\nRtrue(f) = Ex,y[\u2113(y, f(x))] , (8.10)\nwhere y is the label and f(x) is the prediction based on the example x.\nThe notation Rtrue(f) indicates that this is the true risk if we had access to\nan infinite amount of data. The expectation is over the (infinite) set of all Another phrase\ncommonly used for\nexpected risk is\n\u201cpopulation risk\u201d.\npossible data and labels. There are two practical questions that arise from\nour desire to minimize expected risk, which we address in the following\ntwo subsections:\nHow s"
  },
  {
    "vector_id": 1027,
    "chunk_id": "p267_c2",
    "page_number": 267,
    "text": "ed risk is\n\u201cpopulation risk\u201d.\npossible data and labels. There are two practical questions that arise from\nour desire to minimize expected risk, which we address in the following\ntwo subsections:\nHow should we change our training procedure to generalize well?\nHow do we estimate expected risk from (finite) data?\nRemark. Many machine learning tasks are specified with an associated\nperformance measure, e.g., accuracy of prediction or root mean squared\nerror. The performance measure could be more complex, be cost sensitive,\nand capture details about the particular application. In principle, the de-\nsign of the loss function for empirical risk minimization should correspond\ndirectly to the performance measure specified by the machine learning\ntask. In practice, there is often a mismatch between"
  },
  {
    "vector_id": 1028,
    "chunk_id": "p267_c3",
    "page_number": 267,
    "text": "sign of the loss function for empirical risk minimization should correspond\ndirectly to the performance measure specified by the machine learning\ntask. In practice, there is often a mismatch between the design of the loss\nfunction and the performance measure. This could be due to issues such\nas ease of implementation or efficiency of optimization. \u2662\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1029,
    "chunk_id": "p268_c0",
    "page_number": 268,
    "text": "262 When Models Meet Data\n8.2.3 Regularization to Reduce Overfitting\nThis section describes an addition to empirical risk minimization that al-\nlows it to generalize well (approximately minimizing expected risk). Re-\ncall that the aim of training a machine learning predictor is so that we can\nperform well on unseen data, i.e., the predictor generalizes well. We sim-\nulate this unseen data by holding out a proportion of the whole dataset.\nThis hold out set is referred to as thetest set. Given a sufficiently rich classtest set\nEven knowing only\nthe performance of\nthe predictor on the\ntest set leaks\ninformation (Blum\nand Hardt, 2015).\nof functions for the predictor f, we can essentially memorize the training\ndata to obtain zero empirical risk. While this is great to minimize the loss\n(and the"
  },
  {
    "vector_id": 1030,
    "chunk_id": "p268_c1",
    "page_number": 268,
    "text": "aks\ninformation (Blum\nand Hardt, 2015).\nof functions for the predictor f, we can essentially memorize the training\ndata to obtain zero empirical risk. While this is great to minimize the loss\n(and therefore the risk) on the training data, we would not expect the\npredictor to generalize well to unseen data. In practice, we have only a\nfinite set of data, and hence we split our data into a training and a test\nset. The training set is used to fit the model, and the test set (not seen\nby the machine learning algorithm during training) is used to evaluate\ngeneralization performance. It is important for the user to not cycle back\nto a new round of training after having observed the test set. We use the\nsubscripts train and test to denote the training and test sets, respectively .\nWe will revisit"
  },
  {
    "vector_id": 1031,
    "chunk_id": "p268_c2",
    "page_number": 268,
    "text": "for the user to not cycle back\nto a new round of training after having observed the test set. We use the\nsubscripts train and test to denote the training and test sets, respectively .\nWe will revisit this idea of using a finite dataset to evaluate expected risk\nin Section 8.2.4.\nIt turns out that empirical risk minimization can lead to overfitting, i.e.,overfitting\nthe predictor fits too closely to the training data and does not general-\nize well to new data (Mitchell, 1997). This general phenomenon of hav-\ning very small average loss on the training set but large average loss on\nthe test set tends to occur when we have little data and a complex hy-\npothesis class. For a particular predictor f (with parameters fixed), the\nphenomenon of overfitting occurs when the risk estimate from the tr"
  },
  {
    "vector_id": 1032,
    "chunk_id": "p268_c3",
    "page_number": 268,
    "text": "tends to occur when we have little data and a complex hy-\npothesis class. For a particular predictor f (with parameters fixed), the\nphenomenon of overfitting occurs when the risk estimate from the train-\ning data Remp(f, Xtrain, ytrain) underestimates the expected riskRtrue(f).\nSince we estimate the expected risk Rtrue(f) by using the empirical risk\non the test set Remp(f, Xtest, ytest) if the test risk is much larger than\nthe training risk, this is an indication of overfitting. We revisit the idea of\noverfitting in Section 8.3.3.\nTherefore, we need to somehow bias the search for the minimizer of\nempirical risk by introducing a penalty term, which makes it harder for\nthe optimizer to return an overly flexible predictor. In machine learning,\nthe penalty term is referred to as regularizatio"
  },
  {
    "vector_id": 1033,
    "chunk_id": "p268_c4",
    "page_number": 268,
    "text": "r of\nempirical risk by introducing a penalty term, which makes it harder for\nthe optimizer to return an overly flexible predictor. In machine learning,\nthe penalty term is referred to as regularization. Regularization is a wayregularization\nto compromise between accurate solution of empirical risk minimization\nand the size or complexity of the solution.\nExample 8.3 (Regularized Least Squares)\nRegularization is an approach that discourages complex or extreme solu-\ntions to an optimization problem. The simplest regularization strategy is\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1034,
    "chunk_id": "p268_c5",
    "page_number": 268,
    "text": "Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1035,
    "chunk_id": "p269_c0",
    "page_number": 269,
    "text": "8.2 Empirical Risk Minimization 263\nto replace the least-squares problem\nmin\n\u03b8\n1\nN \u2225y \u2212 X\u03b8\u2225\n2\n. (8.11)\nin the previous example with the \u201cregularized\u201d problem by adding a\npenalty term involving only \u03b8:\nmin\n\u03b8\n1\nN \u2225y \u2212 X\u03b8\u2225\n2\n+ \u03bb \u2225\u03b8\u2225\n2\n. (8.12)\nThe additional term \u2225\u03b8\u2225\n2\nis called the regularizer, and the parameter regularizer\n\u03bb is the regularization parameter . The regularization parameter trades regularization\nparameteroff minimizing the loss on the training set and the magnitude of the pa-\nrameters \u03b8. It often happens that the magnitude of the parameter values\nbecomes relatively large if we run into overfitting (Bishop, 2006).\nThe regularization term is sometimes called the penalty term, which bi- penalty term\nases the vector \u03b8 to be closer to the origin. The idea of regularization also\nappe"
  },
  {
    "vector_id": 1036,
    "chunk_id": "p269_c1",
    "page_number": 269,
    "text": "into overfitting (Bishop, 2006).\nThe regularization term is sometimes called the penalty term, which bi- penalty term\nases the vector \u03b8 to be closer to the origin. The idea of regularization also\nappears in probabilistic models as the prior probability of the parameters.\nRecall from Section 6.6 that for the posterior distribution to be of the same\nform as the prior distribution, the prior and the likelihood need to be con-\njugate. We will revisit this idea in Section 8.3.2. We will see in Chapter 12\nthat the idea of the regularizer is equivalent to the idea of a large margin.\n8.2.4 Cross-Validation to Assess the Generalization Performance\nWe mentioned in the previous section that we measure the generalization\nerror by estimating it by applying the predictor on test data. This data is\nalso"
  },
  {
    "vector_id": 1037,
    "chunk_id": "p269_c2",
    "page_number": 269,
    "text": "ation to Assess the Generalization Performance\nWe mentioned in the previous section that we measure the generalization\nerror by estimating it by applying the predictor on test data. This data is\nalso sometimes referred to as thevalidation set. The validation set is a sub- validation set\nset of the available training data that we keep aside. A practical issue with\nthis approach is that the amount of data is limited, and ideally we would\nuse as much of the data available to train the model. This would require\nus to keep our validation set V small, which then would lead to a noisy\nestimate (with high variance) of the predictive performance. One solu-\ntion to these contradictory objectives (large training set, large validation\nset) is to use cross-validation. K-fold cross-validation effectivel"
  },
  {
    "vector_id": 1038,
    "chunk_id": "p269_c3",
    "page_number": 269,
    "text": "high variance) of the predictive performance. One solu-\ntion to these contradictory objectives (large training set, large validation\nset) is to use cross-validation. K-fold cross-validation effectively partitions cross-validation\nthe data into K chunks, K \u2212 1 of which form the training set R, and\nthe last chunk serves as the validation set V (similar to the idea outlined\npreviously). Cross-validation iterates through (ideally) all combinations\nof assignments of chunks to R and V; see Figure 8.4. This procedure is\nrepeated for all K choices for the validation set, and the performance of\nthe model from the K runs is averaged.\nWe partition our dataset into two setsD = R\u222aV , such that they do not\noverlap (R \u2229 V= \u2205), where V is the validation set, and train our model\non R. After training, we as"
  },
  {
    "vector_id": 1039,
    "chunk_id": "p269_c4",
    "page_number": 269,
    "text": "el from the K runs is averaged.\nWe partition our dataset into two setsD = R\u222aV , such that they do not\noverlap (R \u2229 V= \u2205), where V is the validation set, and train our model\non R. After training, we assess the performance of the predictor f on the\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1040,
    "chunk_id": "p270_c0",
    "page_number": 270,
    "text": "264 When Models Meet Data\nFigure 8.4 K-fold\ncross-validation.\nThe dataset is\ndivided into K = 5\nchunks, K \u2212 1 of\nwhich serve as the\ntraining set (blue)\nand one as the\nvalidation set\n(orange hatch).\nTraining\nValidation\nvalidation set V (e.g., by computing root mean square error (RMSE) of\nthe trained model on the validation set). More precisely , for each partition\nk the training data R(k) produces a predictor f(k), which is then applied\nto validation set V(k) to compute the empirical risk R(f(k), V(k)). We cycle\nthrough all possible partitionings of validation and training sets and com-\npute the average generalization error of the predictor. Cross-validation\napproximates the expected generalization error\nEV[R(f, V)] \u2248 1\nK\nKX\nk=1\nR(f(k), V(k)) , (8.13)\nwhere R(f(k), V(k)) is the risk (e.g.,"
  },
  {
    "vector_id": 1041,
    "chunk_id": "p270_c1",
    "page_number": 270,
    "text": "e average generalization error of the predictor. Cross-validation\napproximates the expected generalization error\nEV[R(f, V)] \u2248 1\nK\nKX\nk=1\nR(f(k), V(k)) , (8.13)\nwhere R(f(k), V(k)) is the risk (e.g., RMSE) on the validation set V(k) for\npredictor f(k). The approximation has two sources: first, due to the finite\ntraining set, which results in not the best possiblef(k); and second, due to\nthe finite validation set, which results in an inaccurate estimation of the\nrisk R(f(k), V(k)). A potential disadvantage of K-fold cross-validation is\nthe computational cost of training the model K times, which can be bur-\ndensome if the training cost is computationally expensive. In practice, it\nis often not sufficient to look at the direct parameters alone. For example,\nwe need to explore multiple complex"
  },
  {
    "vector_id": 1042,
    "chunk_id": "p270_c2",
    "page_number": 270,
    "text": "can be bur-\ndensome if the training cost is computationally expensive. In practice, it\nis often not sufficient to look at the direct parameters alone. For example,\nwe need to explore multiple complexity parameters (e.g., multiple regu-\nlarization parameters), which may not be direct parameters of the model.\nEvaluating the quality of the model, depending on these hyperparameters,\nmay result in a number of training runs that is exponential in the number\nof model parameters. One can use nested cross-validation (Section 8.6.1)\nto search for good hyperparameters.\nHowever, cross-validation is anembarrassingly parallel problem, i.e., lit-embarrassingly\nparallel tle effort is needed to separate the problem into a number of parallel\ntasks. Given sufficient computing resources (e.g., cloud computin"
  },
  {
    "vector_id": 1043,
    "chunk_id": "p270_c3",
    "page_number": 270,
    "text": "arrassingly parallel problem, i.e., lit-embarrassingly\nparallel tle effort is needed to separate the problem into a number of parallel\ntasks. Given sufficient computing resources (e.g., cloud computing, server\nfarms), cross-validation does not require longer than a single performance\nassessment.\nIn this section, we saw that empirical risk minimization is based on the\nfollowing concepts: the hypothesis class of functions, the loss function and\nregularization. In Section 8.3, we will see the effect of using a probability\ndistribution to replace the idea of loss functions and regularization.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1044,
    "chunk_id": "p270_c4",
    "page_number": 270,
    "text": "t (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1045,
    "chunk_id": "p271_c0",
    "page_number": 271,
    "text": "8.3 Parameter Estimation 265\n8.2.5 Further Reading\nDue to the fact that the original development of empirical risk minimiza-\ntion (Vapnik, 1998) was couched in heavily theoretical language, many\nof the subsequent developments have been theoretical. The area of study\nis called statistical learning theory (Vapnik, 1999; Evgeniou et al., 2000; statistical learning\ntheoryHastie et al., 2001; von Luxburg and Sch\u00a8olkopf, 2011). A recent machine\nlearning textbook that builds on the theoretical foundations and develops\nefficient learning algorithms is Shalev-Shwartz and Ben-David (2014).\nThe concept of regularization has its roots in the solution of ill-posed in-\nverse problems (Neumaier, 1998). The approach presented here is called\nTikhonov regularization, and there is a closely related constrain"
  },
  {
    "vector_id": 1046,
    "chunk_id": "p271_c1",
    "page_number": 271,
    "text": "f regularization has its roots in the solution of ill-posed in-\nverse problems (Neumaier, 1998). The approach presented here is called\nTikhonov regularization, and there is a closely related constrained version Tikhonov\nregularizationcalled Ivanov regularization. Tikhonov regularization has deep relation-\nships to the bias-variance trade-off and feature selection (B \u00a8uhlmann and\nVan De Geer, 2011). An alternative to cross-validation is bootstrap and\njackknife (Efron and Tibshirani, 1993; Davidson and Hinkley, 1997; Hall,\n1992).\nThinking about empirical risk minimization (Section 8.2) as \u201cprobabil-\nity free\u201d is incorrect. There is an underlying unknown probability distri-\nbution p(x, y) that governs the data generation. However, the approach\nof empirical risk minimization is agnostic to tha"
  },
  {
    "vector_id": 1047,
    "chunk_id": "p271_c2",
    "page_number": 271,
    "text": "bil-\nity free\u201d is incorrect. There is an underlying unknown probability distri-\nbution p(x, y) that governs the data generation. However, the approach\nof empirical risk minimization is agnostic to that choice of distribution.\nThis is in contrast to standard statistical approaches that explicitly re-\nquire the knowledge of p(x, y). Furthermore, since the distribution is a\njoint distribution on both examples x and labels y, the labels can be non-\ndeterministic. In contrast to standard statistics we do not need to specify\nthe noise distribution for the labels y.\n8.3 Parameter Estimation\nIn Section 8.2, we did not explicitly model our problem using probability\ndistributions. In this section, we will see how to use probability distribu-\ntions to model our uncertainty due to the observation proc"
  },
  {
    "vector_id": 1048,
    "chunk_id": "p271_c3",
    "page_number": 271,
    "text": "n 8.2, we did not explicitly model our problem using probability\ndistributions. In this section, we will see how to use probability distribu-\ntions to model our uncertainty due to the observation process and our\nuncertainty in the parameters of our predictors. In Section 8.3.1, we in-\ntroduce the likelihood, which is analogous to the concept of loss functions\n(Section 8.2.2) in empirical risk minimization. The concept of priors (Sec-\ntion 8.3.2) is analogous to the concept of regularization (Section 8.2.3).\n8.3.1 Maximum Likelihood Estimation\nThe idea behind maximum likelihood estimation (MLE) is to define a func- maximum likelihood\nestimationtion of the parameters that enables us to find a model that fits the data\nwell. The estimation problem is focused on the likelihood function, or like"
  },
  {
    "vector_id": 1049,
    "chunk_id": "p271_c4",
    "page_number": 271,
    "text": "is to define a func- maximum likelihood\nestimationtion of the parameters that enables us to find a model that fits the data\nwell. The estimation problem is focused on the likelihood function, or likelihood\nmore precisely its negative logarithm. For data represented by a random\nvariable x and for a family of probability densities p(x|\u03b8) parametrized\nby \u03b8, the negative log-likelihood is given by negative\nlog-likelihood\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1050,
    "chunk_id": "p272_c0",
    "page_number": 272,
    "text": "266 When Models Meet Data\nLx(\u03b8) = \u2212log p(x|\u03b8) . (8.14)\nThe notation Lx(\u03b8) emphasizes the fact that the parameter \u03b8 is varying\nand the datax is fixed. We very often drop the reference tox when writing\nthe negative log-likelihood, as it is really a function of \u03b8, and write it as\nL(\u03b8) when the random variable representing the uncertainty in the data\nis clear from the context.\nLet us interpret what the probability density p(x|\u03b8) is modeling for a\nfixed value of \u03b8. It is a distribution that models the uncertainty of the data\nfor a given parameter setting. For a given dataset x, the likelihood allows\nus to express preferences about different settings of the parameters\u03b8, and\nwe can choose the setting that more \u201clikely\u201d has generated the data.\nIn a complementary view, if we consider the data to be"
  },
  {
    "vector_id": 1051,
    "chunk_id": "p272_c1",
    "page_number": 272,
    "text": "s\nus to express preferences about different settings of the parameters\u03b8, and\nwe can choose the setting that more \u201clikely\u201d has generated the data.\nIn a complementary view, if we consider the data to be fixed (because\nit has been observed), and we vary the parameters \u03b8, what does L(\u03b8) tell\nus? It tells us how likely a particular setting of \u03b8 is for the observations x.\nBased on this second view, the maximum likelihood estimator gives us the\nmost likely parameter \u03b8 for the set of data.\nWe consider the supervised learning setting, where we obtain pairs\n(x1, y1), . . . ,(xN , yN ) with xn \u2208 RD and labels yn \u2208 R. We are inter-\nested in constructing a predictor that takes a feature vector xn as input\nand produces a prediction yn (or something close to it), i.e., given a vec-\ntor xn we want the pro"
  },
  {
    "vector_id": 1052,
    "chunk_id": "p272_c2",
    "page_number": 272,
    "text": "abels yn \u2208 R. We are inter-\nested in constructing a predictor that takes a feature vector xn as input\nand produces a prediction yn (or something close to it), i.e., given a vec-\ntor xn we want the probability distribution of the labelyn. In other words,\nwe specify the conditional probability distribution of the labels given the\nexamples for the particular parameter setting \u03b8.\nExample 8.4\nThe first example that is often used is to specify that the conditional\nprobability of the labels given the examples is a Gaussian distribution. In\nother words, we assume that we can explain our observation uncertainty\nby independent Gaussian noise (refer to Section 6.5) with zero mean,\n\u03b5n \u223c N\n\u0000\n0, \u03c32\u0001\n. We further assume that the linear model x\u22a4\nn \u03b8 is used for\nprediction. This means we specify a Gaussian"
  },
  {
    "vector_id": 1053,
    "chunk_id": "p272_c3",
    "page_number": 272,
    "text": "certainty\nby independent Gaussian noise (refer to Section 6.5) with zero mean,\n\u03b5n \u223c N\n\u0000\n0, \u03c32\u0001\n. We further assume that the linear model x\u22a4\nn \u03b8 is used for\nprediction. This means we specify a Gaussian likelihood for each example\nlabel pair (xn, yn),\np(yn |xn, \u03b8) = N\n\u0000\nyn |x\u22a4\nn \u03b8, \u03c32\u0001\n. (8.15)\nAn illustration of a Gaussian likelihood for a given parameter \u03b8 is shown\nin Figure 8.3. We will see in Section 9.2 how to explicitly expand the\npreceding expression out in terms of the Gaussian distribution.\nWe assume that the set of examples(x1, y1), . . . ,(xN , yN ) are independentindependent and\nidentically\ndistributed\nand identically distributed (i.i.d.). The word \u201cindependent\u201d (Section 6.4.5)\nimplies that the likelihood involving the whole dataset (Y = {y1, . . . , yN }\nand X = {x1, . . . ,xN }"
  },
  {
    "vector_id": 1054,
    "chunk_id": "p272_c4",
    "page_number": 272,
    "text": "tically\ndistributed\nand identically distributed (i.i.d.). The word \u201cindependent\u201d (Section 6.4.5)\nimplies that the likelihood involving the whole dataset (Y = {y1, . . . , yN }\nand X = {x1, . . . ,xN }) factorizes into a product of the likelihoods of\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1055,
    "chunk_id": "p273_c0",
    "page_number": 273,
    "text": "8.3 Parameter Estimation 267\neach individual example\np(Y | X, \u03b8) =\nNY\nn=1\np(yn |xn, \u03b8) , (8.16)\nwhere p(yn |xn, \u03b8) is a particular distribution (which was Gaussian in Ex-\nample 8.4). The expression \u201cidentically distributed\u201d means that each term\nin the product (8.16) is of the same distribution, and all of them share\nthe same parameters. It is often easier from an optimization viewpoint to\ncompute functions that can be decomposed into sums of simpler functions.\nHence, in machine learning we often consider the negative log-likelihood Recall log(ab) =\nlog(a) + log(b)\nL(\u03b8) = \u2212log p(Y | X, \u03b8) = \u2212\nNX\nn=1\nlog p(yn |xn, \u03b8) . (8.17)\nWhile it is temping to interpret the fact that \u03b8 is on the right of the condi-\ntioning in p(yn|xn, \u03b8) (8.15), and hence should be interpreted as observed\nand fixed, thi"
  },
  {
    "vector_id": 1056,
    "chunk_id": "p273_c1",
    "page_number": 273,
    "text": "X\nn=1\nlog p(yn |xn, \u03b8) . (8.17)\nWhile it is temping to interpret the fact that \u03b8 is on the right of the condi-\ntioning in p(yn|xn, \u03b8) (8.15), and hence should be interpreted as observed\nand fixed, this interpretation is incorrect. The negative log-likelihoodL(\u03b8)\nis a function of \u03b8. Therefore, to find a good parameter vector \u03b8 that\nexplains the data (x1, y1), . . . ,(xN , yN ) well, minimize the negative log-\nlikelihood L(\u03b8) with respect to \u03b8.\nRemark. The negative sign in (8.17) is a historical artifact that is due\nto the convention that we want to maximize likelihood, but numerical\noptimization literature tends to study minimization of functions. \u2662\nExample 8.5\nContinuing on our example of Gaussian likelihoods (8.15), the negative\nlog-likelihood can be rewritten as\nL(\u03b8) = \u2212\nNX\nn=1\nlog p(yn"
  },
  {
    "vector_id": 1057,
    "chunk_id": "p273_c2",
    "page_number": 273,
    "text": "n literature tends to study minimization of functions. \u2662\nExample 8.5\nContinuing on our example of Gaussian likelihoods (8.15), the negative\nlog-likelihood can be rewritten as\nL(\u03b8) = \u2212\nNX\nn=1\nlog p(yn |xn, \u03b8) = \u2212\nNX\nn=1\nlog N\n\u0000\nyn |x\u22a4\nn \u03b8, \u03c32\u0001\n(8.18a)\n= \u2212\nNX\nn=1\nlog 1\u221a\n2\u03c0\u03c32\nexp\n\u0012\n\u2212(yn \u2212 x\u22a4\nn \u03b8)2\n2\u03c32\n\u0013\n(8.18b)\n= \u2212\nNX\nn=1\nlog exp\n\u0012\n\u2212(yn \u2212 x\u22a4\nn \u03b8)2\n2\u03c32\n\u0013\n\u2212\nNX\nn=1\nlog 1\u221a\n2\u03c0\u03c32\n(8.18c)\n= 1\n2\u03c32\nNX\nn=1\n(yn \u2212 x\u22a4\nn \u03b8)2 \u2212\nNX\nn=1\nlog 1\u221a\n2\u03c0\u03c32\n. (8.18d)\nAs \u03c3 is given, the second term in (8.18d) is constant, and minimizingL(\u03b8)\ncorresponds to solving the least-squares problem (compare with (8.8))\nexpressed in the first term.\nIt turns out that for Gaussian likelihoods the resulting optimization\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1058,
    "chunk_id": "p273_c3",
    "page_number": 273,
    "text": "the first term.\nIt turns out that for Gaussian likelihoods the resulting optimization\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1059,
    "chunk_id": "p274_c0",
    "page_number": 274,
    "text": "268 When Models Meet Data\nFigure 8.5 For the\ngiven data, the\nmaximum likelihood\nestimate of the\nparameters results\nin the black\ndiagonal line. The\norange square\nshows the value of\nthe maximum\nlikelihood\nprediction at\nx = 60.\n0 10 20 30 40 50 60 70 80\nx\n0\n25\n50\n75\n100\n125\n150y\nFigure 8.6\nComparing the\npredictions with the\nmaximum likelihood\nestimate and the\nMAP estimate at\nx = 60. The prior\nbiases the slope to\nbe less steep and the\nintercept to be\ncloser to zero. In\nthis example, the\nbias that moves the\nintercept closer to\nzero actually\nincreases the slope.\n0 10 20 30 40 50 60 70 80\nx\n0\n25\n50\n75\n100\n125\n150y\nMLE\nMAP\nproblem corresponding to maximum likelihood estimation has a closed-\nform solution. We will see more details on this in Chapter 9. Figure 8.5\nshows a regression dataset and the"
  },
  {
    "vector_id": 1060,
    "chunk_id": "p274_c1",
    "page_number": 274,
    "text": "5\n100\n125\n150y\nMLE\nMAP\nproblem corresponding to maximum likelihood estimation has a closed-\nform solution. We will see more details on this in Chapter 9. Figure 8.5\nshows a regression dataset and the function that is induced by the maxi-\nmum-likelihood parameters. Maximum likelihood estimation may suffer\nfrom overfitting (Section 8.3.3), analogous to unregularized empirical risk\nminimization (Section 8.2.3). For other likelihood functions, i.e., if we\nmodel our noise with non-Gaussian distributions, maximum likelihood es-\ntimation may not have a closed-form analytic solution. In this case, we\nresort to numerical optimization methods discussed in Chapter 7.\n8.3.2 Maximum A Posteriori Estimation\nIf we have prior knowledge about the distribution of the parameters\u03b8, we\ncan multiply an addition"
  },
  {
    "vector_id": 1061,
    "chunk_id": "p274_c2",
    "page_number": 274,
    "text": "resort to numerical optimization methods discussed in Chapter 7.\n8.3.2 Maximum A Posteriori Estimation\nIf we have prior knowledge about the distribution of the parameters\u03b8, we\ncan multiply an additional term to the likelihood. This additional term is\na prior probability distribution on parametersp(\u03b8). For a given prior, after\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1062,
    "chunk_id": "p275_c0",
    "page_number": 275,
    "text": "8.3 Parameter Estimation 269\nobserving some data x, how should we update the distribution of \u03b8? In\nother words, how should we represent the fact that we have more specific\nknowledge of \u03b8 after observing data x? Bayes\u2019 theorem, as discussed in\nSection 6.3, gives us a principled tool to update our probability distribu-\ntions of random variables. It allows us to compute a posterior distribution posterior\np(\u03b8 |x) (the more specific knowledge) on the parameters \u03b8 from general\nprior statements (prior distribution) p(\u03b8) and the function p(x|\u03b8) that prior\nlinks the parameters \u03b8 and the observed data x (called the likelihood): likelihood\np(\u03b8 |x) = p(x|\u03b8)p(\u03b8)\np(x) . (8.19)\nRecall that we are interested in finding the parameter \u03b8 that maximizes\nthe posterior. Since the distribution p(x) does not depe"
  },
  {
    "vector_id": 1063,
    "chunk_id": "p275_c1",
    "page_number": 275,
    "text": "(called the likelihood): likelihood\np(\u03b8 |x) = p(x|\u03b8)p(\u03b8)\np(x) . (8.19)\nRecall that we are interested in finding the parameter \u03b8 that maximizes\nthe posterior. Since the distribution p(x) does not depend on \u03b8, we can\nignore the value of the denominator for the optimization and obtain\np(\u03b8 |x) \u221d p(x|\u03b8)p(\u03b8) . (8.20)\nThe preceding proportion relation hides the density of the data p(x),\nwhich may be difficult to estimate. Instead of estimating the minimum\nof the negative log-likelihood, we now estimate the minimum of the neg-\native log-posterior, which is referred to as maximum a posteriori estima- maximum a\nposteriori\nestimation\ntion (MAP estimation). An illustration of the effect of adding a zero-mean\nMAP estimationGaussian prior is shown in Figure 8.6.\nExample 8.6\nIn addition to the assumptio"
  },
  {
    "vector_id": 1064,
    "chunk_id": "p275_c2",
    "page_number": 275,
    "text": "maximum a\nposteriori\nestimation\ntion (MAP estimation). An illustration of the effect of adding a zero-mean\nMAP estimationGaussian prior is shown in Figure 8.6.\nExample 8.6\nIn addition to the assumption of Gaussian likelihood in the previous exam-\nple, we assume that the parameter vector is distributed as a multivariate\nGaussian with zero mean, i.e., p(\u03b8) = N\n\u0000\n0, \u03a3\n\u0001\n, where \u03a3 is the covari-\nance matrix (Section 6.5). Note that the conjugate prior of a Gaussian\nis also a Gaussian (Section 6.6.1), and therefore we expect the posterior\ndistribution to also be a Gaussian. We will see the details of maximum a\nposteriori estimation in Chapter 9.\nThe idea of including prior knowledge about where good parameters\nlie is widespread in machine learning. An alternative view, which we saw\nin Section 8"
  },
  {
    "vector_id": 1065,
    "chunk_id": "p275_c3",
    "page_number": 275,
    "text": "of maximum a\nposteriori estimation in Chapter 9.\nThe idea of including prior knowledge about where good parameters\nlie is widespread in machine learning. An alternative view, which we saw\nin Section 8.2.3, is the idea of regularization, which introduces an addi-\ntional term that biases the resulting parameters to be close to the origin.\nMaximum a posteriori estimation can be considered to bridge the non-\nprobabilistic and probabilistic worlds as it explicitly acknowledges the\nneed for a prior distribution but it still only produces a point estimate\nof the parameters.\nRemark. The maximum likelihood estimate \u03b8ML possesses the following\nproperties (Lehmann and Casella, 1998; Efron and Hastie, 2016):\nAsymptotic consistency: The MLE converges to the true value in the\n\u00a92024 M. P. Deisenroth, A."
  },
  {
    "vector_id": 1066,
    "chunk_id": "p275_c4",
    "page_number": 275,
    "text": "hood estimate \u03b8ML possesses the following\nproperties (Lehmann and Casella, 1998; Efron and Hastie, 2016):\nAsymptotic consistency: The MLE converges to the true value in the\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1067,
    "chunk_id": "p276_c0",
    "page_number": 276,
    "text": "270 When Models Meet Data\nFigure 8.7 Model\nfitting. In a\nparametrized class\nM\u03b8 of models, we\noptimize the model\nparameters \u03b8 to\nminimize the\ndistance to the true\n(unknown) model\nM\u2217.\nM\u03b8\nM\u2217\nM\u03b8\u2217\nM\u03b80\nlimit of infinitely many observations, plus a random error that is ap-\nproximately normal.\nThe size of the samples necessary to achieve these properties can be\nquite large.\nThe error\u2019s variance decays in 1/N, where N is the number of data\npoints.\nEspecially , in the \u201csmall\u201d data regime, maximum likelihood estimation\ncan lead to overfitting.\n\u2662\nThe principle of maximum likelihood estimation (and maximum a pos-\nteriori estimation) uses probabilistic modeling to reason about the uncer-\ntainty in the data and model parameters. However, we have not yet taken\nprobabilistic modeling to its full extent. In"
  },
  {
    "vector_id": 1068,
    "chunk_id": "p276_c1",
    "page_number": 276,
    "text": "a pos-\nteriori estimation) uses probabilistic modeling to reason about the uncer-\ntainty in the data and model parameters. However, we have not yet taken\nprobabilistic modeling to its full extent. In this section, the resulting train-\ning procedure still produces a point estimate of the predictor, i.e., training\nreturns one single set of parameter values that represent the best predic-\ntor. In Section 8.4, we will take the view that the parameter values should\nalso be treated as random variables, and instead of estimating \u201cbest\u201d val-\nues of that distribution, we will use the full parameter distribution when\nmaking predictions.\n8.3.3 Model Fitting\nConsider the setting where we are given a dataset, and we are interested\nin fitting a parametrized model to the data. When we talk about \u201cfit-\nt"
  },
  {
    "vector_id": 1069,
    "chunk_id": "p276_c2",
    "page_number": 276,
    "text": "stribution when\nmaking predictions.\n8.3.3 Model Fitting\nConsider the setting where we are given a dataset, and we are interested\nin fitting a parametrized model to the data. When we talk about \u201cfit-\nting\u201d, we typically mean optimizing/learning model parameters so that\nthey minimize some loss function, e.g., the negative log-likelihood. With\nmaximum likelihood (Section 8.3.1) and maximum a posteriori estima-\ntion (Section 8.3.2), we already discussed two commonly used algorithms\nfor model fitting.\nThe parametrization of the model defines a model class M\u03b8 with which\nwe can operate. For example, in a linear regression setting, we may define\nthe relationship between inputs x and (noise-free) observations y to be\ny = ax + b, where \u03b8 := {a, b} are the model parameters. In this case, the\nmodel pa"
  },
  {
    "vector_id": 1070,
    "chunk_id": "p276_c3",
    "page_number": 276,
    "text": "in a linear regression setting, we may define\nthe relationship between inputs x and (noise-free) observations y to be\ny = ax + b, where \u03b8 := {a, b} are the model parameters. In this case, the\nmodel parameters \u03b8 describe the family of affine functions, i.e., straight\nlines with slope a, which are offset from 0 by b. Assume the data comes\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1071,
    "chunk_id": "p277_c0",
    "page_number": 277,
    "text": "8.3 Parameter Estimation 271\nFigure 8.8 Fitting\n(by maximum\nlikelihood) of\ndifferent model\nclasses to a\nregression dataset.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE\n(a) Overfitting\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE (b) Underfitting.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE (c) Fitting well.\nfrom a model M\u2217, which is unknown to us. For a given training dataset,\nwe optimize \u03b8 so that M\u03b8 is as close as possible to M\u2217, where the \u201cclose-\nness\u201d is defined by the objective function we optimize (e.g., squared loss\non the training data). Figure 8.7 illustrates a setting where we have a small\nmodel class (indicated by the circle M\u03b8), and the data generation model\nM\u2217 lies outside the set of considered models. We begin our parameter\nsearch at M\u03b80 . After the optimization, i.e., w"
  },
  {
    "vector_id": 1072,
    "chunk_id": "p277_c1",
    "page_number": 277,
    "text": "ave a small\nmodel class (indicated by the circle M\u03b8), and the data generation model\nM\u2217 lies outside the set of considered models. We begin our parameter\nsearch at M\u03b80 . After the optimization, i.e., when we obtain the best pos-\nsible parameters \u03b8\u2217, we distinguish three different cases: (i) overfitting,\n(ii) underfitting, and (iii) fitting well. We will give a high-level intuition\nof what these three concepts mean.\nRoughly speaking, overfitting refers to the situation where the para- overfitting\nmetrized model class is too rich to model the dataset generated by M\u2217,\ni.e., M\u03b8 could model much more complicated datasets. For instance, if the\ndataset was generated by a linear function, and we define M\u03b8 to be the\nclass of seventh-order polynomials, we could model not only linear func-\ntions, but"
  },
  {
    "vector_id": 1073,
    "chunk_id": "p277_c2",
    "page_number": 277,
    "text": "e complicated datasets. For instance, if the\ndataset was generated by a linear function, and we define M\u03b8 to be the\nclass of seventh-order polynomials, we could model not only linear func-\ntions, but also polynomials of degree two, three, etc. Models that over-\nfit typically have a large number of parameters. An observation we often One way to detect\noverfitting in\npractice is to\nobserve that the\nmodel has low\ntraining risk but\nhigh test risk during\ncross validation\n(Section 8.2.4).\nmake is that the overly flexible model classM\u03b8 uses all its modeling power\nto reduce the training error. If the training data is noisy , it will therefore\nfind some useful signal in the noise itself. This will cause enormous prob-\nlems when we predict away from the training data. Figure 8.8(a) gives an\nexample"
  },
  {
    "vector_id": 1074,
    "chunk_id": "p277_c3",
    "page_number": 277,
    "text": "training data is noisy , it will therefore\nfind some useful signal in the noise itself. This will cause enormous prob-\nlems when we predict away from the training data. Figure 8.8(a) gives an\nexample of overfitting in the context of regression where the model pa-\nrameters are learned by means of maximum likelihood (see Section 8.3.1).\nWe will discuss overfitting in regression more in Section 9.2.2.\nWhen we run into underfitting, we encounter the opposite problem underfitting\nwhere the model class M\u03b8 is not rich enough. For example, if our dataset\nwas generated by a sinusoidal function, but \u03b8 only parametrizes straight\nlines, the best optimization procedure will not get us close to the true\nmodel. However, we still optimize the parameters and find the best straight\nline that models the data"
  },
  {
    "vector_id": 1075,
    "chunk_id": "p277_c4",
    "page_number": 277,
    "text": "nly parametrizes straight\nlines, the best optimization procedure will not get us close to the true\nmodel. However, we still optimize the parameters and find the best straight\nline that models the dataset. Figure 8.8(b) shows an example of a model\nthat underfits because it is insufficiently flexible. Models that underfit typ-\nically have few parameters.\nThe third case is when the parametrized model class is about right.\nThen, our model fits well, i.e., it neither overfits nor underfits. This means\nour model class is just rich enough to describe the dataset we are given.\nFigure 8.8(c) shows a model that fits the given dataset fairly well. Ideally ,\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1076,
    "chunk_id": "p277_c5",
    "page_number": 277,
    "text": "del that fits the given dataset fairly well. Ideally ,\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1077,
    "chunk_id": "p278_c0",
    "page_number": 278,
    "text": "272 When Models Meet Data\nthis is the model class we would want to work with since it has good\ngeneralization properties.\nIn practice, we often define very rich model classes M\u03b8 with many pa-\nrameters, such as deep neural networks. To mitigate the problem of over-\nfitting, we can use regularization (Section 8.2.3) or priors (Section 8.3.2).\nWe will discuss how to choose the model class in Section 8.6.\n8.3.4 Further Reading\nWhen considering probabilistic models, the principle of maximum likeli-\nhood estimation generalizes the idea of least-squares regression for linear\nmodels, which we will discuss in detail in Chapter 9. When restricting\nthe predictor to have linear form with an additional nonlinear function \u03c6\napplied to the output, i.e.,\np(yn|xn, \u03b8) = \u03c6(\u03b8\u22a4xn) , (8.21)\nwe can consider othe"
  },
  {
    "vector_id": 1078,
    "chunk_id": "p278_c1",
    "page_number": 278,
    "text": "cuss in detail in Chapter 9. When restricting\nthe predictor to have linear form with an additional nonlinear function \u03c6\napplied to the output, i.e.,\np(yn|xn, \u03b8) = \u03c6(\u03b8\u22a4xn) , (8.21)\nwe can consider other models for other prediction tasks, such as binary\nclassification or modeling count data (McCullagh and Nelder, 1989). An\nalternative view of this is to consider likelihoods that are from the ex-\nponential family (Section 6.6). The class of models, which have linear\ndependence between parameters and data, and have potentially nonlin-\near transformation \u03c6 (called a link function), is referred to as generalizedlink function\ngeneralized linear\nmodel\nlinear models (Agresti, 2002, chapter 4).\nMaximum likelihood estimation has a rich history , and was originally\nproposed by Sir Ronald Fisher in the"
  },
  {
    "vector_id": 1079,
    "chunk_id": "p278_c2",
    "page_number": 278,
    "text": "s generalizedlink function\ngeneralized linear\nmodel\nlinear models (Agresti, 2002, chapter 4).\nMaximum likelihood estimation has a rich history , and was originally\nproposed by Sir Ronald Fisher in the 1930s. We will expand upon the idea\nof a probabilistic model in Section 8.4. One debate among researchers\nwho use probabilistic models, is the discussion between Bayesian and fre-\nquentist statistics. As mentioned in Section 6.1.1, it boils down to the\ndefinition of probability . Recall from Section 6.1 that one can consider\nprobability to be a generalization (by allowing uncertainty) of logical rea-\nsoning (Cheeseman, 1985; Jaynes, 2003). The method of maximum like-\nlihood estimation is frequentist in nature, and the interested reader is\npointed to Efron and Hastie (2016) for a balanced view"
  },
  {
    "vector_id": 1080,
    "chunk_id": "p278_c3",
    "page_number": 278,
    "text": "rea-\nsoning (Cheeseman, 1985; Jaynes, 2003). The method of maximum like-\nlihood estimation is frequentist in nature, and the interested reader is\npointed to Efron and Hastie (2016) for a balanced view of both Bayesian\nand frequentist statistics.\nThere are some probabilistic models where maximum likelihood esti-\nmation may not be possible. The reader is referred to more advanced sta-\ntistical textbooks, e.g., Casella and Berger (2002), for approaches, such as\nmethod of moments, M-estimation, and estimating equations.\n8.4 Probabilistic Modeling and Inference\nIn machine learning, we are frequently concerned with the interpretation\nand analysis of data, e.g., for prediction of future events and decision\nmaking. To make this task more tractable, we often build models that\ndescribe the generativ"
  },
  {
    "vector_id": 1081,
    "chunk_id": "p278_c4",
    "page_number": 278,
    "text": "y concerned with the interpretation\nand analysis of data, e.g., for prediction of future events and decision\nmaking. To make this task more tractable, we often build models that\ndescribe the generative process that generates the observed data.generative process\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1082,
    "chunk_id": "p279_c0",
    "page_number": 279,
    "text": "8.4 Probabilistic Modeling and Inference 273\nFor example, we can describe the outcome of a coin-flip experiment\n(\u201cheads\u201d or \u201ctails\u201d) in two steps. First, we define a parameter \u00b5, which\ndescribes the probability of \u201cheads\u201d as the parameter of a Bernoulli distri-\nbution (Chapter 6); second, we can sample an outcome x \u2208 {head, tail}\nfrom the Bernoulli distribution p(x |\u00b5) = Ber(\u00b5). The parameter \u00b5 gives\nrise to a specific dataset X and depends on the coin used. Since \u00b5 is un-\nknown in advance and can never be observed directly , we need mecha-\nnisms to learn something about \u00b5 given observed outcomes of coin-flip\nexperiments. In the following, we will discuss how probabilistic modeling\ncan be used for this purpose.\n8.4.1 Probabilistic Models A probabilistic\nmodel is specified\nby the joint\ndist"
  },
  {
    "vector_id": 1083,
    "chunk_id": "p279_c1",
    "page_number": 279,
    "text": "mes of coin-flip\nexperiments. In the following, we will discuss how probabilistic modeling\ncan be used for this purpose.\n8.4.1 Probabilistic Models A probabilistic\nmodel is specified\nby the joint\ndistribution of all\nrandom variables.\nProbabilistic models represent the uncertain aspects of an experiment as\nprobability distributions. The benefit of using probabilistic models is that\nthey offer a unified and consistent set of tools from probability theory\n(Chapter 6) for modeling, inference, prediction, and model selection.\nIn probabilistic modeling, the joint distribution p(x, \u03b8) of the observed\nvariables x and the hidden parameters \u03b8 is of central importance: It en-\ncapsulates information from the following:\nThe prior and the likelihood (product rule, Section 6.3).\nThe marginal likelihood p"
  },
  {
    "vector_id": 1084,
    "chunk_id": "p279_c2",
    "page_number": 279,
    "text": "variables x and the hidden parameters \u03b8 is of central importance: It en-\ncapsulates information from the following:\nThe prior and the likelihood (product rule, Section 6.3).\nThe marginal likelihood p(x), which will play an important role in\nmodel selection (Section 8.6), can be computed by taking the joint dis-\ntribution and integrating out the parameters (sum rule, Section 6.3).\nThe posterior, which can be obtained by dividing the joint by the marginal\nlikelihood.\nOnly the joint distribution has this property . Therefore, a probabilistic\nmodel is specified by the joint distribution of all its random variables.\n8.4.2 Bayesian Inference\nParameter\nestimation can be\nphrased as an\noptimization\nproblem.\nA key task in machine learning is to take a model and the data to uncover\nthe values of the"
  },
  {
    "vector_id": 1085,
    "chunk_id": "p279_c3",
    "page_number": 279,
    "text": "s random variables.\n8.4.2 Bayesian Inference\nParameter\nestimation can be\nphrased as an\noptimization\nproblem.\nA key task in machine learning is to take a model and the data to uncover\nthe values of the model\u2019s hidden variables \u03b8 given the observed variables\nx. In Section 8.3.1, we already discussed two ways for estimating model\nparameters \u03b8 using maximum likelihood or maximum a posteriori esti-\nmation. In both cases, we obtain a single-best value for \u03b8 so that the key\nalgorithmic problem of parameter estimation is solving an optimization\nproblem. Once these point estimates \u03b8\u2217 are known, we use them to make\npredictions. More specifically , the predictive distribution will bep(x|\u03b8\u2217),\nwhere we use \u03b8\u2217 in the likelihood function.\nAs discussed in Section 6.3, focusing solely on some statistic of"
  },
  {
    "vector_id": 1086,
    "chunk_id": "p279_c4",
    "page_number": 279,
    "text": "them to make\npredictions. More specifically , the predictive distribution will bep(x|\u03b8\u2217),\nwhere we use \u03b8\u2217 in the likelihood function.\nAs discussed in Section 6.3, focusing solely on some statistic of the pos-\nterior distribution (such as the parameter \u03b8\u2217 that maximizes the poste-\nrior) leads to loss of information, which can be critical in a system that\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1087,
    "chunk_id": "p280_c0",
    "page_number": 280,
    "text": "274 When Models Meet Data\nuses the prediction p(x|\u03b8\u2217) to make decisions. These decision-making\nsystems typically have different objective functions than the likelihood, aBayesian inference\nis about learning the\ndistribution of\nrandom variables.\nsquared-error loss or a mis-classification error. Therefore, having the full\nposterior distribution around can be extremely useful and leads to more\nrobust decisions. Bayesian inference is about finding this posterior distri-Bayesian inference\nbution (Gelman et al., 2004). For a datasetX, a parameter priorp(\u03b8), and\na likelihood function, the posterior\np(\u03b8 | X) = p(X |\u03b8)p(\u03b8)\np(X) , p (X) =\nZ\np(X |\u03b8)p(\u03b8)d\u03b8 , (8.22)\nis obtained by applying Bayes\u2019 theorem. The key idea is to exploit Bayes\u2019Bayesian inference\ninverts the\nrelationship\nbetween parameters\nan"
  },
  {
    "vector_id": 1088,
    "chunk_id": "p280_c1",
    "page_number": 280,
    "text": "(\u03b8 | X) = p(X |\u03b8)p(\u03b8)\np(X) , p (X) =\nZ\np(X |\u03b8)p(\u03b8)d\u03b8 , (8.22)\nis obtained by applying Bayes\u2019 theorem. The key idea is to exploit Bayes\u2019Bayesian inference\ninverts the\nrelationship\nbetween parameters\nand the data.\ntheorem to invert the relationship between the parameters\u03b8 and the data\nX (given by the likelihood) to obtain the posterior distribution p(\u03b8 | X).\nThe implication of having a posterior distribution on the parameters is\nthat it can be used to propagate uncertainty from the parameters to the\ndata. More specifically , with a distribution p(\u03b8) on the parameters our\npredictions will be\np(x) =\nZ\np(x|\u03b8)p(\u03b8)d\u03b8 = E\u03b8[p(x|\u03b8)] , (8.23)\nand they no longer depend on the model parameters \u03b8, which have been\nmarginalized/integrated out. Equation (8.23) reveals that the prediction\nis an average over"
  },
  {
    "vector_id": 1089,
    "chunk_id": "p280_c2",
    "page_number": 280,
    "text": "=\nZ\np(x|\u03b8)p(\u03b8)d\u03b8 = E\u03b8[p(x|\u03b8)] , (8.23)\nand they no longer depend on the model parameters \u03b8, which have been\nmarginalized/integrated out. Equation (8.23) reveals that the prediction\nis an average over all plausible parameter values \u03b8, where the plausibility\nis encapsulated by the parameter distribution p(\u03b8).\nHaving discussed parameter estimation in Section 8.3 and Bayesian in-\nference here, let us compare these two approaches to learning. Parameter\nestimation via maximum likelihood or MAP estimation yields a consistent\npoint estimate \u03b8\u2217 of the parameters, and the key computational problem\nto be solved is optimization. In contrast, Bayesian inference yields a (pos-\nterior) distribution, and the key computational problem to be solved is\nintegration. Predictions with point estimates are strai"
  },
  {
    "vector_id": 1090,
    "chunk_id": "p280_c3",
    "page_number": 280,
    "text": "e solved is optimization. In contrast, Bayesian inference yields a (pos-\nterior) distribution, and the key computational problem to be solved is\nintegration. Predictions with point estimates are straightforward, whereas\npredictions in the Bayesian framework require solving another integration\nproblem; see (8.23). However, Bayesian inference gives us a principled\nway to incorporate prior knowledge, account for side information, and\nincorporate structural knowledge, all of which is not easily done in the\ncontext of parameter estimation. Moreover, the propagation of parameter\nuncertainty to the prediction can be valuable in decision-making systems\nfor risk assessment and exploration in the context of data-efficient learn-\ning (Deisenroth et al., 2015; Kamthe and Deisenroth, 2018).\nWhile Bayes"
  },
  {
    "vector_id": 1091,
    "chunk_id": "p280_c4",
    "page_number": 280,
    "text": "rediction can be valuable in decision-making systems\nfor risk assessment and exploration in the context of data-efficient learn-\ning (Deisenroth et al., 2015; Kamthe and Deisenroth, 2018).\nWhile Bayesian inference is a mathematically principled framework for\nlearning about parameters and making predictions, there are some prac-\ntical challenges that come with it because of the integration problems we\nneed to solve; see (8.22) and (8.23). More specifically , if we do not choose\na conjugate prior on the parameters (Section 6.6.1), the integrals in (8.22)\nand (8.23) are not analytically tractable, and we cannot compute the pos-\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1092,
    "chunk_id": "p280_c5",
    "page_number": 280,
    "text": ", and we cannot compute the pos-\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1093,
    "chunk_id": "p281_c0",
    "page_number": 281,
    "text": "8.4 Probabilistic Modeling and Inference 275\nterior, the predictions, or the marginal likelihood in closed form. In these\ncases, we need to resort to approximations. Here, we can use stochas-\ntic approximations, such as Markov chain Monte Carlo (MCMC) (Gilks\net al., 1996), or deterministic approximations, such as the Laplace ap-\nproximation (Bishop, 2006; Barber, 2012; Murphy, 2012), variational in-\nference (Jordan et al., 1999; Blei et al., 2017), or expectation propaga-\ntion (Minka, 2001a).\nDespite these challenges, Bayesian inference has been successfully ap-\nplied to a variety of problems, including large-scale topic modeling (Hoff-\nman et al., 2013), click-through-rate prediction (Graepel et al., 2010),\ndata-efficient reinforcement learning in control systems (Deisenroth et al.,\n2015)"
  },
  {
    "vector_id": 1094,
    "chunk_id": "p281_c1",
    "page_number": 281,
    "text": "including large-scale topic modeling (Hoff-\nman et al., 2013), click-through-rate prediction (Graepel et al., 2010),\ndata-efficient reinforcement learning in control systems (Deisenroth et al.,\n2015), online ranking systems (Herbrich et al., 2007), and large-scale rec-\nommender systems. There are generic tools, such as Bayesian optimiza-\ntion (Brochu et al., 2009; Snoek et al., 2012; Shahriari et al., 2016), that\nare very useful ingredients for an efficient search of meta parameters of\nmodels or algorithms.\nRemark. In the machine learning literature, there can be a somewhat ar-\nbitrary separation between (random) \u201cvariables\u201d and \u201cparameters\u201d. While\nparameters are estimated (e.g., via maximum likelihood), variables are\nusually marginalized out. In this book, we are not so strict with this"
  },
  {
    "vector_id": 1095,
    "chunk_id": "p281_c2",
    "page_number": 281,
    "text": "tion between (random) \u201cvariables\u201d and \u201cparameters\u201d. While\nparameters are estimated (e.g., via maximum likelihood), variables are\nusually marginalized out. In this book, we are not so strict with this sep-\naration because, in principle, we can place a prior on any parameter and\nintegrate it out, which would then turn the parameter into a random vari-\nable according to the aforementioned separation. \u2662\n8.4.3 Latent-Variable Models\nIn practice, it is sometimes useful to have additional latent variables z latent variable\n(besides the model parameters \u03b8) as part of the model (Moustaki et al.,\n2015). These latent variables are different from the model parameters\n\u03b8 as they do not parametrize the model explicitly . Latent variables may\ndescribe the data-generating process, thereby contributing to t"
  },
  {
    "vector_id": 1096,
    "chunk_id": "p281_c3",
    "page_number": 281,
    "text": "These latent variables are different from the model parameters\n\u03b8 as they do not parametrize the model explicitly . Latent variables may\ndescribe the data-generating process, thereby contributing to the inter-\npretability of the model. They also often simplify the structure of the\nmodel and allow us to define simpler and richer model structures. Sim-\nplification of the model structure often goes hand in hand with a smaller\nnumber of model parameters (Paquet, 2008; Murphy, 2012). Learning in\nlatent-variable models (at least via maximum likelihood) can be done in a\nprincipled way using the expectation maximization (EM) algorithm (Demp-\nster et al., 1977; Bishop, 2006). Examples, where such latent variables\nare helpful, are principal component analysis for dimensionality reduc-\ntion (Chapter"
  },
  {
    "vector_id": 1097,
    "chunk_id": "p281_c4",
    "page_number": 281,
    "text": "tation maximization (EM) algorithm (Demp-\nster et al., 1977; Bishop, 2006). Examples, where such latent variables\nare helpful, are principal component analysis for dimensionality reduc-\ntion (Chapter 10), Gaussian mixture models for density estimation (Chap-\nter 11), hidden Markov models (Maybeck, 1979) or dynamical systems\n(Ghahramani and Roweis, 1999; Ljung, 1999) for time-series modeling,\nand meta learning and task generalization (Hausman et al., 2018; S\u00e6-\nmundsson et al., 2018). Although the introduction of these latent variables\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1098,
    "chunk_id": "p281_c5",
    "page_number": 281,
    "text": "y Cambridge University Press (2020)."
  },
  {
    "vector_id": 1099,
    "chunk_id": "p282_c0",
    "page_number": 282,
    "text": "276 When Models Meet Data\nmay make the model structure and the generative process easier, learning\nin latent-variable models is generally hard, as we will see in Chapter 11.\nSince latent-variable models also allow us to define the process that\ngenerates data from parameters, let us have a look at this generative pro-\ncess. Denoting data by x, the model parameters by \u03b8 and the latent vari-\nables by z, we obtain the conditional distribution\np(x|z, \u03b8) (8.24)\nthat allows us to generate data for any model parameters and latent vari-\nables. Given that z are latent variables, we place a prior p(z) on them.\nAs the models we discussed previously , models with latent variables\ncan be used for parameter learning and inference within the frameworks\nwe discussed in Sections 8.3 and 8.4.2. To facilitate"
  },
  {
    "vector_id": 1100,
    "chunk_id": "p282_c1",
    "page_number": 282,
    "text": "them.\nAs the models we discussed previously , models with latent variables\ncan be used for parameter learning and inference within the frameworks\nwe discussed in Sections 8.3 and 8.4.2. To facilitate learning (e.g., by\nmeans of maximum likelihood estimation or Bayesian inference), we fol-\nlow a two-step procedure. First, we compute the likelihood p(x|\u03b8) of the\nmodel, which does not depend on the latent variables. Second, we use this\nlikelihood for parameter estimation or Bayesian inference, where we use\nexactly the same expressions as in Sections 8.3 and 8.4.2, respectively .\nSince the likelihood functionp(x|\u03b8) is the predictive distribution of the\ndata given the model parameters, we need to marginalize out the latent\nvariables so that\np(x|\u03b8) =\nZ\np(x|z, \u03b8)p(z)dz , (8.25)\nwhere p(x|z, \u03b8) i"
  },
  {
    "vector_id": 1101,
    "chunk_id": "p282_c2",
    "page_number": 282,
    "text": "hood functionp(x|\u03b8) is the predictive distribution of the\ndata given the model parameters, we need to marginalize out the latent\nvariables so that\np(x|\u03b8) =\nZ\np(x|z, \u03b8)p(z)dz , (8.25)\nwhere p(x|z, \u03b8) is given in (8.24) and p(z) is the prior on the latent\nvariables. Note that the likelihood must not depend on the latent variablesThe likelihood is a\nfunction of the data\nand the model\nparameters, but is\nindependent of the\nlatent variables.\nz, but it is only a function of the data x and the model parameters \u03b8.\nThe likelihood in (8.25) directly allows for parameter estimation via\nmaximum likelihood. MAP estimation is also straightforward with an ad-\nditional prior on the model parameters \u03b8 as discussed in Section 8.3.2.\nMoreover, with the likelihood (8.25) Bayesian inference (Section 8.4.2)\nin a"
  },
  {
    "vector_id": 1102,
    "chunk_id": "p282_c3",
    "page_number": 282,
    "text": "MAP estimation is also straightforward with an ad-\nditional prior on the model parameters \u03b8 as discussed in Section 8.3.2.\nMoreover, with the likelihood (8.25) Bayesian inference (Section 8.4.2)\nin a latent-variable model works in the usual way: We place a prior p(\u03b8)\non the model parameters and use Bayes\u2019 theorem to obtain a posterior\ndistribution\np(\u03b8 | X) = p(X |\u03b8)p(\u03b8)\np(X) (8.26)\nover the model parameters given a dataset X. The posterior in (8.26) can\nbe used for predictions within a Bayesian inference framework; see (8.23).\nOne challenge we have in this latent-variable model is that the like-\nlihood p(X |\u03b8) requires the marginalization of the latent variables ac-\ncording to (8.25). Except when we choose a conjugate prior p(z) for\np(x|z, \u03b8), the marginalization in (8.25) is not analytic"
  },
  {
    "vector_id": 1103,
    "chunk_id": "p282_c4",
    "page_number": 282,
    "text": "ke-\nlihood p(X |\u03b8) requires the marginalization of the latent variables ac-\ncording to (8.25). Except when we choose a conjugate prior p(z) for\np(x|z, \u03b8), the marginalization in (8.25) is not analytically tractable, and\nwe need to resort to approximations (Bishop, 2006; Paquet, 2008; Mur-\nphy, 2012; Moustaki et al., 2015).\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1104,
    "chunk_id": "p283_c0",
    "page_number": 283,
    "text": "8.4 Probabilistic Modeling and Inference 277\nSimilar to the parameter posterior (8.26) we can compute a posterior\non the latent variables according to\np(z | X) = p(X |z)p(z)\np(X) , p (X |z) =\nZ\np(X |z, \u03b8)p(\u03b8)d\u03b8 , (8.27)\nwhere p(z) is the prior on the latent variables and p(X |z) requires us to\nintegrate out the model parameters \u03b8.\nGiven the difficulty of solving integrals analytically , it is clear that mar-\nginalizing out both the latent variables and the model parameters at the\nsame time is not possible in general (Bishop, 2006; Murphy, 2012). A\nquantity that is easier to compute is the posterior distribution on the latent\nvariables, but conditioned on the model parameters, i.e.,\np(z | X, \u03b8) = p(X |z, \u03b8)p(z)\np(X |\u03b8) , (8.28)\nwhere p(z) is the prior on the latent variables and p(X |z, \u03b8)"
  },
  {
    "vector_id": 1105,
    "chunk_id": "p283_c1",
    "page_number": 283,
    "text": "erior distribution on the latent\nvariables, but conditioned on the model parameters, i.e.,\np(z | X, \u03b8) = p(X |z, \u03b8)p(z)\np(X |\u03b8) , (8.28)\nwhere p(z) is the prior on the latent variables and p(X |z, \u03b8) is given\nin (8.24).\nIn Chapters 10 and 11, we derive the likelihood functions for PCA and\nGaussian mixture models, respectively . Moreover, we compute the poste-\nrior distributions (8.28) on the latent variables for both PCA and Gaussian\nmixture models.\nRemark. In the following chapters, we may not be drawing such a clear\ndistinction between latent variables z and uncertain model parameters \u03b8\nand call the model parameters \u201clatent\u201d or \u201chidden\u201d as well because they\nare unobserved. In Chapters 10 and 11, where we use the latent variables\nz, we will pay attention to the difference as we will have"
  },
  {
    "vector_id": 1106,
    "chunk_id": "p283_c2",
    "page_number": 283,
    "text": "call the model parameters \u201clatent\u201d or \u201chidden\u201d as well because they\nare unobserved. In Chapters 10 and 11, where we use the latent variables\nz, we will pay attention to the difference as we will have two different\ntypes of hidden variables: model parameters \u03b8 and latent variables z. \u2662\nWe can exploit the fact that all the elements of a probabilistic model are\nrandom variables to define a unified language for representing them. In\nSection 8.5, we will see a concise graphical language for representing the\nstructure of probabilistic models. We will use this graphical language to\ndescribe the probabilistic models in the subsequent chapters.\n8.4.4 Further Reading\nProbabilistic models in machine learning (Bishop, 2006; Barber, 2012;\nMurphy, 2012) provide a way for users to capture uncertainty abo"
  },
  {
    "vector_id": 1107,
    "chunk_id": "p283_c3",
    "page_number": 283,
    "text": "bilistic models in the subsequent chapters.\n8.4.4 Further Reading\nProbabilistic models in machine learning (Bishop, 2006; Barber, 2012;\nMurphy, 2012) provide a way for users to capture uncertainty about data\nand predictive models in a principled fashion. Ghahramani (2015) presents\na short review of probabilistic models in machine learning. Given a proba-\nbilistic model, we may be lucky enough to be able to compute parameters\nof interest analytically . However, in general, analytic solutions are rare,\nand computational methods such as sampling (Gilks et al., 1996; Brooks\net al., 2011) and variational inference (Jordan et al., 1999; Blei et al.,\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1108,
    "chunk_id": "p283_c4",
    "page_number": 283,
    "text": "tional inference (Jordan et al., 1999; Blei et al.,\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1109,
    "chunk_id": "p284_c0",
    "page_number": 284,
    "text": "278 When Models Meet Data\n2017) are used. Moustaki et al. (2015) and Paquet (2008) provide a good\noverview of Bayesian inference in latent-variable models.\nIn recent years, several programming languages have been proposed\nthat aim to treat the variables defined in software as random variables\ncorresponding to probability distributions. The objective is to be able to\nwrite complex functions of probability distributions, while under the hood\nthe compiler automatically takes care of the rules of Bayesian inference.\nThis rapidly changing field is called probabilistic programming.probabilistic\nprogramming\n8.5 Directed Graphical Models\nIn this section, we introduce a graphical language for specifying a prob-\nabilistic model, called the directed graphical model. It provides a compactdirected grap"
  },
  {
    "vector_id": 1110,
    "chunk_id": "p284_c1",
    "page_number": 284,
    "text": "ramming\n8.5 Directed Graphical Models\nIn this section, we introduce a graphical language for specifying a prob-\nabilistic model, called the directed graphical model. It provides a compactdirected graphical\nmodel and succinct way to specify probabilistic models, and allows the reader to\nvisually parse dependencies between random variables. A graphical model\nvisually captures the way in which the joint distribution over all random\nvariables can be decomposed into a product of factors depending only on\na subset of these variables. In Section 8.4, we identified the joint distri-\nbution of a probabilistic model as the key quantity of interest because it\ncomprises information about the prior, the likelihood, and the posterior.\nHowever, the joint distribution by itself can be quite complicated, a"
  },
  {
    "vector_id": 1111,
    "chunk_id": "p284_c2",
    "page_number": 284,
    "text": "ilistic model as the key quantity of interest because it\ncomprises information about the prior, the likelihood, and the posterior.\nHowever, the joint distribution by itself can be quite complicated, andDirected graphical\nmodels are also\nknown as Bayesian\nnetworks.\nit does not tell us anything about structural properties of the probabilis-\ntic model. For example, the joint distribution p(a, b, c) does not tell us\nanything about independence relations. This is the point where graphical\nmodels come into play . This section relies on the concepts of independence\nand conditional independence, as described in Section 6.4.5.\nIn a graphical model, nodes are random variables. In Figure 8.9(a), thegraphical model\nnodes represent the random variables a, b, c. Edges represent probabilistic\nrelations b"
  },
  {
    "vector_id": 1112,
    "chunk_id": "p284_c3",
    "page_number": 284,
    "text": "scribed in Section 6.4.5.\nIn a graphical model, nodes are random variables. In Figure 8.9(a), thegraphical model\nnodes represent the random variables a, b, c. Edges represent probabilistic\nrelations between variables, e.g., conditional probabilities.\nRemark. Not every distribution can be represented in a particular choice of\ngraphical model. A discussion of this can be found in Bishop (2006). \u2662\nProbabilistic graphical models have some convenient properties:\nThey are a simple way to visualize the structure of a probabilistic model.\nThey can be used to design or motivate new kinds of statistical models.\nInspection of the graph alone gives us insight into properties, e.g., con-\nditional independence.\nComplex computations for inference and learning in statistical models\ncan be expressed in ter"
  },
  {
    "vector_id": 1113,
    "chunk_id": "p284_c4",
    "page_number": 284,
    "text": "models.\nInspection of the graph alone gives us insight into properties, e.g., con-\nditional independence.\nComplex computations for inference and learning in statistical models\ncan be expressed in terms of graphical manipulations.\n8.5.1 Graph Semantics\nDirected graphical models/Bayesian networks are a method for representingdirected graphical\nmodel/Bayesian\nnetwork\nconditional dependencies in a probabilistic model. They provide a visual\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1114,
    "chunk_id": "p285_c0",
    "page_number": 285,
    "text": "8.5 Directed Graphical Models 279\ndescription of the conditional probabilities, hence, providing a simple lan-\nguage for describing complex interdependence. The modular description With additional\nassumptions, the\narrows can be used\nto indicate causal\nrelationships (Pearl,\n2009).\nalso entails computational simplification. Directed links (arrows) between\ntwo nodes (random variables) indicate conditional probabilities. For ex-\nample, the arrow between a and b in Figure 8.9(a) gives the conditional\nprobability p(b |a) of b given a.\nFigure 8.9\nExamples of\ndirected graphical\nmodels.\na b\nc\n(a) Fully connected.\nx1 x2\nx3 x4\nx5\n(b) Not fully connected.\nDirected graphical models can be derived from joint distributions if we\nknow something about their factorization.\nExample 8.7\nConsider the joint dis"
  },
  {
    "vector_id": 1115,
    "chunk_id": "p285_c1",
    "page_number": 285,
    "text": "connected.\nx1 x2\nx3 x4\nx5\n(b) Not fully connected.\nDirected graphical models can be derived from joint distributions if we\nknow something about their factorization.\nExample 8.7\nConsider the joint distribution\np(a, b, c) = p(c |a, b)p(b |a)p(a) (8.29)\nof three random variables a, b, c. The factorization of the joint distribution\nin (8.29) tells us something about the relationship between the random\nvariables:\nc depends directly on a and b.\nb depends directly on a.\na depends neither on b nor on c.\nFor the factorization in (8.29), we obtain the directed graphical model in\nFigure 8.9(a).\nIn general, we can construct the corresponding directed graphical model\nfrom a factorized joint distribution as follows:\n1. Create a node for all random variables.\n2. For each conditional distribution, we add"
  },
  {
    "vector_id": 1116,
    "chunk_id": "p285_c2",
    "page_number": 285,
    "text": "al, we can construct the corresponding directed graphical model\nfrom a factorized joint distribution as follows:\n1. Create a node for all random variables.\n2. For each conditional distribution, we add a directed link (arrow) to\nthe graph from the nodes corresponding to the variables on which the\ndistribution is conditioned.\nThe graph layout\ndepends on the\nfactorization of the\njoint distribution.\nThe graph layout depends on the choice of factorization of the joint dis-\ntribution.\nWe discussed how to get from a known factorization of the joint dis-\ntribution to the corresponding directed graphical model. Now, we will do\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1117,
    "chunk_id": "p285_c3",
    "page_number": 285,
    "text": "al model. Now, we will do\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1118,
    "chunk_id": "p286_c0",
    "page_number": 286,
    "text": "280 When Models Meet Data\nexactly the opposite and describe how to extract the joint distribution of\na set of random variables from a given graphical model.\nExample 8.8\nLooking at the graphical model in Figure 8.9(b), we exploit two proper-\nties:\nThe joint distribution p(x1, . . . , x5) we seek is the product of a set of\nconditionals, one for each node in the graph. In this particular example,\nwe will need five conditionals.\nEach conditional depends only on the parents of the corresponding\nnode in the graph. For example, x4 will be conditioned on x2.\nThese two properties yield the desired factorization of the joint distribu-\ntion\np(x1, x2, x3, x4, x5) = p(x1)p(x5)p(x2 |x5)p(x3 |x1, x2)p(x4 |x2) . (8.30)\nIn general, the joint distribution p(x) = p(x1, . . . , xK) is given as\np(x) =\nKY\nk=1\np"
  },
  {
    "vector_id": 1119,
    "chunk_id": "p286_c1",
    "page_number": 286,
    "text": "orization of the joint distribu-\ntion\np(x1, x2, x3, x4, x5) = p(x1)p(x5)p(x2 |x5)p(x3 |x1, x2)p(x4 |x2) . (8.30)\nIn general, the joint distribution p(x) = p(x1, . . . , xK) is given as\np(x) =\nKY\nk=1\np(xk |Pak) , (8.31)\nwhere Pak means \u201cthe parent nodes of xk\u201d. Parent nodes of xk are nodes\nthat have arrows pointing to xk.\nWe conclude this subsection with a concrete example of the coin-flip\nexperiment. Consider a Bernoulli experiment (Example 6.8) where the\nprobability that the outcome x of this experiment is \u201cheads\u201d is\np(x |\u00b5) = Ber(\u00b5) . (8.32)\nWe now repeat this experimentN times and observe outcomesx1, . . . , xN\nso that we obtain the joint distribution\np(x1, . . . , xN |\u00b5) =\nNY\nn=1\np(xn |\u00b5) . (8.33)\nThe expression on the right-hand side is a product of Bernoulli distribu-\ntions on each i"
  },
  {
    "vector_id": 1120,
    "chunk_id": "p286_c2",
    "page_number": 286,
    "text": "utcomesx1, . . . , xN\nso that we obtain the joint distribution\np(x1, . . . , xN |\u00b5) =\nNY\nn=1\np(xn |\u00b5) . (8.33)\nThe expression on the right-hand side is a product of Bernoulli distribu-\ntions on each individual outcome because the experiments are indepen-\ndent. Recall from Section 6.4.5 that statistical independence means that\nthe distribution factorizes. To write the graphical model down for this set-\nting, we make the distinction between unobserved/latent variables and\nobserved variables. Graphically , observed variables are denoted by shaded\nnodes so that we obtain the graphical model in Figure 8.10(a). We see\nthat the single parameter \u00b5 is the same for all xn, n = 1 , . . . , Nas the\noutcomes xn are identically distributed. A more compact, but equivalent,\ngraphical model for this settin"
  },
  {
    "vector_id": 1121,
    "chunk_id": "p286_c3",
    "page_number": 286,
    "text": "ure 8.10(a). We see\nthat the single parameter \u00b5 is the same for all xn, n = 1 , . . . , Nas the\noutcomes xn are identically distributed. A more compact, but equivalent,\ngraphical model for this setting is given in Figure 8.10(b), where we use\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1122,
    "chunk_id": "p287_c0",
    "page_number": 287,
    "text": "8.5 Directed Graphical Models 281\nFigure 8.10\nGraphical models\nfor a repeated\nBernoulli\nexperiment.\n\u00b5\nx1 xN\n(a) Version with xn explicit.\n\u00b5\nxn\nn = 1, . . . , N\n(b) Version with\nplate notation.\n\u00b5\nxn\n\u03b2\u03b1\nn = 1, . . . , N\n(c) Hyperparameters \u03b1\nand \u03b2 on the latent \u00b5.\nthe plate notation. The plate (box) repeats everything inside (in this case, plate\nthe observations xn) N times. Therefore, both graphical models are equiv-\nalent, but the plate notation is more compact. Graphical models immedi-\nately allow us to place a hyperprior on \u00b5. A hyperprior is a second layer hyperprior\nof prior distributions on the parameters of the first layer of priors. Fig-\nure 8.10(c) places a Beta (\u03b1, \u03b2) prior on the latent variable \u00b5. If we treat\n\u03b1 and \u03b2 as deterministic parameters, i.e., not random variables, we om"
  },
  {
    "vector_id": 1123,
    "chunk_id": "p287_c1",
    "page_number": 287,
    "text": "on the parameters of the first layer of priors. Fig-\nure 8.10(c) places a Beta (\u03b1, \u03b2) prior on the latent variable \u00b5. If we treat\n\u03b1 and \u03b2 as deterministic parameters, i.e., not random variables, we omit\nthe circle around it.\n8.5.2 Conditional Independence and d-Separation\nDirected graphical models allow us to find conditional independence (Sec-\ntion 6.4.5) relationship properties of the joint distribution only by looking\nat the graph. A concept called d-separation (Pearl, 1988) is key to this. d-separation\nConsider a general directed graph in which A, B, C are arbitrary nonin-\ntersecting sets of nodes (whose union may be smaller than the complete\nset of nodes in the graph). We wish to ascertain whether a particular con-\nditional independence statement, \u201c A is conditionally independent of B"
  },
  {
    "vector_id": 1124,
    "chunk_id": "p287_c2",
    "page_number": 287,
    "text": "of nodes (whose union may be smaller than the complete\nset of nodes in the graph). We wish to ascertain whether a particular con-\nditional independence statement, \u201c A is conditionally independent of B\ngiven C\u201d, denoted by\nA \u22a5 \u22a5 B | C, (8.34)\nis implied by a given directed acyclic graph. To do so, we consider all\npossible trails (paths that ignore the direction of the arrows) from any\nnode in A to any nodes in B. Any such path is said to be blocked if it\nincludes any node such that either of the following are true:\nThe arrows on the path meet either head to tail or tail to tail at the\nnode, and the node is in the set C.\nThe arrows meet head to head at the node, and neither the node nor\nany of its descendants is in the set C.\nIf all paths are blocked, then A is said to be d-separated from B"
  },
  {
    "vector_id": 1125,
    "chunk_id": "p287_c3",
    "page_number": 287,
    "text": "the node is in the set C.\nThe arrows meet head to head at the node, and neither the node nor\nany of its descendants is in the set C.\nIf all paths are blocked, then A is said to be d-separated from B by C,\nand the joint distribution over all of the variables in the graph will satisfy\nA \u22a5 \u22a5 B | C.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1126,
    "chunk_id": "p288_c0",
    "page_number": 288,
    "text": "282 When Models Meet Data\nFigure 8.12 Three\ntypes of graphical\nmodels: (a) Directed\ngraphical models\n(Bayesian\nnetworks);\n(b) Undirected\ngraphical models\n(Markov random\nfields); (c) Factor\ngraphs.\na b\nc\n(a) Directed graphical model\na b\nc\n(b) Undirected graphical\nmodel\na b\nc\n(c) Factor graph\nExample 8.9 (Conditional Independence)\nFigure 8.11\nD-separation\nexample.\na b c\nd\ne\nConsider the graphical model in Figure 8.11. Visual inspection gives us\nb \u22a5 \u22a5d |a, c (8.35)\na \u22a5 \u22a5c |b (8.36)\nb \u0338\u22a5 \u22a5d |c (8.37)\na \u0338\u22a5 \u22a5c |b, e (8.38)\nDirected graphical models allow a compact representation of proba-\nbilistic models, and we will see examples of directed graphical models in\nChapters 9, 10, and 11. The representation, along with the concept of con-\nditional independence, allows us to factorize the respective"
  },
  {
    "vector_id": 1127,
    "chunk_id": "p288_c1",
    "page_number": 288,
    "text": "odels, and we will see examples of directed graphical models in\nChapters 9, 10, and 11. The representation, along with the concept of con-\nditional independence, allows us to factorize the respective probabilistic\nmodels into expressions that are easier to optimize.\nThe graphical representation of the probabilistic model allows us to\nvisually see the impact of design choices we have made on the structure\nof the model. We often need to make high-level assumptions about the\nstructure of the model. These modeling assumptions (hyperparameters)\naffect the prediction performance, but cannot be selected directly using\nthe approaches we have seen so far. We will discuss different ways to\nchoose the structure in Section 8.6.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https:/"
  },
  {
    "vector_id": 1128,
    "chunk_id": "p288_c2",
    "page_number": 288,
    "text": "ted directly using\nthe approaches we have seen so far. We will discuss different ways to\nchoose the structure in Section 8.6.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1129,
    "chunk_id": "p289_c0",
    "page_number": 289,
    "text": "8.6 Model Selection 283\n8.5.3 Further Reading\nAn introduction to probabilistic graphical models can be found in Bishop\n(2006, chapter 8), and an extensive description of the different applica-\ntions and corresponding algorithmic implications can be found in the book\nby Koller and Friedman (2009). There are three main types of probabilistic\ngraphical models:\ndirected graphical\nmodelDirected graphical models (Bayesian networks); see Figure 8.12(a)\nBayesian network\nundirected graphical\nmodel\nUndirected graphical models (Markov random fields); see Figure 8.12(b)\nMarkov random\nfield\nfactor graph\nFactor graphs; see Figure 8.12(c)\nGraphical models allow for graph-based algorithms for inference and\nlearning, e.g., via local message passing. Applications range from rank-\ning in online games (Herbri"
  },
  {
    "vector_id": 1130,
    "chunk_id": "p289_c1",
    "page_number": 289,
    "text": "ctor graphs; see Figure 8.12(c)\nGraphical models allow for graph-based algorithms for inference and\nlearning, e.g., via local message passing. Applications range from rank-\ning in online games (Herbrich et al., 2007) and computer vision (e.g.,\nimage segmentation, semantic labeling, image denoising, image restora-\ntion (Kittler and F \u00a8oglein, 1984; Sucar and Gillies, 1994; Shotton et al.,\n2006; Szeliski et al., 2008)) to coding theory (McEliece et al., 1998), solv-\ning linear equation systems (Shental et al., 2008), and iterative Bayesian\nstate estimation in signal processing (Bickson et al., 2007; Deisenroth and\nMohamed, 2012).\nOne topic that is particularly important in real applications that we do\nnot discuss in this book is the idea of structured prediction (Bakir et al.,\n2007; Nowozin"
  },
  {
    "vector_id": 1131,
    "chunk_id": "p289_c2",
    "page_number": 289,
    "text": "007; Deisenroth and\nMohamed, 2012).\nOne topic that is particularly important in real applications that we do\nnot discuss in this book is the idea of structured prediction (Bakir et al.,\n2007; Nowozin et al., 2014), which allows machine learning models to\ntackle predictions that are structured, for example sequences, trees, and\ngraphs. The popularity of neural network models has allowed more flex-\nible probabilistic models to be used, resulting in many useful applica-\ntions of structured models (Goodfellow et al., 2016, chapter 16). In recent\nyears, there has been a renewed interest in graphical models due to their\napplications to causal inference (Pearl, 2009; Imbens and Rubin, 2015;\nPeters et al., 2017; Rosenbaum, 2017).\n8.6 Model Selection\nIn machine learning, we often need to make high-"
  },
  {
    "vector_id": 1132,
    "chunk_id": "p289_c3",
    "page_number": 289,
    "text": "l models due to their\napplications to causal inference (Pearl, 2009; Imbens and Rubin, 2015;\nPeters et al., 2017; Rosenbaum, 2017).\n8.6 Model Selection\nIn machine learning, we often need to make high-level modeling decisions\nthat critically influence the performance of the model. The choices we\nmake (e.g., the functional form of the likelihood) influence the number\nand type of free parameters in the model and thereby also the flexibility\nand expressivity of the model. More complex models are more flexible in A polynomial\ny = a0 +a1x+a2x2\ncan also describe\nlinear functions by\nsetting a2 = 0, i.e.,\nit is strictly more\nexpressive than a\nfirst-order\npolynomial.\nthe sense that they can be used to describe more datasets. For instance, a\npolynomial of degree 1 (a line y = a0 + a1x) can only be us"
  },
  {
    "vector_id": 1133,
    "chunk_id": "p289_c4",
    "page_number": 289,
    "text": "e.,\nit is strictly more\nexpressive than a\nfirst-order\npolynomial.\nthe sense that they can be used to describe more datasets. For instance, a\npolynomial of degree 1 (a line y = a0 + a1x) can only be used to describe\nlinear relations between inputs x and observations y. A polynomial of\ndegree 2 can additionally describe quadratic relationships between inputs\nand observations.\nOne would now think that very flexible models are generally preferable\nto simple models because they are more expressive. A general problem\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1134,
    "chunk_id": "p289_c5",
    "page_number": 289,
    "text": "Press (2020)."
  },
  {
    "vector_id": 1135,
    "chunk_id": "p290_c0",
    "page_number": 290,
    "text": "284 When Models Meet Data\nFigure 8.13 Nested\ncross-validation. We\nperform two levels\nof K-fold\ncross-validation.\nAll labeled data\nAll training data Test data\nTo train model Validation\nis that at training time we can only use the training set to evaluate the\nperformance of the model and learn its parameters. However, the per-\nformance on the training set is not really what we are interested in. In\nSection 8.3, we have seen that maximum likelihood estimation can lead\nto overfitting, especially when the training dataset is small. Ideally , our\nmodel (also) works well on the test set (which is not available at training\ntime). Therefore, we need some mechanisms for assessing how a model\ngeneralizes to unseen test data. Model selection is concerned with exactly\nthis problem.\n8.6.1 Nested Cross-V"
  },
  {
    "vector_id": 1136,
    "chunk_id": "p290_c1",
    "page_number": 290,
    "text": "available at training\ntime). Therefore, we need some mechanisms for assessing how a model\ngeneralizes to unseen test data. Model selection is concerned with exactly\nthis problem.\n8.6.1 Nested Cross-Validation\nWe have already seen an approach (cross-validation in Section 8.2.4) that\ncan be used for model selection. Recall that cross-validation provides an\nestimate of the generalization error by repeatedly splitting the dataset into\ntraining and validation sets. We can apply this idea one more time, i.e.,\nfor each split, we can perform another round of cross-validation. This is\nsometimes referred to asnested cross-validation; see Figure 8.13. The innernested\ncross-validation level is used to estimate the performance of a particular choice of model\nor hyperparameter on a internal validation"
  },
  {
    "vector_id": 1137,
    "chunk_id": "p290_c2",
    "page_number": 290,
    "text": "ed to asnested cross-validation; see Figure 8.13. The innernested\ncross-validation level is used to estimate the performance of a particular choice of model\nor hyperparameter on a internal validation set. The outer level is used to\nestimate generalization performance for the best choice of model chosen\nby the inner loop. We can test different model and hyperparameter choices\nin the inner loop. To distinguish the two levels, the set used to estimate\nthe generalization performance is often called the test set and the set usedtest set\nfor choosing the best model is called the validation set. The inner loopvalidation set\nestimates the expected value of the generalization error for a given model\n(8.39), by approximating it using the empirical error on the validation set,\ni.e.,The standard error"
  },
  {
    "vector_id": 1138,
    "chunk_id": "p290_c3",
    "page_number": 290,
    "text": "inner loopvalidation set\nestimates the expected value of the generalization error for a given model\n(8.39), by approximating it using the empirical error on the validation set,\ni.e.,The standard error\nis defined as \u03c3\u221a\nK ,\nwhere K is the\nnumber of\nexperiments and \u03c3\nis the standard\ndeviation of the risk\nof each experiment.\nEV[R(V |M)] \u2248 1\nK\nKX\nk=1\nR(V(k) |M) , (8.39)\nwhere R(V |M) is the empirical risk (e.g., root mean square error) on the\nvalidation set V for model M. We repeat this procedure for all models and\nchoose the model that performs best. Note that cross-validation not only\ngives us the expected generalization error, but we can also obtain high-\norder statistics, e.g., the standard error, an estimate of how uncertain the\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Fee"
  },
  {
    "vector_id": 1139,
    "chunk_id": "p290_c4",
    "page_number": 290,
    "text": "e expected generalization error, but we can also obtain high-\norder statistics, e.g., the standard error, an estimate of how uncertain the\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1140,
    "chunk_id": "p291_c0",
    "page_number": 291,
    "text": "8.6 Model Selection 285\nFigure 8.14\nBayesian inference\nembodies Occam\u2019s\nrazor. The\nhorizontal axis\ndescribes the space\nof all possible\ndatasets D. The\nevidence (vertical\naxis) evaluates how\nwell a model\npredicts available\ndata. Since\np(D |Mi) needs to\nintegrate to 1, we\nshould choose the\nmodel with the\ngreatest evidence.\nAdapted\nfrom MacKay\n(2003).\nEvidence\nDC\np(D |M1)\np(D |M2)\nmean estimate is. Once the model is chosen, we can evaluate the final\nperformance on the test set.\n8.6.2 Bayesian Model Selection\nThere are many approaches to model selection, some of which are covered\nin this section. Generally , they all attempt to trade off model complexity\nand data fit. We assume that simpler models are less prone to overfitting\nthan complex models, and hence the objective of model selection is"
  },
  {
    "vector_id": 1141,
    "chunk_id": "p291_c1",
    "page_number": 291,
    "text": "Generally , they all attempt to trade off model complexity\nand data fit. We assume that simpler models are less prone to overfitting\nthan complex models, and hence the objective of model selection is to find\nthe simplest model that explains the data reasonably well. This concept is\nalso known as Occam\u2019s razor. Occam\u2019s razor\nRemark. If we treat model selection as a hypothesis testing problem, we\nare looking for the simplest hypothesis that is consistent with the data (Mur-\nphy, 2012). \u2662\nOne may consider placing a prior on models that favors simpler models.\nHowever, it is not necessary to do this: An \u201cautomatic Occam\u2019s Razor\u201d is\nquantitatively embodied in the application of Bayesian probability (Smith\nand Spiegelhalter, 1980; Jefferys and Berger, 1992; MacKay, 1992). Fig-\nure 8.14, adapted f"
  },
  {
    "vector_id": 1142,
    "chunk_id": "p291_c2",
    "page_number": 291,
    "text": "s: An \u201cautomatic Occam\u2019s Razor\u201d is\nquantitatively embodied in the application of Bayesian probability (Smith\nand Spiegelhalter, 1980; Jefferys and Berger, 1992; MacKay, 1992). Fig-\nure 8.14, adapted from MacKay (2003), gives us the basic intuition why\ncomplex and very expressive models may turn out to be a less probable\nchoice for modeling a given dataset D. Let us think of the horizontal axis These predictions\nare quantified by a\nnormalized\nprobability\ndistribution on D,\ni.e., it needs to\nintegrate/sum to 1.\nrepresenting the space of all possible datasets D. If we are interested in\nthe posterior probability p(Mi | D) of model Mi given the data D, we can\nemploy Bayes\u2019 theorem. Assuming a uniform prior p(M) over all mod-\nels, Bayes\u2019 theorem rewards models in proportion to how much they pre-"
  },
  {
    "vector_id": 1143,
    "chunk_id": "p291_c3",
    "page_number": 291,
    "text": "ior probability p(Mi | D) of model Mi given the data D, we can\nemploy Bayes\u2019 theorem. Assuming a uniform prior p(M) over all mod-\nels, Bayes\u2019 theorem rewards models in proportion to how much they pre-\ndicted the data that occurred. This prediction of the data given model\nMi, p(D |Mi), is called the evidence for Mi. A simple model M1 can only evidence\npredict a small number of datasets, which is shown by p(D |M1); a more\npowerful model M2 that has, e.g., more free parameters than M1, is able\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1144,
    "chunk_id": "p292_c0",
    "page_number": 292,
    "text": "286 When Models Meet Data\nto predict a greater variety of datasets. This means, however, that M2\ndoes not predict the datasets in region C as well as M1. Suppose that\nequal prior probabilities have been assigned to the two models. Then, if\nthe dataset falls into region C, the less powerful model M1 is the more\nprobable model.\nEarlier in this chapter, we argued that models need to be able to explain\nthe data, i.e., there should be a way to generate data from a given model.\nFurthermore, if the model has been appropriately learned from the data,\nthen we expect that the generated data should be similar to the empirical\ndata. For this, it is helpful to phrase model selection as a hierarchical\ninference problem, which allows us to compute the posterior distribution\nover models.\nLet us consider a"
  },
  {
    "vector_id": 1145,
    "chunk_id": "p292_c1",
    "page_number": 292,
    "text": "milar to the empirical\ndata. For this, it is helpful to phrase model selection as a hierarchical\ninference problem, which allows us to compute the posterior distribution\nover models.\nLet us consider a finite number of models M = {M1, . . . , MK}, where\neach model Mk possesses parameters \u03b8k. In Bayesian model selection, weBayesian model\nselection place a prior p(M) on the set of models. The corresponding generative\ngenerative process process that allows us to generate data from this model is\nFigure 8.15\nIllustration of the\nhierarchical\ngenerative process\nin Bayesian model\nselection. We place\na prior p(M) on the\nset of models. For\neach model, there is\na distribution\np(\u03b8 |M) on the\ncorresponding\nmodel parameters,\nwhich is used to\ngenerate the data D.\nM\n\u03b8\nD\nMk \u223c p(M) (8.40)\n\u03b8k \u223c p(\u03b8 |Mk) (8.41"
  },
  {
    "vector_id": 1146,
    "chunk_id": "p292_c2",
    "page_number": 292,
    "text": "prior p(M) on the\nset of models. For\neach model, there is\na distribution\np(\u03b8 |M) on the\ncorresponding\nmodel parameters,\nwhich is used to\ngenerate the data D.\nM\n\u03b8\nD\nMk \u223c p(M) (8.40)\n\u03b8k \u223c p(\u03b8 |Mk) (8.41)\nD \u223cp(D |\u03b8k) (8.42)\nand illustrated in Figure 8.15. Given a training set D, we apply Bayes\u2019\ntheorem and compute the posterior distribution over models as\np(Mk | D) \u221d p(Mk)p(D |Mk) . (8.43)\nNote that this posterior no longer depends on the model parameters \u03b8k\nbecause they have been integrated out in the Bayesian setting since\np(D |Mk) =\nZ\np(D |\u03b8k)p(\u03b8k |Mk)d\u03b8k , (8.44)\nwhere p(\u03b8k |Mk) is the prior distribution of the model parameters \u03b8k of\nmodel Mk. The term (8.44) is referred to as themodel evidence or marginal\nmodel evidence\nmarginal likelihood\nlikelihood. From the posterior in (8.43), we det"
  },
  {
    "vector_id": 1147,
    "chunk_id": "p292_c3",
    "page_number": 292,
    "text": "distribution of the model parameters \u03b8k of\nmodel Mk. The term (8.44) is referred to as themodel evidence or marginal\nmodel evidence\nmarginal likelihood\nlikelihood. From the posterior in (8.43), we determine the MAP estimate\nM\u2217 = arg max\nMk\np(Mk | D) . (8.45)\nWith a uniform prior p(Mk) = 1\nK , which gives every model equal (prior)\nprobability , determining the MAP estimate over models amounts to pick-\ning the model that maximizes the model evidence (8.44).\nRemark (Likelihood and Marginal Likelihood). There are some important\ndifferences between a likelihood and a marginal likelihood (evidence):\nWhile the likelihood is prone to overfitting, the marginal likelihood is typ-\nically not as the model parameters have been marginalized out (i.e., we\nno longer have to fit the parameters). Furthermor"
  },
  {
    "vector_id": 1148,
    "chunk_id": "p292_c4",
    "page_number": 292,
    "text": "While the likelihood is prone to overfitting, the marginal likelihood is typ-\nically not as the model parameters have been marginalized out (i.e., we\nno longer have to fit the parameters). Furthermore, the marginal likeli-\nhood automatically embodies a trade-off between model complexity and\ndata fit (Occam\u2019s razor). \u2662\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1149,
    "chunk_id": "p293_c0",
    "page_number": 293,
    "text": "8.6 Model Selection 287\n8.6.3 Bayes Factors for Model Comparison\nConsider the problem of comparing two probabilistic models M1, M2,\ngiven a dataset D. If we compute the posteriors p(M1 | D) and p(M2 | D),\nwe can compute the ratio of the posteriors\np(M1 | D)\np(M2 | D)| {z }\nposterior odds\n=\np(D |M1)p(M1)\np(D)\np(D |M2)p(M2)\np(D)\n= p(M1)\np(M2)| {z }\nprior odds\np(D |M1)\np(D |M2)| {z }\nBayes factor\n. (8.46)\nThe ratio of the posteriors is also called the posterior odds. The first frac- posterior odds\ntion on the right-hand side of (8.46), the prior odds, measures how much prior odds\nour prior (initial) beliefs favorM1 over M2. The ratio of the marginal like-\nlihoods (second fraction on the right-hand-side) is called the Bayes factor Bayes factor\nand measures how well the data D is predicted by M"
  },
  {
    "vector_id": 1150,
    "chunk_id": "p293_c1",
    "page_number": 293,
    "text": "al) beliefs favorM1 over M2. The ratio of the marginal like-\nlihoods (second fraction on the right-hand-side) is called the Bayes factor Bayes factor\nand measures how well the data D is predicted by M1 compared to M2.\nRemark. The Jeffreys-Lindley paradox states that the \u201cBayes factor always Jeffreys-Lindley\nparadoxfavors the simpler model since the probability of the data under a complex\nmodel with a diffuse prior will be very small\u201d (Murphy, 2012). Here, a\ndiffuse prior refers to a prior that does not favor specific models, i.e.,\nmany models are a priori plausible under this prior. \u2662\nIf we choose a uniform prior over models, the prior odds term in (8.46)\nis 1, i.e., the posterior odds is the ratio of the marginal likelihoods (Bayes\nfactor)\np(D |M1)\np(D |M2) . (8.47)\nIf the Bayes factor is"
  },
  {
    "vector_id": 1151,
    "chunk_id": "p293_c2",
    "page_number": 293,
    "text": "oose a uniform prior over models, the prior odds term in (8.46)\nis 1, i.e., the posterior odds is the ratio of the marginal likelihoods (Bayes\nfactor)\np(D |M1)\np(D |M2) . (8.47)\nIf the Bayes factor is greater than 1, we choose model M1, otherwise\nmodel M2. In a similar way to frequentist statistics, there are guidelines\non the size of the ratio that one should consider before \u201dsignificance\u201d of\nthe result (Jeffreys, 1961).\nRemark (Computing the Marginal Likelihood) . The marginal likelihood\nplays an important role in model selection: We need to compute Bayes\nfactors (8.46) and posterior distributions over models (8.43).\nUnfortunately , computing the marginal likelihood requires us to solve\nan integral (8.44). This integration is generally analytically intractable,\nand we will have to resort"
  },
  {
    "vector_id": 1152,
    "chunk_id": "p293_c3",
    "page_number": 293,
    "text": "utions over models (8.43).\nUnfortunately , computing the marginal likelihood requires us to solve\nan integral (8.44). This integration is generally analytically intractable,\nand we will have to resort to approximation techniques, e.g., numerical\nintegration (Stoer and Burlirsch, 2002), stochastic approximations using\nMonte Carlo (Murphy, 2012), or Bayesian Monte Carlo techniques (O\u2019Hagan,\n1991; Rasmussen and Ghahramani, 2003).\nHowever, there are special cases in which we can solve it. In Section 6.6.1,\nwe discussed conjugate models. If we choose a conjugate parameter prior\np(\u03b8), we can compute the marginal likelihood in closed form. In Chap-\nter 9, we will do exactly this in the context of linear regression. \u2662\nWe have seen a brief introduction to the basic concepts of machine\nlearning in t"
  },
  {
    "vector_id": 1153,
    "chunk_id": "p293_c4",
    "page_number": 293,
    "text": "the marginal likelihood in closed form. In Chap-\nter 9, we will do exactly this in the context of linear regression. \u2662\nWe have seen a brief introduction to the basic concepts of machine\nlearning in this chapter. For the rest of this part of the book we will see\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1154,
    "chunk_id": "p294_c0",
    "page_number": 294,
    "text": "288 When Models Meet Data\nhow the three different flavors of learning in Sections 8.2, 8.3, and 8.4 are\napplied to the four pillars of machine learning (regression, dimensionality\nreduction, density estimation, and classification).\n8.6.4 Further Reading\nWe mentioned at the start of the section that there are high-level modeling\nchoices that influence the performance of the model. Examples include the\nfollowing:\nThe degree of a polynomial in a regression setting\nThe number of components in a mixture model\nThe network architecture of a (deep) neural network\nThe type of kernel in a support vector machine\nThe dimensionality of the latent space in PCA\nThe learning rate (schedule) in an optimization algorithm\nIn parametric\nmodels, the number\nof parameters is\noften related to the\ncomplexity of th"
  },
  {
    "vector_id": 1155,
    "chunk_id": "p294_c1",
    "page_number": 294,
    "text": "machine\nThe dimensionality of the latent space in PCA\nThe learning rate (schedule) in an optimization algorithm\nIn parametric\nmodels, the number\nof parameters is\noften related to the\ncomplexity of the\nmodel class.\nRasmussen and Ghahramani (2001) showed that the automatic Occam\u2019s\nrazor does not necessarily penalize the number of parameters in a model,\nbut it is active in terms of the complexity of functions. They also showed\nthat the automatic Occam\u2019s razor also holds for Bayesian nonparametric\nmodels with many parameters, e.g., Gaussian processes.\nIf we focus on the maximum likelihood estimate, there exist a number of\nheuristics for model selection that discourage overfitting. They are called\ninformation criteria, and we choose the model with the largest value. The\nAkaike information crit"
  },
  {
    "vector_id": 1156,
    "chunk_id": "p294_c2",
    "page_number": 294,
    "text": "e, there exist a number of\nheuristics for model selection that discourage overfitting. They are called\ninformation criteria, and we choose the model with the largest value. The\nAkaike information criterion (AIC) (Akaike, 1974)Akaike information\ncriterion\nlog p(x|\u03b8) \u2212 M (8.48)\ncorrects for the bias of the maximum likelihood estimator by addition of\na penalty term to compensate for the overfitting of more complex models\nwith lots of parameters. Here, M is the number of model parameters. The\nAIC estimates the relative information lost by a given model.\nThe Bayesian information criterion (BIC) (Schwarz, 1978)Bayesian\ninformation\ncriterion log p(x) = log\nZ\np(x|\u03b8)p(\u03b8)d\u03b8 \u2248 log p(x|\u03b8) \u2212 1\n2M log N (8.49)\ncan be used for exponential family distributions. Here, N is the number\nof data points and M i"
  },
  {
    "vector_id": 1157,
    "chunk_id": "p294_c3",
    "page_number": 294,
    "text": "hwarz, 1978)Bayesian\ninformation\ncriterion log p(x) = log\nZ\np(x|\u03b8)p(\u03b8)d\u03b8 \u2248 log p(x|\u03b8) \u2212 1\n2M log N (8.49)\ncan be used for exponential family distributions. Here, N is the number\nof data points and M is the number of parameters. BIC penalizes model\ncomplexity more heavily than AIC.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1158,
    "chunk_id": "p295_c0",
    "page_number": 295,
    "text": "9\nLinear Regression\nIn the following, we will apply the mathematical concepts from Chap-\nters 2, 5, 6, and 7 to solve linear regression (curve fitting) problems. In\nregression, we aim to find a function f that maps inputs x \u2208 RD to corre- regression\nsponding function valuesf(x) \u2208 R. We assume we are given a set of train-\ning inputs xn and corresponding noisy observationsyn = f(xn)+\u03f5, where\n\u03f5 is an i.i.d. random variable that describes measurement/observation\nnoise and potentially unmodeled processes (which we will not consider\nfurther in this chapter). Throughout this chapter, we assume zero-mean\nGaussian noise. Our task is to find a function that not only models the\ntraining data, but generalizes well to predicting function values at input\nlocations that are not part of the training data"
  },
  {
    "vector_id": 1159,
    "chunk_id": "p295_c1",
    "page_number": 295,
    "text": "ean\nGaussian noise. Our task is to find a function that not only models the\ntraining data, but generalizes well to predicting function values at input\nlocations that are not part of the training data (see Chapter 8). An il-\nlustration of such a regression problem is given in Figure 9.1. A typical\nregression setting is given in Figure 9.1(a): For some input values xn, we\nobserve (noisy) function values yn = f(xn) + \u03f5. The task is to infer the\nfunction f that generated the data and generalizes well to function values\nat new input locations. A possible solution is given in Figure 9.1(b), where\nwe also show three distributions centered at the function valuesf(x) that\nrepresent the noise in the data.\nRegression is a fundamental problem in machine learning, and regres-\nsion problems appear in a"
  },
  {
    "vector_id": 1160,
    "chunk_id": "p295_c2",
    "page_number": 295,
    "text": "also show three distributions centered at the function valuesf(x) that\nrepresent the noise in the data.\nRegression is a fundamental problem in machine learning, and regres-\nsion problems appear in a diverse range of research areas and applica-\nFigure 9.1\n(a) Dataset;\n(b) possible solution\nto the regression\nproblem.\n\u22124 \u22122 0 2 4\nx\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\ny\n(a) Regression problem: observed noisy func-\ntion values from which we wish to infer the\nunderlying function that generated the data.\n\u22124 \u22122 0 2 4\nx\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\ny\n(b) Regression solution: possible function\nthat could have generated the data (blue)\nwith indication of the measurement noise of\nthe function value at the corresponding in-\nputs (orange distributions).\n289\nThis material is published by Cambridge University Press asMathema"
  },
  {
    "vector_id": 1161,
    "chunk_id": "p295_c3",
    "page_number": 295,
    "text": "data (blue)\nwith indication of the measurement noise of\nthe function value at the corresponding in-\nputs (orange distributions).\n289\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 1162,
    "chunk_id": "p296_c0",
    "page_number": 296,
    "text": "290 Linear Regression\ntions, including time-series analysis (e.g., system identification), control\nand robotics (e.g., reinforcement learning, forward/inverse model learn-\ning), optimization (e.g., line searches, global optimization), and deep-\nlearning applications (e.g., computer games, speech-to-text translation,\nimage recognition, automatic video annotation). Regression is also a key\ningredient of classification algorithms. Finding a regression function re-\nquires solving a variety of problems, including the following:\nChoice of the model (type) and the parametrization of the regres-\nsion function. Given a dataset, what function classes (e.g., polynomi-Normally , the type\nof noise could also\nbe a \u201cmodel choice\u201d,\nbut we fix the noise\nto be Gaussian in\nthis chapter.\nals) are good candida"
  },
  {
    "vector_id": 1163,
    "chunk_id": "p296_c1",
    "page_number": 296,
    "text": "function. Given a dataset, what function classes (e.g., polynomi-Normally , the type\nof noise could also\nbe a \u201cmodel choice\u201d,\nbut we fix the noise\nto be Gaussian in\nthis chapter.\nals) are good candidates for modeling the data, and what particular\nparametrization (e.g., degree of the polynomial) should we choose?\nModel selection, as discussed in Section 8.6, allows us to compare var-\nious models to find the simplest model that explains the training data\nreasonably well.\nFinding good parameters. Having chosen a model of the regression\nfunction, how do we find good model parameters? Here, we will need to\nlook at different loss/objective functions (they determine what a \u201cgood\u201d\nfit is) and optimization algorithms that allow us to minimize this loss.\nOverfitting and model selection. Overfitting"
  },
  {
    "vector_id": 1164,
    "chunk_id": "p296_c2",
    "page_number": 296,
    "text": "need to\nlook at different loss/objective functions (they determine what a \u201cgood\u201d\nfit is) and optimization algorithms that allow us to minimize this loss.\nOverfitting and model selection. Overfitting is a problem when the\nregression function fits the training data \u201ctoo well\u201d but does not gen-\neralize to unseen test data. Overfitting typically occurs if the underly-\ning model (or its parametrization) is overly flexible and expressive; see\nSection 8.6. We will look at the underlying reasons and discuss ways to\nmitigate the effect of overfitting in the context of linear regression.\nRelationship between loss functions and parameter priors.Loss func-\ntions (optimization objectives) are often motivated and induced by prob-\nabilistic models. We will look at the connection between loss functions\na"
  },
  {
    "vector_id": 1165,
    "chunk_id": "p296_c3",
    "page_number": 296,
    "text": "etween loss functions and parameter priors.Loss func-\ntions (optimization objectives) are often motivated and induced by prob-\nabilistic models. We will look at the connection between loss functions\nand the underlying prior assumptions that induce these losses.\nUncertainty modeling. In any practical setting, we have access to only\na finite, potentially large, amount of (training) data for selecting the\nmodel class and the corresponding parameters. Given that this finite\namount of training data does not cover all possible scenarios, we may\nwant to describe the remaining parameter uncertainty to obtain a mea-\nsure of confidence of the model\u2019s prediction at test time; the smaller the\ntraining set, the more important uncertainty modeling. Consistent mod-\neling of uncertainty equips model predi"
  },
  {
    "vector_id": 1166,
    "chunk_id": "p296_c4",
    "page_number": 296,
    "text": "obtain a mea-\nsure of confidence of the model\u2019s prediction at test time; the smaller the\ntraining set, the more important uncertainty modeling. Consistent mod-\neling of uncertainty equips model predictions with confidence bounds.\nIn the following, we will be using the mathematical tools from Chap-\nters 3, 5, 6 and 7 to solve linear regression problems. We will discuss\nmaximum likelihood and maximum a posteriori (MAP) estimation to find\noptimal model parameters. Using these parameter estimates, we will have\na brief look at generalization errors and overfitting. Toward the end of\nthis chapter, we will discuss Bayesian linear regression, which allows us to\nreason about model parameters at a higher level, thereby removing some\nof the problems encountered in maximum likelihood and MAP estimati"
  },
  {
    "vector_id": 1167,
    "chunk_id": "p296_c5",
    "page_number": 296,
    "text": "we will discuss Bayesian linear regression, which allows us to\nreason about model parameters at a higher level, thereby removing some\nof the problems encountered in maximum likelihood and MAP estimation.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1168,
    "chunk_id": "p297_c0",
    "page_number": 297,
    "text": "9.1 Problem Formulation 291\n9.1 Problem Formulation\nBecause of the presence of observation noise, we will adopt a probabilis-\ntic approach and explicitly model the noise using a likelihood function.\nMore specifically , throughout this chapter, we consider a regression prob-\nlem with the likelihood function\np(y |x) = N\n\u0000\ny |f(x), \u03c32\u0001\n. (9.1)\nHere, x \u2208 RD are inputs and y \u2208 R are noisy function values (targets).\nWith (9.1), the functional relationship between x and y is given as\ny = f(x) + \u03f5 , (9.2)\nwhere \u03f5 \u223c N\n\u0000\n0, \u03c32\u0001\nis independent, identically distributed (i.i.d.) Gaus-\nsian measurement noise with mean 0 and variance \u03c32. Our objective is\nto find a function that is close (similar) to the unknown function f that\ngenerated the data and that generalizes well.\nIn this chapter, we focus on par"
  },
  {
    "vector_id": 1169,
    "chunk_id": "p297_c1",
    "page_number": 297,
    "text": "e with mean 0 and variance \u03c32. Our objective is\nto find a function that is close (similar) to the unknown function f that\ngenerated the data and that generalizes well.\nIn this chapter, we focus on parametric models, i.e., we choose a para-\nmetrized function and find parameters\u03b8 that \u201cwork well\u201d for modeling the\ndata. For the time being, we assume that the noise variance \u03c32 is known\nand focus on learning the model parameters \u03b8. In linear regression, we\nconsider the special case that the parameters \u03b8 appear linearly in our\nmodel. An example of linear regression is given by\np(y |x, \u03b8) = N\n\u0000\ny |x\u22a4\u03b8, \u03c32\u0001\n(9.3)\n\u21d0 \u21d2y = x\u22a4\u03b8 + \u03f5 , \u03f5 \u223c N\n\u0000\n0, \u03c32\u0001\n, (9.4)\nwhere \u03b8 \u2208 RD are the parameters we seek. The class of functions de-\nscribed by (9.4) are straight lines that pass through the origin. In (9.4),\nwe"
  },
  {
    "vector_id": 1170,
    "chunk_id": "p297_c2",
    "page_number": 297,
    "text": "\u03b8, \u03c32\u0001\n(9.3)\n\u21d0 \u21d2y = x\u22a4\u03b8 + \u03f5 , \u03f5 \u223c N\n\u0000\n0, \u03c32\u0001\n, (9.4)\nwhere \u03b8 \u2208 RD are the parameters we seek. The class of functions de-\nscribed by (9.4) are straight lines that pass through the origin. In (9.4),\nwe chose a parametrization f(x) = x\u22a4\u03b8. A Dirac delta (delta\nfunction) is zero\neverywhere except\nat a single point,\nand its integral is 1.\nIt can be considered\na Gaussian in the\nlimit of \u03c32 \u2192 0.\nThe likelihood in (9.3) is the probability density function of y evalu-\nlikelihood\nated at x\u22a4\u03b8. Note that the only source of uncertainty originates from the\nobservation noise (as x and \u03b8 are assumed known in (9.3)). Without ob-\nservation noise, the relationship between x and y would be deterministic\nand (9.3) would be a Dirac delta.\nExample 9.1\nFor x, \u03b8\u2208 R the linear regression model in (9.4) describes str"
  },
  {
    "vector_id": 1171,
    "chunk_id": "p297_c3",
    "page_number": 297,
    "text": "9.3)). Without ob-\nservation noise, the relationship between x and y would be deterministic\nand (9.3) would be a Dirac delta.\nExample 9.1\nFor x, \u03b8\u2208 R the linear regression model in (9.4) describes straight lines\n(linear functions), and the parameter \u03b8 is the slope of the line. Fig-\nure 9.2(a) shows some example functions for different values of \u03b8.\nLinear regression\nrefers to models that\nare linear in the\nparameters.\nThe linear regression model in (9.3)\u2013(9.4) is not only linear in the pa-\nrameters, but also linear in the inputs x. Figure 9.2(a) shows examples\nof such functions. We will see later that y = \u03d5\u22a4(x)\u03b8 for nonlinear trans-\nformations \u03d5 is also a linear regression model because \u201clinear regression\u201d\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Pre"
  },
  {
    "vector_id": 1172,
    "chunk_id": "p297_c4",
    "page_number": 297,
    "text": "r that y = \u03d5\u22a4(x)\u03b8 for nonlinear trans-\nformations \u03d5 is also a linear regression model because \u201clinear regression\u201d\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1173,
    "chunk_id": "p298_c0",
    "page_number": 298,
    "text": "292 Linear Regression\nFigure 9.2 Linear\nregression example.\n(a) Example\nfunctions that fall\ninto this category;\n(b) training set;\n(c) maximum\nlikelihood estimate.\n\u221210 0 10\nx\n\u221220\n0\n20\ny (a) Example functions (straight\nlines) that can be described us-\ning the linear model in (9.4).\n\u221210 \u22125 0 5 10\nx\n\u221210\n0\n10\ny\n(b) Training set.\n\u221210 \u22125 0 5 10\nx\n\u221210\n0\n10\ny\n (c) Maximum likelihood esti-\nmate.\nrefers to models that are \u201clinear in the parameters\u201d, i.e., models that de-\nscribe a function by a linear combination of input features. Here, a \u201cfea-\nture\u201d is a representation \u03d5(x) of the inputs x.\nIn the following, we will discuss in more detail how to find good pa-\nrameters \u03b8 and how to evaluate whether a parameter set \u201cworks well\u201d.\nFor the time being, we assume that the noise variance \u03c32 is known.\n9.2 Pa"
  },
  {
    "vector_id": 1174,
    "chunk_id": "p298_c1",
    "page_number": 298,
    "text": "wing, we will discuss in more detail how to find good pa-\nrameters \u03b8 and how to evaluate whether a parameter set \u201cworks well\u201d.\nFor the time being, we assume that the noise variance \u03c32 is known.\n9.2 Parameter Estimation\nConsider the linear regression setting (9.4) and assume we are given a\ntraining set D := {(x1, y1), . . . ,(xN , yN )} consisting of N inputs xn \u2208training set\nRD and corresponding observations/targets yn \u2208 R, n = 1, . . . , N. TheFigure 9.3\nProbabilistic\ngraphical model for\nlinear regression.\nObserved random\nvariables are\nshaded,\ndeterministic/\nknown values are\nwithout circles.\n\u03b8\nyn\n\u03c3\nxn\nn = 1, . . . , N\ncorresponding graphical model is given in Figure 9.3. Note that yi and yj\nare conditionally independent given their respective inputs xi, xj so that\nthe likelihood factorize"
  },
  {
    "vector_id": 1175,
    "chunk_id": "p298_c2",
    "page_number": 298,
    "text": "\u03b8\nyn\n\u03c3\nxn\nn = 1, . . . , N\ncorresponding graphical model is given in Figure 9.3. Note that yi and yj\nare conditionally independent given their respective inputs xi, xj so that\nthe likelihood factorizes according to\np(Y | X, \u03b8) = p(y1, . . . , yN |x1, . . . ,xN , \u03b8) (9.5a)\n=\nNY\nn=1\np(yn |xn, \u03b8) =\nNY\nn=1\nN\n\u0000\nyn |x\u22a4\nn \u03b8, \u03c32\u0001\n, (9.5b)\nwhere we defined X := {x1, . . . ,xN } and Y := {y1, . . . , yN } as the sets\nof training inputs and corresponding targets, respectively . The likelihood\nand the factors p(yn |xn, \u03b8) are Gaussian due to the noise distribution;\nsee (9.3).\nIn the following, we will discuss how to find optimal parameters \u03b8\u2217 \u2208\nRD for the linear regression model (9.4). Once the parameters \u03b8\u2217 are\nfound, we can predict function values by using this parameter estimate\nin (9.4) so that at"
  },
  {
    "vector_id": 1176,
    "chunk_id": "p298_c3",
    "page_number": 298,
    "text": "cuss how to find optimal parameters \u03b8\u2217 \u2208\nRD for the linear regression model (9.4). Once the parameters \u03b8\u2217 are\nfound, we can predict function values by using this parameter estimate\nin (9.4) so that at an arbitrary test input x\u2217 the distribution of the corre-\nsponding target y\u2217 is\np(y\u2217 |x\u2217, \u03b8\u2217) = N\n\u0000\ny\u2217 |x\u22a4\n\u2217 \u03b8\u2217, \u03c32\u0001\n. (9.6)\nIn the following, we will have a look at parameter estimation by maxi-\nmizing the likelihood, a topic that we already covered to some degree in\nSection 8.3.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1177,
    "chunk_id": "p299_c0",
    "page_number": 299,
    "text": "9.2 Parameter Estimation 293\n9.2.1 Maximum Likelihood Estimation\nA widely used approach to finding the desired parameters\u03b8ML is maximum maximum likelihood\nestimationlikelihood estimation, where we find parameters \u03b8ML that maximize the\nlikelihood (9.5b). Intuitively , maximizing the likelihood means maximiz- Maximizing the\nlikelihood means\nmaximizing the\npredictive\ndistribution of the\n(training) data\ngiven the\nparameters.\ning the predictive distribution of the training data given the model param-\neters. We obtain the maximum likelihood parameters as\n\u03b8ML \u2208 arg max\n\u03b8\np(Y | X, \u03b8) . (9.7)\nThe likelihood is not\na probability\ndistribution in the\nparameters.\nRemark. The likelihood p(y |x, \u03b8) is not a probability distribution in \u03b8: It\nis simply a function of the parameters \u03b8 but does not integrate"
  },
  {
    "vector_id": 1178,
    "chunk_id": "p299_c1",
    "page_number": 299,
    "text": "ihood is not\na probability\ndistribution in the\nparameters.\nRemark. The likelihood p(y |x, \u03b8) is not a probability distribution in \u03b8: It\nis simply a function of the parameters \u03b8 but does not integrate to 1 (i.e.,\nit is unnormalized), and may not even be integrable with respect to \u03b8.\nHowever, the likelihood in (9.7) is a normalized probability distribution\nin y. \u2662\nTo find the desired parameters \u03b8ML that maximize the likelihood, we\ntypically perform gradient ascent (or gradient descent on the negative\nlikelihood). In the case of linear regression we consider here, however, Since the logarithm\nis a (strictly)\nmonotonically\nincreasing function,\nthe optimum of a\nfunction f is\nidentical to the\noptimum of log f.\na closed-form solution exists, which makes iterative gradient descent un-\nnecessary ."
  },
  {
    "vector_id": 1179,
    "chunk_id": "p299_c2",
    "page_number": 299,
    "text": "a (strictly)\nmonotonically\nincreasing function,\nthe optimum of a\nfunction f is\nidentical to the\noptimum of log f.\na closed-form solution exists, which makes iterative gradient descent un-\nnecessary . In practice, instead of maximizing the likelihood directly , we\napply the log-transformation to the likelihood function and minimize the\nnegative log-likelihood.\nRemark (Log-Transformation). Since the likelihood (9.5b) is a product of\nN Gaussian distributions, the log-transformation is useful since (a) it does\nnot suffer from numerical underflow, and (b) the differentiation rules will\nturn out simpler. More specifically , numerical underflow will be a prob-\nlem when we multiply N probabilities, where N is the number of data\npoints, since we cannot represent very small numbers, such as 10\u2212256."
  },
  {
    "vector_id": 1180,
    "chunk_id": "p299_c3",
    "page_number": 299,
    "text": "pler. More specifically , numerical underflow will be a prob-\nlem when we multiply N probabilities, where N is the number of data\npoints, since we cannot represent very small numbers, such as 10\u2212256.\nFurthermore, the log-transform will turn the product into a sum of log-\nprobabilities such that the corresponding gradient is a sum of individual\ngradients, instead of a repeated application of the product rule (5.46) to\ncompute the gradient of a product of N terms. \u2662\nTo find the optimal parameters \u03b8ML of our linear regression problem,\nwe minimize the negative log-likelihood\n\u2212log p(Y | X, \u03b8) = \u2212log\nNY\nn=1\np(yn |xn, \u03b8) = \u2212\nNX\nn=1\nlog p(yn |xn, \u03b8) , (9.8)\nwhere we exploited that the likelihood (9.5b) factorizes over the number\nof data points due to our independence assumption on the training set"
  },
  {
    "vector_id": 1181,
    "chunk_id": "p299_c4",
    "page_number": 299,
    "text": "g\nNY\nn=1\np(yn |xn, \u03b8) = \u2212\nNX\nn=1\nlog p(yn |xn, \u03b8) , (9.8)\nwhere we exploited that the likelihood (9.5b) factorizes over the number\nof data points due to our independence assumption on the training set.\nIn the linear regression model (9.4), the likelihood is Gaussian (due to\nthe Gaussian additive noise term), such that we arrive at\nlog p(yn |xn, \u03b8) = \u2212 1\n2\u03c32 (yn \u2212 x\u22a4\nn \u03b8)2 + const , (9.9)\nwhere the constant includes all terms independent of\u03b8. Using (9.9) in the\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1182,
    "chunk_id": "p300_c0",
    "page_number": 300,
    "text": "294 Linear Regression\nnegative log-likelihood (9.8), we obtain (ignoring the constant terms)\nL(\u03b8) := 1\n2\u03c32\nNX\nn=1\n(yn \u2212 x\u22a4\nn \u03b8)2 (9.10a)\n= 1\n2\u03c32 (y \u2212 X\u03b8)\u22a4(y \u2212 X\u03b8) = 1\n2\u03c32 \u2225y \u2212 X\u03b8\u2225\n2\n, (9.10b)\nwhere we define the design matrix X := [x1, . . . ,xN ]\u22a4 \u2208 RN\u00d7D as theThe negative\nlog-likelihood\nfunction is also\ncalled error function.\ndesign matrix\ncollection of training inputs and y := [y1, . . . , yN ]\u22a4 \u2208 RN as a vector that\ncollects all training targets. Note that the nth row in the design matrix X\ncorresponds to the training input xn. In (9.10b), we used the fact that the\nThe squared error is\noften used as a\nmeasure of distance.\nsum of squared errors between the observationsyn and the corresponding\nmodel prediction x\u22a4\nn \u03b8 equals the squared distance between y and X\u03b8.\nRecall from\nSection 3.1 t"
  },
  {
    "vector_id": 1183,
    "chunk_id": "p300_c1",
    "page_number": 300,
    "text": "en used as a\nmeasure of distance.\nsum of squared errors between the observationsyn and the corresponding\nmodel prediction x\u22a4\nn \u03b8 equals the squared distance between y and X\u03b8.\nRecall from\nSection 3.1 that\n\u2225x\u22252 = x\u22a4x if we\nchoose the dot\nproduct as the inner\nproduct.\nWith (9.10b), we have now a concrete form of the negative log-likelihood\nfunction we need to optimize. We immediately see that (9.10b) is quadratic\nin \u03b8. This means that we can find a unique global solution \u03b8ML for mini-\nmizing the negative log-likelihood L. We can find the global optimum by\ncomputing the gradient of L, setting it to 0 and solving for \u03b8.\nUsing the results from Chapter 5, we compute the gradient of L with\nrespect to the parameters as\ndL\nd\u03b8 = d\nd\u03b8\n\u0012 1\n2\u03c32 (y \u2212 X\u03b8)\u22a4(y \u2212 X\u03b8)\n\u0013\n(9.11a)\n= 1\n2\u03c32\nd\nd\u03b8\n\u0010\ny\u22a4y \u2212 2y\u22a4X\u03b8 + \u03b8\u22a4"
  },
  {
    "vector_id": 1184,
    "chunk_id": "p300_c2",
    "page_number": 300,
    "text": "o 0 and solving for \u03b8.\nUsing the results from Chapter 5, we compute the gradient of L with\nrespect to the parameters as\ndL\nd\u03b8 = d\nd\u03b8\n\u0012 1\n2\u03c32 (y \u2212 X\u03b8)\u22a4(y \u2212 X\u03b8)\n\u0013\n(9.11a)\n= 1\n2\u03c32\nd\nd\u03b8\n\u0010\ny\u22a4y \u2212 2y\u22a4X\u03b8 + \u03b8\u22a4X\u22a4X\u03b8\n\u0011\n(9.11b)\n= 1\n\u03c32 (\u2212y\u22a4X + \u03b8\u22a4X\u22a4X) \u2208 R1\u00d7D . (9.11c)\nThe maximum likelihood estimator \u03b8ML solves dL\nd\u03b8 = 0\u22a4 (necessary opti-\nmality condition) and we obtainIgnoring the\npossibility of\nduplicate data\npoints, rk(X) = D\nif N \u2a7e D, i.e., we\ndo not have more\nparameters than\ndata points.\ndL\nd\u03b8 = 0\u22a4 (9.11c)\n\u21d0 \u21d2\u03b8\u22a4\nMLX\u22a4X = y\u22a4X (9.12a)\n\u21d0 \u21d2\u03b8\u22a4\nML = y\u22a4X(X\u22a4X)\u22121 (9.12b)\n\u21d0 \u21d2\u03b8ML = (X\u22a4X)\u22121X\u22a4y . (9.12c)\nWe could right-multiply the first equation by (X\u22a4X)\u22121 because X\u22a4X is\npositive definite if rk(X) = D, where rk(X) denotes the rank of X.\nRemark. Setting the gradient to0\u22a4 is a necessary and sufficient condition,\na"
  },
  {
    "vector_id": 1185,
    "chunk_id": "p300_c3",
    "page_number": 300,
    "text": "ht-multiply the first equation by (X\u22a4X)\u22121 because X\u22a4X is\npositive definite if rk(X) = D, where rk(X) denotes the rank of X.\nRemark. Setting the gradient to0\u22a4 is a necessary and sufficient condition,\nand we obtain a global minimum since the Hessian \u22072\n\u03b8L(\u03b8) = X\u22a4X \u2208\nRD\u00d7D is positive definite. \u2662\nRemark. The maximum likelihood solution in (9.12c) requires us to solve\na system of linear equations of the form A\u03b8 = b with A = (X\u22a4X) and\nb = X\u22a4y. \u2662\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1186,
    "chunk_id": "p301_c0",
    "page_number": 301,
    "text": "9.2 Parameter Estimation 295\nExample 9.2 (Fitting Lines)\nLet us have a look at Figure 9.2, where we aim to fit a straight linef(x) =\n\u03b8x, where \u03b8 is an unknown slope, to a dataset using maximum likelihood\nestimation. Examples of functions in this model class (straight lines) are\nshown in Figure 9.2(a). For the dataset shown in Figure 9.2(b), we find\nthe maximum likelihood estimate of the slope parameter \u03b8 using (9.12c)\nand obtain the maximum likelihood linear function in Figure 9.2(c).\nMaximum Likelihood Estimation with Features\nSo far, we considered the linear regression setting described in (9.4),\nwhich allowed us to fit straight lines to data using maximum likelihood\nestimation. However, straight lines are not sufficiently expressive when it Linear regression\nrefers to \u201clinear-in-\nthe-pa"
  },
  {
    "vector_id": 1187,
    "chunk_id": "p301_c1",
    "page_number": 301,
    "text": "9.4),\nwhich allowed us to fit straight lines to data using maximum likelihood\nestimation. However, straight lines are not sufficiently expressive when it Linear regression\nrefers to \u201clinear-in-\nthe-parameters\u201d\nregression models,\nbut the inputs can\nundergo any\nnonlinear\ntransformation.\ncomes to fitting more interesting data. Fortunately , linear regression offers\nus a way to fit nonlinear functions within the linear regression framework:\nSince \u201clinear regression\u201d only refers to \u201clinear in the parameters\u201d, we can\nperform an arbitrary nonlinear transformation \u03d5(x) of the inputs x and\nthen linearly combine the components of this transformation. The corre-\nsponding linear regression model is\np(y |x, \u03b8) = N\n\u0000\ny |\u03d5\u22a4(x)\u03b8, \u03c32\u0001\n\u21d0 \u21d2y = \u03d5\u22a4(x)\u03b8 + \u03f5 =\nK\u22121X\nk=0\n\u03b8k\u03d5k(x) + \u03f5 ,\n(9.13)\nwhere \u03d5 : RD \u2192 RK is a"
  },
  {
    "vector_id": 1188,
    "chunk_id": "p301_c2",
    "page_number": 301,
    "text": "y combine the components of this transformation. The corre-\nsponding linear regression model is\np(y |x, \u03b8) = N\n\u0000\ny |\u03d5\u22a4(x)\u03b8, \u03c32\u0001\n\u21d0 \u21d2y = \u03d5\u22a4(x)\u03b8 + \u03f5 =\nK\u22121X\nk=0\n\u03b8k\u03d5k(x) + \u03f5 ,\n(9.13)\nwhere \u03d5 : RD \u2192 RK is a (nonlinear) transformation of the inputs x and\n\u03d5k : RD \u2192 R is the kth component of the feature vector \u03d5. Note that the feature vector\nmodel parameters \u03b8 still appear only linearly .\nExample 9.3 (Polynomial Regression)\nWe are concerned with a regression problemy = \u03d5\u22a4(x)\u03b8+\u03f5, where x \u2208 R\nand \u03b8 \u2208 RK. A transformation that is often used in this context is\n\u03d5(x) =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n\u03d50(x)\n\u03d51(x)\n...\n\u03d5K\u22121(x)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\nx\nx2\nx3\n...\nxK\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u2208 RK . (9.14)\nThis means that we \u201clift\u201d the original one-dimensional input space into\na K-dimensional feature space consisting of all monomials xk for k =\n0,"
  },
  {
    "vector_id": 1189,
    "chunk_id": "p301_c3",
    "page_number": 301,
    "text": "\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\nx\nx2\nx3\n...\nxK\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u2208 RK . (9.14)\nThis means that we \u201clift\u201d the original one-dimensional input space into\na K-dimensional feature space consisting of all monomials xk for k =\n0, . . . , K\u2212 1. With these features, we can model polynomials of degree\n\u2a7d K\u22121 within the framework of linear regression: A polynomial of degree\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1190,
    "chunk_id": "p302_c0",
    "page_number": 302,
    "text": "296 Linear Regression\nK \u2212 1 is\nf(x) =\nK\u22121X\nk=0\n\u03b8kxk = \u03d5\u22a4(x)\u03b8 , (9.15)\nwhere \u03d5 is defined in (9.14) and \u03b8 = [\u03b80, . . . , \u03b8K\u22121]\u22a4 \u2208 RK contains the\n(linear) parameters \u03b8k.\nLet us now have a look at maximum likelihood estimation of the param-\neters \u03b8 in the linear regression model (9.13). We consider training inputs\nxn \u2208 RD and targets yn \u2208 R, n = 1, . . . , N, and define the feature matrixfeature matrix\n(design matrix) asdesign matrix\n\u03a6 :=\n\uf8ee\n\uf8ef\uf8f0\n\u03d5\u22a4(x1)\n...\n\u03d5\u22a4(xN )\n\uf8f9\n\uf8fa\uf8fb =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n\u03d50(x1) \u00b7 \u00b7\u00b7 \u03d5K\u22121(x1)\n\u03d50(x2) \u00b7 \u00b7\u00b7 \u03d5K\u22121(x2)\n... ...\n\u03d50(xN ) \u00b7 \u00b7\u00b7 \u03d5K\u22121(xN )\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb \u2208 RN\u00d7K , (9.16)\nwhere \u03a6ij = \u03d5j(xi) and \u03d5j : RD \u2192 R.\nExample 9.4 (Feature Matrix for Second-order Polynomials)\nFor a second-order polynomial and N training points xn \u2208 R, n =\n1, . . . , N, the feature matrix is\n\u03a6 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n1 x1 x2\n1\n1 x2 x2\n2"
  },
  {
    "vector_id": 1191,
    "chunk_id": "p302_c1",
    "page_number": 302,
    "text": ": RD \u2192 R.\nExample 9.4 (Feature Matrix for Second-order Polynomials)\nFor a second-order polynomial and N training points xn \u2208 R, n =\n1, . . . , N, the feature matrix is\n\u03a6 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n1 x1 x2\n1\n1 x2 x2\n2\n... ... ...\n1 xN x2\nN\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb . (9.17)\nWith the feature matrix\u03a6 defined in (9.16), the negative log-likelihood\nfor the linear regression model (9.13) can be written as\n\u2212log p(Y | X, \u03b8) = 1\n2\u03c32 (y \u2212 \u03a6\u03b8)\u22a4(y \u2212 \u03a6\u03b8) + const . (9.18)\nComparing (9.18) with the negative log-likelihood in (9.10b) for the \u201cfea-\nture-free\u201d model, we immediately see we just need to replace X with \u03a6.\nSince both X and \u03a6 are independent of the parameters \u03b8 that we wish to\noptimize, we arrive immediately at the maximum likelihood estimatemaximum likelihood\nestimate\n\u03b8ML = (\u03a6\u22a4\u03a6)\u22121\u03a6\u22a4y (9.19)\nfor the linear regression problem with"
  },
  {
    "vector_id": 1192,
    "chunk_id": "p302_c2",
    "page_number": 302,
    "text": "pendent of the parameters \u03b8 that we wish to\noptimize, we arrive immediately at the maximum likelihood estimatemaximum likelihood\nestimate\n\u03b8ML = (\u03a6\u22a4\u03a6)\u22121\u03a6\u22a4y (9.19)\nfor the linear regression problem with nonlinear features defined in (9.13).\nRemark. When we were working without features, we required X\u22a4X to\nbe invertible, which is the case when rk(X) = D, i.e., the columns of X\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1193,
    "chunk_id": "p303_c0",
    "page_number": 303,
    "text": "9.2 Parameter Estimation 297\nare linearly independent. In (9.19), we therefore require \u03a6\u22a4\u03a6 \u2208 RK\u00d7K\nto be invertible. This is the case if and only if rk(\u03a6) = K. \u2662\nExample 9.5 (Maximum Likelihood Polynomial Fit)\nFigure 9.4\nPolynomial\nregression:\n(a) dataset\nconsisting of\n(xn, yn) pairs,\nn = 1, . . . ,10;\n(b) maximum\nlikelihood\npolynomial of\ndegree 4.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\n(a) Regression dataset.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE (b) Polynomial of degree 4 determined by max-\nimum likelihood estimation.\nConsider the dataset in Figure 9.4(a). The dataset consists of N = 10\npairs (xn, yn), where xn \u223c U[\u22125, 5] and yn = \u2212sin(xn/5) + cos(xn) + \u03f5,\nwhere \u03f5 \u223c N\n\u0000\n0, 0.22\u0001\n.\nWe fit a polynomial of degree 4 using maximum likelihood estimation,\ni.e., parameters \u03b8ML are given in (9.19). T"
  },
  {
    "vector_id": 1194,
    "chunk_id": "p303_c1",
    "page_number": 303,
    "text": "n, yn), where xn \u223c U[\u22125, 5] and yn = \u2212sin(xn/5) + cos(xn) + \u03f5,\nwhere \u03f5 \u223c N\n\u0000\n0, 0.22\u0001\n.\nWe fit a polynomial of degree 4 using maximum likelihood estimation,\ni.e., parameters \u03b8ML are given in (9.19). The maximum likelihood estimate\nyields function values \u03d5\u22a4(x\u2217)\u03b8ML at any test location x\u2217. The result is\nshown in Figure 9.4(b).\nEstimating the Noise Variance\nThus far, we assumed that the noise variance \u03c32 is known. However, we\ncan also use the principle of maximum likelihood estimation to obtain the\nmaximum likelihood estimator \u03c32\nML for the noise variance. To do this, we\nfollow the standard procedure: We write down the log-likelihood, com-\npute its derivative with respect to \u03c32 > 0, set it to 0, and solve. The\nlog-likelihood is given by\nlog p(Y | X, \u03b8, \u03c32) =\nNX\nn=1\nlog N\n\u0000\nyn |\u03d5\u22a4(xn)\u03b8, \u03c32\u0001\n(9"
  },
  {
    "vector_id": 1195,
    "chunk_id": "p303_c2",
    "page_number": 303,
    "text": "dure: We write down the log-likelihood, com-\npute its derivative with respect to \u03c32 > 0, set it to 0, and solve. The\nlog-likelihood is given by\nlog p(Y | X, \u03b8, \u03c32) =\nNX\nn=1\nlog N\n\u0000\nyn |\u03d5\u22a4(xn)\u03b8, \u03c32\u0001\n(9.20a)\n=\nNX\nn=1\n\u0012\n\u22121\n2 log(2\u03c0) \u2212 1\n2 log \u03c32 \u2212 1\n2\u03c32 (yn \u2212 \u03d5\u22a4(xn)\u03b8)2\n\u0013\n(9.20b)\n= \u2212N\n2 log \u03c32 \u2212 1\n2\u03c32\nNX\nn=1\n(yn \u2212 \u03d5\u22a4(xn)\u03b8)2\n| {z }\n=:s\n+ const . (9.20c)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1196,
    "chunk_id": "p304_c0",
    "page_number": 304,
    "text": "298 Linear Regression\nThe partial derivative of the log-likelihood with respect to \u03c32 is then\n\u2202 log p(Y | X, \u03b8, \u03c32)\n\u2202\u03c32 = \u2212 N\n2\u03c32 + 1\n2\u03c34 s = 0 (9.21a)\n\u21d0 \u21d2N\n2\u03c32 = s\n2\u03c34 (9.21b)\nso that we identify\n\u03c32\nML = s\nN = 1\nN\nNX\nn=1\n(yn \u2212 \u03d5\u22a4(xn)\u03b8)2 . (9.22)\nTherefore, the maximum likelihood estimate of the noise variance is the\nempirical mean of the squared distances between the noise-free function\nvalues \u03d5\u22a4(xn)\u03b8 and the corresponding noisy observations yn at input lo-\ncations xn.\n9.2.2 Overfitting in Linear Regression\nWe just discussed how to use maximum likelihood estimation to fit lin-\near models (e.g., polynomials) to data. We can evaluate the quality of\nthe model by computing the error/loss incurred. One way of doing this\nis to compute the negative log-likelihood (9.10b), which we minimized\nto d"
  },
  {
    "vector_id": 1197,
    "chunk_id": "p304_c1",
    "page_number": 304,
    "text": "., polynomials) to data. We can evaluate the quality of\nthe model by computing the error/loss incurred. One way of doing this\nis to compute the negative log-likelihood (9.10b), which we minimized\nto determine the maximum likelihood estimator. Alternatively , given that\nthe noise parameter \u03c32 is not a free model parameter, we can ignore the\nscaling by 1/\u03c32, so that we end up with a squared-error-loss function\n\u2225y \u2212 \u03a6\u03b8\u2225\n2\n. Instead of using this squared loss, we often use theroot meanroot mean square\nerror square error (RMSE)\nRMSE\nr\n1\nN \u2225y \u2212 \u03a6\u03b8\u2225\n2\n=\nvuut 1\nN\nNX\nn=1\n(yn \u2212 \u03d5\u22a4(xn)\u03b8)2 , (9.23)\nwhich (a) allows us to compare errors of datasets with different sizes\nand (b) has the same scale and the same units as the observed func-The RMSE is\nnormalized. tion values yn. For example, if we fit a mod"
  },
  {
    "vector_id": 1198,
    "chunk_id": "p304_c2",
    "page_number": 304,
    "text": "(a) allows us to compare errors of datasets with different sizes\nand (b) has the same scale and the same units as the observed func-The RMSE is\nnormalized. tion values yn. For example, if we fit a model that maps post-codes ( x\nis given in latitude, longitude) to house prices ( y-values are EUR) then\nthe RMSE is also measured in EUR, whereas the squared error is given\nin EUR2. If we choose to include the factor \u03c32 from the original negativeThe negative\nlog-likelihood is\nunitless.\nlog-likelihood (9.10b), then we end up with a unitless objective, i.e., in\nthe preceding example, our objective would no longer be in EUR or EUR2.\nFor model selection (see Section 8.6), we can use the RMSE (or the\nnegative log-likelihood) to determine the best degree of the polynomial by\nfinding the polynomial deg"
  },
  {
    "vector_id": 1199,
    "chunk_id": "p304_c3",
    "page_number": 304,
    "text": "ld no longer be in EUR or EUR2.\nFor model selection (see Section 8.6), we can use the RMSE (or the\nnegative log-likelihood) to determine the best degree of the polynomial by\nfinding the polynomial degree M that minimizes the objective. Given that\nthe polynomial degree is a natural number, we can perform a brute-force\nsearch and enumerate all (reasonable) values of M. For a training set of\nsize N it is sufficient to test 0 \u2a7d M \u2a7d N \u2212 1. For M < N, the maximum\nlikelihood estimator is unique. For M \u2a7e N, we have more parameters\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1200,
    "chunk_id": "p304_c4",
    "page_number": 304,
    "text": "s://mml-book.com."
  },
  {
    "vector_id": 1201,
    "chunk_id": "p305_c0",
    "page_number": 305,
    "text": "9.2 Parameter Estimation 299\nFigure 9.5\nMaximum\nlikelihood fits for\ndifferent polynomial\ndegrees M.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE\n(a) M = 0\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE (b) M = 1\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE (c) M = 3\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE\n(d) M = 4\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE (e) M = 6\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE (f) M = 9\nthan data points, and would need to solve an underdetermined system of\nlinear equations ( \u03a6\u22a4\u03a6 in (9.19) would also no longer be invertible) so\nthat there are infinitely many possible maximum likelihood estimators.\nFigure 9.5 shows a number of polynomial fits determined by maximum\nlikelihood for the dataset from Figure 9.4(a) with N = 10 observations.\nWe notice t"
  },
  {
    "vector_id": 1202,
    "chunk_id": "p305_c1",
    "page_number": 305,
    "text": "tely many possible maximum likelihood estimators.\nFigure 9.5 shows a number of polynomial fits determined by maximum\nlikelihood for the dataset from Figure 9.4(a) with N = 10 observations.\nWe notice that polynomials of low degree (e.g., constants ( M = 0 ) or\nlinear (M = 1)) fit the data poorly and, hence, are poor representations\nof the true underlying function. For degrees M = 3, . . . ,6, the fits look\nplausible and smoothly interpolate the data. When we go to higher-degree The case of\nM = N \u2212 1 is\nextreme in the sense\nthat otherwise the\nnull space of the\ncorresponding\nsystem of linear\nequations would be\nnon-trivial, and we\nwould have\ninfinitely many\noptimal solutions to\nthe linear regression\nproblem.\npolynomials, we notice that they fit the data better and better. In the ex-\ntreme case"
  },
  {
    "vector_id": 1203,
    "chunk_id": "p305_c2",
    "page_number": 305,
    "text": "tions would be\nnon-trivial, and we\nwould have\ninfinitely many\noptimal solutions to\nthe linear regression\nproblem.\npolynomials, we notice that they fit the data better and better. In the ex-\ntreme case of M = N \u22121 = 9, the function will pass through every single\ndata point. However, these high-degree polynomials oscillate wildly and\nare a poor representation of the underlying function that generated the\ndata, such that we suffer from overfitting.\noverfitting\nNote that the noise\nvariance \u03c32 > 0.\nRemember that the goal is to achieve good generalization by making\naccurate predictions for new (unseen) data. We obtain some quantita-\ntive insight into the dependence of the generalization performance on the\npolynomial of degree M by considering a separate test set comprising200\ndata points generat"
  },
  {
    "vector_id": 1204,
    "chunk_id": "p305_c3",
    "page_number": 305,
    "text": "en) data. We obtain some quantita-\ntive insight into the dependence of the generalization performance on the\npolynomial of degree M by considering a separate test set comprising200\ndata points generated using exactly the same procedure used to generate\nthe training set. As test inputs, we chose a linear grid of 200 points in the\ninterval of [\u22125, 5]. For each choice ofM, we evaluate the RMSE (9.23) for\nboth the training data and the test data.\nLooking now at the test error, which is a qualitive measure of the gen-\neralization properties of the corresponding polynomial, we notice that ini-\ntially the test error decreases; see Figure 9.6 (orange). For fourth-order\npolynomials, the test error is relatively low and stays relatively constant up\nto degree 5. However, from degree6 onward the test"
  },
  {
    "vector_id": 1205,
    "chunk_id": "p305_c4",
    "page_number": 305,
    "text": "y the test error decreases; see Figure 9.6 (orange). For fourth-order\npolynomials, the test error is relatively low and stays relatively constant up\nto degree 5. However, from degree6 onward the test error increases signif-\nicantly , and high-order polynomials have very bad generalization proper-\nties. In this particular example, this also is evident from the corresponding\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1206,
    "chunk_id": "p306_c0",
    "page_number": 306,
    "text": "300 Linear Regression\nFigure 9.6 Training\nand test error.\n0 2 4 6 8 10\nDegree of polynomial\n0\n2\n4\n6\n8\n10RMSE\nTraining error\nTest error\nmaximum likelihood fits in Figure 9.5. Note that the training error (bluetraining error\ncurve in Figure 9.6) never increases when the degree of the polynomial in-\ncreases. In our example, the best generalization (the point of the smallest\ntest error) is obtained for a polynomial of degree M = 4.test error\n9.2.3 Maximum A Posteriori Estimation\nWe just saw that maximum likelihood estimation is prone to overfitting.\nWe often observe that the magnitude of the parameter values becomes\nrelatively large if we run into overfitting (Bishop, 2006).\nTo mitigate the effect of huge parameter values, we can place a prior\ndistribution p(\u03b8) on the parameters. The prior dis"
  },
  {
    "vector_id": 1207,
    "chunk_id": "p306_c1",
    "page_number": 306,
    "text": "eter values becomes\nrelatively large if we run into overfitting (Bishop, 2006).\nTo mitigate the effect of huge parameter values, we can place a prior\ndistribution p(\u03b8) on the parameters. The prior distribution explicitly en-\ncodes what parameter values are plausible (before having seen any data).\nFor example, a Gaussian prior p(\u03b8) = N\n\u0000\n0, 1\n\u0001\non a single parameter\n\u03b8 encodes that parameter values are expected lie in the interval [\u22122, 2]\n(two standard deviations around the mean value). Once a dataset X, Y\nis available, instead of maximizing the likelihood we seek parameters that\nmaximize the posterior distribution p(\u03b8 | X, Y). This procedure is called\nmaximum a posteriori (MAP) estimation.maximum a\nposteriori\nMAP\nThe posterior over the parameters \u03b8, given the training data X, Y, is\nobtained"
  },
  {
    "vector_id": 1208,
    "chunk_id": "p306_c2",
    "page_number": 306,
    "text": "sterior distribution p(\u03b8 | X, Y). This procedure is called\nmaximum a posteriori (MAP) estimation.maximum a\nposteriori\nMAP\nThe posterior over the parameters \u03b8, given the training data X, Y, is\nobtained by applying Bayes\u2019 theorem (Section 6.3) as\np(\u03b8 | X, Y) = p(Y | X, \u03b8)p(\u03b8)\np(Y | X) . (9.24)\nSince the posterior explicitly depends on the parameter prior p(\u03b8), the\nprior will have an effect on the parameter vector we find as the maximizer\nof the posterior. We will see this more explicitly in the following. The\nparameter vector \u03b8MAP that maximizes the posterior (9.24) is the MAP\nestimate.\nTo find the MAP estimate, we follow steps that are similar in flavor\nto maximum likelihood estimation. We start with the log-transform and\ncompute the log-posterior as\nlog p(\u03b8 | X, Y) = log p(Y | X, \u03b8) + logp"
  },
  {
    "vector_id": 1209,
    "chunk_id": "p306_c3",
    "page_number": 306,
    "text": "the MAP estimate, we follow steps that are similar in flavor\nto maximum likelihood estimation. We start with the log-transform and\ncompute the log-posterior as\nlog p(\u03b8 | X, Y) = log p(Y | X, \u03b8) + logp(\u03b8) + const , (9.25)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1210,
    "chunk_id": "p307_c0",
    "page_number": 307,
    "text": "9.2 Parameter Estimation 301\nwhere the constant comprises the terms that are independent of\u03b8. We see\nthat the log-posterior in (9.25) is the sum of the log-likelihoodp(Y | X, \u03b8)\nand the log-priorlog p(\u03b8) so that the MAP estimate will be a \u201ccompromise\u201d\nbetween the prior (our suggestion for plausible parameter values before\nobserving data) and the data-dependent likelihood.\nTo find the MAP estimate \u03b8MAP, we minimize the negative log-posterior\ndistribution with respect to \u03b8, i.e., we solve\n\u03b8MAP \u2208 arg min\n\u03b8\n{\u2212log p(Y | X, \u03b8) \u2212 log p(\u03b8)}. (9.26)\nThe gradient of the negative log-posterior with respect to \u03b8 is\n\u2212d logp(\u03b8 | X, Y)\nd\u03b8 = \u2212d logp(Y | X, \u03b8)\nd\u03b8 \u2212 d logp(\u03b8)\nd\u03b8 , (9.27)\nwhere we identify the first term on the right-hand side as the gradient of\nthe negative log-likelihood from (9.11c).\nWith"
  },
  {
    "vector_id": 1211,
    "chunk_id": "p307_c1",
    "page_number": 307,
    "text": "ct to \u03b8 is\n\u2212d logp(\u03b8 | X, Y)\nd\u03b8 = \u2212d logp(Y | X, \u03b8)\nd\u03b8 \u2212 d logp(\u03b8)\nd\u03b8 , (9.27)\nwhere we identify the first term on the right-hand side as the gradient of\nthe negative log-likelihood from (9.11c).\nWith a (conjugate) Gaussian priorp(\u03b8) = N\n\u0000\n0, b2I\n\u0001\non the parameters\n\u03b8, the negative log-posterior for the linear regression setting (9.13), we\nobtain the negative log posterior\n\u2212log p(\u03b8 | X, Y) = 1\n2\u03c32 (y \u2212 \u03a6\u03b8)\u22a4(y \u2212 \u03a6\u03b8) + 1\n2b2 \u03b8\u22a4\u03b8 + const . (9.28)\nHere, the first term corresponds to the contribution from the log-likelihood,\nand the second term originates from the log-prior. The gradient of the log-\nposterior with respect to the parameters \u03b8 is then\n\u2212d logp(\u03b8 | X, Y)\nd\u03b8 = 1\n\u03c32 (\u03b8\u22a4\u03a6\u22a4\u03a6 \u2212 y\u22a4\u03a6) + 1\nb2 \u03b8\u22a4 . (9.29)\nWe will find the MAP estimate \u03b8MAP by setting this gradient to 0\u22a4 and\nsolving for \u03b8MAP"
  },
  {
    "vector_id": 1212,
    "chunk_id": "p307_c2",
    "page_number": 307,
    "text": "-\nposterior with respect to the parameters \u03b8 is then\n\u2212d logp(\u03b8 | X, Y)\nd\u03b8 = 1\n\u03c32 (\u03b8\u22a4\u03a6\u22a4\u03a6 \u2212 y\u22a4\u03a6) + 1\nb2 \u03b8\u22a4 . (9.29)\nWe will find the MAP estimate \u03b8MAP by setting this gradient to 0\u22a4 and\nsolving for \u03b8MAP. We obtain\n1\n\u03c32 (\u03b8\u22a4\u03a6\u22a4\u03a6 \u2212 y\u22a4\u03a6) + 1\nb2 \u03b8\u22a4 = 0\u22a4 (9.30a)\n\u21d0 \u21d2\u03b8\u22a4\n\u0012 1\n\u03c32 \u03a6\u22a4\u03a6 + 1\nb2 I\n\u0013\n\u2212 1\n\u03c32 y\u22a4\u03a6 = 0\u22a4 (9.30b)\n\u21d0 \u21d2\u03b8\u22a4\n\u0012\n\u03a6\u22a4\u03a6 + \u03c32\nb2 I\n\u0013\n= y\u22a4\u03a6 (9.30c)\n\u21d0 \u21d2\u03b8\u22a4 = y\u22a4\u03a6\n\u0012\n\u03a6\u22a4\u03a6 + \u03c32\nb2 I\n\u0013\u22121\n(9.30d)\nso that the MAP estimate is (by transposing both sides of the last equality) \u03a6\u22a4\u03a6 is symmetric,\npositive semi\ndefinite. The\nadditional term\nin (9.31) is strictly\npositive definite so\nthat the inverse\nexists.\n\u03b8MAP =\n\u0012\n\u03a6\u22a4\u03a6 + \u03c32\nb2 I\n\u0013\u22121\n\u03a6\u22a4y . (9.31)\nComparing the MAP estimate in (9.31) with the maximum likelihood es-\ntimate in (9.19), we see that the only difference between both solutions\nis the additional term \u03c32\nb"
  },
  {
    "vector_id": 1213,
    "chunk_id": "p307_c3",
    "page_number": 307,
    "text": "\u03a6\u22a4\u03a6 + \u03c32\nb2 I\n\u0013\u22121\n\u03a6\u22a4y . (9.31)\nComparing the MAP estimate in (9.31) with the maximum likelihood es-\ntimate in (9.19), we see that the only difference between both solutions\nis the additional term \u03c32\nb2 I in the inverse matrix. This term ensures that\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1214,
    "chunk_id": "p308_c0",
    "page_number": 308,
    "text": "302 Linear Regression\n\u03a6\u22a4\u03a6 + \u03c32\nb2 I is symmetric and strictly positive definite (i.e., its inverse\nexists and the MAP estimate is the unique solution of a system of linear\nequations). Moreover, it reflects the impact of the regularizer.\nExample 9.6 (MAP Estimation for Polynomial Regression)\nIn the polynomial regression example from Section 9.2.1, we place a Gaus-\nsian prior p(\u03b8) = N\n\u0000\n0, I\n\u0001\non the parameters \u03b8 and determine the MAP\nestimates according to (9.31). In Figure 9.7, we show both the maximum\nlikelihood and the MAP estimates for polynomials of degree 6 (left) and\ndegree 8 (right). The prior (regularizer) does not play a significant role\nfor the low-degree polynomial, but keeps the function relatively smooth\nfor higher-degree polynomials. Although the MAP estimate can push the\nbou"
  },
  {
    "vector_id": 1215,
    "chunk_id": "p308_c1",
    "page_number": 308,
    "text": "e prior (regularizer) does not play a significant role\nfor the low-degree polynomial, but keeps the function relatively smooth\nfor higher-degree polynomials. Although the MAP estimate can push the\nboundaries of overfitting, it is not a general solution to this problem, so\nwe need a more principled approach to tackle overfitting.\nFigure 9.7\nPolynomial\nregression:\nmaximum likelihood\nand MAP estimates.\n(a) Polynomials of\ndegree 6;\n(b) polynomials of\ndegree 8.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE\nMAP\n(a) Polynomials of degree 6.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE\nMAP (b) Polynomials of degree 8.\n9.2.4 MAP Estimation as Regularization\nInstead of placing a prior distribution on the parameters \u03b8, it is also pos-\nsible to mitigate the effect of overfitting by penalizing the amp"
  },
  {
    "vector_id": 1216,
    "chunk_id": "p308_c2",
    "page_number": 308,
    "text": "mials of degree 8.\n9.2.4 MAP Estimation as Regularization\nInstead of placing a prior distribution on the parameters \u03b8, it is also pos-\nsible to mitigate the effect of overfitting by penalizing the amplitude of\nthe parameter by means of regularization. In regularized least squares, weregularization\nregularized least\nsquares\nconsider the loss function\n\u2225y \u2212 \u03a6\u03b8\u2225\n2\n+ \u03bb \u2225\u03b8\u2225\n2\n2 , (9.32)\nwhich we minimize with respect to \u03b8 (see Section 8.2.3). Here, the first\nterm is a data-fit term (also called misfit term), which is proportional todata-fit term\nmisfit term the negative log-likelihood; see (9.10b). The second term is called the\nregularizer, and the regularization parameter \u03bb \u2a7e 0 controls the \u201cstrict-regularizer\nregularization\nparameter\nness\u201d of the regularization.\nRemark. Instead of the Euclidea"
  },
  {
    "vector_id": 1217,
    "chunk_id": "p308_c3",
    "page_number": 308,
    "text": "The second term is called the\nregularizer, and the regularization parameter \u03bb \u2a7e 0 controls the \u201cstrict-regularizer\nregularization\nparameter\nness\u201d of the regularization.\nRemark. Instead of the Euclidean norm \u2225\u00b7\u22252, we can choose any p-norm\n\u2225\u00b7\u2225p in (9.32). In practice, smaller values for p lead to sparser solutions.\nHere, \u201csparse\u201d means that many parameter values \u03b8d = 0, which is also\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1218,
    "chunk_id": "p309_c0",
    "page_number": 309,
    "text": "9.3 Bayesian Linear Regression 303\nuseful for variable selection. For p = 1 , the regularizer is called LASSO LASSO\n(least absolute shrinkage and selection operator) and was proposed by Tib-\nshirani (1996). \u2662\nThe regularizer \u03bb \u2225\u03b8\u2225\n2\n2 in (9.32) can be interpreted as a negative log-\nGaussian prior, which we use in MAP estimation; see (9.26). More specif-\nically , with a Gaussian prior p(\u03b8) = N\n\u0000\n0, b2I\n\u0001\n, we obtain the negative\nlog-Gaussian prior\n\u2212log p(\u03b8) = 1\n2b2 \u2225\u03b8\u2225\n2\n2 + const (9.33)\nso that for \u03bb = 1\n2b2 the regularization term and the negative log-Gaussian\nprior are identical.\nGiven that the regularized least-squares loss function in (9.32) consists\nof terms that are closely related to the negative log-likelihood plus a neg-\native log-prior, it is not surprising that, when we minimize"
  },
  {
    "vector_id": 1219,
    "chunk_id": "p309_c1",
    "page_number": 309,
    "text": "the regularized least-squares loss function in (9.32) consists\nof terms that are closely related to the negative log-likelihood plus a neg-\native log-prior, it is not surprising that, when we minimize this loss, we\nobtain a solution that closely resembles the MAP estimate in (9.31). More\nspecifically , minimizing the regularized least-squares loss function yields\n\u03b8RLS = (\u03a6\u22a4\u03a6 + \u03bbI)\u22121\u03a6\u22a4y , (9.34)\nwhich is identical to the MAP estimate in (9.31) for \u03bb = \u03c32\nb2 , where \u03c32 is\nthe noise variance and b2 the variance of the (isotropic) Gaussian prior\np(\u03b8) = N\n\u0000\n0, b2I\n\u0001\n. A point estimate is a\nsingle specific\nparameter value,\nunlike a distribution\nover plausible\nparameter settings.\nSo far, we have covered parameter estimation using maximum likeli-\nhood and MAP estimation where we found point estima"
  },
  {
    "vector_id": 1220,
    "chunk_id": "p309_c2",
    "page_number": 309,
    "text": "pecific\nparameter value,\nunlike a distribution\nover plausible\nparameter settings.\nSo far, we have covered parameter estimation using maximum likeli-\nhood and MAP estimation where we found point estimates \u03b8\u2217 that op-\ntimize an objective function (likelihood or posterior). We saw that both\nmaximum likelihood and MAP estimation can lead to overfitting. In the\nnext section, we will discuss Bayesian linear regression, where we use\nBayesian inference (Section 8.4) to find a posterior distribution over the\nunknown parameters, which we subsequently use to make predictions.\nMore specifically , for predictions we will average over all plausible sets of\nparameters instead of focusing on a point estimate.\n9.3 Bayesian Linear Regression\nPreviously , we looked at linear regression models where we estima"
  },
  {
    "vector_id": 1221,
    "chunk_id": "p309_c3",
    "page_number": 309,
    "text": "ictions we will average over all plausible sets of\nparameters instead of focusing on a point estimate.\n9.3 Bayesian Linear Regression\nPreviously , we looked at linear regression models where we estimated the\nmodel parameters \u03b8, e.g., by means of maximum likelihood or MAP esti-\nmation. We discovered that MLE can lead to severe overfitting, in particu-\nlar, in the small-data regime. MAP addresses this issue by placing a prior\non the parameters that plays the role of a regularizer. Bayesian linear\nregressionBayesian linear regression pushes the idea of the parameter prior a step\nfurther and does not even attempt to compute a point estimate of the\nparameters, but instead the full posterior distribution over the parameters\nis taken into account when making predictions. This means we do not fit"
  },
  {
    "vector_id": 1222,
    "chunk_id": "p309_c4",
    "page_number": 309,
    "text": "not even attempt to compute a point estimate of the\nparameters, but instead the full posterior distribution over the parameters\nis taken into account when making predictions. This means we do not fit\nany parameters, but we compute a mean over all plausible parameters\nsettings (according to the posterior).\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1223,
    "chunk_id": "p310_c0",
    "page_number": 310,
    "text": "304 Linear Regression\n9.3.1 Model\nIn Bayesian linear regression, we consider the model\nprior p(\u03b8) = N\n\u0000\nm0, S0\n\u0001\n,\nlikelihood p(y |x, \u03b8) = N\n\u0000\ny |\u03d5\u22a4(x)\u03b8, \u03c32\u0001\n,\n(9.35)\nwhere we now explicitly place a Gaussian prior p(\u03b8) = N\n\u0000\nm0, S0\n\u0001\non \u03b8,Figure 9.8\nGraphical model for\nBayesian linear\nregression.\n\u03b8\ny\n\u03c3\nx\nm0 S0\nwhich turns the parameter vector into a random variable. This allows us\nto write down the corresponding graphical model in Figure 9.8, where we\nmade the parameters of the Gaussian prior on \u03b8 explicit. The full proba-\nbilistic model, i.e., the joint distribution of observed and unobserved ran-\ndom variables, y and \u03b8, respectively , is\np(y, \u03b8 |x) = p(y |x, \u03b8)p(\u03b8) . (9.36)\n9.3.2 Prior Predictions\nIn practice, we are usually not so much interested in the parameter values\n\u03b8 themselves. In"
  },
  {
    "vector_id": 1224,
    "chunk_id": "p310_c1",
    "page_number": 310,
    "text": "ran-\ndom variables, y and \u03b8, respectively , is\np(y, \u03b8 |x) = p(y |x, \u03b8)p(\u03b8) . (9.36)\n9.3.2 Prior Predictions\nIn practice, we are usually not so much interested in the parameter values\n\u03b8 themselves. Instead, our focus often lies in the predictions we make\nwith those parameter values. In a Bayesian setting, we take the parameter\ndistribution and average over all plausible parameter settings when we\nmake predictions. More specifically , to make predictions at an input x\u2217,\nwe integrate out \u03b8 and obtain\np(y\u2217 |x\u2217) =\nZ\np(y\u2217 |x\u2217, \u03b8)p(\u03b8)d\u03b8 = E\u03b8[p(y\u2217 |x\u2217, \u03b8)] , (9.37)\nwhich we can interpret as the average prediction of y\u2217 |x\u2217, \u03b8 for all plau-\nsible parameters \u03b8 according to the prior distribution p(\u03b8). Note that pre-\ndictions using the prior distribution only require us to specify the input\nx\u2217, but"
  },
  {
    "vector_id": 1225,
    "chunk_id": "p310_c2",
    "page_number": 310,
    "text": "ge prediction of y\u2217 |x\u2217, \u03b8 for all plau-\nsible parameters \u03b8 according to the prior distribution p(\u03b8). Note that pre-\ndictions using the prior distribution only require us to specify the input\nx\u2217, but no training data.\nIn our model (9.35), we chose a conjugate (Gaussian) prior on \u03b8 so\nthat the predictive distribution is Gaussian as well (and can be computed\nin closed form): With the prior distributionp(\u03b8) = N\n\u0000\nm0, S0\n\u0001\n, we obtain\nthe predictive distribution as\np(y\u2217 |x\u2217) = N\n\u0000\n\u03d5\u22a4(x\u2217)m0, \u03d5\u22a4(x\u2217)S0\u03d5(x\u2217) + \u03c32\u0001\n, (9.38)\nwhere we exploited that (i) the prediction is Gaussian due to conjugacy\n(see Section 6.6) and the marginalization property of Gaussians (see Sec-\ntion 6.5), (ii) the Gaussian noise is independent so that\nV[y\u2217] = V\u03b8[\u03d5\u22a4(x\u2217)\u03b8] + V\u03f5[\u03f5] , (9.39)\nand (iii) y\u2217 is a linear transformatio"
  },
  {
    "vector_id": 1226,
    "chunk_id": "p310_c3",
    "page_number": 310,
    "text": "ction 6.6) and the marginalization property of Gaussians (see Sec-\ntion 6.5), (ii) the Gaussian noise is independent so that\nV[y\u2217] = V\u03b8[\u03d5\u22a4(x\u2217)\u03b8] + V\u03f5[\u03f5] , (9.39)\nand (iii) y\u2217 is a linear transformation of \u03b8 so that we can apply the rules\nfor computing the mean and covariance of the prediction analytically by\nusing (6.50) and (6.51), respectively . In (9.38), the term\u03d5\u22a4(x\u2217)S0\u03d5(x\u2217)\nin the predictive variance explicitly accounts for the uncertainty associated\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1227,
    "chunk_id": "p311_c0",
    "page_number": 311,
    "text": "9.3 Bayesian Linear Regression 305\nwith the parameters \u03b8, whereas \u03c32 is the uncertainty contribution due to\nthe measurement noise.\nIf we are interested in predicting noise-free function values f(x\u2217) =\n\u03d5\u22a4(x\u2217)\u03b8 instead of the noise-corrupted targets y\u2217 we obtain\np(f(x\u2217)) = N\n\u0000\n\u03d5\u22a4(x\u2217)m0, \u03d5\u22a4(x\u2217)S0\u03d5(x\u2217)\n\u0001\n, (9.40)\nwhich only differs from (9.38) in the omission of the noise variance \u03c32 in\nthe predictive variance.\nRemark (Distribution over Functions). Since we can represent the distri- The parameter\ndistribution p(\u03b8)\ninduces a\ndistribution over\nfunctions.\nbution p(\u03b8) using a set of samples \u03b8i and every sample \u03b8i gives rise to a\nfunction fi(\u00b7) = \u03b8\u22a4\ni \u03d5(\u00b7), it follows that the parameter distribution p(\u03b8)\ninduces a distribution p(f(\u00b7)) over functions. Here we use the notation (\u00b7)\nto explicitly denot"
  },
  {
    "vector_id": 1228,
    "chunk_id": "p311_c1",
    "page_number": 311,
    "text": "ry sample \u03b8i gives rise to a\nfunction fi(\u00b7) = \u03b8\u22a4\ni \u03d5(\u00b7), it follows that the parameter distribution p(\u03b8)\ninduces a distribution p(f(\u00b7)) over functions. Here we use the notation (\u00b7)\nto explicitly denote a functional relationship. \u2662\nExample 9.7 (Prior over Functions)\nFigure 9.9 Prior\nover functions.\n(a) Distribution over\nfunctions\nrepresented by the\nmean function\n(black line) and the\nmarginal\nuncertainties\n(shaded),\nrepresenting the\n67% and 95%\nconfidence bounds,\nrespectively;\n(b) samples from\nthe prior over\nfunctions, which are\ninduced by the\nsamples from the\nparameter prior.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\n(a) Prior distribution over functions.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny (b) Samples from the prior distribution over\nfunctions.\nLet us consider a Bayesian linear regression problem with polynomia"
  },
  {
    "vector_id": 1229,
    "chunk_id": "p311_c2",
    "page_number": 311,
    "text": "2\n0\n2\n4\ny\n(a) Prior distribution over functions.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny (b) Samples from the prior distribution over\nfunctions.\nLet us consider a Bayesian linear regression problem with polynomials\nof degree 5. We choose a parameter prior p(\u03b8) = N\n\u0000\n0, 1\n4 I\n\u0001\n. Figure 9.9\nvisualizes the induced prior distribution over functions (shaded area: dark\ngray: 67% confidence bound; light gray: 95% confidence bound) induced\nby this parameter prior, including some function samples from this prior.\nA function sample is obtained by first sampling a parameter vector\n\u03b8i \u223c p(\u03b8) and then computing fi(\u00b7) = \u03b8\u22a4\ni \u03d5(\u00b7). We used 200 input lo-\ncations x\u2217 \u2208 [\u22125, 5] to which we apply the feature function \u03d5(\u00b7). The\nuncertainty (represented by the shaded area) in Figure 9.9 is solely due to\nthe parameter unce"
  },
  {
    "vector_id": 1230,
    "chunk_id": "p311_c3",
    "page_number": 311,
    "text": "= \u03b8\u22a4\ni \u03d5(\u00b7). We used 200 input lo-\ncations x\u2217 \u2208 [\u22125, 5] to which we apply the feature function \u03d5(\u00b7). The\nuncertainty (represented by the shaded area) in Figure 9.9 is solely due to\nthe parameter uncertainty because we considered the noise-free predictive\ndistribution (9.40).\nSo far, we looked at computing predictions using the parameter prior\np(\u03b8). However, when we have a parameter posterior (given some train-\ning data X, Y), the same principles for prediction and inference hold\nas in (9.37) \u2013 we just need to replace the prior p(\u03b8) with the posterior\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1231,
    "chunk_id": "p311_c4",
    "page_number": 311,
    "text": ". Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1232,
    "chunk_id": "p312_c0",
    "page_number": 312,
    "text": "306 Linear Regression\np(\u03b8 | X, Y). In the following, we will derive the posterior distribution in\ndetail before using it to make predictions.\n9.3.3 Posterior Distribution\nGiven a training set of inputs xn \u2208 RD and corresponding observations\nyn \u2208 R, n = 1 , . . . , N, we compute the posterior over the parameters\nusing Bayes\u2019 theorem as\np(\u03b8 | X, Y) = p(Y | X, \u03b8)p(\u03b8)\np(Y | X) , (9.41)\nwhere X is the set of training inputs and Y the collection of correspond-\ning training targets. Furthermore, p(Y | X, \u03b8) is the likelihood, p(\u03b8) the\nparameter prior, and\np(Y | X) =\nZ\np(Y | X, \u03b8)p(\u03b8)d\u03b8 = E\u03b8[p(Y | X, \u03b8)] (9.42)\nthe marginal likelihood/evidence, which is independent of the parametersmarginal likelihood\nevidence \u03b8 and ensures that the posterior is normalized, i.e., it integrates to 1. We\nThe margina"
  },
  {
    "vector_id": 1233,
    "chunk_id": "p312_c1",
    "page_number": 312,
    "text": "\u03b8)] (9.42)\nthe marginal likelihood/evidence, which is independent of the parametersmarginal likelihood\nevidence \u03b8 and ensures that the posterior is normalized, i.e., it integrates to 1. We\nThe marginal\nlikelihood is the\nexpected likelihood\nunder the parameter\nprior.\ncan think of the marginal likelihood as the likelihood averaged over all\npossible parameter settings (with respect to the prior distribution p(\u03b8)).\nTheorem 9.1 (Parameter Posterior). In our model (9.35), the parameter\nposterior (9.41) can be computed in closed form as\np(\u03b8 | X, Y) = N\n\u0000\n\u03b8 |mN , SN\n\u0001\n, (9.43a)\nSN = (S\u22121\n0 + \u03c3\u22122\u03a6\u22a4\u03a6)\u22121 , (9.43b)\nmN = SN (S\u22121\n0 m0 + \u03c3\u22122\u03a6\u22a4y) , (9.43c)\nwhere the subscript N indicates the size of the training set.\nProof Bayes\u2019 theorem tells us that the posterior p(\u03b8 | X, Y) is propor-\ntional to the pro"
  },
  {
    "vector_id": 1234,
    "chunk_id": "p312_c2",
    "page_number": 312,
    "text": ", (9.43b)\nmN = SN (S\u22121\n0 m0 + \u03c3\u22122\u03a6\u22a4y) , (9.43c)\nwhere the subscript N indicates the size of the training set.\nProof Bayes\u2019 theorem tells us that the posterior p(\u03b8 | X, Y) is propor-\ntional to the product of the likelihood p(Y | X, \u03b8) and the prior p(\u03b8):\nPosterior p(\u03b8 | X, Y) = p(Y | X, \u03b8)p(\u03b8)\np(Y | X) (9.44a)\nLikelihood p(Y | X, \u03b8) = N\n\u0000\ny |\u03a6\u03b8, \u03c32I\n\u0001\n(9.44b)\nPrior p(\u03b8) = N\n\u0000\n\u03b8 |m0, S0\n\u0001\n. (9.44c)\nInstead of looking at the product of the prior and the likelihood, we\ncan transform the problem into log-space and solve for the mean and\ncovariance of the posterior by completing the squares.\nThe sum of the log-prior and the log-likelihood is\nlog N\n\u0000\ny |\u03a6\u03b8, \u03c32I\n\u0001\n+ logN\n\u0000\n\u03b8 |m0, S0\n\u0001\n(9.45a)\n= \u22121\n2\n\u0000\n\u03c3\u22122(y \u2212 \u03a6\u03b8)\u22a4(y \u2212 \u03a6\u03b8) + (\u03b8 \u2212 m0)\u22a4S\u22121\n0 (\u03b8 \u2212 m0)\n\u0001\n+ const\n(9.45b)\nDraft (2024-01-15) of \u201cMathemat"
  },
  {
    "vector_id": 1235,
    "chunk_id": "p312_c3",
    "page_number": 312,
    "text": "m of the log-prior and the log-likelihood is\nlog N\n\u0000\ny |\u03a6\u03b8, \u03c32I\n\u0001\n+ logN\n\u0000\n\u03b8 |m0, S0\n\u0001\n(9.45a)\n= \u22121\n2\n\u0000\n\u03c3\u22122(y \u2212 \u03a6\u03b8)\u22a4(y \u2212 \u03a6\u03b8) + (\u03b8 \u2212 m0)\u22a4S\u22121\n0 (\u03b8 \u2212 m0)\n\u0001\n+ const\n(9.45b)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1236,
    "chunk_id": "p313_c0",
    "page_number": 313,
    "text": "9.3 Bayesian Linear Regression 307\nwhere the constant contains terms independent of \u03b8. We will ignore the\nconstant in the following. We now factorize (9.45b), which yields\n\u2212 1\n2\n\u0000\n\u03c3\u22122y\u22a4y \u2212 2\u03c3\u22122y\u22a4\u03a6\u03b8 + \u03b8\u22a4\u03c3\u22122\u03a6\u22a4\u03a6\u03b8 + \u03b8\u22a4S\u22121\n0 \u03b8\n\u2212 2m\u22a4\n0 S\u22121\n0 \u03b8 + m\u22a4\n0 S\u22121\n0 m0\n\u0001 (9.46a)\n= \u2212 1\n2\n\u0000\n\u03b8\u22a4(\u03c3\u22122\u03a6\u22a4\u03a6 + S\u22121\n0 )\u03b8 \u2212 2(\u03c3\u22122\u03a6\u22a4y + S\u22121\n0 m0)\u22a4\u03b8\n\u0001\n+ const ,\n(9.46b)\nwhere the constant contains the black terms in (9.46a), which are inde-\npendent of \u03b8. The orange terms are terms that are linear in \u03b8, and the\nblue terms are the ones that are quadratic in \u03b8. Inspecting (9.46b), we\nfind that this equation is quadratic in \u03b8. The fact that the unnormalized\nlog-posterior distribution is a (negative) quadratic form implies that the\nposterior is Gaussian, i.e.,\np(\u03b8 | X, Y) = exp(log p(\u03b8 | X, Y)) \u221d exp(log p(Y | X, \u03b8) + logp(\u03b8))"
  },
  {
    "vector_id": 1237,
    "chunk_id": "p313_c1",
    "page_number": 313,
    "text": "e fact that the unnormalized\nlog-posterior distribution is a (negative) quadratic form implies that the\nposterior is Gaussian, i.e.,\np(\u03b8 | X, Y) = exp(log p(\u03b8 | X, Y)) \u221d exp(log p(Y | X, \u03b8) + logp(\u03b8))\n(9.47a)\n\u221d exp\n\u0010\n\u2212 1\n2\n\u0000\n\u03b8\u22a4(\u03c3\u22122\u03a6\u22a4\u03a6 + S\u22121\n0 )\u03b8 \u2212 2(\u03c3\u22122\u03a6\u22a4y + S\u22121\n0 m0)\u22a4\u03b8\n\u0001\u0011\n,\n(9.47b)\nwhere we used (9.46b) in the last expression.\nThe remaining task is it to bring this (unnormalized) Gaussian into the\nform that is proportional to N\n\u0000\n\u03b8 |mN , SN\n\u0001\n, i.e., we need to identify the\nmean mN and the covariance matrix SN . To do this, we use the concept\nof completing the squares. The desired log-posterior is completing the\nsquares\nlog N\n\u0000\n\u03b8 |mN , SN\n\u0001\n= \u22121\n2(\u03b8 \u2212 mN )\u22a4S\u22121\nN (\u03b8 \u2212 mN ) + const (9.48a)\n= \u22121\n2\n\u0000\n\u03b8\u22a4S\u22121\nN \u03b8 \u2212 2m\u22a4\nN S\u22121\nN \u03b8 + m\u22a4\nN S\u22121\nN mN\n\u0001\n. (9.48b)\nHere, we factorized the quadratic form"
  },
  {
    "vector_id": 1238,
    "chunk_id": "p313_c2",
    "page_number": 313,
    "text": "or is completing the\nsquares\nlog N\n\u0000\n\u03b8 |mN , SN\n\u0001\n= \u22121\n2(\u03b8 \u2212 mN )\u22a4S\u22121\nN (\u03b8 \u2212 mN ) + const (9.48a)\n= \u22121\n2\n\u0000\n\u03b8\u22a4S\u22121\nN \u03b8 \u2212 2m\u22a4\nN S\u22121\nN \u03b8 + m\u22a4\nN S\u22121\nN mN\n\u0001\n. (9.48b)\nHere, we factorized the quadratic form (\u03b8 \u2212 mN )\u22a4S\u22121\nN (\u03b8 \u2212 mN ) into a Since p(\u03b8 |X , Y) =\nN\n\u0000\nmN , SN\n\u0001\n, it\nholds that\n\u03b8MAP = mN .\nterm that is quadratic in\u03b8 alone (blue), a term that is linear in\u03b8 (orange),\nand a constant term (black). This allows us now to find SN and mN by\nmatching the colored expressions in (9.46b) and (9.48b), which yields\nS\u22121\nN = \u03a6\u22a4\u03c3\u22122I\u03a6 + S\u22121\n0 (9.49a)\n\u21d0 \u21d2SN = (\u03c3\u22122\u03a6\u22a4\u03a6 + S\u22121\n0 )\u22121 (9.49b)\nand\nm\u22a4\nN S\u22121\nN = (\u03c3\u22122\u03a6\u22a4y + S\u22121\n0 m0)\u22a4 (9.50a)\n\u21d0 \u21d2mN = SN (\u03c3\u22122\u03a6\u22a4y + S\u22121\n0 m0) . (9.50b)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1239,
    "chunk_id": "p313_c3",
    "page_number": 313,
    "text": "\u03a6\u22a4y + S\u22121\n0 m0)\u22a4 (9.50a)\n\u21d0 \u21d2mN = SN (\u03c3\u22122\u03a6\u22a4y + S\u22121\n0 m0) . (9.50b)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1240,
    "chunk_id": "p314_c0",
    "page_number": 314,
    "text": "308 Linear Regression\nRemark (General Approach to Completing the Squares) . If we are given\nan equation\nx\u22a4Ax \u2212 2a\u22a4x + const1 , (9.51)\nwhere A is symmetric and positive definite, which we wish to bring into\nthe form\n(x \u2212 \u00b5)\u22a4\u03a3(x \u2212 \u00b5) + const2 , (9.52)\nwe can do this by setting\n\u03a3 := A, (9.53)\n\u00b5 := \u03a3\u22121a (9.54)\nand const2 = const1 \u2212 \u00b5\u22a4\u03a3\u00b5. \u2662\nWe can see that the terms inside the exponential in (9.47b) are of the\nform (9.51) with\nA := \u03c3\u22122\u03a6\u22a4\u03a6 + S\u22121\n0 , (9.55)\na := \u03c3\u22122\u03a6\u22a4y + S\u22121\n0 m0 . (9.56)\nSince A, a can be difficult to identify in equations like (9.46a), it is of-\nten helpful to bring these equations into the form (9.51) that decouples\nquadratic term, linear terms, and constants, which simplifies finding the\ndesired solution.\n9.3.4 Posterior Predictions\nIn (9.37), we computed the predictive distr"
  },
  {
    "vector_id": 1241,
    "chunk_id": "p314_c1",
    "page_number": 314,
    "text": "into the form (9.51) that decouples\nquadratic term, linear terms, and constants, which simplifies finding the\ndesired solution.\n9.3.4 Posterior Predictions\nIn (9.37), we computed the predictive distribution of y\u2217 at a test input\nx\u2217 using the parameter prior p(\u03b8). In principle, predicting with the pa-\nrameter posterior p(\u03b8 | X, Y) is not fundamentally different given that\nin our conjugate model the prior and posterior are both Gaussian (with\ndifferent parameters). Therefore, by following the same reasoning as in\nSection 9.3.2, we obtain the (posterior) predictive distribution\np(y\u2217 | X, Y, x\u2217) =\nZ\np(y\u2217 |x\u2217, \u03b8)p(\u03b8 | X, Y)d\u03b8 (9.57a)\n=\nZ\nN\n\u0000\ny\u2217 |\u03d5\u22a4(x\u2217)\u03b8, \u03c32\u0001\nN\n\u0000\n\u03b8 |mN , SN\n\u0001\nd\u03b8 (9.57b)\n= N\n\u0000\ny\u2217 |\u03d5\u22a4(x\u2217)mN , \u03d5\u22a4(x\u2217)SN \u03d5(x\u2217) + \u03c32\u0001\n. (9.57c)\nThe term \u03d5\u22a4(x\u2217)SN \u03d5(x\u2217) reflects the posterior uncertaint"
  },
  {
    "vector_id": 1242,
    "chunk_id": "p314_c2",
    "page_number": 314,
    "text": "=\nZ\np(y\u2217 |x\u2217, \u03b8)p(\u03b8 | X, Y)d\u03b8 (9.57a)\n=\nZ\nN\n\u0000\ny\u2217 |\u03d5\u22a4(x\u2217)\u03b8, \u03c32\u0001\nN\n\u0000\n\u03b8 |mN , SN\n\u0001\nd\u03b8 (9.57b)\n= N\n\u0000\ny\u2217 |\u03d5\u22a4(x\u2217)mN , \u03d5\u22a4(x\u2217)SN \u03d5(x\u2217) + \u03c32\u0001\n. (9.57c)\nThe term \u03d5\u22a4(x\u2217)SN \u03d5(x\u2217) reflects the posterior uncertainty associatedE[y\u2217 |X , Y, x\u2217] =\n\u03d5\u22a4(x\u2217)mN =\n\u03d5\u22a4(x\u2217)\u03b8MAP.\nwith the parameters \u03b8. Note that SN depends on the training inputs\nthrough \u03a6; see (9.43b). The predictive mean \u03d5\u22a4(x\u2217)mN coincides with\nthe predictions made with the MAP estimate \u03b8MAP.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1243,
    "chunk_id": "p315_c0",
    "page_number": 315,
    "text": "9.3 Bayesian Linear Regression 309\nRemark (Marginal Likelihood and Posterior Predictive Distribution) . By\nreplacing the integral in (9.57a), the predictive distribution can be equiv-\nalently written as the expectation E\u03b8 | X,Y[p(y\u2217 |x\u2217, \u03b8)], where the expec-\ntation is taken with respect to the parameter posterior p(\u03b8 | X, Y).\nWriting the posterior predictive distribution in this way highlights a\nclose resemblance to the marginal likelihood (9.42). The key difference\nbetween the marginal likelihood and the posterior predictive distribution\nare (i) the marginal likelihood can be thought of predicting the training\ntargets y and not the test targets y\u2217, and (ii) the marginal likelihood av-\nerages with respect to the parameter prior and not the parameter poste-\nrior. \u2662\nRemark (Mean and Varianc"
  },
  {
    "vector_id": 1244,
    "chunk_id": "p315_c1",
    "page_number": 315,
    "text": "icting the training\ntargets y and not the test targets y\u2217, and (ii) the marginal likelihood av-\nerages with respect to the parameter prior and not the parameter poste-\nrior. \u2662\nRemark (Mean and Variance of Noise-Free Function Values) . In many\ncases, we are not interested in the predictive distribution p(y\u2217 | X, Y, x\u2217)\nof a (noisy) observation y\u2217. Instead, we would like to obtain the distribu-\ntion of the (noise-free) function values f(x\u2217) = \u03d5\u22a4(x\u2217)\u03b8. We determine\nthe corresponding moments by exploiting the properties of means and\nvariances, which yields\nE[f(x\u2217) | X, Y] = E\u03b8[\u03d5\u22a4(x\u2217)\u03b8 | X, Y] = \u03d5\u22a4(x\u2217)E\u03b8[\u03b8 | X, Y]\n= \u03d5\u22a4(x\u2217)mN = m\u22a4\nN \u03d5(x\u2217) ,\n(9.58)\nV\u03b8[f(x\u2217) | X, Y] = V\u03b8[\u03d5\u22a4(x\u2217)\u03b8 | X, Y]\n= \u03d5\u22a4(x\u2217)V\u03b8[\u03b8 | X, Y]\u03d5(x\u2217)\n= \u03d5\u22a4(x\u2217)SN \u03d5(x\u2217) .\n(9.59)\nWe see that the predictive mean is the same as the predictiv"
  },
  {
    "vector_id": 1245,
    "chunk_id": "p315_c2",
    "page_number": 315,
    "text": "(x\u2217)E\u03b8[\u03b8 | X, Y]\n= \u03d5\u22a4(x\u2217)mN = m\u22a4\nN \u03d5(x\u2217) ,\n(9.58)\nV\u03b8[f(x\u2217) | X, Y] = V\u03b8[\u03d5\u22a4(x\u2217)\u03b8 | X, Y]\n= \u03d5\u22a4(x\u2217)V\u03b8[\u03b8 | X, Y]\u03d5(x\u2217)\n= \u03d5\u22a4(x\u2217)SN \u03d5(x\u2217) .\n(9.59)\nWe see that the predictive mean is the same as the predictive mean for\nnoisy observations as the noise has mean 0, and the predictive variance\nonly differs by \u03c32, which is the variance of the measurement noise: When\nwe predict noisy function values, we need to include \u03c32 as a source of\nuncertainty , but this term is not needed for noise-free predictions. Here,\nthe only remaining uncertainty stems from the parameter posterior. \u2662 Integrating out\nparameters induces\na distribution over\nfunctions.\nRemark (Distribution over Functions). The fact that we integrate out the\nparameters \u03b8 induces a distribution over functions: If we sample \u03b8i \u223c\np(\u03b8 | X, Y) from th"
  },
  {
    "vector_id": 1246,
    "chunk_id": "p315_c3",
    "page_number": 315,
    "text": "nduces\na distribution over\nfunctions.\nRemark (Distribution over Functions). The fact that we integrate out the\nparameters \u03b8 induces a distribution over functions: If we sample \u03b8i \u223c\np(\u03b8 | X, Y) from the parameter posterior, we obtain a single function re-\nalization \u03b8\u22a4\ni \u03d5(\u00b7). The mean function, i.e., the set of all expected function mean function\nvalues E\u03b8[f(\u00b7) |\u03b8, X, Y], of this distribution over functions is m\u22a4\nN \u03d5(\u00b7).\nThe (marginal) variance, i.e., the variance of the functionf(\u00b7), is given by\n\u03d5\u22a4(\u00b7)SN \u03d5(\u00b7). \u2662\nExample 9.8 (Posterior over Functions)\nLet us revisit the Bayesian linear regression problem with polynomials\nof degree 5. We choose a parameter prior p(\u03b8) = N\n\u0000\n0, 1\n4 I\n\u0001\n. Figure 9.9\nvisualizes the prior over functions induced by the parameter prior and\nsample functions from this"
  },
  {
    "vector_id": 1247,
    "chunk_id": "p315_c4",
    "page_number": 315,
    "text": "n problem with polynomials\nof degree 5. We choose a parameter prior p(\u03b8) = N\n\u0000\n0, 1\n4 I\n\u0001\n. Figure 9.9\nvisualizes the prior over functions induced by the parameter prior and\nsample functions from this prior.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1248,
    "chunk_id": "p316_c0",
    "page_number": 316,
    "text": "310 Linear Regression\nFigure 9.10 shows the posterior over functions that we obtain via\nBayesian linear regression. The training dataset is shown in panel (a);\npanel (b) shows the posterior distribution over functions, including the\nfunctions we would obtain via maximum likelihood and MAP estimation.\nThe function we obtain using the MAP estimate also corresponds to the\nposterior mean function in the Bayesian linear regression setting. Panel (c)\nshows some plausible realizations (samples) of functions under that pos-\nterior over functions.\nFigure 9.10\nBayesian linear\nregression and\nposterior over\nfunctions.\n(a) training data;\n(b) posterior\ndistribution over\nfunctions;\n(c) Samples from\nthe posterior over\nfunctions.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\n(a) Training data.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTr"
  },
  {
    "vector_id": 1249,
    "chunk_id": "p316_c1",
    "page_number": 316,
    "text": "er\nfunctions.\n(a) training data;\n(b) posterior\ndistribution over\nfunctions;\n(c) Samples from\nthe posterior over\nfunctions.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\n(a) Training data.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE\nMAP\nBLR (b) Posterior over functions rep-\nresented by the marginal uncer-\ntainties (shaded) showing the\n67% and 95% predictive con-\nfidence bounds, the maximum\nlikelihood estimate (MLE) and\nthe MAP estimate (MAP), the\nlatter of which is identical to\nthe posterior mean function.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\n(c) Samples from the posterior\nover functions, which are in-\nduced by the samples from the\nparameter posterior.\nFigure 9.11 shows some posterior distributions over functions induced\nby the parameter posterior. For different polynomial degrees M, the left\npanels show the maximu"
  },
  {
    "vector_id": 1250,
    "chunk_id": "p316_c2",
    "page_number": 316,
    "text": "ples from the\nparameter posterior.\nFigure 9.11 shows some posterior distributions over functions induced\nby the parameter posterior. For different polynomial degrees M, the left\npanels show the maximum likelihood function \u03b8\u22a4\nML\u03d5(\u00b7), the MAP func-\ntion \u03b8\u22a4\nMAP\u03d5(\u00b7) (which is identical to the posterior mean function), and the\n67% and 95% predictive confidence bounds obtained by Bayesian linear\nregression, represented by the shaded areas.\nThe right panels show samples from the posterior over functions: Here,\nwe sampled parameters \u03b8i from the parameter posterior and computed\nthe function \u03d5\u22a4(x\u2217)\u03b8i, which is a single realization of a function under\nthe posterior distribution over functions. For low-order polynomials, the\nparameter posterior does not allow the parameters to vary much: The\nsampled f"
  },
  {
    "vector_id": 1251,
    "chunk_id": "p316_c3",
    "page_number": 316,
    "text": "hich is a single realization of a function under\nthe posterior distribution over functions. For low-order polynomials, the\nparameter posterior does not allow the parameters to vary much: The\nsampled functions are nearly identical. When we make the model more\nflexible by adding more parameters (i.e., we end up with a higher-order\npolynomial), these parameters are not sufficiently constrained by the pos-\nterior, and the sampled functions can be easily visually separated. We also\nsee in the corresponding panels on the left how the uncertainty increases,\nespecially at the boundaries.\nAlthough for a seventh-order polynomial the MAP estimate yields a rea-\nsonable fit, the Bayesian linear regression model additionally tells us that\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedbac"
  },
  {
    "vector_id": 1252,
    "chunk_id": "p316_c4",
    "page_number": 316,
    "text": "a seventh-order polynomial the MAP estimate yields a rea-\nsonable fit, the Bayesian linear regression model additionally tells us that\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1253,
    "chunk_id": "p317_c0",
    "page_number": 317,
    "text": "9.3 Bayesian Linear Regression 311\nFigure 9.11\nBayesian linear\nregression. Left\npanels: Shaded\nareas indicate the\n67% (dark gray)\nand 95% (light\ngray) predictive\nconfidence bounds.\nThe mean of the\nBayesian linear\nregression model\ncoincides with the\nMAP estimate. The\npredictive\nuncertainty is the\nsum of the noise\nterm and the\nposterior parameter\nuncertainty , which\ndepends on the\nlocation of the test\ninput. Right panels:\nsampled functions\nfrom the posterior\ndistribution.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE\nMAP\nBLR\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\n(a) Posterior distribution for polynomials of degree M = 3 (left) and samples from the pos-\nterior over functions (right).\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE\nMAP\nBLR\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\n(b) Posterior distribution for polynom"
  },
  {
    "vector_id": 1254,
    "chunk_id": "p317_c1",
    "page_number": 317,
    "text": "of degree M = 3 (left) and samples from the pos-\nterior over functions (right).\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE\nMAP\nBLR\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\n(b) Posterior distribution for polynomials of degree M = 5 (left) and samples from the\nposterior over functions (right).\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nTraining data\nMLE\nMAP\nBLR\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\n(c) Posterior distribution for polynomials of degree M = 7 (left) and samples from the pos-\nterior over functions (right).\nthe posterior uncertainty is huge. This information can be critical when\nwe use these predictions in a decision-making system, where bad deci-\nsions can have significant consequences (e.g., in reinforcement learning\nor robotics).\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University"
  },
  {
    "vector_id": 1255,
    "chunk_id": "p317_c2",
    "page_number": 317,
    "text": "making system, where bad deci-\nsions can have significant consequences (e.g., in reinforcement learning\nor robotics).\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1256,
    "chunk_id": "p318_c0",
    "page_number": 318,
    "text": "312 Linear Regression\n9.3.5 Computing the Marginal Likelihood\nIn Section 8.6.2, we highlighted the importance of the marginal likelihood\nfor Bayesian model selection. In the following, we compute the marginal\nlikelihood for Bayesian linear regression with a conjugate Gaussian prior\non the parameters, i.e., exactly the setting we have been discussing in this\nchapter.\nJust to recap, we consider the following generative process:\n\u03b8 \u223c N\n\u0000\nm0, S0\n\u0001\n(9.60a)\nyn |xn, \u03b8 \u223c N\n\u0000\nx\u22a4\nn \u03b8, \u03c32\u0001\n, (9.60b)\nn = 1, . . . , N. The marginal likelihood is given byThe marginal\nlikelihood can be\ninterpreted as the\nexpected likelihood\nunder the prior, i.e.,\nE\u03b8[p(Y |X, \u03b8)].\np(Y | X) =\nZ\np(Y | X, \u03b8)p(\u03b8)d\u03b8 (9.61a)\n=\nZ\nN\n\u0000\ny |X\u03b8, \u03c32I\n\u0001\nN\n\u0000\n\u03b8 |m0, S0\n\u0001\nd\u03b8 , (9.61b)\nwhere we integrate out the model parameters\u03b8. We compute"
  },
  {
    "vector_id": 1257,
    "chunk_id": "p318_c1",
    "page_number": 318,
    "text": "cted likelihood\nunder the prior, i.e.,\nE\u03b8[p(Y |X, \u03b8)].\np(Y | X) =\nZ\np(Y | X, \u03b8)p(\u03b8)d\u03b8 (9.61a)\n=\nZ\nN\n\u0000\ny |X\u03b8, \u03c32I\n\u0001\nN\n\u0000\n\u03b8 |m0, S0\n\u0001\nd\u03b8 , (9.61b)\nwhere we integrate out the model parameters\u03b8. We compute the marginal\nlikelihood in two steps: First, we show that the marginal likelihood is\nGaussian (as a distribution in y); second, we compute the mean and co-\nvariance of this Gaussian.\n1. The marginal likelihood is Gaussian: From Section 6.5.2, we know that\n(i) the product of two Gaussian random variables is an (unnormalized)\nGaussian distribution, and (ii) a linear transformation of a Gaussian\nrandom variable is Gaussian distributed. In (9.61b), we require a linear\ntransformation to bring N\n\u0000\ny |X\u03b8, \u03c32I\n\u0001\ninto the form N\n\u0000\n\u03b8 |\u00b5, \u03a3\n\u0001\nfor\nsome \u00b5, \u03a3. Once this is done, the integral can be solved"
  },
  {
    "vector_id": 1258,
    "chunk_id": "p318_c2",
    "page_number": 318,
    "text": "dom variable is Gaussian distributed. In (9.61b), we require a linear\ntransformation to bring N\n\u0000\ny |X\u03b8, \u03c32I\n\u0001\ninto the form N\n\u0000\n\u03b8 |\u00b5, \u03a3\n\u0001\nfor\nsome \u00b5, \u03a3. Once this is done, the integral can be solved in closed form.\nThe result is the normalizing constant of the product of the two Gaus-\nsians. The normalizing constant itself has Gaussian shape; see (6.76).\n2. Mean and covariance. We compute the mean and covariance matrix\nof the marginal likelihood by exploiting the standard results for means\nand covariances of affine transformations of random variables; see Sec-\ntion 6.4.4. The mean of the marginal likelihood is computed as\nE[Y | X] = E\u03b8,\u03f5[X\u03b8 + \u03f5] = XE\u03b8[\u03b8] = Xm0 . (9.62)\nNote that \u03f5 \u223c N\n\u0000\n0, \u03c32I\n\u0001\nis a vector of i.i.d. random variables. The\ncovariance matrix is given as\nCov[Y|X ] = Cov\u03b8,\u03f5[X"
  },
  {
    "vector_id": 1259,
    "chunk_id": "p318_c3",
    "page_number": 318,
    "text": "inal likelihood is computed as\nE[Y | X] = E\u03b8,\u03f5[X\u03b8 + \u03f5] = XE\u03b8[\u03b8] = Xm0 . (9.62)\nNote that \u03f5 \u223c N\n\u0000\n0, \u03c32I\n\u0001\nis a vector of i.i.d. random variables. The\ncovariance matrix is given as\nCov[Y|X ] = Cov\u03b8,\u03f5[X\u03b8 + \u03f5] = Cov\u03b8[X\u03b8] + \u03c32I (9.63a)\n= X Cov\u03b8[\u03b8]X\u22a4 + \u03c32I = XS0X\u22a4 + \u03c32I . (9.63b)\nHence, the marginal likelihood is\np(Y | X) = (2\u03c0)\u2212N\n2 det(XS0X\u22a4 + \u03c32I)\u22121\n2 (9.64a)\n\u00b7 exp\n\u0000\n\u2212 1\n2 (y \u2212 Xm0)\u22a4(XS0X\u22a4 + \u03c32I)\u22121(y \u2212 Xm0)\n\u0001\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1260,
    "chunk_id": "p319_c0",
    "page_number": 319,
    "text": "9.4 Maximum Likelihood as Orthogonal Projection 313\nFigure 9.12\nGeometric\ninterpretation of\nleast squares.\n(a) Dataset;\n(b) maximum\nlikelihood solution\ninterpreted as a\nprojection.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\n(a) Regression dataset consisting of noisy ob-\nservations yn (blue) of function values f(xn)\nat input locations xn.\n\u22124 \u22122 0 2 4\nx\n\u22124\n\u22122\n0\n2\n4\ny\nProjection\nObservations\nMaximum likelihood estimate\n(b) The orange dots are the projections of\nthe noisy observations (blue dots) onto the\nline \u03b8MLx. The maximum likelihood solution to\na linear regression problem finds a subspace\n(line) onto which the overall projection er-\nror (orange lines) of the observations is mini-\nmized.\n= N\n\u0000\ny |Xm0, XS0X\u22a4 + \u03c32I\n\u0001\n. (9.64b)\nGiven the close connection with the posterior predictive distribution (see\nRema"
  },
  {
    "vector_id": 1261,
    "chunk_id": "p319_c1",
    "page_number": 319,
    "text": "the overall projection er-\nror (orange lines) of the observations is mini-\nmized.\n= N\n\u0000\ny |Xm0, XS0X\u22a4 + \u03c32I\n\u0001\n. (9.64b)\nGiven the close connection with the posterior predictive distribution (see\nRemark on Marginal Likelihood and Posterior Predictive Distribution ear-\nlier in this section), the functional form of the marginal likelihood should\nnot be too surprising.\n9.4 Maximum Likelihood as Orthogonal Projection\nHaving crunched through much algebra to derive maximum likelihood\nand MAP estimates, we will now provide a geometric interpretation of\nmaximum likelihood estimation. Let us consider a simple linear regression\nsetting\ny = x\u03b8 + \u03f5, \u03f5 \u223c N\n\u0000\n0, \u03c32\u0001\n, (9.65)\nin which we consider linear functions f : R \u2192 R that go through the\norigin (we omit features here for clarity). The parameter\u03b8 det"
  },
  {
    "vector_id": 1262,
    "chunk_id": "p319_c2",
    "page_number": 319,
    "text": "simple linear regression\nsetting\ny = x\u03b8 + \u03f5, \u03f5 \u223c N\n\u0000\n0, \u03c32\u0001\n, (9.65)\nin which we consider linear functions f : R \u2192 R that go through the\norigin (we omit features here for clarity). The parameter\u03b8 determines the\nslope of the line. Figure 9.12(a) shows a one-dimensional dataset.\nWith a training data set {(x1, y1), . . . ,(xN , yN )} we recall the results\nfrom Section 9.2.1 and obtain the maximum likelihood estimator for the\nslope parameter as\n\u03b8ML = (X\u22a4X)\u22121X\u22a4y = X\u22a4y\nX\u22a4X \u2208 R, (9.66)\nwhere X = [x1, . . . , xN ]\u22a4 \u2208 RN , y = [y1, . . . , yN ]\u22a4 \u2208 RN .\nThis means for the training inputs X we obtain the optimal (maximum\nlikelihood) reconstruction of the training targets as\nX\u03b8ML = X X\u22a4y\nX\u22a4X = XX\u22a4\nX\u22a4X y , (9.67)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press"
  },
  {
    "vector_id": 1263,
    "chunk_id": "p319_c3",
    "page_number": 319,
    "text": "e optimal (maximum\nlikelihood) reconstruction of the training targets as\nX\u03b8ML = X X\u22a4y\nX\u22a4X = XX\u22a4\nX\u22a4X y , (9.67)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1264,
    "chunk_id": "p320_c0",
    "page_number": 320,
    "text": "314 Linear Regression\ni.e., we obtain the approximation with the minimum least-squares error\nbetween y and X\u03b8.\nAs we are looking for a solution of y = X\u03b8, we can think of linear\nregression as a problem for solving systems of linear equations. There-Linear regression\ncan be thought of as\na method for solving\nsystems of linear\nequations.\nfore, we can relate to concepts from linear algebra and analytic geometry\nthat we discussed in Chapters 2 and 3. In particular, looking carefully\nat (9.67) we see that the maximum likelihood estimator \u03b8ML in our ex-\nample from (9.65) effectively does an orthogonal projection of y onto\nthe one-dimensional subspace spanned by X. Recalling the results on or-Maximum\nlikelihood linear\nregression performs\nan orthogonal\nprojection.\nthogonal projections from Section"
  },
  {
    "vector_id": 1265,
    "chunk_id": "p320_c1",
    "page_number": 320,
    "text": "al projection of y onto\nthe one-dimensional subspace spanned by X. Recalling the results on or-Maximum\nlikelihood linear\nregression performs\nan orthogonal\nprojection.\nthogonal projections from Section 3.8, we identify XX\u22a4\nX\u22a4X as the projection\nmatrix, \u03b8ML as the coordinates of the projection onto the one-dimensional\nsubspace of RN spanned by X and X\u03b8ML as the orthogonal projection of\ny onto this subspace.\nTherefore, the maximum likelihood solution provides also a geometri-\ncally optimal solution by finding the vectors in the subspace spanned by\nX that are \u201cclosest\u201d to the corresponding observations y, where \u201cclos-\nest\u201d means the smallest (squared) distance of the function values yn to\nxn\u03b8. This is achieved by orthogonal projections. Figure 9.12(b) shows the\nprojection of the noisy observat"
  },
  {
    "vector_id": 1266,
    "chunk_id": "p320_c2",
    "page_number": 320,
    "text": "tions y, where \u201cclos-\nest\u201d means the smallest (squared) distance of the function values yn to\nxn\u03b8. This is achieved by orthogonal projections. Figure 9.12(b) shows the\nprojection of the noisy observations onto the subspace that minimizes the\nsquared distance between the original dataset and its projection (note that\nthe x-coordinate is fixed), which corresponds to the maximum likelihood\nsolution.\nIn the general linear regression case where\ny = \u03d5\u22a4(x)\u03b8 + \u03f5, \u03f5 \u223c N\n\u0000\n0, \u03c32\u0001\n(9.68)\nwith vector-valued features \u03d5(x) \u2208 RK, we again can interpret the maxi-\nmum likelihood result\ny \u2248 \u03a6\u03b8ML , (9.69)\n\u03b8ML = (\u03a6\u22a4\u03a6)\u22121\u03a6\u22a4y (9.70)\nas a projection onto a K-dimensional subspace of RN , which is spanned\nby the columns of the feature matrix \u03a6; see Section 3.8.2.\nIf the feature functions \u03d5k that we use to construct"
  },
  {
    "vector_id": 1267,
    "chunk_id": "p320_c3",
    "page_number": 320,
    "text": "(\u03a6\u22a4\u03a6)\u22121\u03a6\u22a4y (9.70)\nas a projection onto a K-dimensional subspace of RN , which is spanned\nby the columns of the feature matrix \u03a6; see Section 3.8.2.\nIf the feature functions \u03d5k that we use to construct the feature ma-\ntrix \u03a6 are orthonormal (see Section 3.7), we obtain a special case where\nthe columns of \u03a6 form an orthonormal basis (see Section 3.5), such that\n\u03a6\u22a4\u03a6 = I. This will then lead to the projection\n\u03a6(\u03a6\u22a4\u03a6)\u22121\u03a6\u22a4y = \u03a6\u03a6\u22a4y =\n KX\nk=1\n\u03d5k\u03d5\u22a4\nk\n!\ny (9.71)\nso that the maximum likelihood projection is simply the sum of projections\nof y onto the individual basis vectors \u03d5k, i.e., the columns of \u03a6. Further-\nmore, the coupling between different features has disappeared due to the\northogonality of the basis. Many popular basis functions in signal process-\ning, such as wavelets and Fourier bases, are"
  },
  {
    "vector_id": 1268,
    "chunk_id": "p320_c4",
    "page_number": 320,
    "text": "rther-\nmore, the coupling between different features has disappeared due to the\northogonality of the basis. Many popular basis functions in signal process-\ning, such as wavelets and Fourier bases, are orthogonal basis functions.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1269,
    "chunk_id": "p321_c0",
    "page_number": 321,
    "text": "9.5 Further Reading 315\nWhen the basis is not orthogonal, one can convert a set of linearly inde-\npendent basis functions to an orthogonal basis by using the Gram-Schmidt\nprocess; see Section 3.8.3 and (Strang, 2003).\n9.5 Further Reading\nIn this chapter, we discussed linear regression for Gaussian likelihoods\nand conjugate Gaussian priors on the parameters of the model. This al-\nlowed for closed-form Bayesian inference. However, in some applications\nwe may want to choose a different likelihood function. For example, in\na binary classification setting, we observe only two possible (categorical) classification\noutcomes, and a Gaussian likelihood is inappropriate in this setting. In-\nstead, we can choose a Bernoulli likelihood that will return a probability of\nthe predicted label to be 1 (or"
  },
  {
    "vector_id": 1270,
    "chunk_id": "p321_c1",
    "page_number": 321,
    "text": "classification\noutcomes, and a Gaussian likelihood is inappropriate in this setting. In-\nstead, we can choose a Bernoulli likelihood that will return a probability of\nthe predicted label to be 1 (or 0). We refer to the books by Barber (2012),\nBishop (2006), and Murphy (2012) for an in-depth introduction to classifi-\ncation problems. A different example where non-Gaussian likelihoods are\nimportant is count data. Counts are non-negative integers, and in this case\na Binomial or Poisson likelihood would be a better choice than a Gaussian.\nAll these examples fall into the category ofgeneralized linear models, a flex- generalized linear\nmodelible generalization of linear regression that allows for response variables\nthat have error distributions other than a Gaussian distribution. The GLM Gener"
  },
  {
    "vector_id": 1271,
    "chunk_id": "p321_c2",
    "page_number": 321,
    "text": "near models, a flex- generalized linear\nmodelible generalization of linear regression that allows for response variables\nthat have error distributions other than a Gaussian distribution. The GLM Generalized linear\nmodels are the\nbuilding blocks of\ndeep neural\nnetworks.\ngeneralizes linear regression by allowing the linear model to be related\nto the observed values via a smooth and invertible function \u03c3(\u00b7) that may\nbe nonlinear so that y = \u03c3(f(x)), where f(x) = \u03b8\u22a4\u03d5(x) is the linear\nregression model from (9.13). We can therefore think of a generalized\nlinear model in terms of function composition y = \u03c3 \u25e6 f, where f is a\nlinear regression model and \u03c3 the activation function. Note that although\nwe are talking about \u201cgeneralized linear models\u201d, the outputs y are no\nlonger linear in the parameter"
  },
  {
    "vector_id": 1272,
    "chunk_id": "p321_c3",
    "page_number": 321,
    "text": "y = \u03c3 \u25e6 f, where f is a\nlinear regression model and \u03c3 the activation function. Note that although\nwe are talking about \u201cgeneralized linear models\u201d, the outputs y are no\nlonger linear in the parameters \u03b8. In logistic regression , we choose the logistic regression\nlogistic sigmoid \u03c3(f) = 1\n1+exp(\u2212f) \u2208 [0, 1], which can be interpreted as the logistic sigmoid\nprobability of observing y = 1 of a Bernoulli random variable y \u2208 {0, 1}.\nThe function \u03c3(\u00b7) is called transfer function or activation function, and its transfer function\nactivation functioninverse is called the canonical link function . From this perspective, it is\ncanonical link\nfunction\nFor ordinary linear\nregression the\nactivation function\nwould simply be the\nidentity .\nalso clear that generalized linear models are the building blocks"
  },
  {
    "vector_id": 1273,
    "chunk_id": "p321_c4",
    "page_number": 321,
    "text": "this perspective, it is\ncanonical link\nfunction\nFor ordinary linear\nregression the\nactivation function\nwould simply be the\nidentity .\nalso clear that generalized linear models are the building blocks of (deep)\nfeedforward neural networks: If we consider a generalized linear model\ny = \u03c3(Ax + b), where A is a weight matrix and b a bias vector, we iden-\ntify this generalized linear model as a single-layer neural network with\nactivation function \u03c3(\u00b7). We can now recursively compose these functions\nvia\nA great post on the\nrelation between\nGLMs and deep\nnetworks is\navailable at\nhttps://tinyurl.\ncom/glm-dnn.\nxk+1 = fk(xk)\nfk(xk) = \u03c3k(Akxk + bk) (9.72)\nfor k = 0, . . . , K\u2212 1, where x0 are the input features and xK = y are\nthe observed outputs, such that fK\u22121 \u25e6 \u00b7 \u00b7\u00b7 \u25e6f0 is a K-layer deep neural\nn"
  },
  {
    "vector_id": 1274,
    "chunk_id": "p321_c5",
    "page_number": 321,
    "text": "/glm-dnn.\nxk+1 = fk(xk)\nfk(xk) = \u03c3k(Akxk + bk) (9.72)\nfor k = 0, . . . , K\u2212 1, where x0 are the input features and xK = y are\nthe observed outputs, such that fK\u22121 \u25e6 \u00b7 \u00b7\u00b7 \u25e6f0 is a K-layer deep neural\nnetwork. Therefore, the building blocks of this deep neural network are\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1275,
    "chunk_id": "p322_c0",
    "page_number": 322,
    "text": "316 Linear Regression\nthe generalized linear models defined in (9.72). Neural networks (Bishop,\n1995; Goodfellow et al., 2016) are significantly more expressive and flexi-\nble than linear regression models. However, maximum likelihood parame-\nter estimation is a non-convex optimization problem, and marginalization\nof the parameters in a fully Bayesian setting is analytically intractable.\nWe briefly hinted at the fact that a distribution over parameters in-\nduces a distribution over regression functions. Gaussian processes (Ras-Gaussian process\nmussen and Williams, 2006) are regression models where the concept of\na distribution over function is central. Instead of placing a distribution\nover parameters, a Gaussian process places a distribution directly on the\nspace of functions without the"
  },
  {
    "vector_id": 1276,
    "chunk_id": "p322_c1",
    "page_number": 322,
    "text": "here the concept of\na distribution over function is central. Instead of placing a distribution\nover parameters, a Gaussian process places a distribution directly on the\nspace of functions without the \u201cdetour\u201d via the parameters. To do so, the\nGaussian process exploits the kernel trick (Sch\u00a8olkopf and Smola, 2002),kernel trick\nwhich allows us to compute inner products between two function values\nf(xi), f(xj) only by looking at the corresponding input xi, xj. A Gaus-\nsian process is closely related to both Bayesian linear regression and sup-\nport vector regression but can also be interpreted as a Bayesian neural\nnetwork with a single hidden layer where the number of units tends to\ninfinity (Neal, 1996; Williams, 1997). Excellent introductions to Gaussian\nprocesses can be found in MacKay (199"
  },
  {
    "vector_id": 1277,
    "chunk_id": "p322_c2",
    "page_number": 322,
    "text": "a Bayesian neural\nnetwork with a single hidden layer where the number of units tends to\ninfinity (Neal, 1996; Williams, 1997). Excellent introductions to Gaussian\nprocesses can be found in MacKay (1998) and Rasmussen and Williams\n(2006).\nWe focused on Gaussian parameter priors in the discussions in this chap-\nter, because they allow for closed-form inference in linear regression mod-\nels. However, even in a regression setting with Gaussian likelihoods, we\nmay choose a non-Gaussian prior. Consider a setting, where the inputs are\nx \u2208 RD and our training set is small and of size N \u226a D. This means that\nthe regression problem is underdetermined. In this case, we can choose\na parameter prior that enforces sparsity , i.e., a prior that tries to set as\nmany parameters to 0 as possible (variable se"
  },
  {
    "vector_id": 1278,
    "chunk_id": "p322_c3",
    "page_number": 322,
    "text": "that\nthe regression problem is underdetermined. In this case, we can choose\na parameter prior that enforces sparsity , i.e., a prior that tries to set as\nmany parameters to 0 as possible (variable selection). This prior providesvariable selection\na stronger regularizer than the Gaussian prior, which often leads to an in-\ncreased prediction accuracy and interpretability of the model. The Laplace\nprior is one example that is frequently used for this purpose. A linear re-\ngression model with the Laplace prior on the parameters is equivalent to\nlinear regression with L1 regularization ( LASSO) (Tibshirani, 1996). TheLASSO\nLaplace distribution is sharply peaked at zero (its first derivative is discon-\ntinuous) and it concentrates its probability mass closer to zero than the\nGaussian distributi"
  },
  {
    "vector_id": 1279,
    "chunk_id": "p322_c4",
    "page_number": 322,
    "text": "ibshirani, 1996). TheLASSO\nLaplace distribution is sharply peaked at zero (its first derivative is discon-\ntinuous) and it concentrates its probability mass closer to zero than the\nGaussian distribution, which encourages parameters to be 0. Therefore,\nthe nonzero parameters are relevant for the regression problem, which is\nthe reason why we also speak of \u201cvariable selection\u201d.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1280,
    "chunk_id": "p323_c0",
    "page_number": 323,
    "text": "10\nDimensionality Reduction with Principal\nComponent Analysis\nWorking directly with high-dimensional data, such as images, comes with A 640 \u00d7 480 pixel\ncolor image is a data\npoint in a\nmillion-dimensional\nspace, where every\npixel responds to\nthree dimensions,\none for each color\nchannel (red, green,\nblue).\nsome difficulties: It is hard to analyze, interpretation is difficult, visualiza-\ntion is nearly impossible, and (from a practical point of view) storage of\nthe data vectors can be expensive. However, high-dimensional data often\nhas properties that we can exploit. For example, high-dimensional data is\noften overcomplete, i.e., many dimensions are redundant and can be ex-\nplained by a combination of other dimensions. Furthermore, dimensions\nin high-dimensional data are often correlated so"
  },
  {
    "vector_id": 1281,
    "chunk_id": "p323_c1",
    "page_number": 323,
    "text": "l data is\noften overcomplete, i.e., many dimensions are redundant and can be ex-\nplained by a combination of other dimensions. Furthermore, dimensions\nin high-dimensional data are often correlated so that the data possesses an\nintrinsic lower-dimensional structure. Dimensionality reduction exploits\nstructure and correlation and allows us to work with a more compact rep-\nresentation of the data, ideally without losing information. We can think\nof dimensionality reduction as a compression technique, similar to jpeg or\nmp3, which are compression algorithms for images and music.\nIn this chapter, we will discuss principal component analysis (PCA), an principal component\nanalysis\nPCA\nalgorithm for linear dimensionality reduction. PCA, proposed by Pearson\ndimensionality\nreduction\n(1901) and Hotel"
  },
  {
    "vector_id": 1282,
    "chunk_id": "p323_c2",
    "page_number": 323,
    "text": "e will discuss principal component analysis (PCA), an principal component\nanalysis\nPCA\nalgorithm for linear dimensionality reduction. PCA, proposed by Pearson\ndimensionality\nreduction\n(1901) and Hotelling (1933), has been around for more than 100 years\nand is still one of the most commonly used techniques for data compres-\nsion and data visualization. It is also used for the identification of simple\npatterns, latent factors, and structures of high-dimensional data. In the\nFigure 10.1\nIllustration:\ndimensionality\nreduction. (a) The\noriginal dataset\ndoes not vary much\nalong the x2\ndirection. (b) The\ndata from (a) can be\nrepresented using\nthe x1-coordinate\nalone with nearly no\nloss.\n\u22125.0 \u22122.5 0.0 2.5 5.0\nx1\n\u22124\n\u22122\n0\n2\n4\nx2\n(a) Dataset with x1 and x2 coordinates.\n\u22125.0 \u22122.5 0.0 2.5 5.0\nx1\n\u22124\n\u22122"
  },
  {
    "vector_id": 1283,
    "chunk_id": "p323_c3",
    "page_number": 323,
    "text": "The\ndata from (a) can be\nrepresented using\nthe x1-coordinate\nalone with nearly no\nloss.\n\u22125.0 \u22122.5 0.0 2.5 5.0\nx1\n\u22124\n\u22122\n0\n2\n4\nx2\n(a) Dataset with x1 and x2 coordinates.\n\u22125.0 \u22122.5 0.0 2.5 5.0\nx1\n\u22124\n\u22122\n0\n2\n4\nx2\n (b) Compressed dataset where only thex1 coor-\ndinate is relevant.\n317\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 1284,
    "chunk_id": "p323_c4",
    "page_number": 323,
    "text": "sal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 1285,
    "chunk_id": "p324_c0",
    "page_number": 324,
    "text": "318 Dimensionality Reduction with Principal Component Analysis\nsignal processing community , PCA is also known as the Karhunen-Lo`eveKarhunen-Lo`eve\ntransform transform. In this chapter, we derive PCA from first principles, drawing on\nour understanding of basis and basis change (Sections 2.6.1 and 2.7.2),\nprojections (Section 3.8), eigenvalues (Section 4.2), Gaussian distribu-\ntions (Section 6.5), and constrained optimization (Section 7.2).\nDimensionality reduction generally exploits a property of high-dimen-\nsional data (e.g., images) that it often lies on a low-dimensional subspace.\nFigure 10.1 gives an illustrative example in two dimensions. Although\nthe data in Figure 10.1(a) does not quite lie on a line, the data does not\nvary much in the x2-direction, so that we can express it as if"
  },
  {
    "vector_id": 1286,
    "chunk_id": "p324_c1",
    "page_number": 324,
    "text": "0.1 gives an illustrative example in two dimensions. Although\nthe data in Figure 10.1(a) does not quite lie on a line, the data does not\nvary much in the x2-direction, so that we can express it as if it were on\na line \u2013 with nearly no loss; see Figure 10.1(b). To describe the data in\nFigure 10.1(b), only the x1-coordinate is required, and the data lies in a\none-dimensional subspace of R2.\n10.1 Problem Setting\nIn PCA, we are interested in finding projections \u02dcxn of data points xn that\nare as similar to the original data points as possible, but which have a sig-\nnificantly lower intrinsic dimensionality . Figure 10.1 gives an illustration\nof what this could look like.\nMore concretely , we consider an i.i.d. datasetX = {x1, . . . ,xN }, xn \u2208\nRD, with mean 0 that possesses the data covariance"
  },
  {
    "vector_id": 1287,
    "chunk_id": "p324_c2",
    "page_number": 324,
    "text": "ionality . Figure 10.1 gives an illustration\nof what this could look like.\nMore concretely , we consider an i.i.d. datasetX = {x1, . . . ,xN }, xn \u2208\nRD, with mean 0 that possesses the data covariance matrix (6.42)data covariance\nmatrix\nS = 1\nN\nNX\nn=1\nxnx\u22a4\nn . (10.1)\nFurthermore, we assume there exists a low-dimensional compressed rep-\nresentation (code)\nzn = B\u22a4xn \u2208 RM (10.2)\nof xn, where we define the projection matrix\nB := [b1, . . . ,bM ] \u2208 RD\u00d7M . (10.3)\nWe assume that the columns ofB are orthonormal (Definition 3.7) so that\nb\u22a4\ni bj = 0 if and only if i \u0338= j and b\u22a4\ni bi = 1. We seek an M-dimensionalThe columns\nb1, . . . ,bM of B\nform a basis of the\nM-dimensional\nsubspace in which\nthe projected data\n\u02dcx = BB\u22a4x \u2208 RD\nlive.\nsubspace U \u2286 RD, dim(U) = M < Donto which we project the data. We\nden"
  },
  {
    "vector_id": 1288,
    "chunk_id": "p324_c3",
    "page_number": 324,
    "text": "ensionalThe columns\nb1, . . . ,bM of B\nform a basis of the\nM-dimensional\nsubspace in which\nthe projected data\n\u02dcx = BB\u22a4x \u2208 RD\nlive.\nsubspace U \u2286 RD, dim(U) = M < Donto which we project the data. We\ndenote the projected data by \u02dcxn \u2208 U, and their coordinates (with respect\nto the basis vectors b1, . . . ,bM of U) by zn. Our aim is to find projections\n\u02dcxn \u2208 RD (or equivalently the codes zn and the basis vectors b1, . . . ,bM )\nso that they are as similar to the original data xn and minimize the loss\ndue to compression.\nExample 10.1 (Coordinate Representation/Code)\nConsider R2 with the canonical basis e1 = [1 , 0]\u22a4, e2 = [0 , 1]\u22a4. From\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1289,
    "chunk_id": "p324_c4",
    "page_number": 324,
    "text": "sis e1 = [1 , 0]\u22a4, e2 = [0 , 1]\u22a4. From\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1290,
    "chunk_id": "p325_c0",
    "page_number": 325,
    "text": "10.1 Problem Setting 319\nFigure 10.2\nGraphical\nillustration of PCA.\nIn PCA, we find a\ncompressed version\nz of original data x.\nThe compressed\ndata can be\nreconstructed into\n\u02dcx, which lives in the\noriginal data space,\nbut has an intrinsic\nlower-dimensional\nrepresentation than\nx.\nx \u02dcxz\nOriginal\nCompressed\nReconstructed\nRD RD\nRM\nChapter 2, we know that x \u2208 R2 can be represented as a linear combina-\ntion of these basis vectors, e.g.,\n\u00145\n3\n\u0015\n= 5e1 + 3e2 . (10.4)\nHowever, when we consider vectors of the form\n\u02dcx =\n\u00140\nz\n\u0015\n\u2208 R2 , z \u2208 R, (10.5)\nthey can always be written as 0e1 + ze2. To represent these vectors it is\nsufficient to remember/store the coordinate/code z of \u02dcx with respect to\nthe e2 vector. The dimension of a\nvector space\ncorresponds to the\nnumber of its basis\nvectors (see\nSection 2.6.1"
  },
  {
    "vector_id": 1291,
    "chunk_id": "p325_c1",
    "page_number": 325,
    "text": "vectors it is\nsufficient to remember/store the coordinate/code z of \u02dcx with respect to\nthe e2 vector. The dimension of a\nvector space\ncorresponds to the\nnumber of its basis\nvectors (see\nSection 2.6.1).\nMore precisely , the set of \u02dcx vectors (with the standard vector addition\nand scalar multiplication) forms a vector subspace U (see Section 2.4)\nwith dim(U) = 1 because U = span[e2].\nIn Section 10.2, we will find low-dimensional representations that re-\ntain as much information as possible and minimize the compression loss.\nAn alternative derivation of PCA is given in Section 10.3, where we will\nbe looking at minimizing the squared reconstruction error\u2225xn \u2212 \u02dcxn\u2225\n2\nbe-\ntween the original data xn and its projection \u02dcxn.\nFigure 10.2 illustrates the setting we consider in PCA, where z repre-\nse"
  },
  {
    "vector_id": 1292,
    "chunk_id": "p325_c2",
    "page_number": 325,
    "text": "l\nbe looking at minimizing the squared reconstruction error\u2225xn \u2212 \u02dcxn\u2225\n2\nbe-\ntween the original data xn and its projection \u02dcxn.\nFigure 10.2 illustrates the setting we consider in PCA, where z repre-\nsents the lower-dimensional representation of the compressed data \u02dcx and\nplays the role of a bottleneck, which controls how much information can\nflow between x and \u02dcx. In PCA, we consider a linear relationship between\nthe original data x and its low-dimensional code z so that z = B\u22a4x and\n\u02dcx = Bz for a suitable matrix B. Based on the motivation of thinking\nof PCA as a data compression technique, we can interpret the arrows in\nFigure 10.2 as a pair of operations representing encoders and decoders.\nThe linear mapping represented by B can be thought of as a decoder,\nwhich maps the low-dimensional co"
  },
  {
    "vector_id": 1293,
    "chunk_id": "p325_c3",
    "page_number": 325,
    "text": "an interpret the arrows in\nFigure 10.2 as a pair of operations representing encoders and decoders.\nThe linear mapping represented by B can be thought of as a decoder,\nwhich maps the low-dimensional codez \u2208 RM back into the original data\nspace RD. Similarly ,B\u22a4 can be thought of an encoder, which encodes the\noriginal data x as a low-dimensional (compressed) code z.\nThroughout this chapter, we will use the MNIST digits dataset as a re-\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1294,
    "chunk_id": "p326_c0",
    "page_number": 326,
    "text": "320 Dimensionality Reduction with Principal Component Analysis\nFigure 10.3\nExamples of\nhandwritten digits\nfrom the MNIST\ndataset. http:\n//yann.lecun.\ncom/exdb/mnist/.\noccurring example, which contains 60,000 examples of handwritten digits\n0 through 9. Each digit is a grayscale image of size28\u00d728, i.e., it contains\n784 pixels so that we can interpret every image in this dataset as a vector\nx \u2208 R784. Examples of these digits are shown in Figure 10.3.\n10.2 Maximum Variance Perspective\nFigure 10.1 gave an example of how a two-dimensional dataset can be\nrepresented using a single coordinate. In Figure 10.1(b), we chose to ig-\nnore the x2-coordinate of the data because it did not add too much in-\nformation so that the compressed data is similar to the original data in\nFigure 10.1(a). We could ha"
  },
  {
    "vector_id": 1295,
    "chunk_id": "p326_c1",
    "page_number": 326,
    "text": "ure 10.1(b), we chose to ig-\nnore the x2-coordinate of the data because it did not add too much in-\nformation so that the compressed data is similar to the original data in\nFigure 10.1(a). We could have chosen to ignore the x1-coordinate, but\nthen the compressed data had been very dissimilar from the original data,\nand much information in the data would have been lost.\nIf we interpret information content in the data as how \u201cspace filling\u201d\nthe dataset is, then we can describe the information contained in the data\nby looking at the spread of the data. From Section 6.4.1, we know that the\nvariance is an indicator of the spread of the data, and we can derive PCA as\na dimensionality reduction algorithm that maximizes the variance in the\nlow-dimensional representation of the data to retain as mu"
  },
  {
    "vector_id": 1296,
    "chunk_id": "p326_c2",
    "page_number": 326,
    "text": "e is an indicator of the spread of the data, and we can derive PCA as\na dimensionality reduction algorithm that maximizes the variance in the\nlow-dimensional representation of the data to retain as much information\nas possible. Figure 10.4 illustrates this.\nConsidering the setting discussed in Section 10.1, our aim is to find\na matrix B (see (10.3)) that retains as much information as possible\nwhen compressing data by projecting it onto the subspace spanned by\nthe columns b1, . . . ,bM of B. Retaining most information after data com-\npression is equivalent to capturing the largest amount of variance in the\nlow-dimensional code (Hotelling, 1933).\nRemark. (Centered Data) For the data covariance matrix in (10.1), we\nassumed centered data. We can make this assumption without loss of gen-\nerali"
  },
  {
    "vector_id": 1297,
    "chunk_id": "p326_c3",
    "page_number": 326,
    "text": "riance in the\nlow-dimensional code (Hotelling, 1933).\nRemark. (Centered Data) For the data covariance matrix in (10.1), we\nassumed centered data. We can make this assumption without loss of gen-\nerality: Let us assume that \u00b5 is the mean of the data. Using the properties\nof the variance, which we discussed in Section 6.4.4, we obtain\nVz[z] = Vx[B\u22a4(x \u2212 \u00b5)] = Vx[B\u22a4x \u2212 B\u22a4\u00b5] = Vx[B\u22a4x] , (10.6)\ni.e., the variance of the low-dimensional code does not depend on the\nmean of the data. Therefore, we assume without loss of generality that the\ndata has mean 0 for the remainder of this section. With this assumption\nthe mean of the low-dimensional code is also0 since Ez[z] = Ex[B\u22a4x] =\nB\u22a4Ex[x] = 0. \u2662\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1298,
    "chunk_id": "p326_c4",
    "page_number": 326,
    "text": "sumption\nthe mean of the low-dimensional code is also0 since Ez[z] = Ex[B\u22a4x] =\nB\u22a4Ex[x] = 0. \u2662\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1299,
    "chunk_id": "p327_c0",
    "page_number": 327,
    "text": "10.2 Maximum Variance Perspective 321\nFigure 10.4 PCA\nfinds a\nlower-dimensional\nsubspace (line) that\nmaintains as much\nvariance (spread of\nthe data) as possible\nwhen the data\n(blue) is projected\nonto this subspace\n(orange).\n10.2.1 Direction with Maximal Variance\nWe maximize the variance of the low-dimensional code using a sequential\napproach. We start by seeking a single vectorb1 \u2208 RD that maximizes the The vector b1 will\nbe the first column\nof the matrix B and\ntherefore the first of\nM orthonormal\nbasis vectors that\nspan the\nlower-dimensional\nsubspace.\nvariance of the projected data, i.e., we aim to maximize the variance of\nthe first coordinate z1 of z \u2208 RM so that\nV1 := V[z1] = 1\nN\nNX\nn=1\nz2\n1n (10.7)\nis maximized, where we exploited the i.i.d. assumption of the data and\ndefined z1n as th"
  },
  {
    "vector_id": 1300,
    "chunk_id": "p327_c1",
    "page_number": 327,
    "text": "aim to maximize the variance of\nthe first coordinate z1 of z \u2208 RM so that\nV1 := V[z1] = 1\nN\nNX\nn=1\nz2\n1n (10.7)\nis maximized, where we exploited the i.i.d. assumption of the data and\ndefined z1n as the first coordinate of the low-dimensional representation\nzn \u2208 RM of xn \u2208 RD. Note that first component of zn is given by\nz1n = b\u22a4\n1 xn , (10.8)\ni.e., it is the coordinate of the orthogonal projection of xn onto the one-\ndimensional subspace spanned by b1 (Section 3.8). We substitute (10.8)\ninto (10.7), which yields\nV1 = 1\nN\nNX\nn=1\n(b\u22a4\n1 xn)2 = 1\nN\nNX\nn=1\nb\u22a4\n1 xnx\u22a4\nn b1 (10.9a)\n= b\u22a4\n1\n \n1\nN\nNX\nn=1\nxnx\u22a4\nn\n!\nb1 = b\u22a4\n1 Sb1 , (10.9b)\nwhere S is the data covariance matrix defined in (10.1). In (10.9a), we\nhave used the fact that the dot product of two vectors is symmetric with\nrespect to its argumen"
  },
  {
    "vector_id": 1301,
    "chunk_id": "p327_c2",
    "page_number": 327,
    "text": "xnx\u22a4\nn\n!\nb1 = b\u22a4\n1 Sb1 , (10.9b)\nwhere S is the data covariance matrix defined in (10.1). In (10.9a), we\nhave used the fact that the dot product of two vectors is symmetric with\nrespect to its arguments, that is, b\u22a4\n1 xn = x\u22a4\nn b1.\nNotice that arbitrarily increasing the magnitude of the vector b1 in-\ncreases V1, that is, a vector b1 that is two times longer can result in V1\nthat is potentially four times larger. Therefore, we restrict all solutions to \u2225b1\u22252 = 1\n\u21d0\u21d2 \u2225b1\u2225 = 1.\u2225b1\u2225\n2\n= 1, which results in a constrained optimization problem in which\nwe seek the direction along which the data varies most.\nWith the restriction of the solution space to unit vectors the vector b1\nthat points in the direction of maximum variance can be found by the\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Pu"
  },
  {
    "vector_id": 1302,
    "chunk_id": "p327_c3",
    "page_number": 327,
    "text": "most.\nWith the restriction of the solution space to unit vectors the vector b1\nthat points in the direction of maximum variance can be found by the\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1303,
    "chunk_id": "p328_c0",
    "page_number": 328,
    "text": "322 Dimensionality Reduction with Principal Component Analysis\nconstrained optimization problem\nmax\nb1\nb\u22a4\n1 Sb1\nsubject to \u2225b1\u2225\n2\n= 1 .\n(10.10)\nFollowing Section 7.2, we obtain the Lagrangian\nL(b1, \u03bb) = b\u22a4\n1 Sb1 + \u03bb1(1 \u2212 b\u22a4\n1 b1) (10.11)\nto solve this constrained optimization problem. The partial derivatives of\nL with respect to b1 and \u03bb1 are\n\u2202L\n\u2202b1\n= 2b\u22a4\n1 S \u2212 2\u03bb1b\u22a4\n1 , \u2202L\n\u2202\u03bb1\n= 1 \u2212 b\u22a4\n1 b1 , (10.12)\nrespectively . Setting these partial derivatives to0 gives us the relations\nSb1 = \u03bb1b1 , (10.13)\nb\u22a4\n1 b1 = 1 . (10.14)\nBy comparing this with the definition of an eigenvalue decomposition\n(Section 4.4), we see that b1 is an eigenvector of the data covariance\nmatrix S, and the Lagrange multiplier\u03bb1 plays the role of the correspond-\ning eigenvalue. This eigenvector property (10.13) allows us to"
  },
  {
    "vector_id": 1304,
    "chunk_id": "p328_c1",
    "page_number": 328,
    "text": "n 4.4), we see that b1 is an eigenvector of the data covariance\nmatrix S, and the Lagrange multiplier\u03bb1 plays the role of the correspond-\ning eigenvalue. This eigenvector property (10.13) allows us to rewrite ourThe quantity \u221a\u03bb1 is\nalso called the\nloading of the unit\nvector b1 and\nrepresents the\nstandard deviation\nof the data\naccounted for by the\nprincipal subspace\nspan[b1].\nvariance objective (10.10) as\nV1 = b\u22a4\n1 Sb1 = \u03bb1b\u22a4\n1 b1 = \u03bb1 , (10.15)\ni.e., the variance of the data projected onto a one-dimensional subspace\nequals the eigenvalue that is associated with the basis vectorb1 that spans\nthis subspace. Therefore, to maximize the variance of the low-dimensional\ncode, we choose the basis vector associated with the largest eigenvalue\nof the data covariance matrix. This eigenvector is calle"
  },
  {
    "vector_id": 1305,
    "chunk_id": "p328_c2",
    "page_number": 328,
    "text": "is subspace. Therefore, to maximize the variance of the low-dimensional\ncode, we choose the basis vector associated with the largest eigenvalue\nof the data covariance matrix. This eigenvector is called the first principalprincipal component\ncomponent. We can determine the effect/contribution of the principal com-\nponent b1 in the original data space by mapping the coordinate z1n back\ninto data space, which gives us the projected data point\n\u02dcxn = b1z1n = b1b\u22a4\n1 xn \u2208 RD (10.16)\nin the original data space.\nRemark. Although \u02dcxn is a D-dimensional vector, it only requires a single\ncoordinate z1n to represent it with respect to the basis vectorb1 \u2208 RD. \u2662\n10.2.2 M-dimensional Subspace with Maximal Variance\nAssume we have found the first m \u2212 1 principal components as the m \u2212 1\neigenvectors of S th"
  },
  {
    "vector_id": 1306,
    "chunk_id": "p328_c3",
    "page_number": 328,
    "text": "represent it with respect to the basis vectorb1 \u2208 RD. \u2662\n10.2.2 M-dimensional Subspace with Maximal Variance\nAssume we have found the first m \u2212 1 principal components as the m \u2212 1\neigenvectors of S that are associated with the largest m \u2212 1 eigenvalues.\nSince S is symmetric, the spectral theorem (Theorem 4.15) states that we\ncan use these eigenvectors to construct an orthonormal eigenbasis of an\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1307,
    "chunk_id": "p329_c0",
    "page_number": 329,
    "text": "10.2 Maximum Variance Perspective 323\n(m \u2212 1)-dimensional subspace of RD. Generally , themth principal com-\nponent can be found by subtracting the effect of the first m \u2212 1 principal\ncomponents b1, . . . ,bm\u22121 from the data, thereby trying to find principal\ncomponents that compress the remaining information. We then arrive at\nthe new data matrix\n\u02c6X := X \u2212\nm\u22121X\ni=1\nbib\u22a4\ni X = X \u2212 Bm\u22121X , (10.17)\nwhere X = [ x1, . . . ,xN ] \u2208 RD\u00d7N contains the data points as column The matrix \u02c6X :=\n[\u02c6x1, . . . ,\u02c6xN ] \u2208\nRD\u00d7N in (10.17)\ncontains the\ninformation in the\ndata that has not yet\nbeen compressed.\nvectors and Bm\u22121 := Pm\u22121\ni=1 bib\u22a4\ni is a projection matrix that projects onto\nthe subspace spanned by b1, . . . ,bm\u22121.\nRemark (Notation). Throughout this chapter, we do not follow the con-\nvention of collect"
  },
  {
    "vector_id": 1308,
    "chunk_id": "p329_c1",
    "page_number": 329,
    "text": "and Bm\u22121 := Pm\u22121\ni=1 bib\u22a4\ni is a projection matrix that projects onto\nthe subspace spanned by b1, . . . ,bm\u22121.\nRemark (Notation). Throughout this chapter, we do not follow the con-\nvention of collecting data x1, . . . ,xN as the rows of the data matrix, but\nwe define them to be the columns of X. This means that our data ma-\ntrix X is a D \u00d7 N matrix instead of the conventional N \u00d7 D matrix. The\nreason for our choice is that the algebra operations work out smoothly\nwithout the need to either transpose the matrix or to redefine vectors as\nrow vectors that are left-multiplied onto matrices. \u2662\nTo find the mth principal component, we maximize the variance\nVm = V[zm] = 1\nN\nNX\nn=1\nz2\nmn = 1\nN\nNX\nn=1\n(b\u22a4\nm \u02c6xn)2 = b\u22a4\nm \u02c6Sbm , (10.18)\nsubject to \u2225bm\u2225\n2\n= 1 , where we followed the same steps as in ("
  },
  {
    "vector_id": 1309,
    "chunk_id": "p329_c2",
    "page_number": 329,
    "text": "find the mth principal component, we maximize the variance\nVm = V[zm] = 1\nN\nNX\nn=1\nz2\nmn = 1\nN\nNX\nn=1\n(b\u22a4\nm \u02c6xn)2 = b\u22a4\nm \u02c6Sbm , (10.18)\nsubject to \u2225bm\u2225\n2\n= 1 , where we followed the same steps as in (10.9b)\nand defined \u02c6S as the data covariance matrix of the transformed dataset\n\u02c6X := {\u02c6x1, . . . ,\u02c6xN }. As previously , when we looked at the first principal\ncomponent alone, we solve a constrained optimization problem and dis-\ncover that the optimal solutionbm is the eigenvector of \u02c6S that is associated\nwith the largest eigenvalue of \u02c6S.\nIt turns out that bm is also an eigenvector of S. More generally , the sets\nof eigenvectors of S and \u02c6S are identical. Since both S and \u02c6S are sym-\nmetric, we can find an ONB of eigenvectors (spectral theorem 4.15), i.e.,\nthere exist D distinct eigenvectors"
  },
  {
    "vector_id": 1310,
    "chunk_id": "p329_c3",
    "page_number": 329,
    "text": "erally , the sets\nof eigenvectors of S and \u02c6S are identical. Since both S and \u02c6S are sym-\nmetric, we can find an ONB of eigenvectors (spectral theorem 4.15), i.e.,\nthere exist D distinct eigenvectors for both S and \u02c6S. Next, we show that\nevery eigenvector of S is an eigenvector of \u02c6S. Assume we have already\nfound eigenvectors b1, . . . ,bm\u22121 of \u02c6S. Consider an eigenvector bi of S,\ni.e., Sbi = \u03bbibi. In general,\n\u02c6Sbi = 1\nN\n\u02c6X \u02c6X\n\u22a4\nbi = 1\nN (X \u2212 Bm\u22121X)(X \u2212 Bm\u22121X)\u22a4bi (10.19a)\n= (S \u2212 SBm\u22121 \u2212 Bm\u22121S + Bm\u22121SBm\u22121)bi . (10.19b)\nWe distinguish between two cases. If i \u2a7e m, i.e., bi is an eigenvector\nthat is not among the firstm\u22121 principal components, then bi is orthogo-\nnal to the firstm\u22121 principal components and Bm\u22121bi = 0. If i < m, i.e.,\nbi is among the first m \u2212 1 principal components, then bi i"
  },
  {
    "vector_id": 1311,
    "chunk_id": "p329_c4",
    "page_number": 329,
    "text": "is not among the firstm\u22121 principal components, then bi is orthogo-\nnal to the firstm\u22121 principal components and Bm\u22121bi = 0. If i < m, i.e.,\nbi is among the first m \u2212 1 principal components, then bi is a basis vector\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1312,
    "chunk_id": "p330_c0",
    "page_number": 330,
    "text": "324 Dimensionality Reduction with Principal Component Analysis\nof the principal subspace onto which Bm\u22121 projects. Since b1, . . . ,bm\u22121\nare an ONB of this principal subspace, we obtain Bm\u22121bi = bi. The two\ncases can be summarized as follows:\nBm\u22121bi = bi if i < m , Bm\u22121bi = 0 if i \u2a7e m . (10.20)\nIn the case i \u2a7e m, by using (10.20) in (10.19b), we obtain \u02c6Sbi = (S \u2212\nBm\u22121S)bi = Sbi = \u03bbibi , i.e., bi is also an eigenvector of \u02c6S with eigen-\nvalue \u03bbi. Specifically ,\n\u02c6Sbm = Sbm = \u03bbmbm . (10.21)\nEquation (10.21) reveals that bm is not only an eigenvector of S but also\nof \u02c6S. Specifically ,\u03bbm is the largest eigenvalue of \u02c6S and \u03bbm is the mth\nlargest eigenvalue of S, and both have the associated eigenvector bm.\nIn the case i < m, by using (10.20) in (10.19b), we obtain\n\u02c6Sbi = (S \u2212 SBm\u22121 \u2212 Bm\u22121S + B"
  },
  {
    "vector_id": 1313,
    "chunk_id": "p330_c1",
    "page_number": 330,
    "text": "argest eigenvalue of \u02c6S and \u03bbm is the mth\nlargest eigenvalue of S, and both have the associated eigenvector bm.\nIn the case i < m, by using (10.20) in (10.19b), we obtain\n\u02c6Sbi = (S \u2212 SBm\u22121 \u2212 Bm\u22121S + Bm\u22121SBm\u22121)bi = 0 = 0bi (10.22)\nThis means that b1, . . . ,bm\u22121 are also eigenvectors of \u02c6S, but they are as-\nsociated with eigenvalue 0 so that b1, . . . ,bm\u22121 span the null space of \u02c6S.\nOverall, every eigenvector of S is also an eigenvector of \u02c6S. However,\nif the eigenvectors of S are part of the (m \u2212 1) dimensional principal\nsubspace, then the associated eigenvalue of \u02c6S is 0.This derivation\nshows that there is\nan intimate\nconnection between\nthe M-dimensional\nsubspace with\nmaximal variance\nand the eigenvalue\ndecomposition. We\nwill revisit this\nconnection in\nSection 10.4.\nWith the relation (10"
  },
  {
    "vector_id": 1314,
    "chunk_id": "p330_c2",
    "page_number": 330,
    "text": "s that there is\nan intimate\nconnection between\nthe M-dimensional\nsubspace with\nmaximal variance\nand the eigenvalue\ndecomposition. We\nwill revisit this\nconnection in\nSection 10.4.\nWith the relation (10.21) and b\u22a4\nmbm = 1, the variance of the data pro-\njected onto the mth principal component is\nVm = b\u22a4\nmSbm\n(10.21)\n= \u03bbmb\u22a4\nmbm = \u03bbm . (10.23)\nThis means that the variance of the data, when projected onto an M-\ndimensional subspace, equals the sum of the eigenvalues that are associ-\nated with the corresponding eigenvectors of the data covariance matrix.\nExample 10.2 (Eigenvalues of MNIST \u201c8\u201d)\nFigure 10.5\nProperties of the\ntraining data of\nMNIST \u201c8\u201d. (a)\nEigenvalues sorted\nin descending order;\n(b) Variance\ncaptured by the\nprincipal\ncomponents\nassociated with the\nlargest eigenvalues.\n0 50 100 150"
  },
  {
    "vector_id": 1315,
    "chunk_id": "p330_c3",
    "page_number": 330,
    "text": "10.5\nProperties of the\ntraining data of\nMNIST \u201c8\u201d. (a)\nEigenvalues sorted\nin descending order;\n(b) Variance\ncaptured by the\nprincipal\ncomponents\nassociated with the\nlargest eigenvalues.\n0 50 100 150 200\nIndex\n0\n10\n20\n30\n40\n50Eigenvalue\n (a) Eigenvalues (sorted in descending order) of\nthe data covariance matrix of all digits \u201c8\u201d in\nthe MNIST training set.\n0 50 100 150 200\nNumber of principal components\n100\n200\n300\n400\n500Captured variance\n(b) Variance captured by the principal compo-\nnents.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1316,
    "chunk_id": "p331_c0",
    "page_number": 331,
    "text": "10.3 Projection Perspective 325\nFigure 10.6\nIllustration of the\nprojection\napproach: Find a\nsubspace (line) that\nminimizes the\nlength of the\ndifference vector\nbetween projected\n(orange) and\noriginal (blue) data.\nTaking all digits \u201c8\u201d in the MNIST training data, we compute the eigen-\nvalues of the data covariance matrix. Figure 10.5(a) shows the200 largest\neigenvalues of the data covariance matrix. We see that only a few of\nthem have a value that differs significantly from 0. Therefore, most of\nthe variance, when projecting data onto the subspace spanned by the cor-\nresponding eigenvectors, is captured by only a few principal components,\nas shown in Figure 10.5(b).\nOverall, to find an M-dimensional subspace of RD that retains as much\ninformation as possible, PCA tells us to choose the colum"
  },
  {
    "vector_id": 1317,
    "chunk_id": "p331_c1",
    "page_number": 331,
    "text": "captured by only a few principal components,\nas shown in Figure 10.5(b).\nOverall, to find an M-dimensional subspace of RD that retains as much\ninformation as possible, PCA tells us to choose the columns of the matrix\nB in (10.3) as the M eigenvectors of the data covariance matrix S that\nare associated with the M largest eigenvalues. The maximum amount of\nvariance PCA can capture with the first M principal components is\nVM =\nMX\nm=1\n\u03bbm , (10.24)\nwhere the \u03bbm are the M largest eigenvalues of the data covariance matrix\nS. Consequently , the variance lost by data compression via PCA is\nJM :=\nDX\nj=M+1\n\u03bbj = VD \u2212 VM . (10.25)\nInstead of these absolute quantities, we can define the relative variance\ncaptured as VM\nVD\n, and the relative variance lost by compression as 1 \u2212 VM\nVD\n.\n10.3 Projection Per"
  },
  {
    "vector_id": 1318,
    "chunk_id": "p331_c2",
    "page_number": 331,
    "text": "+1\n\u03bbj = VD \u2212 VM . (10.25)\nInstead of these absolute quantities, we can define the relative variance\ncaptured as VM\nVD\n, and the relative variance lost by compression as 1 \u2212 VM\nVD\n.\n10.3 Projection Perspective\nIn the following, we will derive PCA as an algorithm that directly mini-\nmizes the average reconstruction error. This perspective allows us to in-\nterpret PCA as implementing an optimal linear auto-encoder. We will draw\nheavily from Chapters 2 and 3.\nIn the previous section, we derived PCA by maximizing the variance\nin the projected space to retain as much information as possible. In the\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1319,
    "chunk_id": "p331_c3",
    "page_number": 331,
    "text": "\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1320,
    "chunk_id": "p332_c0",
    "page_number": 332,
    "text": "326 Dimensionality Reduction with Principal Component Analysis\nFigure 10.7\nSimplified\nprojection setting.\n(a) A vector x \u2208 R2\n(red cross) shall be\nprojected onto a\none-dimensional\nsubspace U \u2286 R2\nspanned by b. (b)\nshows the difference\nvectors between x\nand some\ncandidates \u02dcx.\n\u22121.0 \u22120.5 0.0 0.5 1.0 1.5 2.0\nx1\n\u22120.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nx2\nb\nU\n(a) Setting.\n\u22121.0 \u22120.5 0.0 0.5 1.0 1.5 2.0\nx1\n\u22120.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nx2\nb\nU (b) Differences x \u2212 \u02dcxi for 50 different \u02dcxi are\nshown by the red lines.\nfollowing, we will look at the difference vectors between the original data\nxn and their reconstruction \u02dcxn and minimize this distance so that xn and\n\u02dcxn are as close as possible. Figure 10.6 illustrates this setting.\n10.3.1 Setting and Objective\nAssume an (ordered) orthonormal basis (ONB) B = (b1,"
  },
  {
    "vector_id": 1321,
    "chunk_id": "p332_c1",
    "page_number": 332,
    "text": "tion \u02dcxn and minimize this distance so that xn and\n\u02dcxn are as close as possible. Figure 10.6 illustrates this setting.\n10.3.1 Setting and Objective\nAssume an (ordered) orthonormal basis (ONB) B = (b1, . . . ,bD) of RD,\ni.e., b\u22a4\ni bj = 1 if and only if i = j and 0 otherwise.\nFrom Section 2.5 we know that for a basis (b1, . . . ,bD) of RD any x \u2208\nRD can be written as a linear combination of the basis vectors of RD, i.e.,\nVectors \u02dcx \u2208 U could\nbe vectors on a\nplane in R3. The\ndimensionality of\nthe plane is 2, but\nthe vectors still have\nthree coordinates\nwith respect to the\nstandard basis of\nR3.\nx =\nDX\nd=1\n\u03b6dbd =\nMX\nm=1\n\u03b6mbm +\nDX\nj=M+1\n\u03b6jbj (10.26)\nfor suitable coordinates \u03b6d \u2208 R.\nWe are interested in finding vectors \u02dcx \u2208 RD, which live in lower-\ndimensional subspace U \u2286 RD, dim(U) = M, so that"
  },
  {
    "vector_id": 1322,
    "chunk_id": "p332_c2",
    "page_number": 332,
    "text": "=\nDX\nd=1\n\u03b6dbd =\nMX\nm=1\n\u03b6mbm +\nDX\nj=M+1\n\u03b6jbj (10.26)\nfor suitable coordinates \u03b6d \u2208 R.\nWe are interested in finding vectors \u02dcx \u2208 RD, which live in lower-\ndimensional subspace U \u2286 RD, dim(U) = M, so that\n\u02dcx =\nMX\nm=1\nzmbm \u2208 U \u2286 RD (10.27)\nis as similar to x as possible. Note that at this point we need to assume\nthat the coordinates zm of \u02dcx and \u03b6m of x are not identical.\nIn the following, we use exactly this kind of representation of \u02dcx to find\noptimal coordinates z and basis vectors b1, . . . ,bM such that \u02dcx is as sim-\nilar to the original data point x as possible, i.e., we aim to minimize the\n(Euclidean) distance \u2225x \u2212 \u02dcx\u2225. Figure 10.7 illustrates this setting.\nWithout loss of generality , we assume that the datasetX = {x1, . . . ,xN },\nxn \u2208 RD, is centered at0, i.e., E[X] = 0. Without the z"
  },
  {
    "vector_id": 1323,
    "chunk_id": "p332_c3",
    "page_number": 332,
    "text": "Euclidean) distance \u2225x \u2212 \u02dcx\u2225. Figure 10.7 illustrates this setting.\nWithout loss of generality , we assume that the datasetX = {x1, . . . ,xN },\nxn \u2208 RD, is centered at0, i.e., E[X] = 0. Without the zero-mean assump-\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1324,
    "chunk_id": "p333_c0",
    "page_number": 333,
    "text": "10.3 Projection Perspective 327\ntion, we would arrive at exactly the same solution, but the notation would\nbe substantially more cluttered.\nWe are interested in finding the best linear projection ofX onto a lower-\ndimensional subspace U of RD with dim(U) = M and orthonormal basis\nvectors b1, . . . ,bM . We will call this subspace U the principal subspace. principal subspace\nThe projections of the data points are denoted by\n\u02dcxn :=\nMX\nm=1\nzmnbm = Bzn \u2208 RD , (10.28)\nwhere zn := [z1n, . . . , zMn ]\u22a4 \u2208 RM is the coordinate vector of \u02dcxn with\nrespect to the basis (b1, . . . ,bM ). More specifically , we are interested in\nhaving the \u02dcxn as similar to xn as possible.\nThe similarity measure we use in the following is the squared distance\n(Euclidean norm) \u2225x \u2212 \u02dcx\u2225\n2\nbetween x and \u02dcx. We therefore de"
  },
  {
    "vector_id": 1325,
    "chunk_id": "p333_c1",
    "page_number": 333,
    "text": ", we are interested in\nhaving the \u02dcxn as similar to xn as possible.\nThe similarity measure we use in the following is the squared distance\n(Euclidean norm) \u2225x \u2212 \u02dcx\u2225\n2\nbetween x and \u02dcx. We therefore define our ob-\njective as minimizing the average squared Euclidean distance (reconstruction reconstruction error\nerror) (Pearson, 1901)\nJM := 1\nN\nNX\nn=1\n\u2225xn \u2212 \u02dcxn\u22252 , (10.29)\nwhere we make it explicit that the dimension of the subspace onto which\nwe project the data is M. In order to find this optimal linear projection,\nwe need to find the orthonormal basis of the principal subspace and the\ncoordinates zn \u2208 RM of the projections with respect to this basis.\nTo find the coordinates zn and the ONB of the principal subspace, we\nfollow a two-step approach. First, we optimize the coordinates zn for a"
  },
  {
    "vector_id": 1326,
    "chunk_id": "p333_c2",
    "page_number": 333,
    "text": "tes zn \u2208 RM of the projections with respect to this basis.\nTo find the coordinates zn and the ONB of the principal subspace, we\nfollow a two-step approach. First, we optimize the coordinates zn for a\ngiven ONB (b1, . . . ,bM ); second, we find the optimal ONB.\n10.3.2 Finding Optimal Coordinates\nLet us start by finding the optimal coordinates z1n, . . . , zMn of the projec-\ntions \u02dcxn for n = 1, . . . , N. Consider Figure 10.7(b), where the principal\nsubspace is spanned by a single vector b. Geometrically speaking, finding\nthe optimal coordinates z corresponds to finding the representation of the\nlinear projection \u02dcx with respect to b that minimizes the distance between\n\u02dcx \u2212 x. From Figure 10.7(b), it is clear that this will be the orthogonal\nprojection, and in the following we will show exa"
  },
  {
    "vector_id": 1327,
    "chunk_id": "p333_c3",
    "page_number": 333,
    "text": "e\nlinear projection \u02dcx with respect to b that minimizes the distance between\n\u02dcx \u2212 x. From Figure 10.7(b), it is clear that this will be the orthogonal\nprojection, and in the following we will show exactly this.\nWe assume an ONB (b1, . . . ,bM ) of U \u2286 RD. To find the optimal co-\nordinates zm with respect to this basis, we require the partial derivatives\n\u2202JM\n\u2202zin\n= \u2202JM\n\u2202\u02dcxn\n\u2202\u02dcxn\n\u2202zin\n, (10.30a)\n\u2202JM\n\u2202\u02dcxn\n= \u2212 2\nN (xn \u2212 \u02dcxn)\u22a4 \u2208 R1\u00d7D , (10.30b)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1328,
    "chunk_id": "p334_c0",
    "page_number": 334,
    "text": "328 Dimensionality Reduction with Principal Component Analysis\nFigure 10.8\nOptimal projection\nof a vector x \u2208 R2\nonto a\none-dimensional\nsubspace\n(continuation from\nFigure 10.7).\n(a) Distances\n\u2225x \u2212 \u02dcx\u2225 for some\n\u02dcx \u2208 U.\n(b) Orthogonal\nprojection and\noptimal coordinates.\n\u22121.0 \u22120.5 0.0 0.5 1.0 1.5 2.0\nx1\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\n3.25\n\u2225x \u2212 \u02dcx\u2225\n(a) Distances \u2225x \u2212 \u02dcx\u2225 for some \u02dcx = z1b \u2208\nU = span[b]; see panel (b) for the setting.\n\u22121.0 \u22120.5 0.0 0.5 1.0 1.5 2.0\nx1\n\u22120.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nx2\nb\nU\n\u02dcx\n(b) The vector \u02dcx that minimizes the distance\nin panel (a) is its orthogonal projection onto\nU. The coordinate of the projection \u02dcx with\nrespect to the basis vector b that spans U\nis the factor we need to scale b in order to\n\u201creach\u201d \u02dcx.\n\u2202\u02dcxn\n\u2202zin\n(10.28)\n= \u2202\n\u2202zin\n MX\nm=1\nzmnbm\n!\n= bi"
  },
  {
    "vector_id": 1329,
    "chunk_id": "p334_c1",
    "page_number": 334,
    "text": "on onto\nU. The coordinate of the projection \u02dcx with\nrespect to the basis vector b that spans U\nis the factor we need to scale b in order to\n\u201creach\u201d \u02dcx.\n\u2202\u02dcxn\n\u2202zin\n(10.28)\n= \u2202\n\u2202zin\n MX\nm=1\nzmnbm\n!\n= bi (10.30c)\nfor i = 1, . . . , M, such that we obtain\n\u2202JM\n\u2202zin\n(10.30b)\n(10.30c)\n= \u2212 2\nN (xn \u2212 \u02dcxn)\u22a4bi\n(10.28)\n= \u2212 2\nN\n \nxn \u2212\nMX\nm=1\nzmnbm\n!\u22a4\nbi\n(10.31a)\nONB\n= \u2212 2\nN (x\u22a4\nn bi \u2212 zinb\u22a4\ni bi) = \u2212 2\nN (x\u22a4\nn bi \u2212 zin) . (10.31b)\nsince b\u22a4\ni bi = 1. Setting this partial derivative to 0 yields immediately theThe coordinates of\nthe optimal\nprojection of xn\nwith respect to the\nbasis vectors\nb1, . . . ,bM are the\ncoordinates of the\northogonal\nprojection of xn\nonto the principal\nsubspace.\noptimal coordinates\nzin = x\u22a4\nn bi = b\u22a4\ni xn (10.32)\nfor i = 1 , . . . , Mand n = 1 , . . . , N. This means that the optim"
  },
  {
    "vector_id": 1330,
    "chunk_id": "p334_c2",
    "page_number": 334,
    "text": "he\ncoordinates of the\northogonal\nprojection of xn\nonto the principal\nsubspace.\noptimal coordinates\nzin = x\u22a4\nn bi = b\u22a4\ni xn (10.32)\nfor i = 1 , . . . , Mand n = 1 , . . . , N. This means that the optimal co-\nordinates zin of the projection \u02dcxn are the coordinates of the orthogonal\nprojection (see Section 3.8) of the original data point xn onto the one-\ndimensional subspace that is spanned by bi. Consequently:\nThe optimal linear projection \u02dcxn of xn is an orthogonal projection.\nThe coordinates of \u02dcxn with respect to the basis (b1, . . . ,bM ) are the\ncoordinates of the orthogonal projection of xn onto the principal sub-\nspace.\nAn orthogonal projection is the best linear mapping given the objec-\ntive (10.29).\nThe coordinates \u03b6m of x in (10.26) and the coordinateszm of \u02dcx in (10.27)\nDraft (202"
  },
  {
    "vector_id": 1331,
    "chunk_id": "p334_c3",
    "page_number": 334,
    "text": "n onto the principal sub-\nspace.\nAn orthogonal projection is the best linear mapping given the objec-\ntive (10.29).\nThe coordinates \u03b6m of x in (10.26) and the coordinateszm of \u02dcx in (10.27)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1332,
    "chunk_id": "p335_c0",
    "page_number": 335,
    "text": "10.3 Projection Perspective 329\nmust be identical for m = 1, . . . , Msince U\u22a5 = span[bM+1, . . . ,bD] is\nthe orthogonal complement (see Section 3.6) of U = span[b1, . . . ,bM ].\nRemark (Orthogonal Projections with Orthonormal Basis Vectors). Let us\nbriefly recap orthogonal projections from Section 3.8. If(b1, . . . ,bD) is an\northonormal basis of RD then b\u22a4\nj x is the\ncoordinate of the\northogonal\nprojection of x onto\nthe subspace\nspanned by bj.\n\u02dcx = bj(b\u22a4\nj bj)\u22121b\u22a4\nj x = bjb\u22a4\nj x \u2208 RD (10.33)\nis the orthogonal projection ofx onto the subspace spanned by thejth ba-\nsis vector, andzj = b\u22a4\nj x is the coordinate of this projection with respect to\nthe basis vector bj that spans that subspace sincezjbj = \u02dcx. Figure 10.8(b)\nillustrates this setting.\nMore generally , if we aim to project onto an"
  },
  {
    "vector_id": 1333,
    "chunk_id": "p335_c1",
    "page_number": 335,
    "text": "s the coordinate of this projection with respect to\nthe basis vector bj that spans that subspace sincezjbj = \u02dcx. Figure 10.8(b)\nillustrates this setting.\nMore generally , if we aim to project onto an M-dimensional subspace\nof RD, we obtain the orthogonal projection of x onto the M-dimensional\nsubspace with orthonormal basis vectors b1, . . . ,bM as\n\u02dcx = B(B\u22a4B| {z }\n=I\n)\u22121B\u22a4x = BB\u22a4x, (10.34)\nwhere we defined B := [ b1, . . . ,bM ] \u2208 RD\u00d7M . The coordinates of this\nprojection with respect to the ordered basis (b1, . . . ,bM ) are z := B\u22a4x\nas discussed in Section 3.8.\nWe can think of the coordinates as a representation of the projected\nvector in a new coordinate system defined by (b1, . . . ,bM ). Note that al-\nthough \u02dcx \u2208 RD, we only need M coordinates z1, . . . , zM to represent\nthis vector;"
  },
  {
    "vector_id": 1334,
    "chunk_id": "p335_c2",
    "page_number": 335,
    "text": "es as a representation of the projected\nvector in a new coordinate system defined by (b1, . . . ,bM ). Note that al-\nthough \u02dcx \u2208 RD, we only need M coordinates z1, . . . , zM to represent\nthis vector; the other D \u2212 M coordinates with respect to the basis vectors\n(bM+1, . . . ,bD) are always 0. \u2662\nSo far we have shown that for a given ONB we can find the optimal\ncoordinates of \u02dcx by an orthogonal projection onto the principal subspace.\nIn the following, we will determine what the best basis is.\n10.3.3 Finding the Basis of the Principal Subspace\nTo determine the basis vectors b1, . . . ,bM of the principal subspace, we\nrephrase the loss function (10.29) using the results we have so far. This\nwill make it easier to find the basis vectors. To reformulate the loss func-\ntion, we exploit our resu"
  },
  {
    "vector_id": 1335,
    "chunk_id": "p335_c3",
    "page_number": 335,
    "text": "principal subspace, we\nrephrase the loss function (10.29) using the results we have so far. This\nwill make it easier to find the basis vectors. To reformulate the loss func-\ntion, we exploit our results from before and obtain\n\u02dcxn =\nMX\nm=1\nzmnbm\n(10.32)\n=\nMX\nm=1\n(x\u22a4\nn bm)bm . (10.35)\nWe now exploit the symmetry of the dot product, which yields\n\u02dcxn =\n MX\nm=1\nbmb\u22a4\nm\n!\nxn . (10.36)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1336,
    "chunk_id": "p336_c0",
    "page_number": 336,
    "text": "330 Dimensionality Reduction with Principal Component Analysis\nFigure 10.9\nOrthogonal\nprojection and\ndisplacement\nvectors. When\nprojecting data\npoints xn (blue)\nonto subspace U1,\nwe obtain \u02dcxn\n(orange). The\ndisplacement vector\n\u02dcxn \u2212 xn lies\ncompletely in the\northogonal\ncomplement U2 of\nU1.\n\u22125 0 5\nx1\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\nx2\nU\nU\u22a5\nSince we can generally write the original data point xn as a linear combi-\nnation of all basis vectors, it holds that\nxn =\nDX\nd=1\nzdnbd\n(10.32)\n=\nDX\nd=1\n(x\u22a4\nn bd)bd =\n DX\nd=1\nbdb\u22a4\nd\n!\nxn (10.37a)\n=\n MX\nm=1\nbmb\u22a4\nm\n!\nxn +\n DX\nj=M+1\nbjb\u22a4\nj\n!\nxn , (10.37b)\nwhere we split the sum with D terms into a sum over M and a sum\nover D \u2212 M terms. With this result, we find that the displacement vector\nxn \u2212 \u02dcxn, i.e., the difference vector between the original data point and its\nprojec"
  },
  {
    "vector_id": 1337,
    "chunk_id": "p336_c1",
    "page_number": 336,
    "text": "m with D terms into a sum over M and a sum\nover D \u2212 M terms. With this result, we find that the displacement vector\nxn \u2212 \u02dcxn, i.e., the difference vector between the original data point and its\nprojection, is\nxn \u2212 \u02dcxn =\n DX\nj=M+1\nbjb\u22a4\nj\n!\nxn (10.38a)\n=\nDX\nj=M+1\n(x\u22a4\nn bj)bj . (10.38b)\nThis means the difference is exactly the projection of the data point onto\nthe orthogonal complement of the principal subspace: We identify the ma-\ntrix PD\nj=M+1 bjb\u22a4\nj in (10.38a) as the projection matrix that performs this\nprojection. Hence the displacement vector xn \u2212 \u02dcxn lies in the subspace\nthat is orthogonal to the principal subspace as illustrated in Figure 10.9.\nRemark (Low-Rank Approximation). In (10.38a), we saw that the projec-\ntion matrix, which projects x onto \u02dcx, is given by\nMX\nm=1\nbmb\u22a4\nm = BB\u22a4 ."
  },
  {
    "vector_id": 1338,
    "chunk_id": "p336_c2",
    "page_number": 336,
    "text": "to the principal subspace as illustrated in Figure 10.9.\nRemark (Low-Rank Approximation). In (10.38a), we saw that the projec-\ntion matrix, which projects x onto \u02dcx, is given by\nMX\nm=1\nbmb\u22a4\nm = BB\u22a4 . (10.39)\nBy construction as a sum of rank-one matrices bmb\u22a4\nm we see that BB\u22a4 is\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1339,
    "chunk_id": "p337_c0",
    "page_number": 337,
    "text": "10.3 Projection Perspective 331\nsymmetric and has rankM. Therefore, the average squared reconstruction\nerror can also be written as\n1\nN\nNX\nn=1\n\u2225xn \u2212 \u02dcxn\u2225\n2\n= 1\nN\nNX\nn=1\n\r\r\rxn \u2212 BB\u22a4xn\n\r\r\r\n2\n(10.40a)\n= 1\nN\nNX\nn=1\n\r\r\r(I \u2212 BB\u22a4)xn\n\r\r\r\n2\n. (10.40b)\nFinding orthonormal basis vectors b1, . . . ,bM , which minimize the differ- PCA finds the best\nrank-M\napproximation of\nthe identity matrix.\nence between the original data xn and their projections \u02dcxn, is equivalent\nto finding the best rank- M approximation BB\u22a4 of the identity matrix I\n(see Section 4.6). \u2662\nNow we have all the tools to reformulate the loss function (10.29).\nJM = 1\nN\nNX\nn=1\n\u2225xn \u2212 \u02dcxn\u22252 (10.38b)\n= 1\nN\nNX\nn=1\n\r\r\r\r\r\nDX\nj=M+1\n(b\u22a4\nj xn)bj\n\r\r\r\r\r\n2\n. (10.41)\nWe now explicitly compute the squared norm and exploit the fact that the\nbj form an ON"
  },
  {
    "vector_id": 1340,
    "chunk_id": "p337_c1",
    "page_number": 337,
    "text": "function (10.29).\nJM = 1\nN\nNX\nn=1\n\u2225xn \u2212 \u02dcxn\u22252 (10.38b)\n= 1\nN\nNX\nn=1\n\r\r\r\r\r\nDX\nj=M+1\n(b\u22a4\nj xn)bj\n\r\r\r\r\r\n2\n. (10.41)\nWe now explicitly compute the squared norm and exploit the fact that the\nbj form an ONB, which yields\nJM = 1\nN\nNX\nn=1\nDX\nj=M+1\n(b\u22a4\nj xn)2 = 1\nN\nNX\nn=1\nDX\nj=M+1\nb\u22a4\nj xnb\u22a4\nj xn (10.42a)\n= 1\nN\nNX\nn=1\nDX\nj=M+1\nb\u22a4\nj xnx\u22a4\nn bj , (10.42b)\nwhere we exploited the symmetry of the dot product in the last step to\nwrite b\u22a4\nj xn = x\u22a4\nn bj. We now swap the sums and obtain\nJM =\nDX\nj=M+1\nb\u22a4\nj\n \n1\nN\nNX\nn=1\nxnx\u22a4\nn\n!\n| {z }\n=:S\nbj =\nDX\nj=M+1\nb\u22a4\nj Sbj (10.43a)\n=\nDX\nj=M+1\ntr(b\u22a4\nj Sbj) =\nDX\nj=M+1\ntr(Sbjb\u22a4\nj ) = tr\n\u0010\u0010 DX\nj=M+1\nbjb\u22a4\nj\n\u0011\n| {z }\nprojection matrix\nS\n\u0011\n,\n(10.43b)\nwhere we exploited the property that the trace operator tr(\u00b7) (see (4.18))\nis linear and invariant to cyclic permutations of its"
  },
  {
    "vector_id": 1341,
    "chunk_id": "p337_c2",
    "page_number": 337,
    "text": "\u22a4\nj ) = tr\n\u0010\u0010 DX\nj=M+1\nbjb\u22a4\nj\n\u0011\n| {z }\nprojection matrix\nS\n\u0011\n,\n(10.43b)\nwhere we exploited the property that the trace operator tr(\u00b7) (see (4.18))\nis linear and invariant to cyclic permutations of its arguments. Since we\nassumed that our dataset is centered, i.e., E[X] = 0, we identify S as the\ndata covariance matrix. Since the projection matrix in (10.43b) is con-\nstructed as a sum of rank-one matrices bjb\u22a4\nj it itself is of rank D \u2212 M.\nEquation (10.43a) implies that we can formulate the average squared\nreconstruction error equivalently as the covariance matrix of the data,\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1342,
    "chunk_id": "p337_c3",
    "page_number": 337,
    "text": "roth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1343,
    "chunk_id": "p338_c0",
    "page_number": 338,
    "text": "332 Dimensionality Reduction with Principal Component Analysis\nprojected onto the orthogonal complement of the principal subspace. Min-\nimizing the average squared reconstruction error is therefore equivalent toMinimizing the\naverage squared\nreconstruction error\nis equivalent to\nminimizing the\nprojection of the\ndata covariance\nmatrix onto the\northogonal\ncomplement of the\nprincipal subspace.\nminimizing the variance of the data when projected onto the subspace we\nignore, i.e., the orthogonal complement of the principal subspace. Equiva-\nlently , we maximize the variance of the projection that we retain in the\nprincipal subspace, which links the projection loss immediately to the\nmaximum-variance formulation of PCA discussed in Section 10.2. But this\nthen also means that we will obtain the sa"
  },
  {
    "vector_id": 1344,
    "chunk_id": "p338_c1",
    "page_number": 338,
    "text": "retain in the\nprincipal subspace, which links the projection loss immediately to the\nmaximum-variance formulation of PCA discussed in Section 10.2. But this\nthen also means that we will obtain the same solution that we obtained\nMinimizing the\naverage squared\nreconstruction error\nis equivalent to\nmaximizing the\nvariance of the\nprojected data.\nfor the maximum-variance perspective. Therefore, we omit a derivation\nthat is identical to the one presented in Section 10.2 and summarize the\nresults from earlier in the light of the projection perspective.\nThe average squared reconstruction error, when projecting onto theM-\ndimensional principal subspace, is\nJM =\nDX\nj=M+1\n\u03bbj , (10.44)\nwhere \u03bbj are the eigenvalues of the data covariance matrix. Therefore,\nto minimize (10.44) we need to select the sma"
  },
  {
    "vector_id": 1345,
    "chunk_id": "p338_c2",
    "page_number": 338,
    "text": "projecting onto theM-\ndimensional principal subspace, is\nJM =\nDX\nj=M+1\n\u03bbj , (10.44)\nwhere \u03bbj are the eigenvalues of the data covariance matrix. Therefore,\nto minimize (10.44) we need to select the smallest D \u2212 M eigenvalues,\nwhich then implies that their corresponding eigenvectors are the basis of\nthe orthogonal complement of the principal subspace. Consequently , this\nmeans that the basis of the principal subspace comprises the eigenvectors\nb1, . . . ,bM that are associated with the largest M eigenvalues of the data\ncovariance matrix.\nExample 10.3 (MNIST Digits Embedding)\nFigure 10.10\nEmbedding of\nMNIST digits 0\n(blue) and 1\n(orange) in a\ntwo-dimensional\nprincipal subspace\nusing PCA. Four\nembeddings of the\ndigits \u201c0\u201d and \u201c1\u201d in\nthe principal\nsubspace are\nhighlighted in red\nwith their\ncorr"
  },
  {
    "vector_id": 1346,
    "chunk_id": "p338_c3",
    "page_number": 338,
    "text": "ng of\nMNIST digits 0\n(blue) and 1\n(orange) in a\ntwo-dimensional\nprincipal subspace\nusing PCA. Four\nembeddings of the\ndigits \u201c0\u201d and \u201c1\u201d in\nthe principal\nsubspace are\nhighlighted in red\nwith their\ncorresponding\noriginal digit.\nFigure 10.10 visualizes the training data of the MMIST digits \u201c0\u201d and \u201c1\u201d\nembedded in the vector subspace spanned by the first two principal com-\nponents. We observe a relatively clear separation between \u201c0\u201ds (blue dots)\nand \u201c1\u201ds (orange dots), and we see the variation within each individual\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1347,
    "chunk_id": "p338_c4",
    "page_number": 338,
    "text": "ok.com."
  },
  {
    "vector_id": 1348,
    "chunk_id": "p339_c0",
    "page_number": 339,
    "text": "10.4 Eigenvector Computation and Low-Rank Approximations 333\ncluster. Four embeddings of the digits \u201c0\u201d and \u201c1\u201d in the principal subspace\nare highlighted in red with their corresponding original digit. The figure\nreveals that the variation within the set of \u201c0\u201d is significantly greater than\nthe variation within the set of \u201c1\u201d.\n10.4 Eigenvector Computation and Low-Rank Approximations\nIn the previous sections, we obtained the basis of the principal subspace\nas the eigenvectors that are associated with the largest eigenvalues of the\ndata covariance matrix\nS = 1\nN\nNX\nn=1\nxnx\u22a4\nn = 1\nN XX\u22a4 , (10.45)\nX = [x1, . . . ,xN ] \u2208 RD\u00d7N . (10.46)\nNote that X is a D \u00d7 N matrix, i.e., it is the transpose of the \u201ctypical\u201d\ndata matrix (Bishop, 2006; Murphy, 2012). To get the eigenvalues (and\nthe corresponding"
  },
  {
    "vector_id": 1349,
    "chunk_id": "p339_c1",
    "page_number": 339,
    "text": "X = [x1, . . . ,xN ] \u2208 RD\u00d7N . (10.46)\nNote that X is a D \u00d7 N matrix, i.e., it is the transpose of the \u201ctypical\u201d\ndata matrix (Bishop, 2006; Murphy, 2012). To get the eigenvalues (and\nthe corresponding eigenvectors) of S, we can follow two approaches: Use\neigendecomposition\nor SVD to compute\neigenvectors.\nWe perform an eigendecomposition (see Section 4.2) and compute the\neigenvalues and eigenvectors of S directly .\nWe use a singular value decomposition (see Section 4.5). Since S is\nsymmetric and factorizes into XX\u22a4 (ignoring the factor 1\nN ), the eigen-\nvalues of S are the squared singular values of X.\nMore specifically , the SVD ofX is given by\nX|{z}\nD\u00d7N\n= U|{z}\nD\u00d7D\n\u03a3|{z}\nD\u00d7N\nV \u22a4\n|{z}\nN\u00d7N\n, (10.47)\nwhere U \u2208 RD\u00d7D and V \u22a4 \u2208 RN\u00d7N are orthogonal matrices and \u03a3 \u2208\nRD\u00d7N is a matrix whose only no"
  },
  {
    "vector_id": 1350,
    "chunk_id": "p339_c2",
    "page_number": 339,
    "text": "es of X.\nMore specifically , the SVD ofX is given by\nX|{z}\nD\u00d7N\n= U|{z}\nD\u00d7D\n\u03a3|{z}\nD\u00d7N\nV \u22a4\n|{z}\nN\u00d7N\n, (10.47)\nwhere U \u2208 RD\u00d7D and V \u22a4 \u2208 RN\u00d7N are orthogonal matrices and \u03a3 \u2208\nRD\u00d7N is a matrix whose only nonzero entries are the singular values\u03c3ii \u2a7e\n0. It then follows that\nS = 1\nN XX\u22a4 = 1\nN U\u03a3 V \u22a4V| {z }\n=IN\n\u03a3\u22a4U\u22a4 = 1\nN U\u03a3\u03a3\u22a4U\u22a4 . (10.48)\nWith the results from Section 4.5, we get that the columns of U are the The columns of U\nare the eigenvectors\nof S.\neigenvectors of XX\u22a4 (and therefore S). Furthermore, the eigenvalues\n\u03bbd of S are related to the singular values of X via\n\u03bbd = \u03c32\nd\nN . (10.49)\nThis relationship between the eigenvalues of S and the singular values\nof X provides the connection between the maximum variance view (Sec-\ntion 10.2) and the singular value decomposition.\n\u00a92024 M. P. Deisenroth"
  },
  {
    "vector_id": 1351,
    "chunk_id": "p339_c3",
    "page_number": 339,
    "text": "onship between the eigenvalues of S and the singular values\nof X provides the connection between the maximum variance view (Sec-\ntion 10.2) and the singular value decomposition.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1352,
    "chunk_id": "p340_c0",
    "page_number": 340,
    "text": "334 Dimensionality Reduction with Principal Component Analysis\n10.4.1 PCA Using Low-Rank Matrix Approximations\nTo maximize the variance of the projected data (or minimize the average\nsquared reconstruction error), PCA chooses the columns of U in (10.48)\nto be the eigenvectors that are associated with the M largest eigenvalues\nof the data covariance matrixS so that we identifyU as the projection ma-\ntrix B in (10.3), which projects the original data onto a lower-dimensional\nsubspace of dimension M. The Eckart-Young theorem (Theorem 4.25 inEckart-Young\ntheorem Section 4.6) offers a direct way to estimate the low-dimensional represen-\ntation. Consider the best rank-M approximation\n\u02dcXM := argminrk(A)\u2a7dM \u2225X \u2212 A\u22252 \u2208 RD\u00d7N (10.50)\nof X, where \u2225\u00b7\u22252 is the spectral norm defined in (4.93). The Eckart-"
  },
  {
    "vector_id": 1353,
    "chunk_id": "p340_c1",
    "page_number": 340,
    "text": "estimate the low-dimensional represen-\ntation. Consider the best rank-M approximation\n\u02dcXM := argminrk(A)\u2a7dM \u2225X \u2212 A\u22252 \u2208 RD\u00d7N (10.50)\nof X, where \u2225\u00b7\u22252 is the spectral norm defined in (4.93). The Eckart-Young\ntheorem states that \u02dcXM is given by truncating the SVD at the top- M\nsingular value. In other words, we obtain\n\u02dcXM = UM|{z}\nD\u00d7M\n\u03a3M|{z}\nM\u00d7M\nV \u22a4\nM|{z}\nM\u00d7N\n\u2208 RD\u00d7N (10.51)\nwith orthogonal matrices UM := [ u1, . . . ,uM ] \u2208 RD\u00d7M and V M :=\n[v1, . . . ,vM ] \u2208 RN\u00d7M and a diagonal matrix \u03a3M \u2208 RM\u00d7M whose diago-\nnal entries are the M largest singular values of X.\n10.4.2 Practical Aspects\nFinding eigenvalues and eigenvectors is also important in other funda-\nmental machine learning methods that require matrix decompositions. In\ntheory , as we discussed in Section 4.2, we can solve for the eigenvalu"
  },
  {
    "vector_id": 1354,
    "chunk_id": "p340_c2",
    "page_number": 340,
    "text": "values and eigenvectors is also important in other funda-\nmental machine learning methods that require matrix decompositions. In\ntheory , as we discussed in Section 4.2, we can solve for the eigenvalues as\nroots of the characteristic polynomial. However, for matrices larger than\n4\u00d74 this is not possible because we would need to find the roots of a poly-\nnomial of degree 5 or higher. However, the Abel-Ruffini theorem (Ruffini,Abel-Ruffini\ntheorem 1799; Abel, 1826) states that there exists no algebraic solution to this\nproblem for polynomials of degree 5 or more. Therefore, in practice, wenp.linalg.eigh\nor\nnp.linalg.svd\nsolve for eigenvalues or singular values using iterative methods, which are\nimplemented in all modern packages for linear algebra.\nIn many applications (such as PCA presented"
  },
  {
    "vector_id": 1355,
    "chunk_id": "p340_c3",
    "page_number": 340,
    "text": "alg.eigh\nor\nnp.linalg.svd\nsolve for eigenvalues or singular values using iterative methods, which are\nimplemented in all modern packages for linear algebra.\nIn many applications (such as PCA presented in this chapter), we only\nrequire a few eigenvectors. It would be wasteful to compute the full de-\ncomposition, and then discard all eigenvectors with eigenvalues that are\nbeyond the first few. It turns out that if we are interested in only the first\nfew eigenvectors (with the largest eigenvalues), then iterative processes,\nwhich directly optimize these eigenvectors, are computationally more effi-\ncient than a full eigendecomposition (or SVD). In the extreme case of only\nneeding the first eigenvector, a simple method called the power iterationpower iteration\nis very efficient. Power iteration"
  },
  {
    "vector_id": 1356,
    "chunk_id": "p340_c4",
    "page_number": 340,
    "text": "-\ncient than a full eigendecomposition (or SVD). In the extreme case of only\nneeding the first eigenvector, a simple method called the power iterationpower iteration\nis very efficient. Power iteration chooses a random vectorx0 that is not in\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1357,
    "chunk_id": "p341_c0",
    "page_number": 341,
    "text": "10.5 PCA in High Dimensions 335\nthe null space of S and follows the iteration\nxk+1 = Sxk\n\u2225Sxk\u2225 , k = 0, 1, . . . . (10.52)\nThis means the vector xk is multiplied by S in every iteration and then If S is invertible, it\nis sufficient to\nensure that x0 \u0338= 0.\nnormalized, i.e., we always have \u2225xk\u2225 = 1. This sequence of vectors con-\nverges to the eigenvector associated with the largest eigenvalue of S. The\noriginal Google PageRank algorithm (Page et al., 1999) uses such an al-\ngorithm for ranking web pages based on their hyperlinks.\n10.5 PCA in High Dimensions\nIn order to do PCA, we need to compute the data covariance matrix. In D\ndimensions, the data covariance matrix is aD \u00d7D matrix. Computing the\neigenvalues and eigenvectors of this matrix is computationally expensive\nas it scales cubically i"
  },
  {
    "vector_id": 1358,
    "chunk_id": "p341_c1",
    "page_number": 341,
    "text": "the data covariance matrix. In D\ndimensions, the data covariance matrix is aD \u00d7D matrix. Computing the\neigenvalues and eigenvectors of this matrix is computationally expensive\nas it scales cubically in D. Therefore, PCA, as we discussed earlier, will be\ninfeasible in very high dimensions. For example, if ourxn are images with\n10,000 pixels (e.g., 100 \u00d7 100 pixel images), we would need to compute\nthe eigendecomposition of a 10,000 \u00d7 10,000 covariance matrix. In the\nfollowing, we provide a solution to this problem for the case that we have\nsubstantially fewer data points than dimensions, i.e., N \u226a D.\nAssume we have a centered dataset x1, . . . ,xN , xn \u2208 RD. Then the\ndata covariance matrix is given as\nS = 1\nN XX\u22a4 \u2208 RD\u00d7D , (10.53)\nwhere X = [x1, . . . ,xN ] is a D \u00d7 N matrix whose columns are"
  },
  {
    "vector_id": 1359,
    "chunk_id": "p341_c2",
    "page_number": 341,
    "text": "\u226a D.\nAssume we have a centered dataset x1, . . . ,xN , xn \u2208 RD. Then the\ndata covariance matrix is given as\nS = 1\nN XX\u22a4 \u2208 RD\u00d7D , (10.53)\nwhere X = [x1, . . . ,xN ] is a D \u00d7 N matrix whose columns are the data\npoints.\nWe now assume that N \u226a D, i.e., the number of data points is smaller\nthan the dimensionality of the data. If there are no duplicate data points,\nthe rank of the covariance matrixS is N, so it has D \u2212N +1 many eigen-\nvalues that are 0. Intuitively , this means that there are some redundancies.\nIn the following, we will exploit this and turn theD\u00d7D covariance matrix\ninto an N \u00d7 N covariance matrix whose eigenvalues are all positive.\nIn PCA, we ended up with the eigenvector equation\nSbm = \u03bbmbm , m = 1, . . . , M , (10.54)\nwhere bm is a basis vector of the principal subspace. Let"
  },
  {
    "vector_id": 1360,
    "chunk_id": "p341_c3",
    "page_number": 341,
    "text": "ovariance matrix whose eigenvalues are all positive.\nIn PCA, we ended up with the eigenvector equation\nSbm = \u03bbmbm , m = 1, . . . , M , (10.54)\nwhere bm is a basis vector of the principal subspace. Let us rewrite this\nequation a bit: With S defined in (10.53), we obtain\nSbm = 1\nN XX\u22a4bm = \u03bbmbm . (10.55)\nWe now multiply X\u22a4 \u2208 RN\u00d7D from the left-hand side, which yields\n1\nN X\u22a4X| {z }\nN\u00d7N\nX\u22a4bm| {z }\n=:cm\n= \u03bbmX\u22a4bm \u21d0 \u21d21\nN X\u22a4Xcm = \u03bbmcm , (10.56)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1361,
    "chunk_id": "p342_c0",
    "page_number": 342,
    "text": "336 Dimensionality Reduction with Principal Component Analysis\nand we get a new eigenvector/eigenvalue equation: \u03bbm remains eigen-\nvalue, which confirms our results from Section 4.5.3 that the nonzero\neigenvalues of XX\u22a4 equal the nonzero eigenvalues of X\u22a4X. We obtain\nthe eigenvector of the matrix 1\nN X\u22a4X \u2208 RN\u00d7N associated with \u03bbm as\ncm := X\u22a4bm. Assuming we have no duplicate data points, this matrix\nhas rank N and is invertible. This also implies that 1\nN X\u22a4X has the same\n(nonzero) eigenvalues as the data covariance matrixS. But this is now an\nN \u00d7 N matrix, so that we can compute the eigenvalues and eigenvectors\nmuch more efficiently than for the originalD \u00d7D data covariance matrix.\nNow that we have the eigenvectors of 1\nN X\u22a4X, we are going to re-\ncover the original eigenvectors, which we s"
  },
  {
    "vector_id": 1362,
    "chunk_id": "p342_c1",
    "page_number": 342,
    "text": "s and eigenvectors\nmuch more efficiently than for the originalD \u00d7D data covariance matrix.\nNow that we have the eigenvectors of 1\nN X\u22a4X, we are going to re-\ncover the original eigenvectors, which we still need for PCA. Currently ,\nwe know the eigenvectors of 1\nN X\u22a4X. If we left-multiply our eigenvalue/\neigenvector equation with X, we get\n1\nN XX\u22a4\n| {z }\nS\nXcm = \u03bbmXcm (10.57)\nand we recover the data covariance matrix again. This now also means\nthat we recover Xcm as an eigenvector of S.\nRemark. If we want to apply the PCA algorithm that we discussed in Sec-\ntion 10.6, we need to normalize the eigenvectors Xcm of S so that they\nhave norm 1. \u2662\n10.6 Key Steps of PCA in Practice\nIn the following, we will go through the individual steps of PCA using a\nrunning example, which is summarized in Figur"
  },
  {
    "vector_id": 1363,
    "chunk_id": "p342_c2",
    "page_number": 342,
    "text": "genvectors Xcm of S so that they\nhave norm 1. \u2662\n10.6 Key Steps of PCA in Practice\nIn the following, we will go through the individual steps of PCA using a\nrunning example, which is summarized in Figure 10.11. We are given a\ntwo-dimensional dataset (Figure 10.11(a)), and we want to use PCA to\nproject it onto a one-dimensional subspace.\n1. Mean subtraction We start by centering the data by computing the\nmean \u00b5 of the dataset and subtracting it from every single data point.\nThis ensures that the dataset has mean0 (Figure 10.11(b)). Mean sub-\ntraction is not strictly necessary but reduces the risk of numerical prob-\nlems.\n2. Standardization Divide the data points by the standard deviation\u03c3d\nof the dataset for every dimension d = 1, . . . , D. Now the data is unit\nfree, and it has variance 1 al"
  },
  {
    "vector_id": 1364,
    "chunk_id": "p342_c3",
    "page_number": 342,
    "text": "of numerical prob-\nlems.\n2. Standardization Divide the data points by the standard deviation\u03c3d\nof the dataset for every dimension d = 1, . . . , D. Now the data is unit\nfree, and it has variance 1 along each axis, which is indicated by the\ntwo arrows in Figure 10.11(c). This step completes thestandardizationstandardization\nof the data.\n3. Eigendecomposition of the covariance matrix Compute the data\ncovariance matrix and its eigenvalues and corresponding eigenvectors.\nSince the covariance matrix is symmetric, the spectral theorem (The-\norem 4.15) states that we can find an ONB of eigenvectors. In Fig-\nure 10.11(d), the eigenvectors are scaled by the magnitude of the cor-\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1365,
    "chunk_id": "p342_c4",
    "page_number": 342,
    "text": "In Fig-\nure 10.11(d), the eigenvectors are scaled by the magnitude of the cor-\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1366,
    "chunk_id": "p343_c0",
    "page_number": 343,
    "text": "10.6 Key Steps of PCA in Practice 337\nFigure 10.11 Steps\nof PCA. (a) Original\ndataset;\n(b) centering;\n(c) divide by\nstandard deviation;\n(d) eigendecomposi-\ntion; (e) projection;\n(f) mapping back to\noriginal data space.\n0 5\nx1\n\u22122.5\n0.0\n2.5\n5.0\nx2\n(a) Original dataset.\n0 5\nx1\n\u22122.5\n0.0\n2.5\n5.0\nx2\n (b) Step 1: Centering by sub-\ntracting the mean from each\ndata point.\n0 5\nx1\n\u22122.5\n0.0\n2.5\n5.0\nx2\n(c) Step 2: Dividing by the\nstandard deviation to make\nthe data unit free. Data has\nvariance 1 along each axis.\n0 5\nx1\n\u22122.5\n0.0\n2.5\n5.0\nx2\n(d) Step 3: Compute eigenval-\nues and eigenvectors (arrows)\nof the data covariance matrix\n(ellipse).\n0 5\nx1\n\u22122.5\n0.0\n2.5\n5.0\nx2\n(e) Step 4: Project data onto\nthe principal subspace.\n0 5\nx1\n\u22122.5\n0.0\n2.5\n5.0\nx2\n(f) Undo the standardization\nand move projected data back\ni"
  },
  {
    "vector_id": 1367,
    "chunk_id": "p343_c1",
    "page_number": 343,
    "text": "ata covariance matrix\n(ellipse).\n0 5\nx1\n\u22122.5\n0.0\n2.5\n5.0\nx2\n(e) Step 4: Project data onto\nthe principal subspace.\n0 5\nx1\n\u22122.5\n0.0\n2.5\n5.0\nx2\n(f) Undo the standardization\nand move projected data back\ninto the original data space\nfrom (a).\nresponding eigenvalue. The longer vector spans the principal subspace,\nwhich we denote by U. The data covariance matrix is represented by\nthe ellipse.\n4. Projection We can project any data pointx\u2217 \u2208 RD onto the principal\nsubspace: To get this right, we need to standardize x\u2217 using the mean\n\u00b5d and standard deviation \u03c3d of the training data in thedth dimension,\nrespectively , so that\nx(d)\n\u2217 \u2190 x(d)\n\u2217 \u2212 \u00b5d\n\u03c3d\n, d = 1, . . . , D , (10.58)\nwhere x(d)\n\u2217 is the dth component of x\u2217. We obtain the projection as\n\u02dcx\u2217 = BB\u22a4x\u2217 (10.59)\nwith coordinates\nz\u2217 = B\u22a4x\u2217 (10.60)"
  },
  {
    "vector_id": 1368,
    "chunk_id": "p343_c2",
    "page_number": 343,
    "text": "respectively , so that\nx(d)\n\u2217 \u2190 x(d)\n\u2217 \u2212 \u00b5d\n\u03c3d\n, d = 1, . . . , D , (10.58)\nwhere x(d)\n\u2217 is the dth component of x\u2217. We obtain the projection as\n\u02dcx\u2217 = BB\u22a4x\u2217 (10.59)\nwith coordinates\nz\u2217 = B\u22a4x\u2217 (10.60)\nwith respect to the basis of the principal subspace. Here, B is the ma-\ntrix that contains the eigenvectors that are associated with the largest\neigenvalues of the data covariance matrix as columns. PCA returns the\ncoordinates (10.60), not the projections x\u2217.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1369,
    "chunk_id": "p344_c0",
    "page_number": 344,
    "text": "338 Dimensionality Reduction with Principal Component Analysis\nHaving standardized our dataset, (10.59) only yields the projections in\nthe context of the standardized dataset. To obtain our projection in the\noriginal data space (i.e., before standardization), we need to undo the\nstandardization (10.58) and multiply by the standard deviation before\nadding the mean so that we obtain\n\u02dcx(d)\n\u2217 \u2190 \u02dcx(d)\n\u2217 \u03c3d + \u00b5d , d = 1, . . . , D . (10.61)\nFigure 10.11(f) illustrates the projection in the original data space.\nExample 10.4 (MNIST Digits: Reconstruction)\nIn the following, we will apply PCA to the MNIST digits dataset, which\ncontains 60,000 examples of handwritten digits 0 through 9. Each digit is\nan image of size28\u00d728, i.e., it contains784 pixels so that we can interpret\nevery image in this datas"
  },
  {
    "vector_id": 1370,
    "chunk_id": "p344_c1",
    "page_number": 344,
    "text": "ST digits dataset, which\ncontains 60,000 examples of handwritten digits 0 through 9. Each digit is\nan image of size28\u00d728, i.e., it contains784 pixels so that we can interpret\nevery image in this dataset as a vector x \u2208 R784. Examples of these digits\nare shown in Figure 10.3.\nFigure 10.12 Effect\nof increasing the\nnumber of principal\ncomponents on\nreconstruction.\nOriginal\nPCs: 1\nPCs: 10\nPCs: 100\nPCs: 500\nFor illustration purposes, we apply PCA to a subset of the MNIST digits,\nand we focus on the digit \u201c8\u201d. We used 5,389 training images of the digit\n\u201c8\u201d and determined the principal subspace as detailed in this chapter. We\nthen used the learned projection matrix to reconstruct a set of test im-\nages, which is illustrated in Figure 10.12. The first row of Figure 10.12\nshows a set of four origin"
  },
  {
    "vector_id": 1371,
    "chunk_id": "p344_c2",
    "page_number": 344,
    "text": "tailed in this chapter. We\nthen used the learned projection matrix to reconstruct a set of test im-\nages, which is illustrated in Figure 10.12. The first row of Figure 10.12\nshows a set of four original digits from the test set. The following rows\nshow reconstructions of exactly these digits when using a principal sub-\nspace of dimensions 1, 10, 100, and 500, respectively . We see that even\nwith a single-dimensional principal subspace we get a halfway decent re-\nconstruction of the original digits, which, however, is blurry and generic.\nWith an increasing number of principal components (PCs), the reconstruc-\ntions become sharper and more details are accounted for. With 500 prin-\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1372,
    "chunk_id": "p344_c3",
    "page_number": 344,
    "text": "the reconstruc-\ntions become sharper and more details are accounted for. With 500 prin-\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1373,
    "chunk_id": "p345_c0",
    "page_number": 345,
    "text": "10.7 Latent Variable Perspective 339\ncipal components, we effectively obtain a near-perfect reconstruction. If\nwe were to choose 784 PCs, we would recover the exact digit without any\ncompression loss.\nFigure 10.13 shows the average squared reconstruction error, which is\n1\nN\nNX\nn=1\n\u2225xn \u2212 \u02dcxn\u2225\n2\n=\nDX\ni=M+1\n\u03bbi , (10.62)\nas a function of the number M of principal components. We can see that\nthe importance of the principal components drops off rapidly , and only\nmarginal gains can be achieved by adding more PCs. This matches exactly\nour observation in Figure 10.5, where we discovered that the most of the\nvariance of the projected data is captured by only a few principal compo-\nnents. With about550 PCs, we can essentially fully reconstruct the training\ndata that contains the digit \u201c8\u201d (some pixe"
  },
  {
    "vector_id": 1374,
    "chunk_id": "p345_c1",
    "page_number": 345,
    "text": "of the\nvariance of the projected data is captured by only a few principal compo-\nnents. With about550 PCs, we can essentially fully reconstruct the training\ndata that contains the digit \u201c8\u201d (some pixels around the boundaries show\nno variation across the dataset as they are always black).\nFigure 10.13\nAverage squared\nreconstruction error\nas a function of the\nnumber of principal\ncomponents. The\naverage squared\nreconstruction error\nis the sum of the\neigenvalues in the\northogonal\ncomplement of the\nprincipal subspace.\n0 200 400 600 800\nNumber of PCs\n0\n100\n200\n300\n400\n500Average squared reconstruction error\n10.7 Latent Variable Perspective\nIn the previous sections, we derived PCA without any notion of a prob-\nabilistic model using the maximum-variance and the projection perspec-\ntives. On the on"
  },
  {
    "vector_id": 1375,
    "chunk_id": "p345_c2",
    "page_number": 345,
    "text": "on error\n10.7 Latent Variable Perspective\nIn the previous sections, we derived PCA without any notion of a prob-\nabilistic model using the maximum-variance and the projection perspec-\ntives. On the one hand, this approach may be appealing as it allows us to\nsidestep all the mathematical difficulties that come with probability the-\nory , but on the other hand, a probabilistic model would offer us more flex-\nibility and useful insights. More specifically , a probabilistic model would\nCome with a likelihood function, and we can explicitly deal with noisy\nobservations (which we did not even discuss earlier)\nAllow us to do Bayesian model comparison via the marginal likelihood\nas discussed in Section 8.6\nView PCA as a generative model, which allows us to simulate new data\n\u00a92024 M. P. Deisenroth,"
  },
  {
    "vector_id": 1376,
    "chunk_id": "p345_c3",
    "page_number": 345,
    "text": "s earlier)\nAllow us to do Bayesian model comparison via the marginal likelihood\nas discussed in Section 8.6\nView PCA as a generative model, which allows us to simulate new data\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1377,
    "chunk_id": "p346_c0",
    "page_number": 346,
    "text": "340 Dimensionality Reduction with Principal Component Analysis\nAllow us to make straightforward connections to related algorithms\nDeal with data dimensions that are missing at random by applying\nBayes\u2019 theorem\nGive us a notion of the novelty of a new data point\nGive us a principled way to extend the model, e.g., to a mixture of PCA\nmodels\nHave the PCA we derived in earlier sections as a special case\nAllow for a fully Bayesian treatment by marginalizing out the model\nparameters\nBy introducing a continuous-valued latent variable z \u2208 RM it is possible\nto phrase PCA as a probabilistic latent-variable model. Tipping and Bishop\n(1999) proposed this latent-variable model as probabilistic PCA (PPCA).probabilistic PCA\nPPCA PPCA addresses most of the aforementioned issues, and the PCA solution\nthat"
  },
  {
    "vector_id": 1378,
    "chunk_id": "p346_c1",
    "page_number": 346,
    "text": "ble model. Tipping and Bishop\n(1999) proposed this latent-variable model as probabilistic PCA (PPCA).probabilistic PCA\nPPCA PPCA addresses most of the aforementioned issues, and the PCA solution\nthat we obtained by maximizing the variance in the projected space or\nby minimizing the reconstruction error is obtained as the special case of\nmaximum likelihood estimation in a noise-free setting.\n10.7.1 Generative Process and Probabilistic Model\nIn PPCA, we explicitly write down the probabilistic model for linear di-\nmensionality reduction. For this we assume a continuous latent variable\nz \u2208 RM with a standard-normal prior p(z) = N\n\u0000\n0, I\n\u0001\nand a linear rela-\ntionship between the latent variables and the observed x data where\nx = Bz + \u00b5 + \u03f5 \u2208 RD , (10.63)\nwhere \u03f5 \u223c N\n\u0000\n0, \u03c32I\n\u0001\nis Gaussian obser"
  },
  {
    "vector_id": 1379,
    "chunk_id": "p346_c2",
    "page_number": 346,
    "text": "a standard-normal prior p(z) = N\n\u0000\n0, I\n\u0001\nand a linear rela-\ntionship between the latent variables and the observed x data where\nx = Bz + \u00b5 + \u03f5 \u2208 RD , (10.63)\nwhere \u03f5 \u223c N\n\u0000\n0, \u03c32I\n\u0001\nis Gaussian observation noise and B \u2208 RD\u00d7M\nand \u00b5 \u2208 RD describe the linear/affine mapping from latent to observed\nvariables. Therefore, PPCA links latent and observed variables via\np(x|z, B, \u00b5, \u03c32) = N\n\u0000\nx|Bz + \u00b5, \u03c32I\n\u0001\n. (10.64)\nOverall, PPCA induces the following generative process:\nzn \u223c N\n\u0000\nz |0, I\n\u0001\n(10.65)\nxn |zn \u223c N\n\u0000\nx|Bzn + \u00b5, \u03c32I\n\u0001\n(10.66)\nTo generate a data point that is typical given the model parameters, we\nfollow an ancestral sampling scheme: We first sample a latent variable znancestral sampling\nfrom p(z). Then we use zn in (10.64) to sample a data point conditioned\non the sampled zn, i.e., xn \u223c p"
  },
  {
    "vector_id": 1380,
    "chunk_id": "p346_c3",
    "page_number": 346,
    "text": ", we\nfollow an ancestral sampling scheme: We first sample a latent variable znancestral sampling\nfrom p(z). Then we use zn in (10.64) to sample a data point conditioned\non the sampled zn, i.e., xn \u223c p(x|zn, B, \u00b5, \u03c32).\nThis generative process allows us to write down the probabilistic model\n(i.e., the joint distribution of all random variables; see Section 8.4) as\np(x, z|B, \u00b5, \u03c32) = p(x|z, B, \u00b5, \u03c32)p(z) , (10.67)\nwhich immediately gives rise to the graphical model in Figure 10.14 using\nthe results from Section 8.5.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1381,
    "chunk_id": "p346_c4",
    "page_number": 346,
    "text": "ok.com."
  },
  {
    "vector_id": 1382,
    "chunk_id": "p347_c0",
    "page_number": 347,
    "text": "10.7 Latent Variable Perspective 341\nFigure 10.14\nGraphical model for\nprobabilistic PCA.\nThe observations xn\nexplicitly depend on\ncorresponding\nlatent variables\nzn \u223c N\n\u0000\n0, I\n\u0001\n. The\nmodel parameters\nB, \u00b5 and the\nlikelihood\nparameter \u03c3 are\nshared across the\ndataset.\nxn\nB\nzn\n\u03c3\n\u00b5\nn = 1, . . . , N\nRemark. Note the direction of the arrow that connects the latent variables\nz and the observed data x: The arrow points from z to x, which means\nthat the PPCA model assumes a lower-dimensional latent causez for high-\ndimensional observations x. In the end, we are obviously interested in\nfinding something out about z given some observations. To get there we\nwill apply Bayesian inference to \u201cinvert\u201d the arrow implicitly and go from\nobservations to latent variables. \u2662\nExample 10.5 (Generating New Data U"
  },
  {
    "vector_id": 1383,
    "chunk_id": "p347_c1",
    "page_number": 347,
    "text": "out about z given some observations. To get there we\nwill apply Bayesian inference to \u201cinvert\u201d the arrow implicitly and go from\nobservations to latent variables. \u2662\nExample 10.5 (Generating New Data Using Latent Variables)\nFigure 10.15\nGenerating new\nMNIST digits. The\nlatent variables z\ncan be used to\ngenerate new data\n\u02dcx = Bz. The closer\nwe stay to the\ntraining data, the\nmore realistic the\ngenerated data.\nFigure 10.15 shows the latent coordinates of the MNIST digits \u201c8\u201d found\nby PCA when using a two-dimensional principal subspace (blue dots). We\ncan query any vector z\u2217 in this latent space and generate an image \u02dcx\u2217 =\nBz\u2217 that resembles the digit \u201c8\u201d. We show eight of such generated images\nwith their corresponding latent space representation. Depending on where\nwe query the latent space, t"
  },
  {
    "vector_id": 1384,
    "chunk_id": "p347_c2",
    "page_number": 347,
    "text": "d generate an image \u02dcx\u2217 =\nBz\u2217 that resembles the digit \u201c8\u201d. We show eight of such generated images\nwith their corresponding latent space representation. Depending on where\nwe query the latent space, the generated images look different (shape,\nrotation, size, etc.). If we query away from the training data, we see more\nand more artifacts, e.g., the top-left and top-right digits. Note that the\nintrinsic dimensionality of these generated images is only two.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1385,
    "chunk_id": "p348_c0",
    "page_number": 348,
    "text": "342 Dimensionality Reduction with Principal Component Analysis\n10.7.2 Likelihood and Joint DistributionThe likelihood does\nnot depend on the\nlatent variables z.\nUsing the results from Chapter 6, we obtain the likelihood of this proba-\nbilistic model by integrating out the latent variable z (see Section 8.4.3)\nso that\np(x|B, \u00b5, \u03c32) =\nZ\np(x|z, B, \u00b5, \u03c32)p(z)dz (10.68a)\n=\nZ\nN\n\u0000\nx|Bz + \u00b5, \u03c32I\n\u0001\nN\n\u0000\nz |0, I\n\u0001\ndz . (10.68b)\nFrom Section 6.5, we know that the solution to this integral is a Gaussian\ndistribution with mean\nEx[x] = Ez[Bz + \u00b5] + E\u03f5[\u03f5] = \u00b5 (10.69)\nand with covariance matrix\nV[x] = Vz[Bz + \u00b5] + V\u03f5[\u03f5] = Vz[Bz] + \u03c32I (10.70a)\n= BVz[z]B\u22a4 + \u03c32I = BB\u22a4 + \u03c32I . (10.70b)\nThe likelihood in (10.68b) can be used for maximum likelihood or MAP\nestimation of the model parameters.\nRemark. We cannot us"
  },
  {
    "vector_id": 1386,
    "chunk_id": "p348_c1",
    "page_number": 348,
    "text": "\u00b5] + V\u03f5[\u03f5] = Vz[Bz] + \u03c32I (10.70a)\n= BVz[z]B\u22a4 + \u03c32I = BB\u22a4 + \u03c32I . (10.70b)\nThe likelihood in (10.68b) can be used for maximum likelihood or MAP\nestimation of the model parameters.\nRemark. We cannot use the conditional distribution in (10.64) for maxi-\nmum likelihood estimation as it still depends on the latent variables. The\nlikelihood function we require for maximum likelihood (or MAP) estima-\ntion should only be a function of the data x and the model parameters,\nbut must not depend on the latent variables. \u2662\nFrom Section 6.5, we know that a Gaussian random variable z and\na linear/affine transformation x = Bz of it are jointly Gaussian dis-\ntributed. We already know the marginals p(z) = N\n\u0000\nz |0, I\n\u0001\nand p(x) =\nN\n\u0000\nx|\u00b5, BB\u22a4 + \u03c32I\n\u0001\n. The missing cross-covariance is given as\nCov[x, z] = Co"
  },
  {
    "vector_id": 1387,
    "chunk_id": "p348_c2",
    "page_number": 348,
    "text": "sformation x = Bz of it are jointly Gaussian dis-\ntributed. We already know the marginals p(z) = N\n\u0000\nz |0, I\n\u0001\nand p(x) =\nN\n\u0000\nx|\u00b5, BB\u22a4 + \u03c32I\n\u0001\n. The missing cross-covariance is given as\nCov[x, z] = Covz[Bz + \u00b5] = B Covz[z, z] = B . (10.71)\nTherefore, the probabilistic model of PPCA, i.e., the joint distribution of\nlatent and observed random variables is explicitly given by\np(x, z |B, \u00b5, \u03c32) = N\n\u0012\u0014x\nz\n\u0015 \f\f\f\f\n\u0014\u00b5\n0\n\u0015\n,\n\u0014BB\u22a4 + \u03c32I B\nB\u22a4 I\n\u0015\u0013\n, (10.72)\nwith a mean vector of length D + M and a covariance matrix of size\n(D + M) \u00d7 (D + M).\n10.7.3 Posterior Distribution\nThe joint Gaussian distribution p(x, z |B, \u00b5, \u03c32) in (10.72) allows us to\ndetermine the posterior distribution p(z |x) immediately by applying the\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.co"
  },
  {
    "vector_id": 1388,
    "chunk_id": "p348_c3",
    "page_number": 348,
    "text": "(x, z |B, \u00b5, \u03c32) in (10.72) allows us to\ndetermine the posterior distribution p(z |x) immediately by applying the\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1389,
    "chunk_id": "p349_c0",
    "page_number": 349,
    "text": "10.8 Further Reading 343\nrules of Gaussian conditioning from Section 6.5.1. The posterior distribu-\ntion of the latent variable given an observation x is then\np(z |x) = N\n\u0000\nz |m, C\n\u0001\n, (10.73)\nm = B\u22a4(BB\u22a4 + \u03c32I)\u22121(x \u2212 \u00b5) , (10.74)\nC = I \u2212 B\u22a4(BB\u22a4 + \u03c32I)\u22121B . (10.75)\nNote that the posterior covariance does not depend on the observed data\nx. For a new observation x\u2217 in data space, we use (10.73) to determine\nthe posterior distribution of the corresponding latent variable z\u2217. The co-\nvariance matrix C allows us to assess how confident the embedding is. A\ncovariance matrix C with a small determinant (which measures volumes)\ntells us that the latent embedding z\u2217 is fairly certain. If we obtain a pos-\nterior distribution p(z\u2217 |x\u2217) with much variance, we may be faced with\nan outlier. However, we ca"
  },
  {
    "vector_id": 1390,
    "chunk_id": "p349_c1",
    "page_number": 349,
    "text": "(which measures volumes)\ntells us that the latent embedding z\u2217 is fairly certain. If we obtain a pos-\nterior distribution p(z\u2217 |x\u2217) with much variance, we may be faced with\nan outlier. However, we can explore this posterior distribution to under-\nstand what other data points x are plausible under this posterior. To do\nthis, we exploit the generative process underlying PPCA, which allows us\nto explore the posterior distribution on the latent variables by generating\nnew data that is plausible under this posterior:\n1. Sample a latent variable z\u2217 \u223c p(z |x\u2217) from the posterior distribution\nover the latent variables (10.73).\n2. Sample a reconstructed vector \u02dcx\u2217 \u223c p(x|z\u2217, B, \u00b5, \u03c32) from (10.64).\nIf we repeat this process many times, we can explore the posterior dis-\ntribution (10.73) on the late"
  },
  {
    "vector_id": 1391,
    "chunk_id": "p349_c2",
    "page_number": 349,
    "text": "e latent variables (10.73).\n2. Sample a reconstructed vector \u02dcx\u2217 \u223c p(x|z\u2217, B, \u00b5, \u03c32) from (10.64).\nIf we repeat this process many times, we can explore the posterior dis-\ntribution (10.73) on the latent variables z\u2217 and its implications on the\nobserved data. The sampling process effectively hypothesizes data, which\nis plausible under the posterior distribution.\n10.8 Further Reading\nWe derived PCA from two perspectives: (a) maximizing the variance in the\nprojected space; (b) minimizing the average reconstruction error. How-\never, PCA can also be interpreted from different perspectives. Let us recap\nwhat we have done: We took high-dimensional data x \u2208 RD and used\na matrix B\u22a4 to find a lower-dimensional representation z \u2208 RM . The\ncolumns of B are the eigenvectors of the data covariance matri"
  },
  {
    "vector_id": 1392,
    "chunk_id": "p349_c3",
    "page_number": 349,
    "text": "ecap\nwhat we have done: We took high-dimensional data x \u2208 RD and used\na matrix B\u22a4 to find a lower-dimensional representation z \u2208 RM . The\ncolumns of B are the eigenvectors of the data covariance matrixS that are\nassociated with the largest eigenvalues. Once we have a low-dimensional\nrepresentation z, we can get a high-dimensional version of it (in the orig-\ninal data space) as x \u2248 \u02dcx = Bz = BB\u22a4x \u2208 RD, where BB\u22a4 is a\nprojection matrix.\nWe can also think of PCA as a linear auto-encoder as illustrated in Fig- auto-encoder\nure 10.16. An auto-encoder encodes the dataxn \u2208 RD to a code zn \u2208 RM code\nand decodes it to a \u02dcxn similar to xn. The mapping from the data to the\ncode is called theencoder, and the mapping from the code back to the orig- encoder\ninal data space is called thedecoder. If we co"
  },
  {
    "vector_id": 1393,
    "chunk_id": "p349_c4",
    "page_number": 349,
    "text": "nd decodes it to a \u02dcxn similar to xn. The mapping from the data to the\ncode is called theencoder, and the mapping from the code back to the orig- encoder\ninal data space is called thedecoder. If we consider linear mappings where decoder\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1394,
    "chunk_id": "p350_c0",
    "page_number": 350,
    "text": "344 Dimensionality Reduction with Principal Component Analysis\nFigure 10.16 PCA\ncan be viewed as a\nlinear auto-encoder.\nIt encodes the\nhigh-dimensional\ndata x into a\nlower-dimensional\nrepresentation\n(code) z \u2208 RM and\ndecodes z using a\ndecoder. The\ndecoded vector \u02dcx is\nthe orthogonal\nprojection of the\noriginal data x onto\nthe M-dimensional\nprincipal subspace.\nB\u22a4\nx \u02dcxz B\nEncoder Decoder\nOriginal\nCodeRD RD\nRM\nthe code is given by zn = B\u22a4xn \u2208 RM and we are interested in minimiz-\ning the average squared error between the data xn and its reconstruction\n\u02dcxn = Bzn, n = 1, . . . , N, we obtain\n1\nN\nNX\nn=1\n\u2225xn \u2212 \u02dcxn\u22252 = 1\nN\nNX\nn=1\n\r\r\rxn \u2212 BB\u22a4xn\n\r\r\r\n2\n. (10.76)\nThis means we end up with the same objective function as in (10.29) that\nwe discussed in Section 10.3 so that we obtain the PCA solution when"
  },
  {
    "vector_id": 1395,
    "chunk_id": "p350_c1",
    "page_number": 350,
    "text": "=1\n\u2225xn \u2212 \u02dcxn\u22252 = 1\nN\nNX\nn=1\n\r\r\rxn \u2212 BB\u22a4xn\n\r\r\r\n2\n. (10.76)\nThis means we end up with the same objective function as in (10.29) that\nwe discussed in Section 10.3 so that we obtain the PCA solution when we\nminimize the squared auto-encoding loss. If we replace the linear map-\nping of PCA with a nonlinear mapping, we get a nonlinear auto-encoder.\nA prominent example of this is a deep auto-encoder where the linear func-\ntions are replaced with deep neural networks. In this context, the encoder\nis also known as a recognition network or inference network, whereas therecognition network\ninference network decoder is also called a generator.\ngenerator Another interpretation of PCA is related to information theory . We can\nthink of the code as a smaller or compressed version of the original data\npoin"
  },
  {
    "vector_id": 1396,
    "chunk_id": "p350_c2",
    "page_number": 350,
    "text": "ork decoder is also called a generator.\ngenerator Another interpretation of PCA is related to information theory . We can\nthink of the code as a smaller or compressed version of the original data\npoint. When we reconstruct our original data using the code, we do not\nget the exact data point back, but a slightly distorted or noisy version\nof it. This means that our compression is \u201clossy\u201d. Intuitively , we wantThe code is a\ncompressed version\nof the original data.\nto maximize the correlation between the original data and the lower-\ndimensional code. More formally , this is related to the mutual information.\nWe would then get the same solution to PCA we discussed in Section 10.3\nby maximizing the mutual information, a core concept in information the-\nory (MacKay, 2003).\nIn our discussion on P"
  },
  {
    "vector_id": 1397,
    "chunk_id": "p350_c3",
    "page_number": 350,
    "text": "information.\nWe would then get the same solution to PCA we discussed in Section 10.3\nby maximizing the mutual information, a core concept in information the-\nory (MacKay, 2003).\nIn our discussion on PPCA, we assumed that the parameters of the\nmodel, i.e., B, \u00b5, and the likelihood parameter \u03c32, are known. Tipping\nand Bishop (1999) describe how to derive maximum likelihood estimates\nfor these parameters in the PPCA setting (note that we use a different\nnotation in this chapter). The maximum likelihood parameters, when pro-\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1398,
    "chunk_id": "p350_c4",
    "page_number": 350,
    "text": "//mml-book.com."
  },
  {
    "vector_id": 1399,
    "chunk_id": "p351_c0",
    "page_number": 351,
    "text": "10.8 Further Reading 345\njecting D-dimensional data onto an M-dimensional subspace, are\n\u00b5ML = 1\nN\nNX\nn=1\nxn , (10.77)\nBML = T(\u039b \u2212 \u03c32I)\n1\n2 R , (10.78)\n\u03c32\nML = 1\nD \u2212 M\nDX\nj=M+1\n\u03bbj , (10.79)\nwhere T \u2208 RD\u00d7M contains M eigenvectors of the data covariance matrix, The matrix \u039b \u2212 \u03c32I\nin (10.78) is\nguaranteed to be\npositive semidefinite\nas the smallest\neigenvalue of the\ndata covariance\nmatrix is bounded\nfrom below by the\nnoise variance \u03c32.\n\u039b = diag(\u03bb1, . . . , \u03bbM ) \u2208 RM\u00d7M is a diagonal matrix with the eigenvalues\nassociated with the principal axes on its diagonal, and R \u2208 RM\u00d7M is\nan arbitrary orthogonal matrix. The maximum likelihood solution BML is\nunique up to an arbitrary orthogonal transformation, e.g., we can right-\nmultiply BML with any rotation matrix R so that (10.78) essentially is a\nsing"
  },
  {
    "vector_id": 1400,
    "chunk_id": "p351_c1",
    "page_number": 351,
    "text": "al matrix. The maximum likelihood solution BML is\nunique up to an arbitrary orthogonal transformation, e.g., we can right-\nmultiply BML with any rotation matrix R so that (10.78) essentially is a\nsingular value decomposition (see Section 4.5). An outline of the proof is\ngiven by Tipping and Bishop (1999).\nThe maximum likelihood estimate for \u00b5 given in (10.77) is the sample\nmean of the data. The maximum likelihood estimator for the observation\nnoise variance \u03c32 given in (10.79) is the average variance in the orthog-\nonal complement of the principal subspace, i.e., the average leftover vari-\nance that we cannot capture with the first M principal components is\ntreated as observation noise.\nIn the noise-free limit where \u03c3 \u2192 0, PPCA and PCA provide identical\nsolutions: Since the data covariance"
  },
  {
    "vector_id": 1401,
    "chunk_id": "p351_c2",
    "page_number": 351,
    "text": "e that we cannot capture with the first M principal components is\ntreated as observation noise.\nIn the noise-free limit where \u03c3 \u2192 0, PPCA and PCA provide identical\nsolutions: Since the data covariance matrix S is symmetric, it can be di-\nagonalized (see Section 4.4), i.e., there exists a matrix T of eigenvectors\nof S so that\nS = T\u039bT\u22121 . (10.80)\nIn the PPCA model, the data covariance matrix is the covariance matrix of\nthe Gaussian likelihoodp(x|B, \u00b5, \u03c32), which isBB\u22a4+\u03c32I, see (10.70b).\nFor \u03c3 \u2192 0, we obtain BB\u22a4 so that this data covariance must equal the\nPCA data covariance (and its factorization given in (10.80)) so that\nCov[X] = T\u039bT\u22121 = BB\u22a4 \u21d0 \u21d2B = T\u039b\n1\n2 R , (10.81)\ni.e., we obtain the maximum likelihood estimate in (10.78) for \u03c3 = 0 .\nFrom (10.78) and (10.80), it becomes clear that (P)PCA"
  },
  {
    "vector_id": 1402,
    "chunk_id": "p351_c3",
    "page_number": 351,
    "text": "n given in (10.80)) so that\nCov[X] = T\u039bT\u22121 = BB\u22a4 \u21d0 \u21d2B = T\u039b\n1\n2 R , (10.81)\ni.e., we obtain the maximum likelihood estimate in (10.78) for \u03c3 = 0 .\nFrom (10.78) and (10.80), it becomes clear that (P)PCA performs a de-\ncomposition of the data covariance matrix.\nIn a streaming setting, where data arrives sequentially , it is recom-\nmended to use the iterative expectation maximization (EM) algorithm for\nmaximum likelihood estimation (Roweis, 1998).\nTo determine the dimensionality of the latent variables (the length of\nthe code, the dimensionality of the lower-dimensional subspace onto which\nwe project the data), Gavish and Donoho (2014) suggest the heuristic\nthat, if we can estimate the noise variance \u03c32 of the data, we should\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambri"
  },
  {
    "vector_id": 1403,
    "chunk_id": "p351_c4",
    "page_number": 351,
    "text": "ect the data), Gavish and Donoho (2014) suggest the heuristic\nthat, if we can estimate the noise variance \u03c32 of the data, we should\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1404,
    "chunk_id": "p352_c0",
    "page_number": 352,
    "text": "346 Dimensionality Reduction with Principal Component Analysis\ndiscard all singular values smaller than 4\u03c3\n\u221a\nD\u221a\n3 . Alternatively , we can use\n(nested) cross-validation (Section 8.6.1) or Bayesian model selection cri-\nteria (discussed in Section 8.6.2) to determine a good estimate of the\nintrinsic dimensionality of the data (Minka, 2001b).\nSimilar to our discussion on linear regression in Chapter 9, we can place\na prior distribution on the parameters of the model and integrate them\nout. By doing so, we (a) avoid point estimates of the parameters and the\nissues that come with these point estimates (see Section 8.6) and (b) al-\nlow for an automatic selection of the appropriate dimensionalityM of the\nlatent space. In this Bayesian PCA, which was proposed by Bishop (1999),Bayesian PCA\na prior"
  },
  {
    "vector_id": 1405,
    "chunk_id": "p352_c1",
    "page_number": 352,
    "text": "tes (see Section 8.6) and (b) al-\nlow for an automatic selection of the appropriate dimensionalityM of the\nlatent space. In this Bayesian PCA, which was proposed by Bishop (1999),Bayesian PCA\na prior p(\u00b5, B, \u03c32) is placed on the model parameters. The generative\nprocess allows us to integrate the model parameters out instead of condi-\ntioning on them, which addresses overfitting issues. Since this integration\nis analytically intractable, Bishop (1999) proposes to use approximate in-\nference methods, such as MCMC or variational inference. We refer to the\nwork by Gilks et al. (1996) and Blei et al. (2017) for more details on these\napproximate inference techniques.\nIn PPCA, we considered the linear model p(xn |zn) = N\n\u0000\nxn |Bzn +\n\u00b5, \u03c32I\n\u0001\nwith prior p(zn) = N\n\u0000\n0, I\n\u0001\n, where all observation d"
  },
  {
    "vector_id": 1406,
    "chunk_id": "p352_c2",
    "page_number": 352,
    "text": "l. (2017) for more details on these\napproximate inference techniques.\nIn PPCA, we considered the linear model p(xn |zn) = N\n\u0000\nxn |Bzn +\n\u00b5, \u03c32I\n\u0001\nwith prior p(zn) = N\n\u0000\n0, I\n\u0001\n, where all observation dimensions\nare affected by the same amount of noise. If we allow each observation\ndimension d to have a different variance \u03c32\nd, we obtain factor analysisfactor analysis\n(FA) (Spearman, 1904; Bartholomew et al., 2011). This means that FA\ngives the likelihood some more flexibility than PPCA, but still forces the\ndata to be explained by the model parameters B, \u00b5.However, FA noAn overly flexible\nlikelihood would be\nable to explain more\nthan just the noise.\nlonger allows for a closed-form maximum likelihood solution so that we\nneed to use an iterative scheme, such as the expectation maximization\nal"
  },
  {
    "vector_id": 1407,
    "chunk_id": "p352_c3",
    "page_number": 352,
    "text": "ihood would be\nable to explain more\nthan just the noise.\nlonger allows for a closed-form maximum likelihood solution so that we\nneed to use an iterative scheme, such as the expectation maximization\nalgorithm, to estimate the model parameters. While in PPCA all station-\nary points are global optima, this no longer holds for FA. Compared to\nPPCA, FA does not change if we scale the data, but it does return different\nsolutions if we rotate the data.\nAn algorithm that is also closely related to PCA is independent com-independent\ncomponent analysis ponent analysis (ICA (Hyvarinen et al., 2001)). Starting again with the\nICA latent-variable perspective p(xn |zn) = N\n\u0000\nxn |Bzn + \u00b5, \u03c32I\n\u0001\nwe now\nchange the prior on zn to non-Gaussian distributions. ICA can be used\nfor blind-source separation. Imagin"
  },
  {
    "vector_id": 1408,
    "chunk_id": "p352_c4",
    "page_number": 352,
    "text": "rting again with the\nICA latent-variable perspective p(xn |zn) = N\n\u0000\nxn |Bzn + \u00b5, \u03c32I\n\u0001\nwe now\nchange the prior on zn to non-Gaussian distributions. ICA can be used\nfor blind-source separation. Imagine you are in a busy train station withblind-source\nseparation many people talking. Your ears play the role of microphones, and they\nlinearly mix different speech signals in the train station. The goal of blind-\nsource separation is to identify the constituent parts of the mixed signals.\nAs discussed previously in the context of maximum likelihood estimation\nfor PPCA, the original PCA solution is invariant to any rotation. Therefore,\nPCA can identify the best lower-dimensional subspace in which the sig-\nnals live, but not the signals themselves (Murphy, 2012). ICA addresses\nthis issue by modify"
  },
  {
    "vector_id": 1409,
    "chunk_id": "p352_c5",
    "page_number": 352,
    "text": "nvariant to any rotation. Therefore,\nPCA can identify the best lower-dimensional subspace in which the sig-\nnals live, but not the signals themselves (Murphy, 2012). ICA addresses\nthis issue by modifying the prior distribution p(z) on the latent sources\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1410,
    "chunk_id": "p353_c0",
    "page_number": 353,
    "text": "10.8 Further Reading 347\nto require non-Gaussian priors p(z). We refer to the books by Hyvarinen\net al. (2001) and Murphy (2012) for more details on ICA.\nPCA, factor analysis, and ICA are three examples for dimensionality re-\nduction with linear models. Cunningham and Ghahramani (2015) provide\na broader survey of linear dimensionality reduction.\nThe (P)PCA model we discussed here allows for several important ex-\ntensions. In Section 10.5, we explained how to do PCA when the in-\nput dimensionality D is significantly greater than the number N of data\npoints. By exploiting the insight that PCA can be performed by computing\n(many) inner products, this idea can be pushed to the extreme by consid-\nering infinite-dimensional features. The kernel trick is the basis of kernel kernel trick\nkernel PC"
  },
  {
    "vector_id": 1411,
    "chunk_id": "p353_c1",
    "page_number": 353,
    "text": "n be performed by computing\n(many) inner products, this idea can be pushed to the extreme by consid-\nering infinite-dimensional features. The kernel trick is the basis of kernel kernel trick\nkernel PCAPCA and allows us to implicitly compute inner products between infinite-\ndimensional features (Sch\u00a8olkopf et al., 1998; Sch\u00a8olkopf and Smola, 2002).\nThere are nonlinear dimensionality reduction techniques that are de-\nrived from PCA (Burges (2010) provides a good overview). The auto-\nencoder perspective of PCA that we discussed previously in this section\ncan be used to render PCA as a special case of a deep auto-encoder. In the deep auto-encoder\ndeep auto-encoder, both the encoder and the decoder are represented by\nmultilayer feedforward neural networks, which themselves are nonlinear\nmapping"
  },
  {
    "vector_id": 1412,
    "chunk_id": "p353_c2",
    "page_number": 353,
    "text": "e of a deep auto-encoder. In the deep auto-encoder\ndeep auto-encoder, both the encoder and the decoder are represented by\nmultilayer feedforward neural networks, which themselves are nonlinear\nmappings. If we set the activation functions in these neural networks to be\nthe identity , the model becomes equivalent to PCA. A different approach to\nnonlinear dimensionality reduction is the Gaussian process latent-variable Gaussian process\nlatent-variable\nmodel\nmodel (GP-LVM) proposed by Lawrence (2005). The GP-LVM starts off with\nGP-LVMthe latent-variable perspective that we used to derive PPCA and replaces\nthe linear relationship between the latent variablesz and the observations\nx with a Gaussian process (GP). Instead of estimating the parameters of\nthe mapping (as we do in PPCA), the GP-LVM m"
  },
  {
    "vector_id": 1413,
    "chunk_id": "p353_c3",
    "page_number": 353,
    "text": "replaces\nthe linear relationship between the latent variablesz and the observations\nx with a Gaussian process (GP). Instead of estimating the parameters of\nthe mapping (as we do in PPCA), the GP-LVM marginalizes out the model\nparameters and makes point estimates of the latent variables z. Similar\nto Bayesian PCA, the Bayesian GP-LVM proposed by Titsias and Lawrence Bayesian GP-LVM\n(2010) maintains a distribution on the latent variablesz and uses approx-\nimate inference to integrate them out as well.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1414,
    "chunk_id": "p353_c4",
    "page_number": 353,
    "text": "."
  },
  {
    "vector_id": 1415,
    "chunk_id": "p354_c0",
    "page_number": 354,
    "text": "11\nDensity Estimation with Gaussian Mixture\nModels\nIn earlier chapters, we covered already two fundamental problems in\nmachine learning: regression (Chapter 9) and dimensionality reduction\n(Chapter 10). In this chapter, we will have a look at a third pillar of ma-\nchine learning: density estimation. On our journey , we introduce impor-\ntant concepts, such as the expectation maximization (EM) algorithm and\na latent variable perspective of density estimation with mixture models.\nWhen we apply machine learning to data we often aim to represent\ndata in some way . A straightforward way is to take the data points them-\nselves as the representation of the data; see Figure 11.1 for an example.\nHowever, this approach may be unhelpful if the dataset is huge or if we\nare interested in representing ch"
  },
  {
    "vector_id": 1416,
    "chunk_id": "p354_c1",
    "page_number": 354,
    "text": "he data points them-\nselves as the representation of the data; see Figure 11.1 for an example.\nHowever, this approach may be unhelpful if the dataset is huge or if we\nare interested in representing characteristics of the data. In density esti-\nmation, we represent the data compactly using a density from a paramet-\nric family , e.g., a Gaussian or Beta distribution. For example, we may be\nlooking for the mean and variance of a dataset in order to represent the\ndata compactly using a Gaussian distribution. The mean and variance can\nbe found using tools we discussed in Section 8.3: maximum likelihood or\nmaximum a posteriori estimation. We can then use the mean and variance\nof this Gaussian to represent the distribution underlying the data, i.e., we\nthink of the dataset to be a typical realiza"
  },
  {
    "vector_id": 1417,
    "chunk_id": "p354_c2",
    "page_number": 354,
    "text": "hood or\nmaximum a posteriori estimation. We can then use the mean and variance\nof this Gaussian to represent the distribution underlying the data, i.e., we\nthink of the dataset to be a typical realization from this distribution if we\nwere to sample from it.\nFigure 11.1\nTwo-dimensional\ndataset that cannot\nbe meaningfully\nrepresented by a\nGaussian.\n\u22125 0 5\nx1\n\u22124\n\u22122\n0\n2\n4\nx2\n348\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 1418,
    "chunk_id": "p354_c3",
    "page_number": 354,
    "text": "ly . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 1419,
    "chunk_id": "p355_c0",
    "page_number": 355,
    "text": "11.1 Gaussian Mixture Model 349\nIn practice, the Gaussian (or similarly all other distributions we encoun-\ntered so far) have limited modeling capabilities. For example, a Gaussian\napproximation of the density that generated the data in Figure 11.1 would\nbe a poor approximation. In the following, we will look at a more ex-\npressive family of distributions, which we can use for density estimation:\nmixture models. mixture model\nMixture models can be used to describe a distribution p(x) by a convex\ncombination of K simple (base) distributions\np(x) =\nKX\nk=1\n\u03c0kpk(x) (11.1)\n0 \u2a7d \u03c0k \u2a7d 1 ,\nKX\nk=1\n\u03c0k = 1 , (11.2)\nwhere the components pk are members of a family of basic distributions,\ne.g., Gaussians, Bernoullis, or Gammas, and the \u03c0k are mixture weights. mixture weight\nMixture models are more expres"
  },
  {
    "vector_id": 1420,
    "chunk_id": "p355_c1",
    "page_number": 355,
    "text": "1 , (11.2)\nwhere the components pk are members of a family of basic distributions,\ne.g., Gaussians, Bernoullis, or Gammas, and the \u03c0k are mixture weights. mixture weight\nMixture models are more expressive than the corresponding base distri-\nbutions because they allow for multimodal data representations, i.e., they\ncan describe datasets with multiple \u201cclusters\u201d, such as the example in Fig-\nure 11.1.\nWe will focus on Gaussian mixture models (GMMs), where the basic\ndistributions are Gaussians. For a given dataset, we aim to maximize the\nlikelihood of the model parameters to train the GMM. For this purpose,\nwe will use results from Chapter 5, Chapter 6, and Section 7.2. However,\nunlike other applications we discussed earlier (linear regression or PCA),\nwe will not find a closed-form maximum li"
  },
  {
    "vector_id": 1421,
    "chunk_id": "p355_c2",
    "page_number": 355,
    "text": "s purpose,\nwe will use results from Chapter 5, Chapter 6, and Section 7.2. However,\nunlike other applications we discussed earlier (linear regression or PCA),\nwe will not find a closed-form maximum likelihood solution. Instead, we\nwill arrive at a set of dependent simultaneous equations, which we can\nonly solve iteratively .\n11.1 Gaussian Mixture Model\nA Gaussian mixture model is a density model where we combine a finite Gaussian mixture\nmodelnumber of K Gaussian distributions N\n\u0000\nx|\u00b5k, \u03a3k\n\u0001\nso that\np(x|\u03b8) =\nKX\nk=1\n\u03c0kN\n\u0000\nx|\u00b5k, \u03a3k\n\u0001\n(11.3)\n0 \u2a7d \u03c0k \u2a7d 1 ,\nKX\nk=1\n\u03c0k = 1 , (11.4)\nwhere we defined \u03b8 := {\u00b5k, \u03a3k, \u03c0k : k = 1, . . . , K} as the collection of\nall parameters of the model. This convex combination of Gaussian distri-\nbution gives us significantly more flexibility for modeling complex den"
  },
  {
    "vector_id": 1422,
    "chunk_id": "p355_c3",
    "page_number": 355,
    "text": ":= {\u00b5k, \u03a3k, \u03c0k : k = 1, . . . , K} as the collection of\nall parameters of the model. This convex combination of Gaussian distri-\nbution gives us significantly more flexibility for modeling complex densi-\nties than a simple Gaussian distribution (which we recover from (11.3) for\nK = 1). An illustration is given in Figure 11.2, displaying the weighted\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1423,
    "chunk_id": "p356_c0",
    "page_number": 356,
    "text": "350 Density Estimation with Gaussian Mixture Models\nFigure 11.2\nGaussian mixture\nmodel. The\nGaussian mixture\ndistribution (black)\nis composed of a\nconvex combination\nof Gaussian\ndistributions and is\nmore expressive\nthan any individual\ncomponent. Dashed\nlines represent the\nweighted Gaussian\ncomponents.\n\u22124 \u22122 0 2 4 6 8\nx\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\np(x)\nComponent 1\nComponent 2\nComponent 3\nGMM density\ncomponents and the mixture density , which is given as\np(x |\u03b8) = 0.5N\n\u0000\nx | \u22122, 1\n2\n\u0001\n+ 0.2N\n\u0000\nx |1, 2\n\u0001\n+ 0.3N\n\u0000\nx |4, 1\n\u0001\n. (11.5)\n11.2 Parameter Learning via Maximum Likelihood\nAssume we are given a dataset X = {x1, . . . ,xN }, where xn, n =\n1, . . . , N, are drawn i.i.d. from an unknown distribution p(x). Our ob-\njective is to find a good approximation/representation of this unknown"
  },
  {
    "vector_id": 1424,
    "chunk_id": "p356_c1",
    "page_number": 356,
    "text": "re given a dataset X = {x1, . . . ,xN }, where xn, n =\n1, . . . , N, are drawn i.i.d. from an unknown distribution p(x). Our ob-\njective is to find a good approximation/representation of this unknown\ndistribution p(x) by means of a GMM with K mixture components. The\nparameters of the GMM are the K means \u00b5k, the covariances \u03a3k, and\nmixture weights \u03c0k. We summarize all these free parameters in \u03b8 :=\n{\u03c0k, \u00b5k, \u03a3k : k = 1, . . . , K}.\nExample 11.1 (Initial Setting)\nFigure 11.3 Initial\nsetting: GMM\n(black) with\nmixture three\nmixture components\n(dashed) and seven\ndata points (discs).\n\u22125 0 5 10 15\nx\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\np(x)\n\u03c01N(x|\u00b51, \u03c32\n1)\n\u03c02N(x|\u00b52, \u03c32\n2)\n\u03c03N(x|\u00b53, \u03c32\n3)\nGMM density\nThroughout this chapter, we will have a simple running example that\nhelps us illustrate and visualize"
  },
  {
    "vector_id": 1425,
    "chunk_id": "p356_c2",
    "page_number": 356,
    "text": "00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\np(x)\n\u03c01N(x|\u00b51, \u03c32\n1)\n\u03c02N(x|\u00b52, \u03c32\n2)\n\u03c03N(x|\u00b53, \u03c32\n3)\nGMM density\nThroughout this chapter, we will have a simple running example that\nhelps us illustrate and visualize important concepts.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1426,
    "chunk_id": "p357_c0",
    "page_number": 357,
    "text": "11.2 Parameter Learning via Maximum Likelihood 351\nWe consider a one-dimensional dataset X = {\u22123, \u22122.5, \u22121, 0, 2, 4, 5}\nconsisting of seven data points and wish to find a GMM with K = 3\ncomponents that models the density of the data. We initialize the mixture\ncomponents as\np1(x) = N\n\u0000\nx | \u22124, 1\n\u0001\n(11.6)\np2(x) = N\n\u0000\nx |0, 0.2\n\u0001\n(11.7)\np3(x) = N\n\u0000\nx |8, 3\n\u0001\n(11.8)\nand assign them equal weights \u03c01 = \u03c02 = \u03c03 = 1\n3 . The corresponding\nmodel (and the data points) are shown in Figure 11.3.\nIn the following, we detail how to obtain a maximum likelihood esti-\nmate \u03b8ML of the model parameters \u03b8. We start by writing down the like-\nlihood, i.e., the predictive distribution of the training data given the pa-\nrameters. We exploit our i.i.d. assumption, which leads to the factorized\nlikelihood\np(X |\u03b8) ="
  },
  {
    "vector_id": 1427,
    "chunk_id": "p357_c1",
    "page_number": 357,
    "text": "t by writing down the like-\nlihood, i.e., the predictive distribution of the training data given the pa-\nrameters. We exploit our i.i.d. assumption, which leads to the factorized\nlikelihood\np(X |\u03b8) =\nNY\nn=1\np(xn |\u03b8) , p (xn |\u03b8) =\nKX\nk=1\n\u03c0kN\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\n, (11.9)\nwhere every individual likelihood term p(xn |\u03b8) is a Gaussian mixture\ndensity . Then we obtain the log-likelihood as\nlog p(X |\u03b8) =\nNX\nn=1\nlog p(xn |\u03b8) =\nNX\nn=1\nlog\nKX\nk=1\n\u03c0kN\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\n| {z }\n=:L\n. (11.10)\nWe aim to find parameters\u03b8\u2217\nML that maximize the log-likelihoodL defined\nin (11.10). Our \u201cnormal\u201d procedure would be to compute the gradient\ndL/d\u03b8 of the log-likelihood with respect to the model parameters \u03b8, set\nit to 0, and solve for \u03b8. However, unlike our previous examples for max-\nimum likelihood estimation (e.g., when"
  },
  {
    "vector_id": 1428,
    "chunk_id": "p357_c2",
    "page_number": 357,
    "text": "ute the gradient\ndL/d\u03b8 of the log-likelihood with respect to the model parameters \u03b8, set\nit to 0, and solve for \u03b8. However, unlike our previous examples for max-\nimum likelihood estimation (e.g., when we discussed linear regression in\nSection 9.2), we cannot obtain a closed-form solution. However, we can\nexploit an iterative scheme to find good model parameters\u03b8ML, which will\nturn out to be the EM algorithm for GMMs. The key idea is to update one\nmodel parameter at a time while keeping the others fixed.\nRemark. If we were to consider a single Gaussian as the desired density ,\nthe sum over k in (11.10) vanishes, and the log can be applied directly to\nthe Gaussian component, such that we get\nlog N\n\u0000\nx|\u00b5, \u03a3\n\u0001\n= \u2212D\n2 log(2\u03c0) \u2212 1\n2 log det(\u03a3) \u2212 1\n2 (x \u2212 \u00b5)\u22a4\u03a3\u22121(x \u2212 \u00b5).\n(11.11)\nThis simple form a"
  },
  {
    "vector_id": 1429,
    "chunk_id": "p357_c3",
    "page_number": 357,
    "text": "(11.10) vanishes, and the log can be applied directly to\nthe Gaussian component, such that we get\nlog N\n\u0000\nx|\u00b5, \u03a3\n\u0001\n= \u2212D\n2 log(2\u03c0) \u2212 1\n2 log det(\u03a3) \u2212 1\n2 (x \u2212 \u00b5)\u22a4\u03a3\u22121(x \u2212 \u00b5).\n(11.11)\nThis simple form allows us to find closed-form maximum likelihood esti-\nmates of \u00b5 and \u03a3, as discussed in Chapter 8. In (11.10), we cannot move\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1430,
    "chunk_id": "p358_c0",
    "page_number": 358,
    "text": "352 Density Estimation with Gaussian Mixture Models\nthe log into the sum over k so that we cannot obtain a simple closed-form\nmaximum likelihood solution. \u2662\nAny local optimum of a function exhibits the property that its gradi-\nent with respect to the parameters must vanish (necessary condition); see\nChapter 7. In our case, we obtain the following necessary conditions when\nwe optimize the log-likelihood in (11.10) with respect to the GMM param-\neters \u00b5k, \u03a3k, \u03c0k:\n\u2202L\n\u2202\u00b5k\n= 0\u22a4 \u21d0 \u21d2\nNX\nn=1\n\u2202 log p(xn |\u03b8)\n\u2202\u00b5k\n= 0\u22a4 , (11.12)\n\u2202L\n\u2202\u03a3k\n= 0 \u21d0 \u21d2\nNX\nn=1\n\u2202 log p(xn |\u03b8)\n\u2202\u03a3k\n= 0 , (11.13)\n\u2202L\n\u2202\u03c0k\n= 0 \u21d0 \u21d2\nNX\nn=1\n\u2202 log p(xn |\u03b8)\n\u2202\u03c0k\n= 0 . (11.14)\nFor all three necessary conditions, by applying the chain rule (see Sec-\ntion 5.2.2), we require partial derivatives of the form\n\u2202 log p(xn |\u03b8)\n\u2202\u03b8 = 1\np(xn |\u03b8)\n\u2202p(xn |"
  },
  {
    "vector_id": 1431,
    "chunk_id": "p358_c1",
    "page_number": 358,
    "text": "\u2202 log p(xn |\u03b8)\n\u2202\u03c0k\n= 0 . (11.14)\nFor all three necessary conditions, by applying the chain rule (see Sec-\ntion 5.2.2), we require partial derivatives of the form\n\u2202 log p(xn |\u03b8)\n\u2202\u03b8 = 1\np(xn |\u03b8)\n\u2202p(xn |\u03b8)\n\u2202\u03b8 , (11.15)\nwhere \u03b8 = {\u00b5k, \u03a3k, \u03c0k, k= 1, . . . , K} are the model parameters and\n1\np(xn |\u03b8) = 1\nPK\nj=1 \u03c0jN\n\u0000\nxn |\u00b5j, \u03a3j\n\u0001 . (11.16)\nIn the following, we will compute the partial derivatives (11.12) through\n(11.14). But before we do this, we introduce a quantity that will play a\ncentral role in the remainder of this chapter: responsibilities.\n11.2.1 Responsibilities\nWe define the quantity\nrnk := \u03c0kN\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\nPK\nj=1 \u03c0jN\n\u0000\nxn |\u00b5j, \u03a3j\n\u0001 (11.17)\nas the responsibility of the kth mixture component for the nth data point.responsibility\nThe responsibility rnk of the kth mixture component for"
  },
  {
    "vector_id": 1432,
    "chunk_id": "p358_c2",
    "page_number": 358,
    "text": "= \u03c0kN\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\nPK\nj=1 \u03c0jN\n\u0000\nxn |\u00b5j, \u03a3j\n\u0001 (11.17)\nas the responsibility of the kth mixture component for the nth data point.responsibility\nThe responsibility rnk of the kth mixture component for data point xn is\nproportional to the likelihood\np(xn |\u03c0k, \u00b5k, \u03a3k) = \u03c0kN\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\n(11.18)\nof the mixture component given the data point. Therefore, mixture com-rn follows a\nBoltzmann/Gibbs\ndistribution.\nponents have a high responsibility for a data point when the data point\ncould be a plausible sample from that mixture component. Note that\nrn := [ rn1, . . . , rnK]\u22a4 \u2208 RK is a (normalized) probability vector, i.e.,\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1433,
    "chunk_id": "p358_c3",
    "page_number": 358,
    "text": "bability vector, i.e.,\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1434,
    "chunk_id": "p359_c0",
    "page_number": 359,
    "text": "11.2 Parameter Learning via Maximum Likelihood 353\nP\nk rnk = 1 with rnk \u2a7e 0. This probability vector distributes probabil-\nity mass among the K mixture components, and we can think of rn as a\n\u201csoft assignment\u201d of xn to the K mixture components. Therefore, the re- The responsibility\nrnk is the\nprobability that the\nkth mixture\ncomponent\ngenerated the nth\ndata point.\nsponsibility rnk from (11.17) represents the probability that xn has been\ngenerated by the kth mixture component.\nExample 11.2 (Responsibilities)\nFor our example from Figure 11.3, we compute the responsibilities rnk\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1.0 0 .0 0 .0\n1.0 0 .0 0 .0\n0.057 0 .943 0 .0\n0.001 0 .999 0 .0\n0.0 0 .066 0 .934\n0.0 0 .0 1 .0\n0.0 0 .0 1 .0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u2208 RN\u00d7K . (11.19)\nHere the nth row tells us the responsibilities of all mixture comp"
  },
  {
    "vector_id": 1435,
    "chunk_id": "p359_c1",
    "page_number": 359,
    "text": ".0 0 .0\n1.0 0 .0 0 .0\n0.057 0 .943 0 .0\n0.001 0 .999 0 .0\n0.0 0 .066 0 .934\n0.0 0 .0 1 .0\n0.0 0 .0 1 .0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u2208 RN\u00d7K . (11.19)\nHere the nth row tells us the responsibilities of all mixture components\nfor xn. The sum of all K responsibilities for a data point (sum of every\nrow) is 1. The kth column gives us an overview of the responsibility of\nthe kth mixture component. We can see that the third mixture component\n(third column) is not responsible for any of the first four data points, but\ntakes much responsibility of the remaining data points. The sum of all\nentries of a column gives us the values Nk, i.e., the total responsibility of\nthe kth mixture component. In our example, we get N1 = 2.058, N2 =\n2.008, N3 = 2.934.\nIn the following, we determine the updates of the model parameter"
  },
  {
    "vector_id": 1436,
    "chunk_id": "p359_c2",
    "page_number": 359,
    "text": "the values Nk, i.e., the total responsibility of\nthe kth mixture component. In our example, we get N1 = 2.058, N2 =\n2.008, N3 = 2.934.\nIn the following, we determine the updates of the model parameters\n\u00b5k, \u03a3k, \u03c0k for given responsibilities. We will see that the update equa-\ntions all depend on the responsibilities, which makes a closed-form solu-\ntion to the maximum likelihood estimation problem impossible. However,\nfor given responsibilities we will be updating one model parameter at a\ntime, while keeping the others fixed. After this, we will recompute the\nresponsibilities. Iterating these two steps will eventually converge to a lo-\ncal optimum and is a specific instantiation of the EM algorithm. We will\ndiscuss this in some more detail in Section 11.3.\n11.2.2 Updating the Means\nTheorem 1"
  },
  {
    "vector_id": 1437,
    "chunk_id": "p359_c3",
    "page_number": 359,
    "text": "o steps will eventually converge to a lo-\ncal optimum and is a specific instantiation of the EM algorithm. We will\ndiscuss this in some more detail in Section 11.3.\n11.2.2 Updating the Means\nTheorem 11.1 (Update of the GMM Means). The update of the mean pa-\nrameters \u00b5k, k = 1, . . . , K, of the GMM is given by\n\u00b5new\nk =\nPN\nn=1 rnkxn\nPN\nn=1 rnk\n, (11.20)\nwhere the responsibilities rnk are defined in (11.17).\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1438,
    "chunk_id": "p360_c0",
    "page_number": 360,
    "text": "354 Density Estimation with Gaussian Mixture Models\nRemark. The update of the means \u00b5k of the individual mixture compo-\nnents in (11.20) depends on all means, covariance matrices \u03a3k, and mix-\nture weights \u03c0k via rnk given in (11.17). Therefore, we cannot obtain a\nclosed-form solution for all \u00b5k at once. \u2662\nProof From (11.15), we see that the gradient of the log-likelihood with\nrespect to the mean parameters \u00b5k, k = 1, . . . , K, requires us to compute\nthe partial derivative\n\u2202p(xn |\u03b8)\n\u2202\u00b5k\n=\nKX\nj=1\n\u03c0j\n\u2202N\n\u0000\nxn |\u00b5j, \u03a3j\n\u0001\n\u2202\u00b5k\n= \u03c0k\n\u2202N\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\n\u2202\u00b5k\n(11.21a)\n= \u03c0k(xn \u2212 \u00b5k)\u22a4\u03a3\u22121\nk N\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\n, (11.21b)\nwhere we exploited that only the kth mixture component depends on \u00b5k.\nWe use our result from (11.21b) in (11.15) and put everything together\nso that the desired partial derivative of L with"
  },
  {
    "vector_id": 1439,
    "chunk_id": "p360_c1",
    "page_number": 360,
    "text": "(11.21b)\nwhere we exploited that only the kth mixture component depends on \u00b5k.\nWe use our result from (11.21b) in (11.15) and put everything together\nso that the desired partial derivative of L with respect to \u00b5k is given as\n\u2202L\n\u2202\u00b5k\n=\nNX\nn=1\n\u2202 log p(xn |\u03b8)\n\u2202\u00b5k\n=\nNX\nn=1\n1\np(xn |\u03b8)\n\u2202p(xn |\u03b8)\n\u2202\u00b5k\n(11.22a)\n=\nNX\nn=1\n(xn \u2212 \u00b5k)\u22a4\u03a3\u22121\nk\n\u03c0kN\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\nPK\nj=1 \u03c0jN\n\u0000\nxn |\u00b5j, \u03a3j\n\u0001\n| {z }\n=rnk\n(11.22b)\n=\nNX\nn=1\nrnk(xn \u2212 \u00b5k)\u22a4\u03a3\u22121\nk . (11.22c)\nHere we used the identity from (11.16) and the result of the partial deriva-\ntive in (11.21b) to get to (11.22b). The values rnk are the responsibilities\nwe defined in (11.17).\nWe now solve (11.22c) for \u00b5new\nk so that \u2202L(\u00b5new\nk )\n\u2202\u00b5k\n= 0\u22a4 and obtain\nNX\nn=1\nrnkxn =\nNX\nn=1\nrnk\u00b5new\nk \u21d0 \u21d2\u00b5new\nk =\nPN\nn=1 rnkxn\nPN\nn=1 rnk\n= 1\nNk\nNX\nn=1\nrnkxn ,\n(11.23)\nwhere we defined\nN"
  },
  {
    "vector_id": 1440,
    "chunk_id": "p360_c2",
    "page_number": 360,
    "text": "n (11.17).\nWe now solve (11.22c) for \u00b5new\nk so that \u2202L(\u00b5new\nk )\n\u2202\u00b5k\n= 0\u22a4 and obtain\nNX\nn=1\nrnkxn =\nNX\nn=1\nrnk\u00b5new\nk \u21d0 \u21d2\u00b5new\nk =\nPN\nn=1 rnkxn\nPN\nn=1 rnk\n= 1\nNk\nNX\nn=1\nrnkxn ,\n(11.23)\nwhere we defined\nNk :=\nNX\nn=1\nrnk (11.24)\nas the total responsibility of the kth mixture component for the entire\ndataset. This concludes the proof of Theorem 11.1.\nIntuitively , (11.20) can be interpreted as an importance-weighted Monte\nCarlo estimate of the mean, where the importance weights of data point\nxn are the responsibilities rnk of the kth cluster for xn, k = 1 , . . . , K.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1441,
    "chunk_id": "p360_c3",
    "page_number": 360,
    "text": "ics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1442,
    "chunk_id": "p361_c0",
    "page_number": 361,
    "text": "11.2 Parameter Learning via Maximum Likelihood 355\nTherefore, the mean \u00b5k is pulled toward a data point xn with strength Figure 11.4 Update\nof the mean\nparameter of\nmixture component\nin a GMM. The\nmean \u00b5 is being\npulled toward\nindividual data\npoints with the\nweights given by the\ncorresponding\nresponsibilities.\nr1\nr2\nr3x1\nx2 x3\n\u00b5\ngiven by rnk. The means are pulled stronger toward data points for which\nthe corresponding mixture component has a high responsibility , i.e., a high\nlikelihood. Figure 11.4 illustrates this. We can also interpret the mean up-\ndate in (11.20) as the expected value of all data points under the distri-\nbution given by\nrk := [r1k, . . . , rNk ]\u22a4/Nk , (11.25)\nwhich is a normalized probability vector, i.e.,\n\u00b5k \u2190 Erk [X] . (11.26)\nExample 11.3 (Mean Updates)\nFigure 11.5"
  },
  {
    "vector_id": 1443,
    "chunk_id": "p361_c1",
    "page_number": 361,
    "text": "ll data points under the distri-\nbution given by\nrk := [r1k, . . . , rNk ]\u22a4/Nk , (11.25)\nwhich is a normalized probability vector, i.e.,\n\u00b5k \u2190 Erk [X] . (11.26)\nExample 11.3 (Mean Updates)\nFigure 11.5 Effect\nof updating the\nmean values in a\nGMM. (a) GMM\nbefore updating the\nmean values;\n(b) GMM after\nupdating the mean\nvalues \u00b5k while\nretaining the\nvariances and\nmixture weights.\n\u22125 0 5 10 15\nx\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\np(x)\n\u03c01N(x|\u00b51, \u03c32\n1)\n\u03c02N(x|\u00b52, \u03c32\n2)\n\u03c03N(x|\u00b53, \u03c32\n3)\nGMM density(a) GMM density and individual components\nprior to updating the mean values.\n\u22125 0 5 10 15\nx\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\np(x)\n\u03c01N(x|\u00b51, \u03c32\n1)\n\u03c02N(x|\u00b52, \u03c32\n2)\n\u03c03N(x|\u00b53, \u03c32\n3)\nGMM density\n(b) GMM density and individual components\nafter updating the mean values.\nIn our example from Figure 11.3, the mean"
  },
  {
    "vector_id": 1444,
    "chunk_id": "p361_c2",
    "page_number": 361,
    "text": "0.15\n0.20\n0.25\n0.30\np(x)\n\u03c01N(x|\u00b51, \u03c32\n1)\n\u03c02N(x|\u00b52, \u03c32\n2)\n\u03c03N(x|\u00b53, \u03c32\n3)\nGMM density\n(b) GMM density and individual components\nafter updating the mean values.\nIn our example from Figure 11.3, the mean values are updated as fol-\nlows:\n\u00b51 : \u22124 \u2192 \u22122.7 (11.27)\n\u00b52 : 0 \u2192 \u22120.4 (11.28)\n\u00b53 : 8 \u2192 3.7 (11.29)\nHere we see that the means of the first and third mixture component\nmove toward the regime of the data, whereas the mean of the second\ncomponent does not change so dramatically . Figure 11.5 illustrates this\nchange, where Figure 11.5(a) shows the GMM density prior to updating\nthe means and Figure 11.5(b) shows the GMM density after updating the\nmean values \u00b5k.\nThe update of the mean parameters in (11.20) look fairly straight-\nforward. However, note that the responsibilities rnk are a function of"
  },
  {
    "vector_id": 1445,
    "chunk_id": "p361_c3",
    "page_number": 361,
    "text": ".5(b) shows the GMM density after updating the\nmean values \u00b5k.\nThe update of the mean parameters in (11.20) look fairly straight-\nforward. However, note that the responsibilities rnk are a function of\n\u03c0j, \u00b5j, \u03a3j for all j = 1, . . . , K, such that the updates in (11.20) depend\non all parameters of the GMM, and a closed-form solution, which we ob-\ntained for linear regression in Section 9.2 or PCA in Chapter 10, cannot\nbe obtained.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1446,
    "chunk_id": "p362_c0",
    "page_number": 362,
    "text": "356 Density Estimation with Gaussian Mixture Models\n11.2.3 Updating the Covariances\nTheorem 11.2 (Updates of the GMM Covariances). The update of the co-\nvariance parameters \u03a3k, k = 1, . . . , Kof the GMM is given by\n\u03a3new\nk = 1\nNk\nNX\nn=1\nrnk(xn \u2212 \u00b5k)(xn \u2212 \u00b5k)\u22a4 , (11.30)\nwhere rnk and Nk are defined in (11.17) and (11.24), respectively.\nProof To prove Theorem 11.2, our approach is to compute the partial\nderivatives of the log-likelihood L with respect to the covariances \u03a3k, set\nthem to 0, and solve for \u03a3k. We start with our general approach\n\u2202L\n\u2202\u03a3k\n=\nNX\nn=1\n\u2202 log p(xn |\u03b8)\n\u2202\u03a3k\n=\nNX\nn=1\n1\np(xn |\u03b8)\n\u2202p(xn |\u03b8)\n\u2202\u03a3k\n. (11.31)\nWe already know 1/p(xn |\u03b8) from (11.16). To obtain the remaining par-\ntial derivative \u2202p(xn |\u03b8)/\u2202\u03a3k, we write down the definition of the Gaus-\nsian distribution p(xn |\u03b8) (see ("
  },
  {
    "vector_id": 1447,
    "chunk_id": "p362_c1",
    "page_number": 362,
    "text": "\u2202p(xn |\u03b8)\n\u2202\u03a3k\n. (11.31)\nWe already know 1/p(xn |\u03b8) from (11.16). To obtain the remaining par-\ntial derivative \u2202p(xn |\u03b8)/\u2202\u03a3k, we write down the definition of the Gaus-\nsian distribution p(xn |\u03b8) (see (11.9)) and drop all terms but the kth. We\nthen obtain\n\u2202p(xn |\u03b8)\n\u2202\u03a3k\n(11.32a)\n= \u2202\n\u2202\u03a3k\n\u0012\n\u03c0k(2\u03c0)\u2212D\n2 det(\u03a3k)\u22121\n2 exp\n\u0000\n\u22121\n2 (xn \u2212 \u00b5k)\u22a4\u03a3\u22121\nk (xn \u2212 \u00b5k)\n\u0001\u0013\n(11.32b)\n= \u03c0k(2\u03c0)\u2212D\n2\n\u0014 \u2202\n\u2202\u03a3k\ndet(\u03a3k)\u22121\n2 exp\n\u0000\n\u22121\n2 (xn \u2212 \u00b5k)\u22a4\u03a3\u22121\nk (xn \u2212 \u00b5k)\n\u0001\n+ det(\u03a3k)\u22121\n2\n\u2202\n\u2202\u03a3k\nexp\n\u0000\n\u22121\n2 (xn \u2212 \u00b5k)\u22a4\u03a3\u22121\nk (xn \u2212 \u00b5k)\n\u0001\u0015\n. (11.32c)\nWe now use the identities\n\u2202\n\u2202\u03a3k\ndet(\u03a3k)\u22121\n2 (5.101)\n= \u22121\n2 det(\u03a3k)\u22121\n2 \u03a3\u22121\nk , (11.33)\n\u2202\n\u2202\u03a3k\n(xn \u2212 \u00b5k)\u22a4\u03a3\u22121\nk (xn \u2212 \u00b5k)\n(5.103)\n= \u2212\u03a3\u22121\nk (xn \u2212 \u00b5k)(xn \u2212 \u00b5k)\u22a4\u03a3\u22121\nk\n(11.34)\nand obtain (after some rearranging) the desired partial derivative required\nin (11.31) as\n\u2202p(xn |\u03b8)\n\u2202\u03a3k\n= \u03c0k N\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\n\u00b7\n\u0002"
  },
  {
    "vector_id": 1448,
    "chunk_id": "p362_c2",
    "page_number": 362,
    "text": "\u00b5k)\u22a4\u03a3\u22121\nk (xn \u2212 \u00b5k)\n(5.103)\n= \u2212\u03a3\u22121\nk (xn \u2212 \u00b5k)(xn \u2212 \u00b5k)\u22a4\u03a3\u22121\nk\n(11.34)\nand obtain (after some rearranging) the desired partial derivative required\nin (11.31) as\n\u2202p(xn |\u03b8)\n\u2202\u03a3k\n= \u03c0k N\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\n\u00b7\n\u0002\n\u22121\n2 (\u03a3\u22121\nk \u2212 \u03a3\u22121\nk (xn \u2212 \u00b5k)(xn \u2212 \u00b5k)\u22a4\u03a3\u22121\nk )\n\u0003\n. (11.35)\nPutting everything together, the partial derivative of the log-likelihood\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1449,
    "chunk_id": "p363_c0",
    "page_number": 363,
    "text": "11.2 Parameter Learning via Maximum Likelihood 357\nwith respect to \u03a3k is given by\n\u2202L\n\u2202\u03a3k\n=\nNX\nn=1\n\u2202 log p(xn |\u03b8)\n\u2202\u03a3k\n=\nNX\nn=1\n1\np(xn |\u03b8)\n\u2202p(xn |\u03b8)\n\u2202\u03a3k\n(11.36a)\n=\nNX\nn=1\n\u03c0kN\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\nPK\nj=1 \u03c0jN\n\u0000\nxn |\u00b5j, \u03a3j\n\u0001\n| {z }\n=rnk\n\u00b7\n\u0002\n\u22121\n2 (\u03a3\u22121\nk \u2212 \u03a3\u22121\nk (xn \u2212 \u00b5k)(xn \u2212 \u00b5k)\u22a4\u03a3\u22121\nk )\n\u0003\n(11.36b)\n= \u22121\n2\nNX\nn=1\nrnk(\u03a3\u22121\nk \u2212 \u03a3\u22121\nk (xn \u2212 \u00b5k)(xn \u2212 \u00b5k)\u22a4\u03a3\u22121\nk ) (11.36c)\n= \u22121\n2\u03a3\u22121\nk\nNX\nn=1\nrnk\n| {z }\n=Nk\n+ 1\n2\u03a3\u22121\nk\n NX\nn=1\nrnk(xn \u2212 \u00b5k)(xn \u2212 \u00b5k)\u22a4\n!\n\u03a3\u22121\nk .\n(11.36d)\nWe see that the responsibilities rnk also appear in this partial derivative.\nSetting this partial derivative to 0, we obtain the necessary optimality\ncondition\nNk\u03a3\u22121\nk = \u03a3\u22121\nk\n NX\nn=1\nrnk(xn \u2212 \u00b5k)(xn \u2212 \u00b5k)\u22a4\n!\n\u03a3\u22121\nk (11.37a)\n\u21d0 \u21d2NkI =\n NX\nn=1\nrnk(xn \u2212 \u00b5k)(xn \u2212 \u00b5k)\u22a4\n!\n\u03a3\u22121\nk . (11.37b)\nBy solving for \u03a3k, we obtain\n\u03a3new\nk = 1\nNk\nNX\nn=1\nrnk(xn \u2212 \u00b5k)(xn"
  },
  {
    "vector_id": 1450,
    "chunk_id": "p363_c1",
    "page_number": 363,
    "text": "condition\nNk\u03a3\u22121\nk = \u03a3\u22121\nk\n NX\nn=1\nrnk(xn \u2212 \u00b5k)(xn \u2212 \u00b5k)\u22a4\n!\n\u03a3\u22121\nk (11.37a)\n\u21d0 \u21d2NkI =\n NX\nn=1\nrnk(xn \u2212 \u00b5k)(xn \u2212 \u00b5k)\u22a4\n!\n\u03a3\u22121\nk . (11.37b)\nBy solving for \u03a3k, we obtain\n\u03a3new\nk = 1\nNk\nNX\nn=1\nrnk(xn \u2212 \u00b5k)(xn \u2212 \u00b5k)\u22a4 , (11.38)\nwhere rk is the probability vector defined in (11.25). This gives us a sim-\nple update rule for \u03a3k for k = 1, . . . , Kand proves Theorem 11.2.\nSimilar to the update of \u00b5k in (11.20), we can interpret the update of\nthe covariance in (11.30) as an importance-weighted expected value of\nthe square of the centered data \u02dcXk := {x1 \u2212 \u00b5k, . . . ,xN \u2212 \u00b5k}.\nExample 11.4 (Variance Updates)\nIn our example from Figure 11.3, the variances are updated as follows:\n\u03c32\n1 : 1 \u2192 0.14 (11.39)\n\u03c32\n2 : 0.2 \u2192 0.44 (11.40)\n\u03c32\n3 : 3 \u2192 1.53 (11.41)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Publis"
  },
  {
    "vector_id": 1451,
    "chunk_id": "p363_c2",
    "page_number": 363,
    "text": "In our example from Figure 11.3, the variances are updated as follows:\n\u03c32\n1 : 1 \u2192 0.14 (11.39)\n\u03c32\n2 : 0.2 \u2192 0.44 (11.40)\n\u03c32\n3 : 3 \u2192 1.53 (11.41)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1452,
    "chunk_id": "p364_c0",
    "page_number": 364,
    "text": "358 Density Estimation with Gaussian Mixture Models\nHere we see that the variances of the first and third component shrink\nsignificantly , whereas the variance of the second component increases\nslightly .\nFigure 11.6 illustrates this setting. Figure 11.6(a) is identical (but\nzoomed in) to Figure 11.5(b) and shows the GMM density and its indi-\nvidual components prior to updating the variances. Figure 11.6(b) shows\nthe GMM density after updating the variances.\nFigure 11.6 Effect\nof updating the\nvariances in a GMM.\n(a) GMM before\nupdating the\nvariances; (b) GMM\nafter updating the\nvariances while\nretaining the means\nand mixture\nweights.\n\u22124 \u22122 0 2 4 6 8\nx\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\np(x)\n\u03c01N(x|\u00b51, \u03c32\n1)\n\u03c02N(x|\u00b52, \u03c32\n2)\n\u03c03N(x|\u00b53, \u03c32\n3)\nGMM density\n(a) GMM density and individual components"
  },
  {
    "vector_id": 1453,
    "chunk_id": "p364_c1",
    "page_number": 364,
    "text": "retaining the means\nand mixture\nweights.\n\u22124 \u22122 0 2 4 6 8\nx\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\np(x)\n\u03c01N(x|\u00b51, \u03c32\n1)\n\u03c02N(x|\u00b52, \u03c32\n2)\n\u03c03N(x|\u00b53, \u03c32\n3)\nGMM density\n(a) GMM density and individual components\nprior to updating the variances.\n\u22124 \u22122 0 2 4 6 8\nx\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\np(x)\n\u03c01N(x|\u00b51, \u03c32\n1)\n\u03c02N(x|\u00b52, \u03c32\n2)\n\u03c03N(x|\u00b53, \u03c32\n3)\nGMM density\n(b) GMM density and individual components\nafter updating the variances.\nSimilar to the update of the mean parameters, we can interpret (11.30)\nas a Monte Carlo estimate of the weighted covariance of data points xn\nassociated with the kth mixture component, where the weights are the\nresponsibilities rnk. As with the updates of the mean parameters, this up-\ndate depends on all \u03c0j, \u00b5j, \u03a3j, j= 1, . . . , K, through the responsibilities\nrnk, wh"
  },
  {
    "vector_id": 1454,
    "chunk_id": "p364_c2",
    "page_number": 364,
    "text": "re component, where the weights are the\nresponsibilities rnk. As with the updates of the mean parameters, this up-\ndate depends on all \u03c0j, \u00b5j, \u03a3j, j= 1, . . . , K, through the responsibilities\nrnk, which prohibits a closed-form solution.\n11.2.4 Updating the Mixture Weights\nTheorem 11.3 (Update of the GMM Mixture Weights). The mixture weights\nof the GMM are updated as\n\u03c0new\nk = Nk\nN , k = 1, . . . , K , (11.42)\nwhere N is the number of data points and Nk is defined in (11.24).\nProof To find the partial derivative of the log-likelihood with respect\nto the weight parameters \u03c0k, k = 1 , . . . , K, we account for the con-\nstraint P\nk \u03c0k = 1 by using Lagrange multipliers (see Section 7.2). The\nLagrangian is\nL = L + \u03bb\n KX\nk=1\n\u03c0k \u2212 1\n!\n(11.43a)\nDraft (2024-01-15) of \u201cMathematics for Machine Learnin"
  },
  {
    "vector_id": 1455,
    "chunk_id": "p364_c3",
    "page_number": 364,
    "text": "we account for the con-\nstraint P\nk \u03c0k = 1 by using Lagrange multipliers (see Section 7.2). The\nLagrangian is\nL = L + \u03bb\n KX\nk=1\n\u03c0k \u2212 1\n!\n(11.43a)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1456,
    "chunk_id": "p365_c0",
    "page_number": 365,
    "text": "11.2 Parameter Learning via Maximum Likelihood 359\n=\nNX\nn=1\nlog\nKX\nk=1\n\u03c0kN\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\n+ \u03bb\n KX\nk=1\n\u03c0k \u2212 1\n!\n, (11.43b)\nwhere L is the log-likelihood from (11.10) and the second term encodes\nfor the equality constraint that all the mixture weights need to sum up to\n1. We obtain the partial derivative with respect to \u03c0k as\n\u2202L\n\u2202\u03c0k\n=\nNX\nn=1\nN\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\nPK\nj=1 \u03c0jN\n\u0000\nxn |\u00b5j, \u03a3j\n\u0001 + \u03bb (11.44a)\n= 1\n\u03c0k\nNX\nn=1\n\u03c0kN\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\nPK\nj=1 \u03c0jN\n\u0000\nxn |\u00b5j, \u03a3j\n\u0001\n| {z }\n=Nk\n+\u03bb = Nk\n\u03c0k\n+ \u03bb , (11.44b)\nand the partial derivative with respect to the Lagrange multiplier \u03bb as\n\u2202L\n\u2202\u03bb =\nKX\nk=1\n\u03c0k \u2212 1 . (11.45)\nSetting both partial derivatives to 0 (necessary condition for optimum)\nyields the system of equations\n\u03c0k = \u2212Nk\n\u03bb , (11.46)\n1 =\nKX\nk=1\n\u03c0k . (11.47)\nUsing (11.46) in (11.47) and solving for \u03c0k, we obtain"
  },
  {
    "vector_id": 1457,
    "chunk_id": "p365_c1",
    "page_number": 365,
    "text": "tting both partial derivatives to 0 (necessary condition for optimum)\nyields the system of equations\n\u03c0k = \u2212Nk\n\u03bb , (11.46)\n1 =\nKX\nk=1\n\u03c0k . (11.47)\nUsing (11.46) in (11.47) and solving for \u03c0k, we obtain\nKX\nk=1\n\u03c0k = 1 \u21d0 \u21d2 \u2212\nKX\nk=1\nNk\n\u03bb = 1 \u21d0 \u21d2 \u2212N\n\u03bb = 1 \u21d0 \u21d2\u03bb = \u2212N .\n(11.48)\nThis allows us to substitute \u2212N for \u03bb in (11.46) to obtain\n\u03c0new\nk = Nk\nN , (11.49)\nwhich gives us the update for the weight parameters \u03c0k and proves Theo-\nrem 11.3.\nWe can identify the mixture weight in (11.42) as the ratio of the to-\ntal responsibility of the kth cluster and the number of data points. Since\nN = P\nk Nk, the number of data points can also be interpreted as the\ntotal responsibility of all mixture components together, such that\u03c0k is the\nrelative importance of the kth mixture component for the dataset.\nRemark. S"
  },
  {
    "vector_id": 1458,
    "chunk_id": "p365_c2",
    "page_number": 365,
    "text": "er of data points can also be interpreted as the\ntotal responsibility of all mixture components together, such that\u03c0k is the\nrelative importance of the kth mixture component for the dataset.\nRemark. Since Nk = PN\ni=1 rnk, the update equation (11.42) for the mix-\nture weights \u03c0k also depends on all \u03c0j, \u00b5j, \u03a3j, j= 1, . . . , Kvia the re-\nsponsibilities rnk. \u2662\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1459,
    "chunk_id": "p366_c0",
    "page_number": 366,
    "text": "360 Density Estimation with Gaussian Mixture Models\nExample 11.5 (Weight Parameter Updates)\nFigure 11.7 Effect\nof updating the\nmixture weights in a\nGMM. (a) GMM\nbefore updating the\nmixture weights;\n(b) GMM after\nupdating the\nmixture weights\nwhile retaining the\nmeans and\nvariances. Note the\ndifferent scales of\nthe vertical axes.\n\u22124 \u22122 0 2 4 6 8\nx\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\np(x)\n\u03c01N(x|\u00b51, \u03c32\n1)\n\u03c02N(x|\u00b52, \u03c32\n2)\n\u03c03N(x|\u00b53, \u03c32\n3)\nGMM density\n(a) GMM density and individual components\nprior to updating the mixture weights.\n\u22124 \u22122 0 2 4 6 8\nx\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\np(x)\n\u03c01N(x|\u00b51, \u03c32\n1)\n\u03c02N(x|\u00b52, \u03c32\n2)\n\u03c03N(x|\u00b53, \u03c32\n3)\nGMM density\n(b) GMM density and individual components\nafter updating the mixture weights.\nIn our running example from Figure 11.3, the mixture weights are up-\nd"
  },
  {
    "vector_id": 1460,
    "chunk_id": "p366_c1",
    "page_number": 366,
    "text": ", \u03c32\n1)\n\u03c02N(x|\u00b52, \u03c32\n2)\n\u03c03N(x|\u00b53, \u03c32\n3)\nGMM density\n(b) GMM density and individual components\nafter updating the mixture weights.\nIn our running example from Figure 11.3, the mixture weights are up-\ndated as follows:\n\u03c01 : 1\n3 \u2192 0.29 (11.50)\n\u03c02 : 1\n3 \u2192 0.29 (11.51)\n\u03c03 : 1\n3 \u2192 0.42 (11.52)\nHere we see that the third component gets more weight/importance,\nwhile the other components become slightly less important. Figure 11.7\nillustrates the effect of updating the mixture weights. Figure 11.7(a) is\nidentical to Figure 11.6(b) and shows the GMM density and its individual\ncomponents prior to updating the mixture weights. Figure 11.7(b) shows\nthe GMM density after updating the mixture weights.\nOverall, having updated the means, the variances, and the weights\nonce, we obtain the GMM shown in Figur"
  },
  {
    "vector_id": 1461,
    "chunk_id": "p366_c2",
    "page_number": 366,
    "text": "g the mixture weights. Figure 11.7(b) shows\nthe GMM density after updating the mixture weights.\nOverall, having updated the means, the variances, and the weights\nonce, we obtain the GMM shown in Figure 11.7(b). Compared with the\ninitialization shown in Figure 11.3, we can see that the parameter updates\ncaused the GMM density to shift some of its mass toward the data points.\nAfter updating the means, variances, and weights once, the GMM fit\nin Figure 11.7(b) is already remarkably better than its initialization from\nFigure 11.3. This is also evidenced by the log-likelihood values, which\nincreased from \u221228.3 (initialization) to \u221214.4 after a full update cycle.\n11.3 EM Algorithm\nUnfortunately , the updates in (11.20), (11.30), and (11.42) do not consti-\ntute a closed-form solution for the upda"
  },
  {
    "vector_id": 1462,
    "chunk_id": "p366_c3",
    "page_number": 366,
    "text": "d from \u221228.3 (initialization) to \u221214.4 after a full update cycle.\n11.3 EM Algorithm\nUnfortunately , the updates in (11.20), (11.30), and (11.42) do not consti-\ntute a closed-form solution for the updates of the parameters \u00b5k, \u03a3k, \u03c0k\nof the mixture model because the responsibilities rnk depend on those pa-\nrameters in a complex way . However, the results suggest a simpleiterative\nscheme for finding a solution to the parameters estimation problem via\nmaximum likelihood. The expectation maximization algorithm ( EM algo-EM algorithm\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1463,
    "chunk_id": "p366_c4",
    "page_number": 366,
    "text": "k:https://mml-book.com."
  },
  {
    "vector_id": 1464,
    "chunk_id": "p367_c0",
    "page_number": 367,
    "text": "11.3 EM Algorithm 361\nrithm) was proposed by Dempster et al. (1977) and is a general iterative\nscheme for learning parameters (maximum likelihood or MAP) in mixture\nmodels and, more generally , latent-variable models.\nIn our example of the Gaussian mixture model, we choose initial values\nfor \u00b5k, \u03a3k, \u03c0k and alternate until convergence between\nE-step: Evaluate the responsibilities rnk (posterior probability of data\npoint n belonging to mixture component k).\nM-step: Use the updated responsibilities to reestimate the parameters\n\u00b5k, \u03a3k, \u03c0k.\nEvery step in the EM algorithm increases the log-likelihood function (Neal\nand Hinton, 1999). For convergence, we can check the log-likelihood or\nthe parameters directly . A concrete instantiation of the EM algorithm for\nestimating the parameters of a GMM is"
  },
  {
    "vector_id": 1465,
    "chunk_id": "p367_c1",
    "page_number": 367,
    "text": "d function (Neal\nand Hinton, 1999). For convergence, we can check the log-likelihood or\nthe parameters directly . A concrete instantiation of the EM algorithm for\nestimating the parameters of a GMM is as follows:\n1. Initialize \u00b5k, \u03a3k, \u03c0k.\n2. E-step: Evaluate responsibilities rnk for every data point xn using cur-\nrent parameters \u03c0k, \u00b5k, \u03a3k:\nrnk = \u03c0kN\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\nP\nj \u03c0jN\n\u0000\nxn |\u00b5j, \u03a3j\n\u0001 . (11.53)\n3. M-step: Reestimate parameters \u03c0k, \u00b5k, \u03a3k using the current responsi-\nbilities rnk (from E-step): Having updated the\nmeans \u00b5k\nin (11.54), they are\nsubsequently used\nin (11.55) to update\nthe corresponding\ncovariances.\n\u00b5k = 1\nNk\nNX\nn=1\nrnkxn , (11.54)\n\u03a3k = 1\nNk\nNX\nn=1\nrnk(xn \u2212 \u00b5k)(xn \u2212 \u00b5k)\u22a4 , (11.55)\n\u03c0k = Nk\nN . (11.56)\nExample 11.6 (GMM Fit)\nFigure 11.8 EM\nalgorithm applied to\nthe GMM from\nFigur"
  },
  {
    "vector_id": 1466,
    "chunk_id": "p367_c2",
    "page_number": 367,
    "text": "onding\ncovariances.\n\u00b5k = 1\nNk\nNX\nn=1\nrnkxn , (11.54)\n\u03a3k = 1\nNk\nNX\nn=1\nrnk(xn \u2212 \u00b5k)(xn \u2212 \u00b5k)\u22a4 , (11.55)\n\u03c0k = Nk\nN . (11.56)\nExample 11.6 (GMM Fit)\nFigure 11.8 EM\nalgorithm applied to\nthe GMM from\nFigure 11.2. (a)\nFinal GMM fit;\n(b) negative\nlog-likelihood as a\nfunction of the EM\niteration.\n\u22125 0 5 10 15\nx\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\np(x)\n\u03c01N(x|\u00b51, \u03c32\n1)\n\u03c02N(x|\u00b52, \u03c32\n2)\n\u03c03N(x|\u00b53, \u03c32\n3)\nGMM density\n(a) Final GMM fit. After five iterations, the EM\nalgorithm converges and returns this GMM.\n0 1 2 3 4 5\nIteration\n14\n16\n18\n20\n22\n24\n26\n28Negative log-likelihood\n(b) Negative log-likelihood as a function of the\nEM iterations.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1467,
    "chunk_id": "p367_c3",
    "page_number": 367,
    "text": "nction of the\nEM iterations.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1468,
    "chunk_id": "p368_c0",
    "page_number": 368,
    "text": "362 Density Estimation with Gaussian Mixture Models\nFigure 11.9\nIllustration of the\nEM algorithm for\nfitting a Gaussian\nmixture model with\nthree components to\na two-dimensional\ndataset. (a) Dataset;\n(b) negative\nlog-likelihood\n(lower is better) as\na function of the EM\niterations. The red\ndots indicate the\niterations for which\nthe mixture\ncomponents of the\ncorresponding GMM\nfits are shown in (c)\nthrough (f). The\nyellow discs indicate\nthe means of the\nGaussian mixture\ncomponents.\nFigure 11.10(a)\nshows the final\nGMM fit.\n\u221210 \u22125 0 5 10\nx1\n\u221210\n\u22125\n0\n5\n10\nx2\n(a) Dataset.\n0 20 40 60\nEM iteration\n104\n4 \u00d7 103\n6 \u00d7 103\nNegative log-likelihood (b) Negative log-likelihood.\n\u221210 \u22125 0 5 10\nx1\n\u221210\n\u22125\n0\n5\n10\nx2\n(c) EM initialization.\n\u221210 \u22125 0 5 10\nx1\n\u221210\n\u22125\n0\n5\n10\nx2\n (d) EM after one iteration.\n\u221210 \u22125 0 5 1"
  },
  {
    "vector_id": 1469,
    "chunk_id": "p368_c1",
    "page_number": 368,
    "text": "\u00d7 103\n6 \u00d7 103\nNegative log-likelihood (b) Negative log-likelihood.\n\u221210 \u22125 0 5 10\nx1\n\u221210\n\u22125\n0\n5\n10\nx2\n(c) EM initialization.\n\u221210 \u22125 0 5 10\nx1\n\u221210\n\u22125\n0\n5\n10\nx2\n (d) EM after one iteration.\n\u221210 \u22125 0 5 10\nx1\n\u221210\n\u22125\n0\n5\n10\nx2\n(e) EM after 10 iterations.\n\u221210 \u22125 0 5 10\nx1\n\u221210\n\u22125\n0\n5\n10\nx2\n (f) EM after 62 iterations.\nWhen we run EM on our example from Figure 11.3, we obtain the final\nresult shown in Figure 11.8(a) after five iterations, and Figure 11.8(b)\nshows how the negative log-likelihood evolves as a function of the EM\niterations. The final GMM is given as\np(x) = 0.29N\n\u0000\nx | \u22122.75, 0.06\n\u0001\n+ 0.28N\n\u0000\nx | \u22120.50, 0.25\n\u0001\n+ 0.43N\n\u0000\nx |3.64, 1.63\n\u0001\n.\n(11.57)\nWe applied the EM algorithm to the two-dimensional dataset shown\nin Figure 11.1 with K = 3 mixture components. Figure 11.9 illustrates\nsome s"
  },
  {
    "vector_id": 1470,
    "chunk_id": "p368_c2",
    "page_number": 368,
    "text": "8N\n\u0000\nx | \u22120.50, 0.25\n\u0001\n+ 0.43N\n\u0000\nx |3.64, 1.63\n\u0001\n.\n(11.57)\nWe applied the EM algorithm to the two-dimensional dataset shown\nin Figure 11.1 with K = 3 mixture components. Figure 11.9 illustrates\nsome steps of the EM algorithm and shows the negative log-likelihood as\na function of the EM iteration (Figure 11.9(b)). Figure 11.10(a) shows\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1471,
    "chunk_id": "p369_c0",
    "page_number": 369,
    "text": "11.4 Latent-Variable Perspective 363\nFigure 11.10 GMM\nfit and\nresponsibilities\nwhen EM converges.\n(a) GMM fit when\nEM converges;\n(b) each data point\nis colored according\nto the\nresponsibilities of\nthe mixture\ncomponents.\n\u22125 0 5\nx1\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\nx2\n(a) GMM fit after 62 iterations.\n\u22125 0 5\nx1\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\nx2 (b) Dataset colored according to the respon-\nsibilities of the mixture components.\nthe corresponding final GMM fit. Figure 11.10(b) visualizes the final re-\nsponsibilities of the mixture components for the data points. The dataset is\ncolored according to the responsibilities of the mixture components when\nEM converges. While a single mixture component is clearly responsible\nfor the data on the left, the overlap of the two data clusters on the right\ncould have been generated by two"
  },
  {
    "vector_id": 1472,
    "chunk_id": "p369_c1",
    "page_number": 369,
    "text": "ture components when\nEM converges. While a single mixture component is clearly responsible\nfor the data on the left, the overlap of the two data clusters on the right\ncould have been generated by two mixture components. It becomes clear\nthat there are data points that cannot be uniquely assigned to a single\ncomponent (either blue or yellow), such that the responsibilities of these\ntwo clusters for those points are around 0.5.\n11.4 Latent-Variable Perspective\nWe can look at the GMM from the perspective of a discrete latent-variable\nmodel, i.e., where the latent variable z can attain only a finite set of val-\nues. This is in contrast to PCA, where the latent variables were continuous-\nvalued numbers in RM .\nThe advantages of the probabilistic perspective are that (i) it will jus-\ntify some a"
  },
  {
    "vector_id": 1473,
    "chunk_id": "p369_c2",
    "page_number": 369,
    "text": "te set of val-\nues. This is in contrast to PCA, where the latent variables were continuous-\nvalued numbers in RM .\nThe advantages of the probabilistic perspective are that (i) it will jus-\ntify some ad hoc decisions we made in the previous sections, (ii) it allows\nfor a concrete interpretation of the responsibilities as posterior probabil-\nities, and (iii) the iterative algorithm for updating the model parameters\ncan be derived in a principled manner as the EM algorithm for maximum\nlikelihood parameter estimation in latent-variable models.\n11.4.1 Generative Process and Probabilistic Model\nTo derive the probabilistic model for GMMs, it is useful to think about the\ngenerative process, i.e., the process that allows us to generate data, using\na probabilistic model.\nWe assume a mixture model wi"
  },
  {
    "vector_id": 1474,
    "chunk_id": "p369_c3",
    "page_number": 369,
    "text": "erive the probabilistic model for GMMs, it is useful to think about the\ngenerative process, i.e., the process that allows us to generate data, using\na probabilistic model.\nWe assume a mixture model with K components and that a data point\nx can be generated by exactly one mixture component. We introduce a\nbinary indicator variable zk \u2208 {0, 1} with two states (see Section 6.2) that\nindicates whether the kth mixture component generated that data point\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1475,
    "chunk_id": "p370_c0",
    "page_number": 370,
    "text": "364 Density Estimation with Gaussian Mixture Models\nso that\np(x|zk = 1) = N\n\u0000\nx|\u00b5k, \u03a3k\n\u0001\n. (11.58)\nWe define z := [z1, . . . , zK]\u22a4 \u2208 RK as a probability vector consisting of\nK \u22121 many 0s and exactly one1. For example, forK = 3, a valid z would\nbe z = [z1, z2, z3]\u22a4 = [0, 1, 0]\u22a4, which would select the second mixture\ncomponent since z2 = 1.\nRemark. Sometimes this kind of probability distribution is called \u201cmulti-\nnoulli\u201d, a generalization of the Bernoulli distribution to more than two\nvalues (Murphy, 2012). \u2662\nThe properties of z imply that PK\nk=1 zk = 1. Therefore, z is a one-hotone-hot encoding\nencoding (also: 1-of-K representation).1-of-K\nrepresentation Thus far, we assumed that the indicator variables zk are known. How-\never, in practice, this is not the case, and we place a prior distri"
  },
  {
    "vector_id": 1476,
    "chunk_id": "p370_c1",
    "page_number": 370,
    "text": "g\nencoding (also: 1-of-K representation).1-of-K\nrepresentation Thus far, we assumed that the indicator variables zk are known. How-\never, in practice, this is not the case, and we place a prior distribution\np(z) = \u03c0 = [\u03c01, . . . , \u03c0K]\u22a4 ,\nKX\nk=1\n\u03c0k = 1 , (11.59)\non the latent variable z. Then the kth entry\n\u03c0k = p(zk = 1) (11.60)\nof this probability vector describes the probability that the kth mixture\ncomponent generated data point x.Figure 11.11\nGraphical model for\na GMM with a single\ndata point.\n\u03c0\nz\nx\u03a3k\n\u00b5k\nk= 1, . . . , K\nRemark (Sampling from a GMM). The construction of this latent-variable\nmodel (see the corresponding graphical model in Figure 11.11) lends it-\nself to a very simple sampling procedure (generative process) to generate\ndata:\n1. Sample z(i) \u223c p(z).\n2. Sample x(i) \u223c p(x|z(i)"
  },
  {
    "vector_id": 1477,
    "chunk_id": "p370_c2",
    "page_number": 370,
    "text": "model (see the corresponding graphical model in Figure 11.11) lends it-\nself to a very simple sampling procedure (generative process) to generate\ndata:\n1. Sample z(i) \u223c p(z).\n2. Sample x(i) \u223c p(x|z(i) = 1).\nIn the first step, we select a mixture component i (via the one-hot encod-\ning z) at random according to p(z) = \u03c0; in the second step we draw a\nsample from the corresponding mixture component. When we discard the\nsamples of the latent variable so that we are left with the x(i), we have\nvalid samples from the GMM. This kind of sampling, where samples of\nrandom variables depend on samples from the variable\u2019s parents in the\ngraphical model, is called ancestral sampling. \u2662ancestral sampling\nGenerally , a probabilistic model is defined by the joint distribution of\nthe data and the latent var"
  },
  {
    "vector_id": 1478,
    "chunk_id": "p370_c3",
    "page_number": 370,
    "text": "m the variable\u2019s parents in the\ngraphical model, is called ancestral sampling. \u2662ancestral sampling\nGenerally , a probabilistic model is defined by the joint distribution of\nthe data and the latent variables (see Section 8.4). With the prior p(z)\ndefined in (11.59) and (11.60) and the conditional p(x|z) from (11.58),\nwe obtain all K components of this joint distribution via\np(x, zk = 1) = p(x|zk = 1)p(zk = 1) = \u03c0kN\n\u0000\nx|\u00b5k, \u03a3k\n\u0001\n(11.61)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1479,
    "chunk_id": "p371_c0",
    "page_number": 371,
    "text": "11.4 Latent-Variable Perspective 365\nfor k = 1, . . . , K, so that\np(x, z) =\n\uf8ee\n\uf8ef\uf8f0\np(x, z1 = 1)\n...\np(x, zK = 1)\n\uf8f9\n\uf8fa\uf8fb =\n\uf8ee\n\uf8ef\uf8f0\n\u03c01N\n\u0000\nx|\u00b51, \u03a31\n\u0001\n...\n\u03c0KN\n\u0000\nx|\u00b5K, \u03a3K\n\u0001\n\uf8f9\n\uf8fa\uf8fb , (11.62)\nwhich fully specifies the probabilistic model.\n11.4.2 Likelihood\nTo obtain the likelihood p(x|\u03b8) in a latent-variable model, we need to\nmarginalize out the latent variables (see Section 8.4.3). In our case, this\ncan be done by summing out all latent variables from the joint p(x, z)\nin (11.62) so that\np(x|\u03b8) =\nX\nz\np(x|\u03b8, z)p(z |\u03b8) , \u03b8 := {\u00b5k, \u03a3k, \u03c0k : k = 1, . . . , K}.\n(11.63)\nWe now explicitly condition on the parameters\u03b8 of the probabilistic model,\nwhich we previously omitted. In (11.63), we sum over allK possible one-\nhot encodings of z, which is denoted by P\nz. Since there is only a single\nnonzero single entry i"
  },
  {
    "vector_id": 1480,
    "chunk_id": "p371_c1",
    "page_number": 371,
    "text": "rs\u03b8 of the probabilistic model,\nwhich we previously omitted. In (11.63), we sum over allK possible one-\nhot encodings of z, which is denoted by P\nz. Since there is only a single\nnonzero single entry in each z there are only K possible configurations/\nsettings of z. For example, if K = 3, then z can have the configurations\n\uf8ee\n\uf8f0\n1\n0\n0\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n0\n1\n0\n\uf8f9\n\uf8fb,\n\uf8ee\n\uf8f0\n0\n0\n1\n\uf8f9\n\uf8fb . (11.64)\nSumming over all possible configurations of z in (11.63) is equivalent to\nlooking at the nonzero entry of the z-vector and writing\np(x|\u03b8) =\nX\nz\np(x|\u03b8, z)p(z |\u03b8) (11.65a)\n=\nKX\nk=1\np(x|\u03b8, zk = 1)p(zk = 1 |\u03b8) (11.65b)\nso that the desired marginal distribution is given as\np(x|\u03b8)\n(11.65b)\n=\nKX\nk=1\np(x|\u03b8, zk = 1)p(zk = 1|\u03b8) (11.66a)\n=\nKX\nk=1\n\u03c0kN\n\u0000\nx|\u00b5k, \u03a3k\n\u0001\n, (11.66b)\nwhich we identify as the GMM model from (11.3). Given a"
  },
  {
    "vector_id": 1481,
    "chunk_id": "p371_c2",
    "page_number": 371,
    "text": "the desired marginal distribution is given as\np(x|\u03b8)\n(11.65b)\n=\nKX\nk=1\np(x|\u03b8, zk = 1)p(zk = 1|\u03b8) (11.66a)\n=\nKX\nk=1\n\u03c0kN\n\u0000\nx|\u00b5k, \u03a3k\n\u0001\n, (11.66b)\nwhich we identify as the GMM model from (11.3). Given a dataset X, we\nimmediately obtain the likelihood\np(X |\u03b8) =\nNY\nn=1\np(xn |\u03b8)\n(11.66b)\n=\nNY\nn=1\nKX\nk=1\n\u03c0kN\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\n, (11.67)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1482,
    "chunk_id": "p372_c0",
    "page_number": 372,
    "text": "366 Density Estimation with Gaussian Mixture Models\nFigure 11.12\nGraphical model for\na GMM with N data\npoints.\n\u03c0\nzn\nxn\u03a3k\n\u00b5k\nn = 1, . . . , Nk = 1, . . . , K\nwhich is exactly the GMM likelihood from (11.9). Therefore, the latent-\nvariable model with latent indicators zk is an equivalent way of thinking\nabout a Gaussian mixture model.\n11.4.3 Posterior Distribution\nLet us have a brief look at the posterior distribution on the latent variable\nz. According to Bayes\u2019 theorem, the posterior of thekth component having\ngenerated data point x\np(zk = 1 |x) = p(zk = 1)p(x|zk = 1)\np(x) , (11.68)\nwhere the marginal p(x) is given in (11.66b). This yields the posterior\ndistribution for the kth indicator variable zk\np(zk = 1 |x) = p(zk = 1)p(x|zk = 1)\nPK\nj=1 p(zj = 1)p(x|zj = 1)\n= \u03c0kN\n\u0000\nx|\u00b5k, \u03a3k\n\u0001\nPK\nj=1 \u03c0"
  },
  {
    "vector_id": 1483,
    "chunk_id": "p372_c1",
    "page_number": 372,
    "text": "marginal p(x) is given in (11.66b). This yields the posterior\ndistribution for the kth indicator variable zk\np(zk = 1 |x) = p(zk = 1)p(x|zk = 1)\nPK\nj=1 p(zj = 1)p(x|zj = 1)\n= \u03c0kN\n\u0000\nx|\u00b5k, \u03a3k\n\u0001\nPK\nj=1 \u03c0jN\n\u0000\nx|\u00b5j, \u03a3j\n\u0001 ,\n(11.69)\nwhich we identify as the responsibility of the kth mixture component for\ndata point x. Note that we omitted the explicit conditioning on the GMM\nparameters \u03c0k, \u00b5k, \u03a3k where k = 1, . . . , K.\n11.4.4 Extension to a Full Dataset\nThus far, we have only discussed the case where the dataset consists only\nof a single data point x. However, the concepts of the prior and posterior\ncan be directly extended to the case of N data points X := {x1, . . . ,xN }.\nIn the probabilistic interpretation of the GMM, every data pointxn pos-\nsesses its own latent variable\nzn = [zn1, . . . ,"
  },
  {
    "vector_id": 1484,
    "chunk_id": "p372_c2",
    "page_number": 372,
    "text": "can be directly extended to the case of N data points X := {x1, . . . ,xN }.\nIn the probabilistic interpretation of the GMM, every data pointxn pos-\nsesses its own latent variable\nzn = [zn1, . . . , znK]\u22a4 \u2208 RK . (11.70)\nPreviously (when we only considered a single data point x), we omitted\nthe index n, but now this becomes important.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1485,
    "chunk_id": "p373_c0",
    "page_number": 373,
    "text": "11.4 Latent-Variable Perspective 367\nWe share the same prior distribution \u03c0 across all latent variables zn.\nThe corresponding graphical model is shown in Figure 11.12, where we\nuse the plate notation.\nThe conditional distribution p(x1, . . . ,xN |z1, . . . ,zN ) factorizes over\nthe data points and is given as\np(x1, . . . ,xN |z1, . . . ,zN ) =\nNY\nn=1\np(xn |zn) . (11.71)\nTo obtain the posterior distribution p(znk = 1 |xn), we follow the same\nreasoning as in Section 11.4.3 and apply Bayes\u2019 theorem to obtain\np(znk = 1 |xn) = p(xn |znk = 1)p(znk = 1)\nPK\nj=1 p(xn |znj = 1)p(znj = 1)\n(11.72a)\n= \u03c0kN\n\u0000\nxn |\u00b5k, \u03a3k\n\u0001\nPK\nj=1 \u03c0jN\n\u0000\nxn |\u00b5j, \u03a3j\n\u0001 = rnk . (11.72b)\nThis means that p(zk = 1 |xn) is the (posterior) probability that the kth\nmixture component generated data point xn and corresponds to the re-"
  },
  {
    "vector_id": 1486,
    "chunk_id": "p373_c1",
    "page_number": 373,
    "text": "\u0000\nxn |\u00b5k, \u03a3k\n\u0001\nPK\nj=1 \u03c0jN\n\u0000\nxn |\u00b5j, \u03a3j\n\u0001 = rnk . (11.72b)\nThis means that p(zk = 1 |xn) is the (posterior) probability that the kth\nmixture component generated data point xn and corresponds to the re-\nsponsibility rnk we introduced in (11.17). Now the responsibilities also\nhave not only an intuitive but also a mathematically justified interpreta-\ntion as posterior probabilities.\n11.4.5 EM Algorithm Revisited\nThe EM algorithm that we introduced as an iterative scheme for maximum\nlikelihood estimation can be derived in a principled way from the latent-\nvariable perspective. Given a current setting\u03b8(t) of model parameters, the\nE-step calculates the expected log-likelihood\nQ(\u03b8 |\u03b8(t)) = Ez |x,\u03b8(t) [log p(x, z |\u03b8)] (11.73a)\n=\nZ\nlog p(x, z |\u03b8)p(z |x, \u03b8(t))dz , (11.73b)\nwhere the expectation of lo"
  },
  {
    "vector_id": 1487,
    "chunk_id": "p373_c2",
    "page_number": 373,
    "text": "ng\u03b8(t) of model parameters, the\nE-step calculates the expected log-likelihood\nQ(\u03b8 |\u03b8(t)) = Ez |x,\u03b8(t) [log p(x, z |\u03b8)] (11.73a)\n=\nZ\nlog p(x, z |\u03b8)p(z |x, \u03b8(t))dz , (11.73b)\nwhere the expectation of log p(x, z |\u03b8) is taken with respect to the poste-\nrior p(z |x, \u03b8(t)) of the latent variables. The M-step selects an updated set\nof model parameters \u03b8(t+1) by maximizing (11.73b).\nAlthough an EM iteration does increase the log-likelihood, there are\nno guarantees that EM converges to the maximum likelihood solution.\nIt is possible that the EM algorithm converges to a local maximum of\nthe log-likelihood. Different initializations of the parameters \u03b8 could be\nused in multiple EM runs to reduce the risk of ending up in a bad local\noptimum. We do not go into further details here, but refer to the exc"
  },
  {
    "vector_id": 1488,
    "chunk_id": "p373_c3",
    "page_number": 373,
    "text": "od. Different initializations of the parameters \u03b8 could be\nused in multiple EM runs to reduce the risk of ending up in a bad local\noptimum. We do not go into further details here, but refer to the excellent\nexpositions by Rogers and Girolami (2016) and Bishop (2006).\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1489,
    "chunk_id": "p374_c0",
    "page_number": 374,
    "text": "368 Density Estimation with Gaussian Mixture Models\n11.5 Further Reading\nThe GMM can be considered a generative model in the sense that it is\nstraightforward to generate new data using ancestral sampling (Bishop,\n2006). For given GMM parameters \u03c0k, \u00b5k, \u03a3k, k = 1, . . . , K, we sample\nan index k from the probability vector [\u03c01, . . . , \u03c0K]\u22a4 and then sample a\ndata point x \u223c N\n\u0000\n\u00b5k, \u03a3k\n\u0001\n. If we repeat thisN times, we obtain a dataset\nthat has been generated by a GMM. Figure 11.1 was generated using this\nprocedure.\nThroughout this chapter, we assumed that the number of components\nK is known. In practice, this is often not the case. However, we could use\nnested cross-validation, as discussed in Section 8.6.1, to find good models.\nGaussian mixture models are closely related to the K-means clust"
  },
  {
    "vector_id": 1490,
    "chunk_id": "p374_c1",
    "page_number": 374,
    "text": "ractice, this is often not the case. However, we could use\nnested cross-validation, as discussed in Section 8.6.1, to find good models.\nGaussian mixture models are closely related to the K-means clustering\nalgorithm. K-means also uses the EM algorithm to assign data points to\nclusters. If we treat the means in the GMM as cluster centers and ignore\nthe covariances (or set them to I), we arrive at K-means. As also nicely\ndescribed by MacKay (2003),K-means makes a \u201chard\u201d assignment of data\npoints to cluster centers \u00b5k, whereas a GMM makes a \u201csoft\u201d assignment\nvia the responsibilities.\nWe only touched upon the latent-variable perspective of GMMs and the\nEM algorithm. Note that EM can be used for parameter learning in general\nlatent-variable models, e.g., nonlinear state-space models (Ghahramani"
  },
  {
    "vector_id": 1491,
    "chunk_id": "p374_c2",
    "page_number": 374,
    "text": "hed upon the latent-variable perspective of GMMs and the\nEM algorithm. Note that EM can be used for parameter learning in general\nlatent-variable models, e.g., nonlinear state-space models (Ghahramani\nand Roweis, 1999; Roweis and Ghahramani, 1999) and for reinforcement\nlearning as discussed by Barber (2012). Therefore, the latent-variable per-\nspective of a GMM is useful to derive the corresponding EM algorithm in\na principled way (Bishop, 2006; Barber, 2012; Murphy, 2012).\nWe only discussed maximum likelihood estimation (via the EM algo-\nrithm) for finding GMM parameters. The standard criticisms of maximum\nlikelihood also apply here:\nAs in linear regression, maximum likelihood can suffer from severe\noverfitting. In the GMM case, this happens when the mean of a mix-\nture component is ident"
  },
  {
    "vector_id": 1492,
    "chunk_id": "p374_c3",
    "page_number": 374,
    "text": "sms of maximum\nlikelihood also apply here:\nAs in linear regression, maximum likelihood can suffer from severe\noverfitting. In the GMM case, this happens when the mean of a mix-\nture component is identical to a data point and the covariance tends to\n0. Then, the likelihood approaches infinity . Bishop (2006) and Barber\n(2012) discuss this issue in detail.\nWe only obtain a point estimate of the parameters \u03c0k, \u00b5k, \u03a3k for k =\n1, . . . , K, which does not give any indication of uncertainty in the pa-\nrameter values. A Bayesian approach would place a prior on the param-\neters, which can be used to obtain a posterior distribution on the param-\neters. This posterior allows us to compute the model evidence (marginal\nlikelihood), which can be used for model comparison, which gives us a\nprincipled wa"
  },
  {
    "vector_id": 1493,
    "chunk_id": "p374_c4",
    "page_number": 374,
    "text": "btain a posterior distribution on the param-\neters. This posterior allows us to compute the model evidence (marginal\nlikelihood), which can be used for model comparison, which gives us a\nprincipled way to determine the number of mixture components. Un-\nfortunately , closed-form inference is not possible in this setting because\nthere is no conjugate prior for this model. However, approximations,\nsuch as variational inference, can be used to obtain an approximate\nposterior (Bishop, 2006).\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1494,
    "chunk_id": "p375_c0",
    "page_number": 375,
    "text": "11.5 Further Reading 369\nFigure 11.13\nHistogram (orange\nbars) and kernel\ndensity estimation\n(blue line). The\nkernel density\nestimator produces\na smooth estimate\nof the underlying\ndensity , whereas the\nhistogram is an\nunsmoothed count\nmeasure of how\nmany data points\n(black) fall into a\nsingle bin.\n\u22124 \u22122 0 2 4 6 8\nx\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\np(x)\nData\nKDE\nHistogramIn this chapter, we discussed mixture models for density estimation.\nThere is a plethora of density estimation techniques available. In practice,\nwe often use histograms and kernel density estimation. histogram\nHistograms provide a nonparametric way to represent continuous den-\nsities and have been proposed by Pearson (1895). A histogram is con-\nstructed by \u201cbinning\u201d the data space and count, how many data points fall\ninto"
  },
  {
    "vector_id": 1495,
    "chunk_id": "p375_c1",
    "page_number": 375,
    "text": "ide a nonparametric way to represent continuous den-\nsities and have been proposed by Pearson (1895). A histogram is con-\nstructed by \u201cbinning\u201d the data space and count, how many data points fall\ninto each bin. Then a bar is drawn at the center of each bin, and the height\nof the bar is proportional to the number of data points within that bin. The\nbin size is a critical hyperparameter, and a bad choice can lead to overfit-\nting and underfitting. Cross-validation, as discussed in Section 8.2.4, can\nbe used to determine a good bin size. kernel density\nestimationKernel density estimation, independently proposed by Rosenblatt (1956)\nand Parzen (1962), is a nonparametric way for density estimation. Given\nN i.i.d. samples, the kernel density estimator represents the underlying\ndistribution as\np("
  },
  {
    "vector_id": 1496,
    "chunk_id": "p375_c2",
    "page_number": 375,
    "text": "dently proposed by Rosenblatt (1956)\nand Parzen (1962), is a nonparametric way for density estimation. Given\nN i.i.d. samples, the kernel density estimator represents the underlying\ndistribution as\np(x) = 1\nNh\nNX\nn=1\nk\n\u0012x \u2212 xn\nh\n\u0013\n, (11.74)\nwhere k is a kernel function, i.e., a nonnegative function that integrates to\n1 and h >0 is a smoothing/bandwidth parameter, which plays a similar\nrole as the bin size in histograms. Note that we place a kernel on every\nsingle data point xn in the dataset. Commonly used kernel functions are\nthe uniform distribution and the Gaussian distribution. Kernel density esti-\nmates are closely related to histograms, but by choosing a suitable kernel,\nwe can guarantee smoothness of the density estimate. Figure 11.13 illus-\ntrates the difference between a histogram"
  },
  {
    "vector_id": 1497,
    "chunk_id": "p375_c3",
    "page_number": 375,
    "text": "ity esti-\nmates are closely related to histograms, but by choosing a suitable kernel,\nwe can guarantee smoothness of the density estimate. Figure 11.13 illus-\ntrates the difference between a histogram and a kernel density estimator\n(with a Gaussian-shaped kernel) for a given dataset of 250 data points.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1498,
    "chunk_id": "p376_c0",
    "page_number": 376,
    "text": "12\nClassification with Support Vector Machines\nIn many situations, we want our machine learning algorithm to predict\none of a number of (discrete) outcomes. For example, an email client sorts\nmail into personal mail and junk mail, which has two outcomes. Another\nexample is a telescope that identifies whether an object in the night sky\nis a galaxy , star, or planet. There are usually a small number of outcomes,\nand more importantly there is usually no additional structure on these\noutcomes. In this chapter, we consider predictors that output binary val-An example of\nstructure is if the\noutcomes were\nordered, like in the\ncase of small,\nmedium, and large\nt-shirts.\nues, i.e., there are only two possible outcomes. This machine learning task\nis called binary classification. This is in contrast t"
  },
  {
    "vector_id": 1499,
    "chunk_id": "p376_c1",
    "page_number": 376,
    "text": "were\nordered, like in the\ncase of small,\nmedium, and large\nt-shirts.\nues, i.e., there are only two possible outcomes. This machine learning task\nis called binary classification. This is in contrast to Chapter 9, where we\nbinary classification\nconsidered a prediction problem with continuous-valued outputs.\nFor binary classification, the set of possible values that the label/output\ncan attain is binary , and for this chapter we denote them by{+1, \u22121}. In\nother words, we consider predictors of the form\nf : RD \u2192 {+1, \u22121}. (12.1)\nRecall from Chapter 8 that we represent each example (data point) xn\nas a feature vector of D real numbers. The labels are often referred to asInput example xn\nmay also be referred\nto as inputs, data\npoints, features, or\ninstances.\nthe positive and negative classes, r"
  },
  {
    "vector_id": 1500,
    "chunk_id": "p376_c2",
    "page_number": 376,
    "text": "as a feature vector of D real numbers. The labels are often referred to asInput example xn\nmay also be referred\nto as inputs, data\npoints, features, or\ninstances.\nthe positive and negative classes, respectively . One should be careful not\nclass\nto infer intuitive attributes of positiveness of the +1 class. For example,\nin a cancer detection task, a patient with cancer is often labeled +1. In\nprinciple, any two distinct values can be used, e.g., {True, False}, {0, 1}\nor {red, blue}. The problem of binary classification is well studied, andFor probabilistic\nmodels, it is\nmathematically\nconvenient to use\n{0, 1} as a binary\nrepresentation; see\nthe remark after\nExample 6.12.\nwe defer a survey of other approaches to Section 12.6.\nWe present an approach known as the support vector machine (SVM),"
  },
  {
    "vector_id": 1501,
    "chunk_id": "p376_c3",
    "page_number": 376,
    "text": "nt to use\n{0, 1} as a binary\nrepresentation; see\nthe remark after\nExample 6.12.\nwe defer a survey of other approaches to Section 12.6.\nWe present an approach known as the support vector machine (SVM),\nwhich solves the binary classification task. As in regression, we have a su-\npervised learning task, where we have a set of examples xn \u2208 RD along\nwith their corresponding (binary) labels yn \u2208 {+1, \u22121}. Given a train-\ning data set consisting of example\u2013label pairs{(x1, y1), . . . ,(xN , yN )}, we\nwould like to estimate parameters of the model that will give the smallest\nclassification error. Similar to Chapter 9, we consider a linear model, and\nhide away the nonlinearity in a transformation \u03d5 of the examples (9.13).\nWe will revisit \u03d5 in Section 12.4.\nThe SVM provides state-of-the-art results"
  },
  {
    "vector_id": 1502,
    "chunk_id": "p376_c4",
    "page_number": 376,
    "text": "lar to Chapter 9, we consider a linear model, and\nhide away the nonlinearity in a transformation \u03d5 of the examples (9.13).\nWe will revisit \u03d5 in Section 12.4.\nThe SVM provides state-of-the-art results in many applications, with\nsound theoretical guarantees (Steinwart and Christmann, 2008). There\nare two main reasons why we chose to illustrate binary classification using\n370\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 1503,
    "chunk_id": "p376_c5",
    "page_number": 376,
    "text": ". Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 1504,
    "chunk_id": "p377_c0",
    "page_number": 377,
    "text": "Classification with Support Vector Machines 371\nFigure 12.1\nExample 2D data,\nillustrating the\nintuition of data\nwhere we can find a\nlinear classifier that\nseparates orange\ncrosses from blue\ndiscs.\nx(1)\nx(2)\nSVMs. First, the SVM allows for a geometric way to think about supervised\nmachine learning. While in Chapter 9 we considered the machine learning\nproblem in terms of probabilistic models and attacked it using maximum\nlikelihood estimation and Bayesian inference, here we will consider an\nalternative approach where we reason geometrically about the machine\nlearning task. It relies heavily on concepts, such as inner products and\nprojections, which we discussed in Chapter 3. The second reason why we\nfind SVMs instructive is that in contrast to Chapter 9, the optimization\nproblem for SVM doe"
  },
  {
    "vector_id": 1505,
    "chunk_id": "p377_c1",
    "page_number": 377,
    "text": "concepts, such as inner products and\nprojections, which we discussed in Chapter 3. The second reason why we\nfind SVMs instructive is that in contrast to Chapter 9, the optimization\nproblem for SVM does not admit an analytic solution so that we need to\nresort to a variety of optimization tools introduced in Chapter 7.\nThe SVM view of machine learning is subtly different from the max-\nimum likelihood view of Chapter 9. The maximum likelihood view pro-\nposes a model based on a probabilistic view of the data distribution, from\nwhich an optimization problem is derived. In contrast, the SVM view starts\nby designing a particular function that is to be optimized during training,\nbased on geometric intuitions. We have seen something similar already\nin Chapter 10, where we derived PCA from geometric"
  },
  {
    "vector_id": 1506,
    "chunk_id": "p377_c2",
    "page_number": 377,
    "text": "rts\nby designing a particular function that is to be optimized during training,\nbased on geometric intuitions. We have seen something similar already\nin Chapter 10, where we derived PCA from geometric principles. In the\nSVM case, we start by designing a loss function that is to be minimized\non training data, following the principles of empirical risk minimization\n(Section 8.2).\nLet us derive the optimization problem corresponding to training an\nSVM on example\u2013label pairs. Intuitively , we imagine binary classification\ndata, which can be separated by a hyperplane as illustrated in Figure 12.1.\nHere, every example xn (a vector of dimension 2) is a two-dimensional\nlocation (x(1)\nn and x(2)\nn ), and the corresponding binary label yn is one of\ntwo different symbols (orange cross or blue disc)."
  },
  {
    "vector_id": 1507,
    "chunk_id": "p377_c3",
    "page_number": 377,
    "text": "Here, every example xn (a vector of dimension 2) is a two-dimensional\nlocation (x(1)\nn and x(2)\nn ), and the corresponding binary label yn is one of\ntwo different symbols (orange cross or blue disc). \u201cHyperplane\u201d is a word\nthat is commonly used in machine learning, and we encountered hyper-\nplanes already in Section 2.8. A hyperplane is an affine subspace of di-\nmension D \u2212 1 (if the corresponding vector space is of dimension D).\nThe examples consist of two classes (there are two possible labels) that\nhave features (the components of the vector representing the example)\narranged in such a way as to allow us to separate/classify them by draw-\ning a straight line.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1508,
    "chunk_id": "p377_c4",
    "page_number": 377,
    "text": "as to allow us to separate/classify them by draw-\ning a straight line.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1509,
    "chunk_id": "p378_c0",
    "page_number": 378,
    "text": "372 Classification with Support Vector Machines\nIn the following, we formalize the idea of finding a linear separator\nof the two classes. We introduce the idea of the margin and then extend\nlinear separators to allow for examples to fall on the \u201cwrong\u201d side, incur-\nring a classification error. We present two equivalent ways of formalizing\nthe SVM: the geometric view (Section 12.2.4) and the loss function view\n(Section 12.2.5). We derive the dual version of the SVM using Lagrange\nmultipliers (Section 7.2). The dual SVM allows us to observe a third way\nof formalizing the SVM: in terms of the convex hulls of the examples of\neach class (Section 12.3.2). We conclude by briefly describing kernels and\nhow to numerically solve the nonlinear kernel-SVM optimization problem.\n12.1 Separating Hyperpla"
  },
  {
    "vector_id": 1510,
    "chunk_id": "p378_c1",
    "page_number": 378,
    "text": "vex hulls of the examples of\neach class (Section 12.3.2). We conclude by briefly describing kernels and\nhow to numerically solve the nonlinear kernel-SVM optimization problem.\n12.1 Separating Hyperplanes\nGiven two examples represented as vectorsxi and xj, one way to compute\nthe similarity between them is using an inner product\u27e8xi, xj\u27e9. Recall from\nSection 3.2 that inner products are closely related to the angle between\ntwo vectors. The value of the inner product between two vectors depends\non the length (norm) of each vector. Furthermore, inner products allow\nus to rigorously define geometric concepts such as orthogonality and pro-\njections.\nThe main idea behind many classification algorithms is to represent\ndata in RD and then partition this space, ideally in a way that examples\nwith the"
  },
  {
    "vector_id": 1511,
    "chunk_id": "p378_c2",
    "page_number": 378,
    "text": "concepts such as orthogonality and pro-\njections.\nThe main idea behind many classification algorithms is to represent\ndata in RD and then partition this space, ideally in a way that examples\nwith the same label (and no other examples) are in the same partition.\nIn the case of binary classification, the space would be divided into two\nparts corresponding to the positive and negative classes, respectively . We\nconsider a particularly convenient partition, which is to (linearly) split\nthe space into two halves using a hyperplane. Let example x \u2208 RD be an\nelement of the data space. Consider a function\nf : RD \u2192 R (12.2a)\nx 7\u2192 f(x) := \u27e8w, x\u27e9 + b , (12.2b)\nparametrized by w \u2208 RD and b \u2208 R. Recall from Section 2.8 that hy-\nperplanes are affine subspaces. Therefore, we define the hyperplane that\nse"
  },
  {
    "vector_id": 1512,
    "chunk_id": "p378_c3",
    "page_number": 378,
    "text": "tion\nf : RD \u2192 R (12.2a)\nx 7\u2192 f(x) := \u27e8w, x\u27e9 + b , (12.2b)\nparametrized by w \u2208 RD and b \u2208 R. Recall from Section 2.8 that hy-\nperplanes are affine subspaces. Therefore, we define the hyperplane that\nseparates the two classes in our binary classification problem as\n\b\nx \u2208 RD : f(x) = 0\n\t\n. (12.3)\nAn illustration of the hyperplane is shown in Figure 12.2, where the\nvector w is a vector normal to the hyperplane and b the intercept. We can\nderive that w is a normal vector to the hyperplane in (12.3) by choosing\nany two examples xa and xb on the hyperplane and showing that the\nvector between them is orthogonal to w. In the form of an equation,\nf(xa) \u2212 f(xb) = \u27e8w, xa\u27e9 + b \u2212 (\u27e8w, xb\u27e9 + b) (12.4a)\n= \u27e8w, xa \u2212 xb\u27e9 , (12.4b)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml"
  },
  {
    "vector_id": 1513,
    "chunk_id": "p378_c4",
    "page_number": 378,
    "text": "orthogonal to w. In the form of an equation,\nf(xa) \u2212 f(xb) = \u27e8w, xa\u27e9 + b \u2212 (\u27e8w, xb\u27e9 + b) (12.4a)\n= \u27e8w, xa \u2212 xb\u27e9 , (12.4b)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1514,
    "chunk_id": "p379_c0",
    "page_number": 379,
    "text": "12.1 Separating Hyperplanes 373\nFigure 12.2\nEquation of a\nseparating\nhyperplane (12.3).\n(a) The standard\nway of representing\nthe equation in 3D.\n(b) For ease of\ndrawing, we look at\nthe hyperplane edge\non.\nw\n(a) Separating hyperplane in 3D\nw\n.0\n.Positive\n.Negative\nb\n(b) Projection of the setting in (a) onto\na plane\nwhere the second line is obtained by the linearity of the inner product\n(Section 3.2). Since we have chosen xa and xb to be on the hyperplane,\nthis implies that f(xa) = 0 and f(xb) = 0 and hence \u27e8w, xa \u2212 xb\u27e9 = 0.\nRecall that two vectors are orthogonal when their inner product is zero. w is orthogonal to\nany vector on the\nhyperplane.\nTherefore, we obtain thatw is orthogonal to any vector on the hyperplane.\nRemark. Recall from Chapter 2 that we can think of vectors in different\nway"
  },
  {
    "vector_id": 1515,
    "chunk_id": "p379_c1",
    "page_number": 379,
    "text": ". w is orthogonal to\nany vector on the\nhyperplane.\nTherefore, we obtain thatw is orthogonal to any vector on the hyperplane.\nRemark. Recall from Chapter 2 that we can think of vectors in different\nways. In this chapter, we think of the parameter vector w as an arrow\nindicating a direction, i.e., we consider w to be a geometric vector. In\ncontrast, we think of the example vector x as a data point (as indicated\nby its coordinates), i.e., we consider x to be the coordinates of a vector\nwith respect to the standard basis. \u2662\nWhen presented with a test example, we classify the example as pos-\nitive or negative depending on the side of the hyperplane on which it\noccurs. Note that (12.3) not only defines a hyperplane; it additionally de-\nfines a direction. In other words, it defines the positive a"
  },
  {
    "vector_id": 1516,
    "chunk_id": "p379_c2",
    "page_number": 379,
    "text": "or negative depending on the side of the hyperplane on which it\noccurs. Note that (12.3) not only defines a hyperplane; it additionally de-\nfines a direction. In other words, it defines the positive and negative side\nof the hyperplane. Therefore, to classify a test example xtest, we calcu-\nlate the value of the function f(xtest) and classify the example as +1 if\nf(xtest) \u2a7e 0 and \u22121 otherwise. Thinking geometrically , the positive ex-\namples lie \u201cabove\u201d the hyperplane and the negative examples \u201cbelow\u201d the\nhyperplane.\nWhen training the classifier, we want to ensure that the examples with\npositive labels are on the positive side of the hyperplane, i.e.,\n\u27e8w, xn\u27e9 + b \u2a7e 0 when yn = +1 (12.5)\nand the examples with negative labels are on the negative side, i.e.,\n\u27e8w, xn\u27e9 + b <0 when yn = \u22121 . (12.6"
  },
  {
    "vector_id": 1517,
    "chunk_id": "p379_c3",
    "page_number": 379,
    "text": "e labels are on the positive side of the hyperplane, i.e.,\n\u27e8w, xn\u27e9 + b \u2a7e 0 when yn = +1 (12.5)\nand the examples with negative labels are on the negative side, i.e.,\n\u27e8w, xn\u27e9 + b <0 when yn = \u22121 . (12.6)\nRefer to Figure 12.2 for a geometric intuition of positive and negative\nexamples. These two conditions are often presented in a single equation\nyn(\u27e8w, xn\u27e9 + b) \u2a7e 0 . (12.7)\nEquation (12.7) is equivalent to (12.5) and (12.6) when we multiply both\nsides of (12.5) and (12.6) with yn = 1 and yn = \u22121, respectively .\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1518,
    "chunk_id": "p379_c4",
    "page_number": 379,
    "text": "ess (2020)."
  },
  {
    "vector_id": 1519,
    "chunk_id": "p380_c0",
    "page_number": 380,
    "text": "374 Classification with Support Vector Machines\nFigure 12.3\nPossible separating\nhyperplanes. There\nare many linear\nclassifiers (green\nlines) that separate\norange crosses from\nblue discs.\nx(1)\nx(2)\n12.2 Primal Support Vector Machine\nBased on the concept of distances from points to a hyperplane, we now\nare in a position to discuss the support vector machine. For a dataset\n{(x1, y1), . . . ,(xN , yN )} that is linearly separable, we have infinitely many\ncandidate hyperplanes (refer to Figure 12.3), and therefore classifiers,\nthat solve our classification problem without any (training) errors. To find\na unique solution, one idea is to choose the separating hyperplane that\nmaximizes the margin between the positive and negative examples. In\nother words, we want the positive and negative examples"
  },
  {
    "vector_id": 1520,
    "chunk_id": "p380_c1",
    "page_number": 380,
    "text": "find\na unique solution, one idea is to choose the separating hyperplane that\nmaximizes the margin between the positive and negative examples. In\nother words, we want the positive and negative examples to be separated\nby a large margin (Section 12.2.1). In the following, we compute the dis-A classifier with\nlarge margin turns\nout to generalize\nwell (Steinwart and\nChristmann, 2008).\ntance between an example and a hyperplane to derive the margin. Recall\nthat the closest point on the hyperplane to a given point (example xn) is\nobtained by the orthogonal projection (Section 3.8).\n12.2.1 Concept of the Margin\nThe concept of the margin is intuitively simple: It is the distance of themargin\nseparating hyperplane to the closest examples in the dataset, assumingThere could be two\nor more closest\nexa"
  },
  {
    "vector_id": 1521,
    "chunk_id": "p380_c2",
    "page_number": 380,
    "text": "the Margin\nThe concept of the margin is intuitively simple: It is the distance of themargin\nseparating hyperplane to the closest examples in the dataset, assumingThere could be two\nor more closest\nexamples to a\nhyperplane.\nthat the dataset is linearly separable. However, when trying to formalize\nthis distance, there is a technical wrinkle that may be confusing. The tech-\nnical wrinkle is that we need to define a scale at which to measure the\ndistance. A potential scale is to consider the scale of the data, i.e., the raw\nvalues of xn. There are problems with this, as we could change the units\nof measurement of xn and change the values in xn, and, hence, change\nthe distance to the hyperplane. As we will see shortly , we define the scale\nbased on the equation of the hyperplane (12.3) itself."
  },
  {
    "vector_id": 1522,
    "chunk_id": "p380_c3",
    "page_number": 380,
    "text": "f measurement of xn and change the values in xn, and, hence, change\nthe distance to the hyperplane. As we will see shortly , we define the scale\nbased on the equation of the hyperplane (12.3) itself.\nConsider a hyperplane \u27e8w, x\u27e9 + b, and an example xa as illustrated in\nFigure 12.4. Without loss of generality , we can consider the example xa\nto be on the positive side of the hyperplane, i.e., \u27e8w, xa\u27e9 + b > 0. We\nwould like to compute the distance r >0 of xa from the hyperplane. We\ndo so by considering the orthogonal projection (Section 3.8) of xa onto\nthe hyperplane, which we denote by x\u2032\na. Since w is orthogonal to the\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1523,
    "chunk_id": "p380_c4",
    "page_number": 380,
    "text": "nce w is orthogonal to the\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1524,
    "chunk_id": "p381_c0",
    "page_number": 381,
    "text": "12.2 Primal Support Vector Machine 375\nFigure 12.4 Vector\naddition to express\ndistance to\nhyperplane:\nxa = x\u2032\na + r w\n\u2225w\u2225.\n.0\n.xa\nw.x\u2032a\nr\nhyperplane, we know that the distance r is just a scaling of this vector w.\nIf the length of w is known, then we can use this scaling factor r factor\nto work out the absolute distance between xa and x\u2032\na. For convenience,\nwe choose to use a vector of unit length (its norm is 1) and obtain this\nby dividing w by its norm, w\n\u2225w\u2225. Using vector addition (Section 2.4), we\nobtain\nxa = x\u2032\na + r w\n\u2225w\u2225 . (12.8)\nAnother way of thinking about r is that it is the coordinate of xa in the\nsubspace spanned by w/ \u2225w\u2225. We have now expressed the distance of xa\nfrom the hyperplane as r, and if we choose xa to be the point closest to\nthe hyperplane, this distance r is the ma"
  },
  {
    "vector_id": 1525,
    "chunk_id": "p381_c1",
    "page_number": 381,
    "text": "ate of xa in the\nsubspace spanned by w/ \u2225w\u2225. We have now expressed the distance of xa\nfrom the hyperplane as r, and if we choose xa to be the point closest to\nthe hyperplane, this distance r is the margin.\nRecall that we would like the positive examples to be further than r\nfrom the hyperplane, and the negative examples to be further than dis-\ntance r (in the negative direction) from the hyperplane. Analogously to\nthe combination of (12.5) and (12.6) into (12.7), we formulate this ob-\njective as\nyn(\u27e8w, xn\u27e9 + b) \u2a7e r . (12.9)\nIn other words, we combine the requirements that examples are at least\nr away from the hyperplane (in the positive and negative direction) into\none single inequality .\nSince we are interested only in the direction, we add an assumption to\nour model that the parameter ve"
  },
  {
    "vector_id": 1526,
    "chunk_id": "p381_c2",
    "page_number": 381,
    "text": "r away from the hyperplane (in the positive and negative direction) into\none single inequality .\nSince we are interested only in the direction, we add an assumption to\nour model that the parameter vector w is of unit length, i.e., \u2225w\u2225 = 1,\nwhere we use the Euclidean norm \u2225w\u2225 =\n\u221a\nw\u22a4w (Section 3.1). This We will see other\nchoices of inner\nproducts\n(Section 3.2) in\nSection 12.4.\nassumption also allows a more intuitive interpretation of the distance r\n(12.8) since it is the scaling factor of a vector of length 1.\nRemark. A reader familiar with other presentations of the margin would\nnotice that our definition of \u2225w\u2225 = 1 is different from the standard\npresentation if the SVM was the one provided by Sch \u00a8olkopf and Smola\n(2002), for example. In Section 12.2.3, we will show the equivalence of\nbo"
  },
  {
    "vector_id": 1527,
    "chunk_id": "p381_c3",
    "page_number": 381,
    "text": "ur definition of \u2225w\u2225 = 1 is different from the standard\npresentation if the SVM was the one provided by Sch \u00a8olkopf and Smola\n(2002), for example. In Section 12.2.3, we will show the equivalence of\nboth approaches. \u2662\nCollecting the three requirements into a single constrained optimization\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1528,
    "chunk_id": "p382_c0",
    "page_number": 382,
    "text": "376 Classification with Support Vector Machines\nFigure 12.5\nDerivation of the\nmargin: r = 1\n\u2225w\u2225.\n.xa\nw\n\u27e8w,x\u27e9+b= 0\n\u27e8w,x\u27e9+b= 1\n.x\u2032a\nr\nproblem, we obtain the objective\nmax\nw,b,r\nr|{z}\nmargin\nsubject to yn(\u27e8w, xn\u27e9 + b) \u2a7e r| {z }\ndata fitting\n, \u2225w\u2225 = 1| {z }\nnormalization\n, r > 0 , (12.10)\nwhich says that we want to maximize the margin r while ensuring that\nthe data lies on the correct side of the hyperplane.\nRemark. The concept of the margin turns out to be highly pervasive in ma-\nchine learning. It was used by Vladimir Vapnik and Alexey Chervonenkis\nto show that when the margin is large, the \u201ccomplexity\u201d of the function\nclass is low, and hence learning is possible (Vapnik, 2000). It turns out\nthat the concept is useful for various different approaches for theoret-\nically analyzing generalizat"
  },
  {
    "vector_id": 1529,
    "chunk_id": "p382_c1",
    "page_number": 382,
    "text": "plexity\u201d of the function\nclass is low, and hence learning is possible (Vapnik, 2000). It turns out\nthat the concept is useful for various different approaches for theoret-\nically analyzing generalization error (Steinwart and Christmann, 2008;\nShalev-Shwartz and Ben-David, 2014). \u2662\n12.2.2 Traditional Derivation of the Margin\nIn the previous section, we derived (12.10) by making the observation that\nwe are only interested in the direction of w and not its length, leading to\nthe assumption that \u2225w\u2225 = 1. In this section, we derive the margin max-\nimization problem by making a different assumption. Instead of choosing\nthat the parameter vector is normalized, we choose a scale for the data.\nWe choose this scale such that the value of the predictor\u27e8w, x\u27e9 + b is 1 at\nthe closest example. Let us al"
  },
  {
    "vector_id": 1530,
    "chunk_id": "p382_c2",
    "page_number": 382,
    "text": "Instead of choosing\nthat the parameter vector is normalized, we choose a scale for the data.\nWe choose this scale such that the value of the predictor\u27e8w, x\u27e9 + b is 1 at\nthe closest example. Let us also denote the example in the dataset that isRecall that we\ncurrently consider\nlinearly separable\ndata.\nclosest to the hyperplane by xa.\nFigure 12.5 is identical to Figure 12.4, except that now we rescaled the\naxes, such that the example xa lies exactly on the margin, i.e., \u27e8w, xa\u27e9 +\nb = 1. Since x\u2032\na is the orthogonal projection of xa onto the hyperplane, it\nmust by definition lie on the hyperplane, i.e.,\n\u27e8w, x\u2032\na\u27e9 + b = 0 . (12.11)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1531,
    "chunk_id": "p382_c3",
    "page_number": 382,
    "text": "e, i.e.,\n\u27e8w, x\u2032\na\u27e9 + b = 0 . (12.11)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1532,
    "chunk_id": "p383_c0",
    "page_number": 383,
    "text": "12.2 Primal Support Vector Machine 377\nBy substituting (12.8) into (12.11), we obtain\n\u001c\nw, xa \u2212 r w\n\u2225w\u2225\n\u001d\n+ b = 0 . (12.12)\nExploiting the bilinearity of the inner product (see Section 3.2), we get\n\u27e8w, xa\u27e9 + b \u2212 r\u27e8w, w\u27e9\n\u2225w\u2225 = 0 . (12.13)\nObserve that the first term is 1 by our assumption of scale, i.e., \u27e8w, xa\u27e9 +\nb = 1. From (3.16) in Section 3.1, we know that \u27e8w, w\u27e9 = \u2225w\u22252. Hence,\nthe second term reduces to r\u2225w\u2225. Using these simplifications, we obtain\nr = 1\n\u2225w\u2225 . (12.14)\nThis means we derived the distance r in terms of the normal vector w\nof the hyperplane. At first glance, this equation is counterintuitive as we We can also think of\nthe distance as the\nprojection error that\nincurs when\nprojecting xa onto\nthe hyperplane.\nseem to have derived the distance from the hyperplane in terms of th"
  },
  {
    "vector_id": 1533,
    "chunk_id": "p383_c1",
    "page_number": 383,
    "text": "ounterintuitive as we We can also think of\nthe distance as the\nprojection error that\nincurs when\nprojecting xa onto\nthe hyperplane.\nseem to have derived the distance from the hyperplane in terms of the\nlength of the vector w, but we do not yet know this vector. One way to\nthink about it is to consider the distance r to be a temporary variable\nthat we only use for this derivation. Therefore, for the rest of this section\nwe will denote the distance to the hyperplane by 1\n\u2225w\u2225. In Section 12.2.3,\nwe will see that the choice that the margin equals 1 is equivalent to our\nprevious assumption of \u2225w\u2225 = 1 in Section 12.2.1.\nSimilar to the argument to obtain (12.9), we want the positive and\nnegative examples to be at least1 away from the hyperplane, which yields\nthe condition\nyn(\u27e8w, xn\u27e9 + b) \u2a7e 1 . (1"
  },
  {
    "vector_id": 1534,
    "chunk_id": "p383_c2",
    "page_number": 383,
    "text": "1 in Section 12.2.1.\nSimilar to the argument to obtain (12.9), we want the positive and\nnegative examples to be at least1 away from the hyperplane, which yields\nthe condition\nyn(\u27e8w, xn\u27e9 + b) \u2a7e 1 . (12.15)\nCombining the margin maximization with the fact that examples need to\nbe on the correct side of the hyperplane (based on their labels) gives us\nmax\nw,b\n1\n\u2225w\u2225 (12.16)\nsubject to yn(\u27e8w, xn\u27e9 + b) \u2a7e 1 for all n = 1, . . . , N.(12.17)\nInstead of maximizing the reciprocal of the norm as in (12.16), we often\nminimize the squared norm. We also often include a constant 1\n2 that does The squared norm\nresults in a convex\nquadratic\nprogramming\nproblem for the\nSVM (Section 12.5).\nnot affect the optimal w, bbut yields a tidier form when we compute the\ngradient. Then, our objective becomes\nmin\nw,b\n1\n2\u2225"
  },
  {
    "vector_id": 1535,
    "chunk_id": "p383_c3",
    "page_number": 383,
    "text": "results in a convex\nquadratic\nprogramming\nproblem for the\nSVM (Section 12.5).\nnot affect the optimal w, bbut yields a tidier form when we compute the\ngradient. Then, our objective becomes\nmin\nw,b\n1\n2\u2225w\u22252 (12.18)\nsubject to yn(\u27e8w, xn\u27e9 + b) \u2a7e 1 for all n = 1, . . . , N .(12.19)\nEquation (12.18) is known as the hard margin SVM . The reason for the hard margin SVM\nexpression \u201chard\u201d is because the formulation does not allow for any vi-\nolations of the margin condition. We will see in Section 12.2.4 that this\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1536,
    "chunk_id": "p383_c4",
    "page_number": 383,
    "text": "020)."
  },
  {
    "vector_id": 1537,
    "chunk_id": "p384_c0",
    "page_number": 384,
    "text": "378 Classification with Support Vector Machines\n\u201chard\u201d condition can be relaxed to accommodate violations if the data is\nnot linearly separable.\n12.2.3 Why We Can Set the Margin to 1\nIn Section 12.2.1, we argued that we would like to maximize some value\nr, which represents the distance of the closest example to the hyperplane.\nIn Section 12.2.2, we scaled the data such that the closest example is of\ndistance 1 to the hyperplane. In this section, we relate the two derivations,\nand show that they are equivalent.\nTheorem 12.1. Maximizing the margin r, where we consider normalized\nweights as in (12.10),\nmax\nw,b,r\nr|{z}\nmargin\nsubject to yn(\u27e8w, xn\u27e9 + b) \u2a7e r| {z }\ndata fitting\n, \u2225w\u2225 = 1| {z }\nnormalization\n, r > 0 , (12.20)\nis equivalent to scaling the data, such that the margin is unity:\nmin\nw,"
  },
  {
    "vector_id": 1538,
    "chunk_id": "p384_c1",
    "page_number": 384,
    "text": "2.10),\nmax\nw,b,r\nr|{z}\nmargin\nsubject to yn(\u27e8w, xn\u27e9 + b) \u2a7e r| {z }\ndata fitting\n, \u2225w\u2225 = 1| {z }\nnormalization\n, r > 0 , (12.20)\nis equivalent to scaling the data, such that the margin is unity:\nmin\nw,b\n1\n2 \u2225w\u2225\n2\n| {z }\nmargin\nsubject to yn(\u27e8w, xn\u27e9 + b) \u2a7e 1| {z }\ndata fitting\n.\n(12.21)\nProof Consider (12.20). Since the square is a strictly monotonic trans-\nformation for non-negative arguments, the maximum stays the same if we\nconsider r2 in the objective. Since \u2225w\u2225 = 1 we can reparametrize the\nequation with a new weight vector w\u2032 that is not normalized by explicitly\nusing w\u2032\n\u2225w\u2032\u2225. We obtain\nmax\nw\u2032,b,r\nr2\nsubject to yn\n\u0012\u001c w\u2032\n\u2225w\u2032\u2225, xn\n\u001d\n+ b\n\u0013\n\u2a7e r, r > 0 .\n(12.22)\nEquation (12.22) explicitly states that the distancer is positive. Therefore,\nwe can divide the first constraint by r, which yields"
  },
  {
    "vector_id": 1539,
    "chunk_id": "p384_c2",
    "page_number": 384,
    "text": "w\u2032,b,r\nr2\nsubject to yn\n\u0012\u001c w\u2032\n\u2225w\u2032\u2225, xn\n\u001d\n+ b\n\u0013\n\u2a7e r, r > 0 .\n(12.22)\nEquation (12.22) explicitly states that the distancer is positive. Therefore,\nwe can divide the first constraint by r, which yieldsNote that r >0\nbecause we\nassumed linear\nseparability , and\nhence there is no\nissue to divide by r.\nmax\nw\u2032,b,r\nr2\nsubject to yn\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ed\n*\nw\u2032\n\u2225w\u2032\u2225r| {z }\nw\u2032\u2032\n, xn\n+\n+ b\nr|{z}\nb\u2032\u2032\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8 \u2a7e 1, r > 0\n(12.23)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1540,
    "chunk_id": "p385_c0",
    "page_number": 385,
    "text": "12.2 Primal Support Vector Machine 379\nFigure 12.6\n(a) Linearly\nseparable and\n(b) non-linearly\nseparable data.\nx(1)\nx(2)\n(a) Linearly separable data, with a large\nmargin\nx(1)\nx(2)\n(b) Non-linearly separable data\nrenaming the parameters tow\u2032\u2032 and b\u2032\u2032. Since w\u2032\u2032 = w\u2032\n\u2225w\u2032\u2225r , rearranging for\nr gives\n\u2225w\u2032\u2032\u2225 =\n\r\r\r\r\nw\u2032\n\u2225w\u2032\u2225r\n\r\r\r\r = 1\nr \u00b7\n\r\r\r\r\nw\u2032\n\u2225w\u2032\u2225\n\r\r\r\r = 1\nr . (12.24)\nBy substituting this result into (12.23), we obtain\nmax\nw\u2032\u2032,b\u2032\u2032\n1\n\u2225w\u2032\u2032\u2225\n2\nsubject to yn (\u27e8w\u2032\u2032, xn\u27e9 + b\u2032\u2032) \u2a7e 1 .\n(12.25)\nThe final step is to observe that maximizing 1\n\u2225w\u2032\u2032\u22252 yields the same solution\nas minimizing 1\n2 \u2225w\u2032\u2032\u2225\n2\n, which concludes the proof of Theorem 12.1.\n12.2.4 Soft Margin SVM: Geometric View\nIn the case where data is not linearly separable, we may wish to allow\nsome examples to fall within the margin region, or ev"
  },
  {
    "vector_id": 1541,
    "chunk_id": "p385_c1",
    "page_number": 385,
    "text": "concludes the proof of Theorem 12.1.\n12.2.4 Soft Margin SVM: Geometric View\nIn the case where data is not linearly separable, we may wish to allow\nsome examples to fall within the margin region, or even to be on the\nwrong side of the hyperplane as illustrated in Figure 12.6.\nThe model that allows for some classification errors is called the soft soft margin SVM\nmargin SVM. In this section, we derive the resulting optimization problem\nusing geometric arguments. In Section 12.2.5, we will derive an equiv-\nalent optimization problem using the idea of a loss function. Using La-\ngrange multipliers (Section 7.2), we will derive the dual optimization\nproblem of the SVM in Section 12.3. This dual optimization problem al-\nlows us to observe a third interpretation of the SVM: as a hyperplane that\nbi"
  },
  {
    "vector_id": 1542,
    "chunk_id": "p385_c2",
    "page_number": 385,
    "text": "(Section 7.2), we will derive the dual optimization\nproblem of the SVM in Section 12.3. This dual optimization problem al-\nlows us to observe a third interpretation of the SVM: as a hyperplane that\nbisects the line between convex hulls corresponding to the positive and\nnegative data examples (Section 12.3.2).\nThe key geometric idea is to introduce aslack variable \u03ben corresponding slack variable\nto each example\u2013label pair (xn, yn) that allows a particular example to be\nwithin the margin or even on the wrong side of the hyperplane (refer to\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1543,
    "chunk_id": "p385_c3",
    "page_number": 385,
    "text": "hed by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1544,
    "chunk_id": "p386_c0",
    "page_number": 386,
    "text": "380 Classification with Support Vector Machines\nFigure 12.7 Soft\nmargin SVM allows\nexamples to be\nwithin the margin or\non the wrong side of\nthe hyperplane. The\nslack variable \u03be\nmeasures the\ndistance of a\npositive example\nx+ to the positive\nmargin hyperplane\n\u27e8w, x\u27e9 + b = 1\nwhen x+ is on the\nwrong side.\n.x+\nw\n\u27e8w,x\u27e9+b= 0\n\u27e8w,x\u27e9+b= 1\n.\n\u03be\nFigure 12.7). We subtract the value of \u03ben from the margin, constraining\n\u03ben to be non-negative. To encourage correct classification of the samples,\nwe add \u03ben to the objective\nmin\nw,b,\u03be\n1\n2\u2225w\u22252 + C\nNX\nn=1\n\u03ben (12.26a)\nsubject to yn(\u27e8w, xn\u27e9 + b) \u2a7e 1 \u2212 \u03ben (12.26b)\n\u03ben \u2a7e 0 (12.26c)\nfor n = 1, . . . , N. In contrast to the optimization problem (12.18) for the\nhard margin SVM, this one is called the soft margin SVM. The parametersoft margin SVM\nC >0 trades off the size"
  },
  {
    "vector_id": 1545,
    "chunk_id": "p386_c1",
    "page_number": 386,
    "text": "0 (12.26c)\nfor n = 1, . . . , N. In contrast to the optimization problem (12.18) for the\nhard margin SVM, this one is called the soft margin SVM. The parametersoft margin SVM\nC >0 trades off the size of the margin and the total amount of slack that\nwe have. This parameter is called the regularization parameter since, asregularization\nparameter we will see in the following section, the margin term in the objective func-\ntion (12.26a) is a regularization term. The margin term \u2225w\u22252 is called\nthe regularizer, and in many books on numerical optimization, the reg-regularizer\nularization parameter is multiplied with this term (Section 8.2.3). This\nis in contrast to our formulation in this section. Here a large value of C\nimplies low regularization, as we give the slack variables larger weight,\nhe"
  },
  {
    "vector_id": 1546,
    "chunk_id": "p386_c2",
    "page_number": 386,
    "text": "multiplied with this term (Section 8.2.3). This\nis in contrast to our formulation in this section. Here a large value of C\nimplies low regularization, as we give the slack variables larger weight,\nhence giving more priority to examples that do not lie on the correct side\nof the margin.There are\nalternative\nparametrizations of\nthis regularization,\nwhich is\nwhy (12.26a) is also\noften referred to as\nthe C-SVM.\nRemark. In the formulation of the soft margin SVM (12.26a) w is reg-\nularized, but b is not regularized. We can see this by observing that the\nregularization term does not contain b. The unregularized term b com-\nplicates theoretical analysis (Steinwart and Christmann, 2008, chapter 1)\nand decreases computational efficiency (Fan et al., 2008). \u2662\n12.2.5 Soft Margin SVM: Loss Function Vi"
  },
  {
    "vector_id": 1547,
    "chunk_id": "p386_c3",
    "page_number": 386,
    "text": "nregularized term b com-\nplicates theoretical analysis (Steinwart and Christmann, 2008, chapter 1)\nand decreases computational efficiency (Fan et al., 2008). \u2662\n12.2.5 Soft Margin SVM: Loss Function View\nLet us consider a different approach for deriving the SVM, following the\nprinciple of empirical risk minimization (Section 8.2). For the SVM, we\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1548,
    "chunk_id": "p387_c0",
    "page_number": 387,
    "text": "12.2 Primal Support Vector Machine 381\nchoose hyperplanes as the hypothesis class, that is\nf(x) = \u27e8w, x\u27e9 + b. (12.27)\nWe will see in this section that the margin corresponds to the regulariza-\ntion term. The remaining question is, what is the loss function? In con- loss function\ntrast to Chapter 9, where we consider regression problems (the output\nof the predictor is a real number), in this chapter, we consider binary\nclassification problems (the output of the predictor is one of two labels\n{+1, \u22121}). Therefore, the error/loss function for each single example\u2013\nlabel pair needs to be appropriate for binary classification. For example,\nthe squared loss that is used for regression (9.10b) is not suitable for bi-\nnary classification.\nRemark. The ideal loss function between binary labels is to"
  },
  {
    "vector_id": 1549,
    "chunk_id": "p387_c1",
    "page_number": 387,
    "text": "e for binary classification. For example,\nthe squared loss that is used for regression (9.10b) is not suitable for bi-\nnary classification.\nRemark. The ideal loss function between binary labels is to count the num-\nber of mismatches between the prediction and the label. This means that\nfor a predictor f applied to an example xn, we compare the output f(xn)\nwith the label yn. We define the loss to be zero if they match, and one if\nthey do not match. This is denoted by 1(f(xn) \u0338= yn) and is called the\nzero-one loss. Unfortunately , the zero-one loss results in a combinatorial zero-one loss\noptimization problem for finding the best parameters w, b. Combinatorial\noptimization problems (in contrast to continuous optimization problems\ndiscussed in Chapter 7) are in general more challenging to so"
  },
  {
    "vector_id": 1550,
    "chunk_id": "p387_c2",
    "page_number": 387,
    "text": "ization problem for finding the best parameters w, b. Combinatorial\noptimization problems (in contrast to continuous optimization problems\ndiscussed in Chapter 7) are in general more challenging to solve. \u2662\nWhat is the loss function corresponding to the SVM? Consider the error\nbetween the output of a predictor f(xn) and the label yn. The loss de-\nscribes the error that is made on the training data. An equivalent way to\nderive (12.26a) is to use the hinge loss hinge loss\n\u2113(t) = max{0, 1 \u2212 t} where t = yf(x) = y(\u27e8w, x\u27e9 + b) . (12.28)\nIf f(x) is on the correct side (based on the corresponding label y) of the\nhyperplane, and further than distance 1, this means that t \u2a7e 1 and the\nhinge loss returns a value of zero. If f(x) is on the correct side but too\nclose to the hyperplane (0 < t <1), the e"
  },
  {
    "vector_id": 1551,
    "chunk_id": "p387_c3",
    "page_number": 387,
    "text": "el y) of the\nhyperplane, and further than distance 1, this means that t \u2a7e 1 and the\nhinge loss returns a value of zero. If f(x) is on the correct side but too\nclose to the hyperplane (0 < t <1), the example x is within the margin,\nand the hinge loss returns a positive value. When the example is on the\nwrong side of the hyperplane (t <0), the hinge loss returns an even larger\nvalue, which increases linearly . In other words, we pay a penalty once we\nare closer than the margin to the hyperplane, even if the prediction is\ncorrect, and the penalty increases linearly . An alternative way to express\nthe hinge loss is by considering it as two linear pieces\n\u2113(t) =\n(\n0 if t \u2a7e 1\n1 \u2212 t if t <1 , (12.29)\nas illustrated in Figure 12.8. The loss corresponding to the hard margin\nSVM 12.18 is defined as\n\u2113"
  },
  {
    "vector_id": 1552,
    "chunk_id": "p387_c4",
    "page_number": 387,
    "text": "the hinge loss is by considering it as two linear pieces\n\u2113(t) =\n(\n0 if t \u2a7e 1\n1 \u2212 t if t <1 , (12.29)\nas illustrated in Figure 12.8. The loss corresponding to the hard margin\nSVM 12.18 is defined as\n\u2113(t) =\n(\n0 if t \u2a7e 1\n\u221e if t <1 . (12.30)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1553,
    "chunk_id": "p388_c0",
    "page_number": 388,
    "text": "382 Classification with Support Vector Machines\nFigure 12.8 The\nhinge loss is a\nconvex upper bound\nof zero-one loss.\n\u22122 0 2\nt\n0\n2\n4max{0, 1 \u2212 t} Zero-one loss\nHinge loss\nThis loss can be interpreted as never allowing any examples inside the\nmargin.\nFor a given training set {(x1, y1), . . . ,(xN , yN )}, we seek to minimize\nthe total loss, while regularizing the objective with \u21132-regularization (see\nSection 8.2.3). Using the hinge loss (12.28) gives us the unconstrained\noptimization problem\nmin\nw,b\n1\n2\u2225w\u22252\n| {z }\nregularizer\n+ C\nNX\nn=1\nmax{0, 1 \u2212 yn(\u27e8w, xn\u27e9 + b)}\n| {z }\nerror term\n. (12.31)\nThe first term in (12.31) is called the regularization term or theregularizerregularizer\n(see Section 8.2.3), and the second term is called theloss term or the errorloss term\nerror term term. Recall from"
  },
  {
    "vector_id": 1554,
    "chunk_id": "p388_c1",
    "page_number": 388,
    "text": "first term in (12.31) is called the regularization term or theregularizerregularizer\n(see Section 8.2.3), and the second term is called theloss term or the errorloss term\nerror term term. Recall from Section 12.2.4 that the term 1\n2 \u2225w\u2225\n2\narises directly from\nthe margin. In other words, margin maximization can be interpreted as\nregularization.regularization\nIn principle, the unconstrained optimization problem in (12.31) can\nbe directly solved with (sub-)gradient descent methods as described in\nSection 7.1. To see that (12.31) and (12.26a) are equivalent, observe that\nthe hinge loss (12.28) essentially consists of two linear parts, as expressed\nin (12.29). Consider the hinge loss for a single example-label pair (12.28).\nWe can equivalently replace minimization of the hinge loss over t with"
  },
  {
    "vector_id": 1555,
    "chunk_id": "p388_c2",
    "page_number": 388,
    "text": "sentially consists of two linear parts, as expressed\nin (12.29). Consider the hinge loss for a single example-label pair (12.28).\nWe can equivalently replace minimization of the hinge loss over t with a\nminimization of a slack variable \u03be with two constraints. In equation form,\nmin\nt\nmax{0, 1 \u2212 t} (12.32)\nis equivalent to\nmin\n\u03be,t\n\u03be\nsubject to \u03be \u2a7e 0 , \u03be \u2a7e 1 \u2212 t .\n(12.33)\nBy substituting this expression into (12.31) and rearranging one of the\nconstraints, we obtain exactly the soft margin SVM (12.26a).\nRemark. Let us contrast our choice of the loss function in this section to the\nloss function for linear regression in Chapter 9. Recall from Section 9.2.1\nthat for finding maximum likelihood estimators, we usually minimize the\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:h"
  },
  {
    "vector_id": 1556,
    "chunk_id": "p388_c3",
    "page_number": 388,
    "text": "r linear regression in Chapter 9. Recall from Section 9.2.1\nthat for finding maximum likelihood estimators, we usually minimize the\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1557,
    "chunk_id": "p389_c0",
    "page_number": 389,
    "text": "12.3 Dual Support Vector Machine 383\nnegative log-likelihood. Furthermore, since the likelihood term for linear\nregression with Gaussian noise is Gaussian, the negative log-likelihood for\neach example is a squared error function. The squared error function is the\nloss function that is minimized when looking for the maximum likelihood\nsolution. \u2662\n12.3 Dual Support Vector Machine\nThe description of the SVM in the previous sections, in terms of the vari-\nables w and b, is known as the primal SVM. Recall that we consider inputs\nx \u2208 RD with D features. Since w is of the same dimension as x, this\nmeans that the number of parameters (the dimension of w) of the opti-\nmization problem grows linearly with the number of features.\nIn the following, we consider an equivalent optimization problem (the\ns"
  },
  {
    "vector_id": 1558,
    "chunk_id": "p389_c1",
    "page_number": 389,
    "text": "ans that the number of parameters (the dimension of w) of the opti-\nmization problem grows linearly with the number of features.\nIn the following, we consider an equivalent optimization problem (the\nso-called dual view), which is independent of the number of features. In-\nstead, the number of parameters increases with the number of examples\nin the training set. We saw a similar idea appear in Chapter 10, where we\nexpressed the learning problem in a way that does not scale with the num-\nber of features. This is useful for problems where we have more features\nthan the number of examples in the training dataset. The dual SVM also\nhas the additional advantage that it easily allows kernels to be applied,\nas we shall see at the end of this chapter. The word \u201cdual\u201d appears often\nin mathematical l"
  },
  {
    "vector_id": 1559,
    "chunk_id": "p389_c2",
    "page_number": 389,
    "text": "raining dataset. The dual SVM also\nhas the additional advantage that it easily allows kernels to be applied,\nas we shall see at the end of this chapter. The word \u201cdual\u201d appears often\nin mathematical literature, and in this particular case it refers to convex\nduality . The following subsections are essentially an application of convex\nduality , which we discussed in Section 7.2.\n12.3.1 Convex Duality via Lagrange Multipliers\nRecall the primal soft margin SVM (12.26a). We call the variables w, b,\nand \u03be corresponding to the primal SVM the primal variables. We use\u03b1n \u2a7e In Chapter 7, we\nused \u03bb as Lagrange\nmultipliers. In this\nsection, we follow\nthe notation\ncommonly chosen in\nSVM literature, and\nuse \u03b1 and \u03b3.\n0 as the Lagrange multiplier corresponding to the constraint (12.26b) that\nthe examples"
  },
  {
    "vector_id": 1560,
    "chunk_id": "p389_c3",
    "page_number": 389,
    "text": "grange\nmultipliers. In this\nsection, we follow\nthe notation\ncommonly chosen in\nSVM literature, and\nuse \u03b1 and \u03b3.\n0 as the Lagrange multiplier corresponding to the constraint (12.26b) that\nthe examples are classified correctly and \u03b3n \u2a7e 0 as the Lagrange multi-\nplier corresponding to the non-negativity constraint of the slack variable;\nsee (12.26c). The Lagrangian is then given by\nL(w, b, \u03be, \u03b1, \u03b3) = 1\n2\u2225w\u22252 + C\nNX\nn=1\n\u03ben (12.34)\n\u2212\nNX\nn=1\n\u03b1n(yn(\u27e8w, xn\u27e9 + b) \u2212 1 + \u03ben)\n| {z }\nconstraint (12.26b)\n\u2212\nNX\nn=1\n\u03b3n\u03ben\n| {z }\nconstraint (12.26c)\n.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1561,
    "chunk_id": "p389_c4",
    "page_number": 389,
    "text": "Cambridge University Press (2020)."
  },
  {
    "vector_id": 1562,
    "chunk_id": "p390_c0",
    "page_number": 390,
    "text": "384 Classification with Support Vector Machines\nBy differentiating the Lagrangian (12.34) with respect to the three primal\nvariables w, b, and \u03be respectively , we obtain\n\u2202L\n\u2202w = w\u22a4 \u2212\nNX\nn=1\n\u03b1nynxn\n\u22a4 , (12.35)\n\u2202L\n\u2202b = \u2212\nNX\nn=1\n\u03b1nyn , (12.36)\n\u2202L\n\u2202\u03ben\n= C \u2212 \u03b1n \u2212 \u03b3n . (12.37)\nWe now find the maximum of the Lagrangian by setting each of these\npartial derivatives to zero. By setting (12.35) to zero, we find\nw =\nNX\nn=1\n\u03b1nynxn , (12.38)\nwhich is a particular instance of the representer theorem (Kimeldorf andrepresenter theorem\nWahba, 1970). Equation (12.38) states that the optimal weight vector inThe representer\ntheorem is actually\na collection of\ntheorems saying\nthat the solution of\nminimizing\nempirical risk lies in\nthe subspace\n(Section 2.4.3)\ndefined by the\nexamples.\nthe primal is a linear combi"
  },
  {
    "vector_id": 1563,
    "chunk_id": "p390_c1",
    "page_number": 390,
    "text": "epresenter\ntheorem is actually\na collection of\ntheorems saying\nthat the solution of\nminimizing\nempirical risk lies in\nthe subspace\n(Section 2.4.3)\ndefined by the\nexamples.\nthe primal is a linear combination of the examples xn. Recall from Sec-\ntion 2.6.1 that this means that the solution of the optimization problem\nlies in the span of training data. Additionally , the constraint obtained by\nsetting (12.36) to zero implies that the optimal weight vector is an affine\ncombination of the examples. The representer theorem turns out to hold\nfor very general settings of regularized empirical risk minimization (Hof-\nmann et al., 2008; Argyriou and Dinuzzo, 2014). The theorem has more\ngeneral versions (Sch \u00a8olkopf et al., 2001), and necessary and sufficient\nconditions on its existence can be found"
  },
  {
    "vector_id": 1564,
    "chunk_id": "p390_c2",
    "page_number": 390,
    "text": "imization (Hof-\nmann et al., 2008; Argyriou and Dinuzzo, 2014). The theorem has more\ngeneral versions (Sch \u00a8olkopf et al., 2001), and necessary and sufficient\nconditions on its existence can be found in Yu et al. (2013).\nRemark. The representer theorem (12.38) also provides an explanation\nof the name \u201csupport vector machine.\u201d The examples xn, for which the\ncorresponding parameters \u03b1n = 0, do not contribute to the solution w at\nall. The other examples, where \u03b1n > 0, are called support vectors sincesupport vector\nthey \u201csupport\u201d the hyperplane. \u2662\nBy substituting the expression for w into the Lagrangian (12.34), we\nobtain the dual\nD(\u03be, \u03b1, \u03b3) = 1\n2\nNX\ni=1\nNX\nj=1\nyiyj\u03b1i\u03b1j \u27e8xi, xj\u27e9 \u2212\nNX\ni=1\nyi\u03b1i\n* NX\nj=1\nyj\u03b1jxj, xi\n+\n+ C\nNX\ni=1\n\u03bei \u2212 b\nNX\ni=1\nyi\u03b1i +\nNX\ni=1\n\u03b1i \u2212\nNX\ni=1\n\u03b1i\u03bei \u2212\nNX\ni=1\n\u03b3i\u03bei .\n(12.39)"
  },
  {
    "vector_id": 1565,
    "chunk_id": "p390_c3",
    "page_number": 390,
    "text": "angian (12.34), we\nobtain the dual\nD(\u03be, \u03b1, \u03b3) = 1\n2\nNX\ni=1\nNX\nj=1\nyiyj\u03b1i\u03b1j \u27e8xi, xj\u27e9 \u2212\nNX\ni=1\nyi\u03b1i\n* NX\nj=1\nyj\u03b1jxj, xi\n+\n+ C\nNX\ni=1\n\u03bei \u2212 b\nNX\ni=1\nyi\u03b1i +\nNX\ni=1\n\u03b1i \u2212\nNX\ni=1\n\u03b1i\u03bei \u2212\nNX\ni=1\n\u03b3i\u03bei .\n(12.39)\nNote that there are no longer any terms involving the primal variable w.\nBy setting (12.36) to zero, we obtainPN\nn=1 yn\u03b1n = 0. Therefore, the term\ninvolving b also vanishes. Recall that inner products are symmetric and\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1566,
    "chunk_id": "p391_c0",
    "page_number": 391,
    "text": "12.3 Dual Support Vector Machine 385\nbilinear (see Section 3.2). Therefore, the first two terms in (12.39) are\nover the same objects. These terms (colored blue) can be simplified, and\nwe obtain the Lagrangian\nD(\u03be, \u03b1, \u03b3) = \u22121\n2\nNX\ni=1\nNX\nj=1\nyiyj\u03b1i\u03b1j \u27e8xi, xj\u27e9 +\nNX\ni=1\n\u03b1i +\nNX\ni=1\n(C \u2212 \u03b1i \u2212 \u03b3i)\u03bei .\n(12.40)\nThe last term in this equation is a collection of all terms that contain slack\nvariables \u03bei. By setting (12.37) to zero, we see that the last term in (12.40)\nis also zero. Furthermore, by using the same equation and recalling that\nthe Lagrange multiplers \u03b3i are non-negative, we conclude that \u03b1i \u2a7d C.\nWe now obtain the dual optimization problem of the SVM, which is ex-\npressed exclusively in terms of the Lagrange multipliers \u03b1i. Recall from\nLagrangian duality (Definition 7.1) that we maximiz"
  },
  {
    "vector_id": 1567,
    "chunk_id": "p391_c1",
    "page_number": 391,
    "text": "i \u2a7d C.\nWe now obtain the dual optimization problem of the SVM, which is ex-\npressed exclusively in terms of the Lagrange multipliers \u03b1i. Recall from\nLagrangian duality (Definition 7.1) that we maximize the dual problem.\nThis is equivalent to minimizing the negative dual problem, such that we\nend up with the dual SVM dual SVM\nmin\n\u03b1\n1\n2\nNX\ni=1\nNX\nj=1\nyiyj\u03b1i\u03b1j \u27e8xi, xj\u27e9 \u2212\nNX\ni=1\n\u03b1i\nsubject to\nNX\ni=1\nyi\u03b1i = 0\n0 \u2a7d \u03b1i \u2a7d C for all i = 1, . . . , N .\n(12.41)\nThe equality constraint in (12.41) is obtained from setting (12.36) to\nzero. The inequality constraint \u03b1i \u2a7e 0 is the condition imposed on La-\ngrange multipliers of inequality constraints (Section 7.2). The inequality\nconstraint \u03b1i \u2a7d C is discussed in the previous paragraph.\nThe set of inequality constraints in the SVM are called \u201cbox constraint"
  },
  {
    "vector_id": 1568,
    "chunk_id": "p391_c2",
    "page_number": 391,
    "text": "ge multipliers of inequality constraints (Section 7.2). The inequality\nconstraint \u03b1i \u2a7d C is discussed in the previous paragraph.\nThe set of inequality constraints in the SVM are called \u201cbox constraints\u201d\nbecause they limit the vector \u03b1 = [\u03b11, \u00b7 \u00b7\u00b7, \u03b1N ]\u22a4 \u2208 RN of Lagrange mul-\ntipliers to be inside the box defined by 0 and C on each axis. These\naxis-aligned boxes are particularly efficient to implement in numerical\nsolvers (Dost\u00b4al, 2009, chapter 5). It turns out that\nexamples that lie\nexactly on the\nmargin are\nexamples whose\ndual parameters lie\nstrictly inside the\nbox constraints,\n0 < \u03b1i < C. This is\nderived using the\nKarush Kuhn Tucker\nconditions, for\nexample in\nSch\u00a8olkopf and\nSmola (2002).\nOnce we obtain the dual parameters \u03b1, we can recover the primal pa-\nrameters w by using the represen"
  },
  {
    "vector_id": 1569,
    "chunk_id": "p391_c3",
    "page_number": 391,
    "text": "his is\nderived using the\nKarush Kuhn Tucker\nconditions, for\nexample in\nSch\u00a8olkopf and\nSmola (2002).\nOnce we obtain the dual parameters \u03b1, we can recover the primal pa-\nrameters w by using the representer theorem (12.38). Let us call the op-\ntimal primal parameter w\u2217. However, there remains the question on how\nto obtain the parameter b\u2217. Consider an example xn that lies exactly on\nthe margin\u2019s boundary , i.e.,\u27e8w\u2217, xn\u27e9 + b = yn. Recall that yn is either +1\nor \u22121. Therefore, the only unknown is b, which can be computed by\nb\u2217 = yn \u2212 \u27e8w\u2217, xn\u27e9. (12.42)\nRemark. In principle, there may be no examples that lie exactly on the\nmargin. In this case, we should compute |yn \u2212 \u27e8w\u2217, xn\u27e9 |for all support\nvectors and take the median value of this absolute value difference to be\n\u00a92024 M. P. Deisenroth, A. A."
  },
  {
    "vector_id": 1570,
    "chunk_id": "p391_c4",
    "page_number": 391,
    "text": "that lie exactly on the\nmargin. In this case, we should compute |yn \u2212 \u27e8w\u2217, xn\u27e9 |for all support\nvectors and take the median value of this absolute value difference to be\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1571,
    "chunk_id": "p392_c0",
    "page_number": 392,
    "text": "386 Classification with Support Vector Machines\nFigure 12.9 Convex\nhulls. (a) Convex\nhull of points, some\nof which lie within\nthe boundary;\n(b) convex hulls\naround positive and\nnegative examples.\n(a) Convex hull.\nc\nd (b) Convex hulls around positive (blue) and\nnegative (orange) examples. The distance be-\ntween the two convex sets is the length of the\ndifference vector c \u2212 d.\nthe value of b\u2217. A derivation of this can be found in http://fouryears.\neu/2012/06/07/the-svm-bias-term-conspiracy/ . \u2662\n12.3.2 Dual SVM: Convex Hull View\nAnother approach to obtain the dual SVM is to consider an alternative\ngeometric argument. Consider the set of examplesxn with the same label.\nWe would like to build a convex set that contains all the examples such\nthat it is the smallest possible set. This is called t"
  },
  {
    "vector_id": 1572,
    "chunk_id": "p392_c1",
    "page_number": 392,
    "text": "e\ngeometric argument. Consider the set of examplesxn with the same label.\nWe would like to build a convex set that contains all the examples such\nthat it is the smallest possible set. This is called the convex hull and is\nillustrated in Figure 12.9.\nLet us first build some intuition about a convex combination of points.\nConsider two points x1 and x2 and corresponding non-negative weights\n\u03b11, \u03b12 \u2a7e 0 such that \u03b11+\u03b12 = 1. The equation\u03b11x1+\u03b12x2 describes each\npoint on a line between x1 and x2. Consider what happens when we add\na third point x3 along with a weight \u03b13 \u2a7e 0 such that P3\nn=1 \u03b1n = 1 .\nThe convex combination of these three points x1, x2, x3 spans a two-\ndimensional area. The convex hull of this area is the triangle formed byconvex hull\nthe edges corresponding to each pair of of point"
  },
  {
    "vector_id": 1573,
    "chunk_id": "p392_c2",
    "page_number": 392,
    "text": "he convex combination of these three points x1, x2, x3 spans a two-\ndimensional area. The convex hull of this area is the triangle formed byconvex hull\nthe edges corresponding to each pair of of points. As we add more points,\nand the number of points becomes greater than the number of dimen-\nsions, some of the points will be inside the convex hull, as we can see in\nFigure 12.9(a).\nIn general, building a convex convex hull can be done by introducing\nnon-negative weights \u03b1n \u2a7e 0 corresponding to each example xn. Then\nthe convex hull can be described as the set\nconv (X) =\n( NX\nn=1\n\u03b1nxn\n)\nwith\nNX\nn=1\n\u03b1n = 1 and \u03b1n \u2a7e 0, (12.43)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1574,
    "chunk_id": "p392_c3",
    "page_number": 392,
    "text": "=1\n\u03b1n = 1 and \u03b1n \u2a7e 0, (12.43)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1575,
    "chunk_id": "p393_c0",
    "page_number": 393,
    "text": "12.3 Dual Support Vector Machine 387\nfor all n = 1 , . . . , N. If the two clouds of points corresponding to the\npositive and negative classes are separated, then the convex hulls do not\noverlap. Given the training data (x1, y1), . . . ,(xN , yN ), we form two con-\nvex hulls, corresponding to the positive and negative classes respectively .\nWe pick a point c, which is in the convex hull of the set of positive exam-\nples, and is closest to the negative class distribution. Similarly , we pick a\npoint d in the convex hull of the set of negative examples and is closest to\nthe positive class distribution; see Figure 12.9(b). We define a difference\nvector between d and c as\nw := c \u2212 d. (12.44)\nPicking the points c and d as in the preceding cases, and requiring them\nto be closest to each other is"
  },
  {
    "vector_id": 1576,
    "chunk_id": "p393_c1",
    "page_number": 393,
    "text": "bution; see Figure 12.9(b). We define a difference\nvector between d and c as\nw := c \u2212 d. (12.44)\nPicking the points c and d as in the preceding cases, and requiring them\nto be closest to each other is equivalent to minimizing the length/norm of\nw, so that we end up with the corresponding optimization problem\narg min\nw\n\u2225w\u2225 = arg min\nw\n1\n2 \u2225w\u2225\n2\n. (12.45)\nSince c must be in the positive convex hull, it can be expressed as a convex\ncombination of the positive examples, i.e., for non-negative coefficients\n\u03b1+\nn\nc =\nX\nn:yn=+1\n\u03b1+\nn xn . (12.46)\nIn (12.46), we use the notation n : yn = +1 to indicate the set of indices\nn for which yn = +1. Similarly , for the examples with negative labels, we\nobtain\nd =\nX\nn:yn=\u22121\n\u03b1\u2212\nn xn . (12.47)\nBy substituting (12.44), (12.46), and (12.47) into (12.45), we obta"
  },
  {
    "vector_id": 1577,
    "chunk_id": "p393_c2",
    "page_number": 393,
    "text": "the set of indices\nn for which yn = +1. Similarly , for the examples with negative labels, we\nobtain\nd =\nX\nn:yn=\u22121\n\u03b1\u2212\nn xn . (12.47)\nBy substituting (12.44), (12.46), and (12.47) into (12.45), we obtain the\nobjective\nmin\n\u03b1\n1\n2\n\r\r\r\r\r\nX\nn:yn=+1\n\u03b1+\nn xn \u2212\nX\nn:yn=\u22121\n\u03b1\u2212\nn xn\n\r\r\r\r\r\n2\n. (12.48)\nLet \u03b1 be the set of all coefficients, i.e., the concatenation of\u03b1+ and \u03b1\u2212.\nRecall that we require that for each convex hull that their coefficients sum\nto one,\nX\nn:yn=+1\n\u03b1+\nn = 1 and\nX\nn:yn=\u22121\n\u03b1\u2212\nn = 1 . (12.49)\nThis implies the constraint\nNX\nn=1\nyn\u03b1n = 0 . (12.50)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1578,
    "chunk_id": "p393_c3",
    "page_number": 393,
    "text": "ng. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1579,
    "chunk_id": "p394_c0",
    "page_number": 394,
    "text": "388 Classification with Support Vector Machines\nThis result can be seen by multiplying out the individual classes\nNX\nn=1\nyn\u03b1n =\nX\nn:yn=+1\n(+1)\u03b1+\nn +\nX\nn:yn=\u22121\n(\u22121)\u03b1\u2212\nn (12.51a)\n=\nX\nn:yn=+1\n\u03b1+\nn \u2212\nX\nn:yn=\u22121\n\u03b1\u2212\nn = 1 \u2212 1 = 0 . (12.51b)\nThe objective function (12.48) and the constraint (12.50), along with the\nassumption that \u03b1 \u2a7e 0, give us a constrained (convex) optimization prob-\nlem. This optimization problem can be shown to be the same as that of\nthe dual hard margin SVM (Bennett and Bredensteiner, 2000a).\nRemark. To obtain the soft margin dual, we consider the reduced hull. The\nreduced hull is similar to the convex hull but has an upper bound to thereduced hull\nsize of the coefficients \u03b1. The maximum possible value of the elements\nof \u03b1 restricts the size that the convex hull can take. In"
  },
  {
    "vector_id": 1580,
    "chunk_id": "p394_c1",
    "page_number": 394,
    "text": "s similar to the convex hull but has an upper bound to thereduced hull\nsize of the coefficients \u03b1. The maximum possible value of the elements\nof \u03b1 restricts the size that the convex hull can take. In other words, the\nbound on \u03b1 shrinks the convex hull to a smaller volume (Bennett and\nBredensteiner, 2000b). \u2662\n12.4 Kernels\nConsider the formulation of the dual SVM (12.41). Notice that the in-\nner product in the objective occurs only between examples xi and xj.\nThere are no inner products between the examples and the parameters.\nTherefore, if we consider a set of features \u03d5(xi) to represent xi, the only\nchange in the dual SVM will be to replace the inner product. This mod-\nularity , where the choice of the classification method (the SVM) and the\nchoice of the feature representation \u03d5(x) can be"
  },
  {
    "vector_id": 1581,
    "chunk_id": "p394_c2",
    "page_number": 394,
    "text": "e only\nchange in the dual SVM will be to replace the inner product. This mod-\nularity , where the choice of the classification method (the SVM) and the\nchoice of the feature representation \u03d5(x) can be considered separately ,\nprovides flexibility for us to explore the two problems independently . In\nthis section, we discuss the representation \u03d5(x) and briefly introduce the\nidea of kernels, but do not go into the technical details.\nSince \u03d5(x) could be a non-linear function, we can use the SVM (which\nassumes a linear classifier) to construct classifiers that are nonlinear in\nthe examples xn. This provides a second avenue, in addition to the soft\nmargin, for users to deal with a dataset that is not linearly separable. It\nturns out that there are many algorithms and statistical methods that hav"
  },
  {
    "vector_id": 1582,
    "chunk_id": "p394_c3",
    "page_number": 394,
    "text": "provides a second avenue, in addition to the soft\nmargin, for users to deal with a dataset that is not linearly separable. It\nturns out that there are many algorithms and statistical methods that have\nthis property that we observed in the dual SVM: the only inner products\nare those that occur between examples. Instead of explicitly defining a\nnon-linear feature map \u03d5(\u00b7) and computing the resulting inner product\nbetween examples xi and xj, we define a similarity functionk(xi, xj) be-\ntween xi and xj. For a certain class of similarity functions, called kernels,kernel\nthe similarity function implicitly defines a non-linear feature map \u03d5(\u00b7).\nKernels are by definition functions k : X \u00d7 X \u2192R for which there existsThe inputs X of the\nkernel function can\nbe very general and\nare not necessarily\nre"
  },
  {
    "vector_id": 1583,
    "chunk_id": "p394_c4",
    "page_number": 394,
    "text": "licitly defines a non-linear feature map \u03d5(\u00b7).\nKernels are by definition functions k : X \u00d7 X \u2192R for which there existsThe inputs X of the\nkernel function can\nbe very general and\nare not necessarily\nrestricted to RD.\na Hilbert space H and \u03d5 : X \u2192 Ha feature map such that\nk(xi, xj) = \u27e8\u03d5(xi), \u03d5(xj)\u27e9H . (12.52)\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1584,
    "chunk_id": "p395_c0",
    "page_number": 395,
    "text": "12.4 Kernels 389\nFigure 12.10 SVM\nwith different\nkernels. Note that\nwhile the decision\nboundary is\nnonlinear, the\nunderlying problem\nbeing solved is for a\nlinear separating\nhyperplane (albeit\nwith a nonlinear\nkernel).\nFirst feature\nSecond feature\n(a) SVM with linear kernel\nFirst feature\nSecond feature (b) SVM with RBF kernel\nFirst feature\nSecond feature\n(c) SVM with polynomial (degree 2) kernel\nFirst feature\nSecond feature (d) SVM with polynomial (degree 3) kernel\nThere is a unique reproducing kernel Hilbert space associated with every\nkernel k (Aronszajn, 1950; Berlinet and Thomas-Agnan, 2004). In this\nunique association, \u03d5(x) = k(\u00b7, x) is called the canonical feature map . canonical feature\nmapThe generalization from an inner product to a kernel function (12.52) is\nknown as the kernel tr"
  },
  {
    "vector_id": 1585,
    "chunk_id": "p395_c1",
    "page_number": 395,
    "text": "). In this\nunique association, \u03d5(x) = k(\u00b7, x) is called the canonical feature map . canonical feature\nmapThe generalization from an inner product to a kernel function (12.52) is\nknown as the kernel trick (Sch\u00a8olkopf and Smola, 2002; Shawe-Taylor and kernel trick\nCristianini, 2004), as it hides away the explicit non-linear feature map.\nThe matrix K \u2208 RN\u00d7N , resulting from the inner products or the appli-\ncation of k(\u00b7, \u00b7) to a dataset, is called the Gram matrix, and is often just Gram matrix\nreferred to as the kernel matrix. Kernels must be symmetric and positive kernel matrix\nsemidefinite functions so that every kernel matrix K is symmetric and\npositive semidefinite (Section 3.2.3):\n\u2200z \u2208 RN : z\u22a4Kz \u2a7e 0 . (12.53)\nSome popular examples of kernels for multivariate real-valued data xi \u2208\nRD are"
  },
  {
    "vector_id": 1586,
    "chunk_id": "p395_c2",
    "page_number": 395,
    "text": "tions so that every kernel matrix K is symmetric and\npositive semidefinite (Section 3.2.3):\n\u2200z \u2208 RN : z\u22a4Kz \u2a7e 0 . (12.53)\nSome popular examples of kernels for multivariate real-valued data xi \u2208\nRD are the polynomial kernel, the Gaussian radial basis function kernel,\nand the rational quadratic kernel (Sch\u00a8olkopf and Smola, 2002; Rasmussen\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1587,
    "chunk_id": "p396_c0",
    "page_number": 396,
    "text": "390 Classification with Support Vector Machines\nand Williams, 2006). Figure 12.10 illustrates the effect of different kernels\non separating hyperplanes on an example dataset. Note that we are still\nsolving for hyperplanes, that is, the hypothesis class of functions are still\nlinear. The non-linear surfaces are due to the kernel function.\nRemark. Unfortunately for the fledgling machine learner, there are mul-\ntiple meanings of the word \u201ckernel.\u201d In this chapter, the word \u201ckernel\u201d\ncomes from the idea of the reproducing kernel Hilbert space (RKHS) (Aron-\nszajn, 1950; Saitoh, 1988). We have discussed the idea of the kernel in lin-\near algebra (Section 2.7.3), where the kernel is another word for the null\nspace. The third common use of the word \u201ckernel\u201d in machine learning is\nthe smoothing kern"
  },
  {
    "vector_id": 1588,
    "chunk_id": "p396_c1",
    "page_number": 396,
    "text": "sed the idea of the kernel in lin-\near algebra (Section 2.7.3), where the kernel is another word for the null\nspace. The third common use of the word \u201ckernel\u201d in machine learning is\nthe smoothing kernel in kernel density estimation (Section 11.5). \u2662\nSince the explicit representation \u03d5(x) is mathematically equivalent to\nthe kernel representation k(xi, xj), a practitioner will often design the\nkernel function such that it can be computed more efficiently than the\ninner product between explicit feature maps. For example, consider the\npolynomial kernel (Sch \u00a8olkopf and Smola, 2002), where the number of\nterms in the explicit expansion grows very quickly (even for polynomials\nof low degree) when the input dimension is large. The kernel function\nonly requires one multiplication per input dimensio"
  },
  {
    "vector_id": 1589,
    "chunk_id": "p396_c2",
    "page_number": 396,
    "text": "er of\nterms in the explicit expansion grows very quickly (even for polynomials\nof low degree) when the input dimension is large. The kernel function\nonly requires one multiplication per input dimension, which can provide\nsignificant computational savings. Another example is the Gaussian ra-\ndial basis function kernel (Sch \u00a8olkopf and Smola, 2002; Rasmussen and\nWilliams, 2006), where the corresponding feature space is infinite dimen-\nsional. In this case, we cannot explicitly represent the feature space but\ncan still compute similarities between a pair of examples using the kernel.The choice of\nkernel, as well as\nthe parameters of\nthe kernel, is often\nchosen using nested\ncross-validation\n(Section 8.6.1).\nAnother useful aspect of the kernel trick is that there is no need for\nthe original dat"
  },
  {
    "vector_id": 1590,
    "chunk_id": "p396_c3",
    "page_number": 396,
    "text": "kernel, as well as\nthe parameters of\nthe kernel, is often\nchosen using nested\ncross-validation\n(Section 8.6.1).\nAnother useful aspect of the kernel trick is that there is no need for\nthe original data to be already represented as multivariate real-valued\ndata. Note that the inner product is defined on the output of the function\n\u03d5(\u00b7), but does not restrict the input to real numbers. Hence, the function\n\u03d5(\u00b7) and the kernel function k(\u00b7, \u00b7) can be defined on any object, e.g.,\nsets, sequences, strings, graphs, and distributions (Ben-Hur et al., 2008;\nG\u00a8artner, 2008; Shi et al., 2009; Sriperumbudur et al., 2010; Vishwanathan\net al., 2010).\n12.5 Numerical Solution\nWe conclude our discussion of SVMs by looking at how to express the\nproblems derived in this chapter in terms of the concepts presen"
  },
  {
    "vector_id": 1591,
    "chunk_id": "p396_c4",
    "page_number": 396,
    "text": "r et al., 2010; Vishwanathan\net al., 2010).\n12.5 Numerical Solution\nWe conclude our discussion of SVMs by looking at how to express the\nproblems derived in this chapter in terms of the concepts presented in\nChapter 7. We consider two different approaches for finding the optimal\nsolution for the SVM. First we consider the loss view of SVM 8.2.2 and ex-\npress this as an unconstrained optimization problem. Then we express the\nconstrained versions of the primal and dual SVMs as quadratic programs\nin standard form 7.3.2.\nConsider the loss function view of the SVM (12.31). This is a convex\nunconstrained optimization problem, but the hinge loss (12.28) is not dif-\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1592,
    "chunk_id": "p396_c5",
    "page_number": 396,
    "text": "ined optimization problem, but the hinge loss (12.28) is not dif-\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1593,
    "chunk_id": "p397_c0",
    "page_number": 397,
    "text": "12.5 Numerical Solution 391\nferentiable. Therefore, we apply a subgradient approach for solving it.\nHowever, the hinge loss is differentiable almost everywhere, except for\none single point at the hinge t = 1. At this point, the gradient is a set of\npossible values that lie between 0 and \u22121. Therefore, the subgradient g of\nthe hinge loss is given by\ng(t) =\n\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3\n\u22121 t <1\n[\u22121, 0] t = 1\n0 t >1\n. (12.54)\nUsing this subgradient, we can apply the optimization methods presented\nin Section 7.1.\nBoth the primal and the dual SVM result in a convex quadratic pro-\ngramming problem (constrained optimization). Note that the primal SVM\nin (12.26a) has optimization variables that have the size of the dimen-\nsion D of the input examples. The dual SVM in (12.41) has optimization\nvariables that have the siz"
  },
  {
    "vector_id": 1594,
    "chunk_id": "p397_c1",
    "page_number": 397,
    "text": "on). Note that the primal SVM\nin (12.26a) has optimization variables that have the size of the dimen-\nsion D of the input examples. The dual SVM in (12.41) has optimization\nvariables that have the size of the number N of examples.\nTo express the primal SVM in the standard form (7.45) for quadratic\nprogramming, let us assume that we use the dot product (3.5) as the\ninner product. We rearrange the equation for the primal SVM (12.26a), Recall from\nSection 3.2 that we\nuse the phrase dot\nproduct to mean the\ninner product on\nEuclidean vector\nspace.\nsuch that the optimization variables are all on the right and the inequality\nof the constraint matches the standard form. This yields the optimization\nmin\nw,b,\u03be\n1\n2\u2225w\u22252 + C\nNX\nn=1\n\u03ben\nsubject to \u2212ynx\u22a4\nn w \u2212 ynb \u2212 \u03ben \u2a7d \u22121\n\u2212\u03ben \u2a7d 0\n(12.55)\nn = 1, . . . ,"
  },
  {
    "vector_id": 1595,
    "chunk_id": "p397_c2",
    "page_number": 397,
    "text": "right and the inequality\nof the constraint matches the standard form. This yields the optimization\nmin\nw,b,\u03be\n1\n2\u2225w\u22252 + C\nNX\nn=1\n\u03ben\nsubject to \u2212ynx\u22a4\nn w \u2212 ynb \u2212 \u03ben \u2a7d \u22121\n\u2212\u03ben \u2a7d 0\n(12.55)\nn = 1, . . . , N. By concatenating the variables w, b,xn into a single vector,\nand carefully collecting the terms, we obtain the following matrix form of\nthe soft margin SVM:\nmin\nw,b,\u03be\n1\n2\n\uf8ee\n\uf8f0\nw\nb\n\u03be\n\uf8f9\n\uf8fb\n\u22a4 \u0014 ID 0D,N+1\n0N+1,D 0N+1,N+1\n\u0015\uf8ee\n\uf8f0\nw\nb\n\u03be\n\uf8f9\n\uf8fb +\n\u0002\n0D+1,1 C1N,1\n\u0003\u22a4\n\uf8ee\n\uf8f0\nw\nb\n\u03be\n\uf8f9\n\uf8fb\nsubject to\n\u0014\u2212Y X \u2212y \u2212IN\n0N,D+1 \u2212IN\n\u0015\uf8ee\n\uf8f0\nw\nb\n\u03be\n\uf8f9\n\uf8fb \u2a7d\n\u0014\u22121N,1\n0N,1\n\u0015\n.\n(12.56)\nIn the preceding optimization problem, the minimization is over the pa-\nrameters [w\u22a4, b,\u03be\u22a4]\u22a4 \u2208 RD+1+N , and we use the notation: Im to rep-\nresent the identity matrix of size m \u00d7 m, 0m,n to represent the matrix\nof zeros of size m \u00d7 n, and 1m,n to represent"
  },
  {
    "vector_id": 1596,
    "chunk_id": "p397_c3",
    "page_number": 397,
    "text": "s over the pa-\nrameters [w\u22a4, b,\u03be\u22a4]\u22a4 \u2208 RD+1+N , and we use the notation: Im to rep-\nresent the identity matrix of size m \u00d7 m, 0m,n to represent the matrix\nof zeros of size m \u00d7 n, and 1m,n to represent the matrix of ones of size\nm \u00d7 n. In addition, y is the vector of labels [y1, \u00b7 \u00b7\u00b7, yN ]\u22a4, Y = diag(y)\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1597,
    "chunk_id": "p398_c0",
    "page_number": 398,
    "text": "392 Classification with Support Vector Machines\nis an N by N matrix where the elements of the diagonal are from y, and\nX \u2208 RN\u00d7D is the matrix obtained by concatenating all the examples.\nWe can similarly perform a collection of terms for the dual version of the\nSVM (12.41). To express the dual SVM in standard form, we first have to\nexpress the kernel matrix K such that each entry isKij = k(xi, xj). If we\nhave an explicit feature representation xi then we define Kij = \u27e8xi, xj\u27e9.\nFor convenience of notation we introduce a matrix with zeros everywhere\nexcept on the diagonal, where we store the labels, that is, Y = diag(y).\nThe dual SVM can be written as\nmin\n\u03b1\n1\n2\u03b1\u22a4Y KY \u03b1\u2212 1\u22a4\nN,1\u03b1\nsubject to\n\uf8ee\n\uf8ef\uf8ef\uf8f0\ny\u22a4\n\u2212y\u22a4\n\u2212IN\nIN\n\uf8f9\n\uf8fa\uf8fa\uf8fb\u03b1 \u2a7d\n\u00140N+2,1\nC1N,1\n\u0015\n.\n(12.57)\nRemark. In Sections 7.3.1 and 7.3.2, we introduced"
  },
  {
    "vector_id": 1598,
    "chunk_id": "p398_c1",
    "page_number": 398,
    "text": "ls, that is, Y = diag(y).\nThe dual SVM can be written as\nmin\n\u03b1\n1\n2\u03b1\u22a4Y KY \u03b1\u2212 1\u22a4\nN,1\u03b1\nsubject to\n\uf8ee\n\uf8ef\uf8ef\uf8f0\ny\u22a4\n\u2212y\u22a4\n\u2212IN\nIN\n\uf8f9\n\uf8fa\uf8fa\uf8fb\u03b1 \u2a7d\n\u00140N+2,1\nC1N,1\n\u0015\n.\n(12.57)\nRemark. In Sections 7.3.1 and 7.3.2, we introduced the standard forms\nof the constraints to be inequality constraints. We will express the dual\nSVM\u2019s equality constraint as two inequality constraints, i.e.,\nAx = b is replaced by Ax \u2a7d b and Ax \u2a7e b. (12.58)\nParticular software implementations of convex optimization methods may\nprovide the ability to express equality constraints. \u2662\nSince there are many different possible views of the SVM, there are\nmany approaches for solving the resulting optimization problem. The ap-\nproach presented here, expressing the SVM problem in standard convex\noptimization form, is not often used in practice. The two m"
  },
  {
    "vector_id": 1599,
    "chunk_id": "p398_c2",
    "page_number": 398,
    "text": "many approaches for solving the resulting optimization problem. The ap-\nproach presented here, expressing the SVM problem in standard convex\noptimization form, is not often used in practice. The two main implemen-\ntations of SVM solvers are Chang and Lin (2011) (which is open source)\nand Joachims (1999). Since SVMs have a clear and well-defined optimiza-\ntion problem, many approaches based on numerical optimization tech-\nniques (Nocedal and Wright, 2006) can be applied (Shawe-Taylor and\nSun, 2011).\n12.6 Further Reading\nThe SVM is one of many approaches for studying binary classification.\nOther approaches include the perceptron, logistic regression, Fisher dis-\ncriminant, nearest neighbor, naive Bayes, and random forest (Bishop, 2006;\nMurphy, 2012). A short tutorial on SVMs and kernels on d"
  },
  {
    "vector_id": 1600,
    "chunk_id": "p398_c3",
    "page_number": 398,
    "text": "approaches include the perceptron, logistic regression, Fisher dis-\ncriminant, nearest neighbor, naive Bayes, and random forest (Bishop, 2006;\nMurphy, 2012). A short tutorial on SVMs and kernels on discrete se-\nquences can be found in Ben-Hur et al. (2008). The development of SVMs\nis closely linked to empirical risk minimization, discussed in Section 8.2.\nHence, the SVM has strong theoretical properties (Vapnik, 2000; Stein-\nwart and Christmann, 2008). The book about kernel methods (Sch\u00a8olkopf\nand Smola, 2002) includes many details of support vector machines and\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1601,
    "chunk_id": "p398_c4",
    "page_number": 398,
    "text": "tics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1602,
    "chunk_id": "p399_c0",
    "page_number": 399,
    "text": "12.6 Further Reading 393\nhow to optimize them. A broader book about kernel methods (Shawe-\nTaylor and Cristianini, 2004) also includes many linear algebra approaches\nfor different machine learning problems.\nAn alternative derivation of the dual SVM can be obtained using the\nidea of the Legendre\u2013Fenchel transform (Section 7.3.3). The derivation\nconsiders each term of the unconstrained formulation of the SVM (12.31)\nseparately and calculates their convex conjugates (Rifkin and Lippert,\n2007). Readers interested in the functional analysis view (also the reg-\nularization methods view) of SVMs are referred to the work by Wahba\n(1990). Theoretical exposition of kernels (Aronszajn, 1950; Schwartz,\n1964; Saitoh, 1988; Manton and Amblard, 2015) requires a basic ground-\ning in linear operators (Akhi"
  },
  {
    "vector_id": 1603,
    "chunk_id": "p399_c1",
    "page_number": 399,
    "text": "referred to the work by Wahba\n(1990). Theoretical exposition of kernels (Aronszajn, 1950; Schwartz,\n1964; Saitoh, 1988; Manton and Amblard, 2015) requires a basic ground-\ning in linear operators (Akhiezer and Glazman, 1993). The idea of kernels\nhave been generalized to Banach spaces (Zhang et al., 2009) and Kre \u02d8\u0131n\nspaces (Ong et al., 2004; Loosli et al., 2016).\nObserve that the hinge loss has three equivalent representations, as\nshown in (12.28) and (12.29), as well as the constrained optimization\nproblem in (12.33). The formulation (12.28) is often used when compar-\ning the SVM loss function with other loss functions (Steinwart, 2007).\nThe two-piece formulation (12.29) is convenient for computing subgra-\ndients, as each piece is linear. The third formulation (12.33), as seen\nin Section 1"
  },
  {
    "vector_id": 1604,
    "chunk_id": "p399_c2",
    "page_number": 399,
    "text": "with other loss functions (Steinwart, 2007).\nThe two-piece formulation (12.29) is convenient for computing subgra-\ndients, as each piece is linear. The third formulation (12.33), as seen\nin Section 12.5, enables the use of convex quadratic programming (Sec-\ntion 7.3.2) tools.\nSince binary classification is a well-studied task in machine learning,\nother words are also sometimes used, such as discrimination, separation,\nand decision. Furthermore, there are three quantities that can be the out-\nput of a binary classifier. First is the output of the linear function itself\n(often called the score), which can take any real value. This output can be\nused for ranking the examples, and binary classification can be thought\nof as picking a threshold on the ranked examples (Shawe-Taylor and Cris-\ntia"
  },
  {
    "vector_id": 1605,
    "chunk_id": "p399_c3",
    "page_number": 399,
    "text": ", which can take any real value. This output can be\nused for ranking the examples, and binary classification can be thought\nof as picking a threshold on the ranked examples (Shawe-Taylor and Cris-\ntianini, 2004). The second quantity that is often considered the output\nof a binary classifier is the output determined after it is passed through\na non-linear function to constrain its value to a bounded range, for ex-\nample in the interval [0, 1]. A common non-linear function is the sigmoid\nfunction (Bishop, 2006). When the non-linearity results in well-calibrated\nprobabilities (Gneiting and Raftery, 2007; Reid and Williamson, 2011),\nthis is called class probability estimation. The third output of a binary\nclassifier is the final binary decision{+1, \u22121}, which is the one most com-\nmonly assumed"
  },
  {
    "vector_id": 1606,
    "chunk_id": "p399_c4",
    "page_number": 399,
    "text": "y, 2007; Reid and Williamson, 2011),\nthis is called class probability estimation. The third output of a binary\nclassifier is the final binary decision{+1, \u22121}, which is the one most com-\nmonly assumed to be the output of the classifier.\nThe SVM is a binary classifier that does not naturally lend itself to a\nprobabilistic interpretation. There are several approaches for converting\nthe raw output of the linear function (the score) into a calibrated class\nprobability estimate ( P(Y = 1|X = x)) that involve an additional cal-\nibration step (Platt, 2000; Zadrozny and Elkan, 2001; Lin et al., 2007).\nFrom the training perspective, there are many related probabilistic ap-\nproaches. We mentioned at the end of Section 12.2.5 that there is a re-\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Publis"
  },
  {
    "vector_id": 1607,
    "chunk_id": "p399_c5",
    "page_number": 399,
    "text": "From the training perspective, there are many related probabilistic ap-\nproaches. We mentioned at the end of Section 12.2.5 that there is a re-\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1608,
    "chunk_id": "p400_c0",
    "page_number": 400,
    "text": "394 Classification with Support Vector Machines\nlationship between loss function and the likelihood (also compare Sec-\ntions 8.2 and 8.3). The maximum likelihood approach corresponding to\na well-calibrated transformation during training is called logistic regres-\nsion, which comes from a class of methods called generalized linear mod-\nels. Details of logistic regression from this point of view can be found in\nAgresti (2002, chapter 5) and McCullagh and Nelder (1989, chapter 4).\nNaturally , one could take a more Bayesian view of the classifier output by\nestimating a posterior distribution using Bayesian logistic regression. The\nBayesian view also includes the specification of the prior, which includes\ndesign choices such as conjugacy (Section 6.6.1) with the likelihood. Ad-\nditionally , one"
  },
  {
    "vector_id": 1609,
    "chunk_id": "p400_c1",
    "page_number": 400,
    "text": "Bayesian logistic regression. The\nBayesian view also includes the specification of the prior, which includes\ndesign choices such as conjugacy (Section 6.6.1) with the likelihood. Ad-\nditionally , one could consider latent functions as priors, which results in\nGaussian process classification (Rasmussen and Williams, 2006, chapter\n3).\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1610,
    "chunk_id": "p401_c0",
    "page_number": 401,
    "text": "References\nAbel, Niels H. 1826. D\u00b4emonstration de l\u2019Impossibilit \u00b4e de la R \u00b4esolution Alg\u00b4ebrique des\n\u00b4Equations G\u00b4en\u00b4erales qui Passent le Quatri`eme Degr\u00b4e. Gr\u00f8ndahl and S\u00f8n.\nAdhikari, Ani, and DeNero, John. 2018. Computational and Inferential Thinking: The\nFoundations of Data Science. Gitbooks.\nAgarwal, Arvind, and Daum \u00b4e III, Hal. 2010. A Geometric View of Conjugate Priors.\nMachine Learning, 81(1), 99\u2013113.\nAgresti, A. 2002. Categorical Data Analysis. Wiley .\nAkaike, Hirotugu. 1974. A New Look at the Statistical Model Identification. IEEE\nTransactions on Automatic Control, 19(6), 716\u2013723.\nAkhiezer, Naum I., and Glazman, Izrail M. 1993. Theory of Linear Operators in Hilbert\nSpace. Dover Publications.\nAlpaydin, Ethem. 2010. Introduction to Machine Learning. MIT Press.\nAmari, Shun-ichi."
  },
  {
    "vector_id": 1611,
    "chunk_id": "p401_c1",
    "page_number": 401,
    "text": "Akhiezer, Naum I., and Glazman, Izrail M. 1993. Theory of Linear Operators in Hilbert\nSpace. Dover Publications.\nAlpaydin, Ethem. 2010. Introduction to Machine Learning. MIT Press.\nAmari, Shun-ichi. 2016. Information Geometry and Its Applications. Springer.\nArgyriou, Andreas, and Dinuzzo, Francesco. 2014. A Unifying View of Representer\nTheorems. In: Proceedings of the International Conference on Machine Learning.\nAronszajn, Nachman. 1950. Theory of Reproducing Kernels. Transactions of the Amer-\nican Mathematical Society, 68, 337\u2013404.\nAxler, Sheldon. 2015. Linear Algebra Done Right. Springer.\nBakir, G\u00a8okhan, Hofmann, Thomas, Sch\u00a8olkopf, Bernhard, Smola, Alexander J., Taskar,\nBen, and Vishwanathan, S. V. N. (eds). 2007.Predicting Structured Data. MIT Press.\nBarber, David. 2012. Bayesian Rea"
  },
  {
    "vector_id": 1612,
    "chunk_id": "p401_c2",
    "page_number": 401,
    "text": "Bakir, G\u00a8okhan, Hofmann, Thomas, Sch\u00a8olkopf, Bernhard, Smola, Alexander J., Taskar,\nBen, and Vishwanathan, S. V. N. (eds). 2007.Predicting Structured Data. MIT Press.\nBarber, David. 2012. Bayesian Reasoning and Machine Learning. Cambridge University\nPress.\nBarndorff-Nielsen, Ole. 2014. Information and Exponential Families: In Statistical The-\nory. Wiley .\nBartholomew, David, Knott, Martin, and Moustaki, Irini. 2011. Latent Variable Models\nand Factor Analysis: A Unified Approach. Wiley .\nBaydin, At\u0131l\u0131m G., Pearlmutter, Barak A., Radul, Alexey A., and Siskind, Jeffrey M.\n2018. Automatic Differentiation in Machine Learning: A Survey.Journal of Machine\nLearning Research, 18, 1\u201343.\nBeck, Amir, and Teboulle, Marc. 2003. Mirror Descent and Nonlinear Projected Subgra-\ndient Methods for Convex Opti"
  },
  {
    "vector_id": 1613,
    "chunk_id": "p401_c3",
    "page_number": 401,
    "text": "tiation in Machine Learning: A Survey.Journal of Machine\nLearning Research, 18, 1\u201343.\nBeck, Amir, and Teboulle, Marc. 2003. Mirror Descent and Nonlinear Projected Subgra-\ndient Methods for Convex Optimization. Operations Research Letters, 31(3), 167\u2013\n175.\nBelabbas, Mohamed-Ali, and Wolfe, Patrick J. 2009. Spectral Methods in Machine\nLearning and New Strategies for Very Large Datasets. Proceedings of the National\nAcademy of Sciences, 0810600105.\nBelkin, Mikhail, and Niyogi, Partha. 2003. Laplacian Eigenmaps for Dimensionality\nReduction and Data Representation. Neural Computation, 15(6), 1373\u20131396.\nBen-Hur, Asa, Ong, Cheng Soon, Sonnenburg, S\u00a8oren, Sch\u00a8olkopf, Bernhard, and R\u00a8atsch,\nGunnar. 2008. Support Vector Machines and Kernels for Computational Biology.\nPLoS Computational Biology, 4(10)"
  },
  {
    "vector_id": 1614,
    "chunk_id": "p401_c4",
    "page_number": 401,
    "text": "96.\nBen-Hur, Asa, Ong, Cheng Soon, Sonnenburg, S\u00a8oren, Sch\u00a8olkopf, Bernhard, and R\u00a8atsch,\nGunnar. 2008. Support Vector Machines and Kernels for Computational Biology.\nPLoS Computational Biology, 4(10), e1000173.\n395\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 1615,
    "chunk_id": "p402_c0",
    "page_number": 402,
    "text": "396 References\nBennett, Kristin P., and Bredensteiner, Erin J. 2000a. Duality and Geometry in SVM\nClassifiers. In: Proceedings of the International Conference on Machine Learning.\nBennett, Kristin P., and Bredensteiner, Erin J. 2000b. Geometry in Learning. Pages\n132\u2013145 of: Geometry at Work. Mathematical Association of America.\nBerlinet, Alain, and Thomas-Agnan, Christine. 2004. Reproducing Kernel Hilbert Spaces\nin Probability and Statistics. Springer.\nBertsekas, Dimitri P. 1999. Nonlinear Programming. Athena Scientific.\nBertsekas, Dimitri P. 2009. Convex Optimization Theory. Athena Scientific.\nBickel, Peter J., and Doksum, Kjell. 2006. Mathematical Statistics, Basic Ideas and\nSelected Topics. Vol. 1. Prentice Hall.\nBickson, Danny , Dolev, Danny , Shental, Ori, Siegel, Paul H., and Wolf, J"
  },
  {
    "vector_id": 1616,
    "chunk_id": "p402_c1",
    "page_number": 402,
    "text": ".\nBickel, Peter J., and Doksum, Kjell. 2006. Mathematical Statistics, Basic Ideas and\nSelected Topics. Vol. 1. Prentice Hall.\nBickson, Danny , Dolev, Danny , Shental, Ori, Siegel, Paul H., and Wolf, Jack K. 2007.\nLinear Detection via Belief Propagation. In: Proceedings of the Annual Allerton Con-\nference on Communication, Control, and Computing.\nBillingsley , Patrick. 1995.Probability and Measure. Wiley .\nBishop, Christopher M. 1995. Neural Networks for Pattern Recognition . Clarendon\nPress.\nBishop, Christopher M. 1999. Bayesian PCA. In: Advances in Neural Information Pro-\ncessing Systems.\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\nBlei, David M., Kucukelbir, Alp, and McAuliffe, Jon D. 2017. Variational Inference: A\nReview for Statisticians. Journal of"
  },
  {
    "vector_id": 1617,
    "chunk_id": "p402_c2",
    "page_number": 402,
    "text": "hop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\nBlei, David M., Kucukelbir, Alp, and McAuliffe, Jon D. 2017. Variational Inference: A\nReview for Statisticians. Journal of the American Statistical Association , 112(518),\n859\u2013877.\nBlum, Arvim, and Hardt, Moritz. 2015. The Ladder: A Reliable Leaderboard for Ma-\nchine Learning Competitions. In: International Conference on Machine Learning.\nBonnans, J. Fr\u00b4ed\u00b4eric, Gilbert, J. Charles, Lemar\u00b4echal, Claude, and Sagastiz\u00b4abal, Clau-\ndia A. 2006. Numerical Optimization: Theoretical and Practical Aspects. Springer.\nBorwein, Jonathan M., and Lewis, Adrian S. 2006. Convex Analysis and Nonlinear\nOptimization. 2nd edn. Canadian Mathematical Society .\nBottou, L\u00b4eon. 1998. Online Algorithms and Stochastic Approximations. Page"
  },
  {
    "vector_id": 1618,
    "chunk_id": "p402_c3",
    "page_number": 402,
    "text": "Jonathan M., and Lewis, Adrian S. 2006. Convex Analysis and Nonlinear\nOptimization. 2nd edn. Canadian Mathematical Society .\nBottou, L\u00b4eon. 1998. Online Algorithms and Stochastic Approximations. Pages 9\u201342\nof: Online Learning and Neural Networks. Cambridge University Press.\nBottou, L\u00b4eon, Curtis, Frank E., and Nocedal, Jorge. 2018. Optimization Methods for\nLarge-Scale Machine Learning. SIAM Review, 60(2), 223\u2013311.\nBoucheron, Stephane, Lugosi, Gabor, and Massart, Pascal. 2013. Concentration In-\nequalities: A Nonasymptotic Theory of Independence. Oxford University Press.\nBoyd, Stephen, and Vandenberghe, Lieven. 2004. Convex Optimization. Cambridge\nUniversity Press.\nBoyd, Stephen, and Vandenberghe, Lieven. 2018. Introduction to Applied Linear Alge-\nbra. Cambridge University Press.\nBrochu, Er"
  },
  {
    "vector_id": 1619,
    "chunk_id": "p402_c4",
    "page_number": 402,
    "text": "enberghe, Lieven. 2004. Convex Optimization. Cambridge\nUniversity Press.\nBoyd, Stephen, and Vandenberghe, Lieven. 2018. Introduction to Applied Linear Alge-\nbra. Cambridge University Press.\nBrochu, Eric, Cora, Vlad M., and de Freitas, Nando. 2009. A Tutorial on Bayesian\nOptimization of Expensive Cost Functions, with Application to Active User Modeling\nand Hierarchical Reinforcement Learning . Tech. rept. TR-2009-023. Department of\nComputer Science, University of British Columbia.\nBrooks, Steve, Gelman, Andrew, Jones, Galin L., and Meng, Xiao-Li (eds). 2011.Hand-\nbook of Markov Chain Monte Carlo. Chapman and Hall/CRC.\nBrown, Lawrence D. 1986. Fundamentals of Statistical Exponential Families: With Ap-\nplications in Statistical Decision Theory. Institute of Mathematical Statistics.\nBryson, Ar"
  },
  {
    "vector_id": 1620,
    "chunk_id": "p402_c5",
    "page_number": 402,
    "text": "o. Chapman and Hall/CRC.\nBrown, Lawrence D. 1986. Fundamentals of Statistical Exponential Families: With Ap-\nplications in Statistical Decision Theory. Institute of Mathematical Statistics.\nBryson, Arthur E. 1961. A Gradient Method for Optimizing Multi-Stage Allocation\nProcesses. In: Proceedings of the Harvard University Symposium on Digital Computers\nand Their Applications.\nBubeck, S\u00b4ebastien. 2015. Convex Optimization: Algorithms and Complexity . Founda-\ntions and Trends in Machine Learning, 8(3-4), 231\u2013357.\nB\u00a8uhlmann, Peter, and Van De Geer, Sara. 2011. Statistics for High-Dimensional Data .\nSpringer.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1621,
    "chunk_id": "p402_c6",
    "page_number": 402,
    "text": ".\nSpringer.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1622,
    "chunk_id": "p403_c0",
    "page_number": 403,
    "text": "References 397\nBurges, Christopher. 2010. Dimension Reduction: A Guided Tour. Foundations and\nTrends in Machine Learning, 2(4), 275\u2013365.\nCarroll, J Douglas, and Chang, Jih-Jie. 1970. Analysis of Individual Differences in\nMultidimensional Scaling via an N-Way Generalization of \u201cEckart-Young\u201d Decom-\nposition. Psychometrika, 35(3), 283\u2013319.\nCasella, George, and Berger, Roger L. 2002. Statistical Inference. Duxbury .\nC \u00b8inlar, Erhan. 2011.Probability and Stochastics. Springer.\nChang, Chih-Chung, and Lin, Chih-Jen. 2011. LIBSVM: A Library for Support Vector\nMachines. ACM Transactions on Intelligent Systems and Technology, 2, 27:1\u201327:27.\nCheeseman, Peter. 1985. In Defense of Probability . In:Proceedings of the International\nJoint Conference on Artificial Intelligence.\nChollet, Francois, and Alla"
  },
  {
    "vector_id": 1623,
    "chunk_id": "p403_c1",
    "page_number": 403,
    "text": "Systems and Technology, 2, 27:1\u201327:27.\nCheeseman, Peter. 1985. In Defense of Probability . In:Proceedings of the International\nJoint Conference on Artificial Intelligence.\nChollet, Francois, and Allaire, J. J. 2018. Deep Learning with R. Manning Publications.\nCodd, Edgar F. 1990. The Relational Model for Database Management. Addison-Wesley\nLongman Publishing.\nCunningham, John P., and Ghahramani, Zoubin. 2015. Linear Dimensionality Reduc-\ntion: Survey , Insights, and Generalizations. Journal of Machine Learning Research ,\n16, 2859\u20132900.\nDatta, Biswa N. 2010. Numerical Linear Algebra and Applications. SIAM.\nDavidson, Anthony C., and Hinkley , David V. 1997.Bootstrap Methods and Their Appli-\ncation. Cambridge University Press.\nDean, Jeffrey , Corrado, Greg S., Monga, Rajat, and Chen, et al."
  },
  {
    "vector_id": 1624,
    "chunk_id": "p403_c2",
    "page_number": 403,
    "text": "ations. SIAM.\nDavidson, Anthony C., and Hinkley , David V. 1997.Bootstrap Methods and Their Appli-\ncation. Cambridge University Press.\nDean, Jeffrey , Corrado, Greg S., Monga, Rajat, and Chen, et al. 2012. Large Scale\nDistributed Deep Networks. In: Advances in Neural Information Processing Systems.\nDeisenroth, Marc P., and Mohamed, Shakir. 2012. Expectation Propagation in Gaus-\nsian Process Dynamical Systems. Pages 2618\u20132626 of: Advances in Neural Informa-\ntion Processing Systems.\nDeisenroth, Marc P., and Ohlsson, Henrik. 2011. A General Perspective on Gaussian\nFiltering and Smoothing: Explaining Current and Deriving New Algorithms. In:\nProceedings of the American Control Conference.\nDeisenroth, Marc P., Fox, Dieter, and Rasmussen, Carl E. 2015. Gaussian Processes\nfor Data-Efficient Learni"
  },
  {
    "vector_id": 1625,
    "chunk_id": "p403_c3",
    "page_number": 403,
    "text": "ing Current and Deriving New Algorithms. In:\nProceedings of the American Control Conference.\nDeisenroth, Marc P., Fox, Dieter, and Rasmussen, Carl E. 2015. Gaussian Processes\nfor Data-Efficient Learning in Robotics and Control. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 37(2), 408\u2013423.\nDempster, Arthur P., Laird, Nan M., and Rubin, Donald B. 1977. Maximum Likelihood\nfrom Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society,\n39(1), 1\u201338.\nDeng, Li, Seltzer, Michael L., Yu, Dong, Acero, Alex, Mohamed, Abdel-rahman, and\nHinton, Geoffrey E. 2010. Binary Coding of Speech Spectrograms Using a Deep\nAuto-Encoder. In: Proceedings of Interspeech.\nDevroye, Luc. 1986. Non-Uniform Random Variate Generation. Springer.\nDonoho, David L., and Grimes, Carrie. 20"
  },
  {
    "vector_id": 1626,
    "chunk_id": "p403_c4",
    "page_number": 403,
    "text": "y Coding of Speech Spectrograms Using a Deep\nAuto-Encoder. In: Proceedings of Interspeech.\nDevroye, Luc. 1986. Non-Uniform Random Variate Generation. Springer.\nDonoho, David L., and Grimes, Carrie. 2003. Hessian Eigenmaps: Locally Linear\nEmbedding Techniques for High-Dimensional Data. Proceedings of the National\nAcademy of Sciences, 100(10), 5591\u20135596.\nDost\u00b4al, Zden\u02d8ek. 2009. Optimal Quadratic Programming Algorithms: With Applications\nto Variational Inequalities. Springer.\nDouven, Igor. 2017. Abduction. In: The Stanford Encyclopedia of Philosophy . Meta-\nphysics Research Lab, Stanford University .\nDowney , Allen B. 2014. Think Stats: Exploratory Data Analysis . 2nd edn. O\u2019Reilly\nMedia.\nDreyfus, Stuart. 1962. The Numerical Solution of Variational Problems. Journal of\nMathematical Analysis a"
  },
  {
    "vector_id": 1627,
    "chunk_id": "p403_c5",
    "page_number": 403,
    "text": "ty .\nDowney , Allen B. 2014. Think Stats: Exploratory Data Analysis . 2nd edn. O\u2019Reilly\nMedia.\nDreyfus, Stuart. 1962. The Numerical Solution of Variational Problems. Journal of\nMathematical Analysis and Applications, 5(1), 30\u201345.\nDrumm, Volker, and Weil, Wolfgang. 2001.Lineare Algebra und Analytische Geometrie.\nLecture Notes, Universit\u00a8at Karlsruhe (TH).\nDudley , Richard M. 2002.Real Analysis and Probability. Cambridge University Press.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1628,
    "chunk_id": "p404_c0",
    "page_number": 404,
    "text": "398 References\nEaton, Morris L. 2007. Multivariate Statistics: A Vector Space Approach . Institute of\nMathematical Statistics Lecture Notes.\nEckart, Carl, and Young, Gale. 1936. The Approximation of One Matrix by Another of\nLower Rank. Psychometrika, 1(3), 211\u2013218.\nEfron, Bradley , and Hastie, Trevor. 2016.Computer Age Statistical Inference: Algorithms,\nEvidence and Data Science. Cambridge University Press.\nEfron, Bradley , and Tibshirani, Robert J. 1993.An Introduction to the Bootstrap. Chap-\nman and Hall/CRC.\nElliott, Conal. 2009. Beautiful Differentiation. In: International Conference on Func-\ntional Programming.\nEvgeniou, Theodoros, Pontil, Massimiliano, and Poggio, Tomaso. 2000. Statistical\nLearning Theory: A Primer. International Journal of Computer Vision, 38(1), 9\u201313.\nFan, Rong-En,"
  },
  {
    "vector_id": 1629,
    "chunk_id": "p404_c1",
    "page_number": 404,
    "text": "nc-\ntional Programming.\nEvgeniou, Theodoros, Pontil, Massimiliano, and Poggio, Tomaso. 2000. Statistical\nLearning Theory: A Primer. International Journal of Computer Vision, 38(1), 9\u201313.\nFan, Rong-En, Chang, Kai-Wei, Hsieh, Cho-Jui, Wang, Xiang-Rui, and Lin, Chih-Jen.\n2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine\nLearning Research, 9, 1871\u20131874.\nGal, Yarin, van der Wilk, Mark, and Rasmussen, Carl E. 2014. Distributed Variational\nInference in Sparse Gaussian Process Regression and Latent Variable Models. In:\nAdvances in Neural Information Processing Systems.\nG\u00a8artner, Thomas. 2008. Kernels for Structured Data. World Scientific.\nGavish, Matan, and Donoho, David L. 2014. The Optimal Hard Threshold for Singular\nValues is 4\n\u221a\n3. IEEE Transactions on Information"
  },
  {
    "vector_id": 1630,
    "chunk_id": "p404_c2",
    "page_number": 404,
    "text": "rtner, Thomas. 2008. Kernels for Structured Data. World Scientific.\nGavish, Matan, and Donoho, David L. 2014. The Optimal Hard Threshold for Singular\nValues is 4\n\u221a\n3. IEEE Transactions on Information Theory, 60(8), 5040\u20135053.\nGelman, Andrew, Carlin, John B., Stern, Hal S., and Rubin, Donald B. 2004. Bayesian\nData Analysis. Chapman and Hall/CRC.\nGentle, James E. 2004. Random Number Generation and Monte Carlo Methods .\nSpringer.\nGhahramani, Zoubin. 2015. Probabilistic Machine Learning and Artificial Intelligence.\nNature, 521, 452\u2013459.\nGhahramani, Zoubin, and Roweis, Sam T. 1999. Learning Nonlinear Dynamical Sys-\ntems Using an EM Algorithm. In:Advances in Neural Information Processing Systems.\nMIT Press.\nGilks, Walter R., Richardson, Sylvia, and Spiegelhalter, David J. 1996. Markov Chain\nMont"
  },
  {
    "vector_id": 1631,
    "chunk_id": "p404_c3",
    "page_number": 404,
    "text": "ar Dynamical Sys-\ntems Using an EM Algorithm. In:Advances in Neural Information Processing Systems.\nMIT Press.\nGilks, Walter R., Richardson, Sylvia, and Spiegelhalter, David J. 1996. Markov Chain\nMonte Carlo in Practice. Chapman and Hall/CRC.\nGneiting, Tilmann, and Raftery , Adrian E. 2007. Strictly Proper Scoring Rules, Pre-\ndiction, and Estimation. Journal of the American Statistical Association , 102(477),\n359\u2013378.\nGoh, Gabriel. 2017. Why Momentum Really Works. Distill.\nGohberg, Israel, Goldberg, Seymour, and Krupnik, Nahum. 2012. Traces and Determi-\nnants of Linear Operators. Birkh\u00a8auser.\nGolan, Jonathan S. 2007. The Linear Algebra a Beginning Graduate Student Ought to\nKnow. Springer.\nGolub, Gene H., and Van Loan, Charles F. 2012. Matrix Computations. JHU Press.\nGoodfellow, Ian, Bengio"
  },
  {
    "vector_id": 1632,
    "chunk_id": "p404_c4",
    "page_number": 404,
    "text": "Golan, Jonathan S. 2007. The Linear Algebra a Beginning Graduate Student Ought to\nKnow. Springer.\nGolub, Gene H., and Van Loan, Charles F. 2012. Matrix Computations. JHU Press.\nGoodfellow, Ian, Bengio, Yoshua, and Courville, Aaron. 2016. Deep Learning. MIT\nPress.\nGraepel, Thore, Candela, Joaquin Qui \u02dcnonero-Candela, Borchert, Thomas, and Her-\nbrich, Ralf. 2010. Web-Scale Bayesian Click-through Rate Prediction for Sponsored\nSearch Advertising in Microsoft\u2019s Bing Search Engine. In: Proceedings of the Interna-\ntional Conference on Machine Learning.\nGriewank, Andreas, and Walther, Andrea. 2003. Introduction to Automatic Differenti-\nation. In: Proceedings in Applied Mathematics and Mechanics.\nGriewank, Andreas, and Walther, Andrea. 2008. Evaluating Derivatives, Principles and\nTechniques of Algo"
  },
  {
    "vector_id": 1633,
    "chunk_id": "p404_c5",
    "page_number": 404,
    "text": "troduction to Automatic Differenti-\nation. In: Proceedings in Applied Mathematics and Mechanics.\nGriewank, Andreas, and Walther, Andrea. 2008. Evaluating Derivatives, Principles and\nTechniques of Algorithmic Differentiation. SIAM.\nGrimmett, Geoffrey R., and Welsh, Dominic. 2014.Probability: An Introduction. Oxford\nUniversity Press.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1634,
    "chunk_id": "p405_c0",
    "page_number": 405,
    "text": "References 399\nGrinstead, Charles M., and Snell, J. Laurie. 1997.Introduction to Probability. American\nMathematical Society .\nHacking, Ian. 2001. Probability and Inductive Logic. Cambridge University Press.\nHall, Peter. 1992. The Bootstrap and Edgeworth Expansion. Springer.\nHallin, Marc, Paindaveine, Davy , and \u02c7Siman, Miroslav. 2010. Multivariate Quan-\ntiles and Multiple-Output Regression Quantiles: From \u21131 Optimization to Halfspace\nDepth. Annals of Statistics, 38, 635\u2013669.\nHasselblatt, Boris, and Katok, Anatole. 2003. A First Course in Dynamics with a\nPanorama of Recent Developments. Cambridge University Press.\nHastie, Trevor, Tibshirani, Robert, and Friedman, Jerome. 2001. The Elements of Sta-\ntistical Learning \u2013 Data Mining, Inference, and Prediction. Springer.\nHausman, Karol, Springen"
  },
  {
    "vector_id": 1635,
    "chunk_id": "p405_c1",
    "page_number": 405,
    "text": "ge University Press.\nHastie, Trevor, Tibshirani, Robert, and Friedman, Jerome. 2001. The Elements of Sta-\ntistical Learning \u2013 Data Mining, Inference, and Prediction. Springer.\nHausman, Karol, Springenberg, Jost T., Wang, Ziyu, Heess, Nicolas, and Riedmiller,\nMartin. 2018. Learning an Embedding Space for Transferable Robot Skills. In:\nProceedings of the International Conference on Learning Representations.\nHazan, Elad. 2015. Introduction to Online Convex Optimization. Foundations and\nTrends in Optimization, 2(3\u20134), 157\u2013325.\nHensman, James, Fusi, Nicol `o, and Lawrence, Neil D. 2013. Gaussian Processes for\nBig Data. In: Proceedings of the Conference on Uncertainty in Artificial Intelligence.\nHerbrich, Ralf, Minka, Tom, and Graepel, Thore. 2007. TrueSkill(TM): A Bayesian\nSkill Rating System."
  },
  {
    "vector_id": 1636,
    "chunk_id": "p405_c2",
    "page_number": 405,
    "text": "ocesses for\nBig Data. In: Proceedings of the Conference on Uncertainty in Artificial Intelligence.\nHerbrich, Ralf, Minka, Tom, and Graepel, Thore. 2007. TrueSkill(TM): A Bayesian\nSkill Rating System. In: Advances in Neural Information Processing Systems.\nHiriart-Urruty , Jean-Baptiste, and Lemar\u00b4echal, Claude. 2001. Fundamentals of Convex\nAnalysis. Springer.\nHoffman, Matthew D., Blei, David M., and Bach, Francis. 2010. Online Learning for\nLatent Dirichlet Allocation. Advances in Neural Information Processing Systems.\nHoffman, Matthew D., Blei, David M., Wang, Chong, and Paisley , John. 2013. Stochas-\ntic Variational Inference. Journal of Machine Learning Research, 14(1), 1303\u20131347.\nHofmann, Thomas, Sch\u00a8olkopf, Bernhard, and Smola, Alexander J. 2008. Kernel Meth-\nods in Machine Learning. An"
  },
  {
    "vector_id": 1637,
    "chunk_id": "p405_c3",
    "page_number": 405,
    "text": "tochas-\ntic Variational Inference. Journal of Machine Learning Research, 14(1), 1303\u20131347.\nHofmann, Thomas, Sch\u00a8olkopf, Bernhard, and Smola, Alexander J. 2008. Kernel Meth-\nods in Machine Learning. Annals of Statistics, 36(3), 1171\u20131220.\nHogben, Leslie. 2013. Handbook of Linear Algebra. Chapman and Hall/CRC.\nHorn, Roger A., and Johnson, Charles R. 2013. Matrix Analysis. Cambridge University\nPress.\nHotelling, Harold. 1933. Analysis of a Complex of Statistical Variables into Principal\nComponents. Journal of Educational Psychology, 24, 417\u2013441.\nHyvarinen, Aapo, Oja, Erkki, and Karhunen, Juha. 2001.Independent Component Anal-\nysis. Wiley .\nImbens, Guido W., and Rubin, Donald B. 2015. Causal Inference for Statistics, Social\nand Biomedical Sciences. Cambridge University Press.\nJacod, Jean, and P"
  },
  {
    "vector_id": 1638,
    "chunk_id": "p405_c4",
    "page_number": 405,
    "text": "1.Independent Component Anal-\nysis. Wiley .\nImbens, Guido W., and Rubin, Donald B. 2015. Causal Inference for Statistics, Social\nand Biomedical Sciences. Cambridge University Press.\nJacod, Jean, and Protter, Philip. 2004. Probability Essentials. Springer.\nJaynes, Edwin T. 2003. Probability Theory: The Logic of Science. Cambridge University\nPress.\nJefferys, William H., and Berger, James O. 1992. Ockham\u2019s Razor and Bayesian Anal-\nysis. American Scientist, 80, 64\u201372.\nJeffreys, Harold. 1961. Theory of Probability. Oxford University Press.\nJimenez Rezende, Danilo, and Mohamed, Shakir. 2015. Variational Inference with Nor-\nmalizing Flows. In: Proceedings of the International Conference on Machine Learning.\nJimenez Rezende, Danilo, Mohamed, Shakir, and Wierstra, Daan. 2014. Stochastic\nBackpropaga"
  },
  {
    "vector_id": 1639,
    "chunk_id": "p405_c5",
    "page_number": 405,
    "text": "onal Inference with Nor-\nmalizing Flows. In: Proceedings of the International Conference on Machine Learning.\nJimenez Rezende, Danilo, Mohamed, Shakir, and Wierstra, Daan. 2014. Stochastic\nBackpropagation and Approximate Inference in Deep Generative Models. In: Pro-\nceedings of the International Conference on Machine Learning.\nJoachims, Thorsten. 1999. Advances in Kernel Methods \u2013 Support Vector Learning. MIT\nPress. Chap. Making Large-Scale SVM Learning Practical, pages 169\u2013184.\nJordan, Michael I., Ghahramani, Zoubin, Jaakkola, Tommi S., and Saul, Lawrence K.\n1999. An Introduction to Variational Methods for Graphical Models.Machine Learn-\ning, 37, 183\u2013233.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1640,
    "chunk_id": "p405_c6",
    "page_number": 405,
    "text": "al Methods for Graphical Models.Machine Learn-\ning, 37, 183\u2013233.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1641,
    "chunk_id": "p406_c0",
    "page_number": 406,
    "text": "400 References\nJulier, Simon J., and Uhlmann, Jeffrey K. 1997. A New Extension of the Kalman Filter\nto Nonlinear Systems. In: Proceedings of AeroSense Symposium on Aerospace/Defense\nSensing, Simulation and Controls.\nKaiser, Marcus, and Hilgetag, Claus C. 2006. Nonoptimal Component Placement, but\nShort Processing Paths, Due to Long-Distance Projections in Neural Systems. PLoS\nComputational Biology, 2(7), e95.\nKalman, Dan. 1996. A Singularly Valuable Decomposition: The SVD of a Matrix. Col-\nlege Mathematics Journal, 27(1), 2\u201323.\nKalman, Rudolf E. 1960. A New Approach to Linear Filtering and Prediction Problems.\nTransactions of the ASME \u2013 Journal of Basic Engineering, 82(Series D), 35\u201345.\nKamthe, Sanket, and Deisenroth, Marc P. 2018. Data-Efficient Reinforcement Learning\nwith Probabilistic Mo"
  },
  {
    "vector_id": 1642,
    "chunk_id": "p406_c1",
    "page_number": 406,
    "text": "iction Problems.\nTransactions of the ASME \u2013 Journal of Basic Engineering, 82(Series D), 35\u201345.\nKamthe, Sanket, and Deisenroth, Marc P. 2018. Data-Efficient Reinforcement Learning\nwith Probabilistic Model Predictive Control. In: Proceedings of the International\nConference on Artificial Intelligence and Statistics.\nKatz, Victor J. 2004. A History of Mathematics. Pearson/Addison-Wesley .\nKelley , Henry J. 1960. Gradient Theory of Optimal Flight Paths. Ars Journal, 30(10),\n947\u2013954.\nKimeldorf, George S., and Wahba, Grace. 1970. A Correspondence between Bayesian\nEstimation on Stochastic Processes and Smoothing by Splines. Annals of Mathemat-\nical Statistics, 41(2), 495\u2013502.\nKingma, Diederik P., and Welling, Max. 2014. Auto-Encoding Variational Bayes. In:\nProceedings of the International Conferen"
  },
  {
    "vector_id": 1643,
    "chunk_id": "p406_c2",
    "page_number": 406,
    "text": "d Smoothing by Splines. Annals of Mathemat-\nical Statistics, 41(2), 495\u2013502.\nKingma, Diederik P., and Welling, Max. 2014. Auto-Encoding Variational Bayes. In:\nProceedings of the International Conference on Learning Representations.\nKittler, Josef, and F\u00a8oglein, Janos. 1984. Contextual Classification of Multispectral Pixel\nData. Image and Vision Computing, 2(1), 13\u201329.\nKolda, Tamara G., and Bader, Brett W. 2009. Tensor Decompositions and Applications.\nSIAM Review, 51(3), 455\u2013500.\nKoller, Daphne, and Friedman, Nir. 2009. Probabilistic Graphical Models. MIT Press.\nKong, Linglong, and Mizera, Ivan. 2012. Quantile Tomography: Using Quantiles with\nMultivariate Data. Statistica Sinica, 22, 1598\u20131610.\nLang, Serge. 1987. Linear Algebra. Springer.\nLawrence, Neil D. 2005. Probabilistic Non-Linear Pri"
  },
  {
    "vector_id": 1644,
    "chunk_id": "p406_c3",
    "page_number": 406,
    "text": ". 2012. Quantile Tomography: Using Quantiles with\nMultivariate Data. Statistica Sinica, 22, 1598\u20131610.\nLang, Serge. 1987. Linear Algebra. Springer.\nLawrence, Neil D. 2005. Probabilistic Non-Linear Principal Component Analysis with\nGaussian Process Latent Variable Models. Journal of Machine Learning Research ,\n6(Nov.), 1783\u20131816.\nLeemis, Lawrence M., and McQueston, Jacquelyn T. 2008. Univariate Distribution\nRelationships. American Statistician, 62(1), 45\u201353.\nLehmann, Erich L., and Romano, Joseph P. 2005. Testing Statistical Hypotheses .\nSpringer.\nLehmann, Erich Leo, and Casella, George. 1998. Theory of Point Estimation. Springer.\nLiesen, J\u00a8org, and Mehrmann, Volker. 2015. Linear Algebra. Springer.\nLin, Hsuan-Tien, Lin, Chih-Jen, and Weng, Ruby C. 2007. A Note on Platt\u2019s Probabilistic\nOutput"
  },
  {
    "vector_id": 1645,
    "chunk_id": "p406_c4",
    "page_number": 406,
    "text": "heory of Point Estimation. Springer.\nLiesen, J\u00a8org, and Mehrmann, Volker. 2015. Linear Algebra. Springer.\nLin, Hsuan-Tien, Lin, Chih-Jen, and Weng, Ruby C. 2007. A Note on Platt\u2019s Probabilistic\nOutputs for Support Vector Machines. Machine Learning, 68, 267\u2013276.\nLjung, Lennart. 1999. System Identification: Theory for the User. Prentice Hall.\nLoosli, Ga\u00a8elle, Canu, St\u00b4ephane, and Ong, Cheng Soon. 2016. Learning SVM in Kre \u02d8\u0131n\nSpaces. IEEE Transactions of Pattern Analysis and Machine Intelligence, 38(6), 1204\u2013\n1216.\nLuenberger, David G. 1969. Optimization by Vector Space Methods. Wiley .\nMacKay , David J. C. 1992. Bayesian Interpolation.Neural Computation, 4, 415\u2013447.\nMacKay , David J. C. 1998. Introduction to Gaussian Processes. Pages 133\u2013165 of:\nBishop, C. M. (ed), Neural Networks and Machi"
  },
  {
    "vector_id": 1646,
    "chunk_id": "p406_c5",
    "page_number": 406,
    "text": "David J. C. 1992. Bayesian Interpolation.Neural Computation, 4, 415\u2013447.\nMacKay , David J. C. 1998. Introduction to Gaussian Processes. Pages 133\u2013165 of:\nBishop, C. M. (ed), Neural Networks and Machine Learning. Springer.\nMacKay , David J. C. 2003. Information Theory, Inference, and Learning Algorithms .\nCambridge University Press.\nMagnus, Jan R., and Neudecker, Heinz. 2007. Matrix Differential Calculus with Appli-\ncations in Statistics and Econometrics. Wiley .\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1647,
    "chunk_id": "p407_c0",
    "page_number": 407,
    "text": "References 401\nManton, Jonathan H., and Amblard, Pierre-Olivier. 2015. A Primer on Reproducing\nKernel Hilbert Spaces. Foundations and Trends in Signal Processing, 8(1\u20132), 1\u2013126.\nMarkovsky , Ivan. 2011. Low Rank Approximation: Algorithms, Implementation, Appli-\ncations. Springer.\nMaybeck, Peter S. 1979. Stochastic Models, Estimation, and Control. Academic Press.\nMcCullagh, Peter, and Nelder, John A. 1989.Generalized Linear Models. CRC Press.\nMcEliece, Robert J., MacKay , David J. C., and Cheng, Jung-Fu. 1998. Turbo Decoding\nas an Instance of Pearl\u2019s \u201cBelief Propagation\u201d Algorithm. IEEE Journal on Selected\nAreas in Communications, 16(2), 140\u2013152.\nMika, Sebastian, R \u00a8atsch, Gunnar, Weston, Jason, Sch \u00a8olkopf, Bernhard, and M \u00a8uller,\nKlaus-Robert. 1999. Fisher Discriminant Analysis with Kernel"
  },
  {
    "vector_id": 1648,
    "chunk_id": "p407_c1",
    "page_number": 407,
    "text": "on Selected\nAreas in Communications, 16(2), 140\u2013152.\nMika, Sebastian, R \u00a8atsch, Gunnar, Weston, Jason, Sch \u00a8olkopf, Bernhard, and M \u00a8uller,\nKlaus-Robert. 1999. Fisher Discriminant Analysis with Kernels. Pages 41\u201348 of:\nProceedings of the Workshop on Neural Networks for Signal Processing.\nMinka, Thomas P. 2001a. A Family of Algorithms for Approximate Bayesian Inference .\nPh.D. thesis, Massachusetts Institute of Technology .\nMinka, Tom. 2001b. Automatic Choice of Dimensionality of PCA. In: Advances in\nNeural Information Processing Systems.\nMitchell, Tom. 1997. Machine Learning. McGraw-Hill.\nMnih, Volodymyr, Kavukcuoglu, Koray , and Silver, David, et al. 2015. Human-Level\nControl through Deep Reinforcement Learning. Nature, 518, 529\u2013533.\nMoonen, Marc, and De Moor, Bart. 1995. SVD and Signal P"
  },
  {
    "vector_id": 1649,
    "chunk_id": "p407_c2",
    "page_number": 407,
    "text": ", Volodymyr, Kavukcuoglu, Koray , and Silver, David, et al. 2015. Human-Level\nControl through Deep Reinforcement Learning. Nature, 518, 529\u2013533.\nMoonen, Marc, and De Moor, Bart. 1995. SVD and Signal Processing, III: Algorithms,\nArchitectures and Applications. Elsevier.\nMoustaki, Irini, Knott, Martin, and Bartholomew, David J. 2015. Latent-Variable Mod-\neling. American Cancer Society . Pages 1\u201310.\nM\u00a8uller, Andreas C., and Guido, Sarah. 2016. Introduction to Machine Learning with\nPython: A Guide for Data Scientists. O\u2019Reilly Publishing.\nMurphy , Kevin P. 2012.Machine Learning: A Probabilistic Perspective. MIT Press.\nNeal, Radford M. 1996. Bayesian Learning for Neural Networks. Ph.D. thesis, Depart-\nment of Computer Science, University of Toronto.\nNeal, Radford M., and Hinton, Geoffrey E. 199"
  },
  {
    "vector_id": 1650,
    "chunk_id": "p407_c3",
    "page_number": 407,
    "text": "rspective. MIT Press.\nNeal, Radford M. 1996. Bayesian Learning for Neural Networks. Ph.D. thesis, Depart-\nment of Computer Science, University of Toronto.\nNeal, Radford M., and Hinton, Geoffrey E. 1999. A View of the EM Algorithm that\nJustifies Incremental, Sparse, and Other Variants. Pages 355\u2013368 of: Learning in\nGraphical Models. MIT Press.\nNelsen, Roger. 2006. An Introduction to Copulas. Springer.\nNesterov, Yuri. 2018. Lectures on Convex Optimization. Springer.\nNeumaier, Arnold. 1998. Solving Ill-Conditioned and Singular Linear Systems: A Tu-\ntorial on Regularization. SIAM Review, 40, 636\u2013666.\nNocedal, Jorge, and Wright, Stephen J. 2006. Numerical Optimization. Springer.\nNowozin, Sebastian, Gehler, Peter V., Jancsary , Jeremy , and Lampert, Christoph H.\n(eds). 2014. Advanced Structured"
  },
  {
    "vector_id": 1651,
    "chunk_id": "p407_c4",
    "page_number": 407,
    "text": "66.\nNocedal, Jorge, and Wright, Stephen J. 2006. Numerical Optimization. Springer.\nNowozin, Sebastian, Gehler, Peter V., Jancsary , Jeremy , and Lampert, Christoph H.\n(eds). 2014. Advanced Structured Prediction. MIT Press.\nO\u2019Hagan, Anthony . 1991. Bayes-Hermite Quadrature. Journal of Statistical Planning\nand Inference, 29, 245\u2013260.\nOng, Cheng Soon, Mary , Xavier, Canu, St\u00b4ephane, and Smola, Alexander J. 2004. Learn-\ning with Non-Positive Kernels. In: Proceedings of the International Conference on\nMachine Learning.\nOrmoneit, Dirk, Sidenbladh, Hedvig, Black, Michael J., and Hastie, Trevor. 2001.\nLearning and Tracking Cyclic Human Motion. In: Advances in Neural Information\nProcessing Systems.\nPage, Lawrence, Brin, Sergey , Motwani, Rajeev, and Winograd, Terry . 1999. The\nPageRank Citation Ran"
  },
  {
    "vector_id": 1652,
    "chunk_id": "p407_c5",
    "page_number": 407,
    "text": "Learning and Tracking Cyclic Human Motion. In: Advances in Neural Information\nProcessing Systems.\nPage, Lawrence, Brin, Sergey , Motwani, Rajeev, and Winograd, Terry . 1999. The\nPageRank Citation Ranking: Bringing Order to the Web . Tech. rept. Stanford Info-\nLab.\nPaquet, Ulrich. 2008. Bayesian Inference for Latent Variable Models. Ph.D. thesis, Uni-\nversity of Cambridge.\nParzen, Emanuel. 1962. On Estimation of a Probability Density Function and Mode.\nAnnals of Mathematical Statistics, 33(3), 1065\u20131076.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1653,
    "chunk_id": "p407_c6",
    "page_number": 407,
    "text": "2020)."
  },
  {
    "vector_id": 1654,
    "chunk_id": "p408_c0",
    "page_number": 408,
    "text": "402 References\nPearl, Judea. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible\nInference. Morgan Kaufmann.\nPearl, Judea. 2009. Causality: Models, Reasoning and Inference . 2nd edn. Cambridge\nUniversity Press.\nPearson, Karl. 1895. Contributions to the Mathematical Theory of Evolution. II. Skew\nVariation in Homogeneous Material. Philosophical Transactions of the Royal Society\nA: Mathematical, Physical and Engineering Sciences, 186, 343\u2013414.\nPearson, Karl. 1901. On Lines and Planes of Closest Fit to Systems of Points in Space.\nPhilosophical Magazine, 2(11), 559\u2013572.\nPeters, Jonas, Janzing, Dominik, and Sch \u00a8olkopf, Bernhard. 2017. Elements of Causal\nInference: Foundations and Learning Algorithms. MIT Press.\nPetersen, Kaare B., and Pedersen, Michael S. 2012. The Matri"
  },
  {
    "vector_id": 1655,
    "chunk_id": "p408_c1",
    "page_number": 408,
    "text": "rs, Jonas, Janzing, Dominik, and Sch \u00a8olkopf, Bernhard. 2017. Elements of Causal\nInference: Foundations and Learning Algorithms. MIT Press.\nPetersen, Kaare B., and Pedersen, Michael S. 2012. The Matrix Cookbook. Tech. rept.\nTechnical University of Denmark.\nPlatt, John C. 2000. Probabilistic Outputs for Support Vector Machines and Compar-\nisons to Regularized Likelihood Methods. In: Advances in Large Margin Classifiers.\nPollard, David. 2002. A User\u2019s Guide to Measure Theoretic Probability . Cambridge\nUniversity Press.\nPolyak, Roman A. 2016. The Legendre Transformation in Modern Optimization. Pages\n437\u2013507 of: Goldengorin, B. (ed), Optimization and Its Applications in Control and\nData Sciences. Springer.\nPress, William H., Teukolsky , Saul A., Vetterling, William T., and Flannery , Brian P."
  },
  {
    "vector_id": 1656,
    "chunk_id": "p408_c2",
    "page_number": 408,
    "text": "ges\n437\u2013507 of: Goldengorin, B. (ed), Optimization and Its Applications in Control and\nData Sciences. Springer.\nPress, William H., Teukolsky , Saul A., Vetterling, William T., and Flannery , Brian P.\n2007. Numerical Recipes: The Art of Scientific Computing . Cambridge University\nPress.\nProschan, Michael A., and Presnell, Brett. 1998. Expect the Unexpected from Condi-\ntional Expectation. American Statistician, 52(3), 248\u2013252.\nRaschka, Sebastian, and Mirjalili, Vahid. 2017. Python Machine Learning: Machine\nLearning and Deep Learning with Python, scikit-learn, and TensorFlow. Packt Publish-\ning.\nRasmussen, Carl E., and Ghahramani, Zoubin. 2001. Occam\u2019s Razor. In: Advances in\nNeural Information Processing Systems.\nRasmussen, Carl E., and Ghahramani, Zoubin. 2003. Bayesian Monte Carlo. In: Ad-"
  },
  {
    "vector_id": 1657,
    "chunk_id": "p408_c3",
    "page_number": 408,
    "text": "Rasmussen, Carl E., and Ghahramani, Zoubin. 2001. Occam\u2019s Razor. In: Advances in\nNeural Information Processing Systems.\nRasmussen, Carl E., and Ghahramani, Zoubin. 2003. Bayesian Monte Carlo. In: Ad-\nvances in Neural Information Processing Systems.\nRasmussen, Carl E., and Williams, Christopher K. I. 2006. Gaussian Processes for Ma-\nchine Learning. MIT Press.\nReid, Mark, and Williamson, Robert C. 2011. Information, Divergence and Risk for\nBinary Experiments. Journal of Machine Learning Research, 12, 731\u2013817.\nRifkin, Ryan M., and Lippert, Ross A. 2007. Value Regularization and Fenchel Duality .\nJournal of Machine Learning Research, 8, 441\u2013479.\nRockafellar, Ralph T. 1970. Convex Analysis. Princeton University Press.\nRogers, Simon, and Girolami, Mark. 2016. A First Course in Machine Learning."
  },
  {
    "vector_id": 1658,
    "chunk_id": "p408_c4",
    "page_number": 408,
    "text": "Journal of Machine Learning Research, 8, 441\u2013479.\nRockafellar, Ralph T. 1970. Convex Analysis. Princeton University Press.\nRogers, Simon, and Girolami, Mark. 2016. A First Course in Machine Learning. Chap-\nman and Hall/CRC.\nRosenbaum, Paul R. 2017. Observation and Experiment: An Introduction to Causal\nInference. Harvard University Press.\nRosenblatt, Murray . 1956. Remarks on Some Nonparametric Estimates of a Density\nFunction. Annals of Mathematical Statistics, 27(3), 832\u2013837.\nRoweis, Sam T. 1998. EM Algorithms for PCA and SPCA. Pages 626\u2013632 of: Advances\nin Neural Information Processing Systems.\nRoweis, Sam T., and Ghahramani, Zoubin. 1999. A Unifying Review of Linear Gaussian\nModels. Neural Computation, 11(2), 305\u2013345.\nRoy , Anindya, and Banerjee, Sudipto. 2014. Linear Algebra and Matrix"
  },
  {
    "vector_id": 1659,
    "chunk_id": "p408_c5",
    "page_number": 408,
    "text": "s.\nRoweis, Sam T., and Ghahramani, Zoubin. 1999. A Unifying Review of Linear Gaussian\nModels. Neural Computation, 11(2), 305\u2013345.\nRoy , Anindya, and Banerjee, Sudipto. 2014. Linear Algebra and Matrix Analysis for\nStatistics. Chapman and Hall/CRC.\nRubinstein, Reuven Y., and Kroese, Dirk P. 2016. Simulation and the Monte Carlo\nMethod. Wiley .\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1660,
    "chunk_id": "p409_c0",
    "page_number": 409,
    "text": "References 403\nRuffini, Paolo. 1799. Teoria Generale delle Equazioni, in cui si Dimostra Impossibile la\nSoluzione Algebraica delle Equazioni Generali di Grado Superiore al Quarto. Stampe-\nria di S. Tommaso d\u2019Aquino.\nRumelhart, David E., Hinton, Geoffrey E., and Williams, Ronald J. 1986. Learning\nRepresentations by Back-Propagating Errors. Nature, 323(6088), 533\u2013536.\nS\u00e6mundsson, Steind\u00b4or, Hofmann, Katja, and Deisenroth, Marc P. 2018. Meta Rein-\nforcement Learning with Latent Variable Gaussian Processes. In: Proceedings of the\nConference on Uncertainty in Artificial Intelligence.\nSaitoh, Saburou. 1988. Theory of Reproducing Kernels and its Applications . Longman\nScientific and Technical.\nS\u00a8arkk\u00a8a, Simo. 2013. Bayesian Filtering and Smoothing. Cambridge University Press.\nSch\u00a8olkopf, Bernhard"
  },
  {
    "vector_id": 1661,
    "chunk_id": "p409_c1",
    "page_number": 409,
    "text": "u. 1988. Theory of Reproducing Kernels and its Applications . Longman\nScientific and Technical.\nS\u00a8arkk\u00a8a, Simo. 2013. Bayesian Filtering and Smoothing. Cambridge University Press.\nSch\u00a8olkopf, Bernhard, and Smola, Alexander J. 2002. Learning with Kernels \u2013 Support\nVector Machines, Regularization, Optimization, and Beyond. MIT Press.\nSch\u00a8olkopf, Bernhard, Smola, Alexander J., and M \u00a8uller, Klaus-Robert. 1997. Kernel\nPrincipal Component Analysis. In: Proceedings of the International Conference on\nArtificial Neural Networks.\nSch\u00a8olkopf, Bernhard, Smola, Alexander J., and M\u00a8uller, Klaus-Robert. 1998. Nonlinear\nComponent Analysis as a Kernel Eigenvalue Problem. Neural Computation, 10(5),\n1299\u20131319.\nSch\u00a8olkopf, Bernhard, Herbrich, Ralf, and Smola, Alexander J. 2001. A Generalized\nRepresenter Theo"
  },
  {
    "vector_id": 1662,
    "chunk_id": "p409_c2",
    "page_number": 409,
    "text": "8. Nonlinear\nComponent Analysis as a Kernel Eigenvalue Problem. Neural Computation, 10(5),\n1299\u20131319.\nSch\u00a8olkopf, Bernhard, Herbrich, Ralf, and Smola, Alexander J. 2001. A Generalized\nRepresenter Theorem. In: Proceedings of the International Conference on Computa-\ntional Learning Theory.\nSchwartz, Laurent. 1964. Sous Espaces Hilbertiens d\u2019Espaces Vectoriels Topologiques\net Noyaux Associ\u00b4es. Journal d\u2019Analyse Math\u00b4ematique, 13, 115\u2013256.\nSchwarz, Gideon E. 1978. Estimating the Dimension of a Model. Annals of Statistics,\n6(2), 461\u2013464.\nShahriari, Bobak, Swersky , Kevin, Wang, Ziyu, Adams, Ryan P., and De Freitas, Nando.\n2016. Taking the Human out of the Loop: A Review of Bayesian Optimization.\nProceedings of the IEEE, 104(1), 148\u2013175.\nShalev-Shwartz, Shai, and Ben-David, Shai. 2014. Understan"
  },
  {
    "vector_id": 1663,
    "chunk_id": "p409_c3",
    "page_number": 409,
    "text": ", and De Freitas, Nando.\n2016. Taking the Human out of the Loop: A Review of Bayesian Optimization.\nProceedings of the IEEE, 104(1), 148\u2013175.\nShalev-Shwartz, Shai, and Ben-David, Shai. 2014. Understanding Machine Learning:\nFrom Theory to Algorithms. Cambridge University Press.\nShawe-Taylor, John, and Cristianini, Nello. 2004. Kernel Methods for Pattern Analysis.\nCambridge University Press.\nShawe-Taylor, John, and Sun, Shiliang. 2011. A Review of Optimization Methodologies\nin Support Vector Machines. Neurocomputing, 74(17), 3609\u20133618.\nShental, Ori, Siegel, Paul H., Wolf, Jack K., Bickson, Danny , and Dolev, Danny . 2008.\nGaussian Belief Propagation Solver for Systems of Linear Equations. Pages 1863\u2013\n1867 of: Proceedings of the International Symposium on Information Theory.\nShewchuk, Jonatha"
  },
  {
    "vector_id": 1664,
    "chunk_id": "p409_c4",
    "page_number": 409,
    "text": ", and Dolev, Danny . 2008.\nGaussian Belief Propagation Solver for Systems of Linear Equations. Pages 1863\u2013\n1867 of: Proceedings of the International Symposium on Information Theory.\nShewchuk, Jonathan R. 1994. An Introduction to the Conjugate Gradient Method with-\nout the Agonizing Pain.\nShi, Jianbo, and Malik, Jitendra. 2000. Normalized Cuts and Image Segmentation.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8), 888\u2013905.\nShi, Qinfeng, Petterson, James, Dror, Gideon, Langford, John, Smola, Alexander J.,\nand Vishwanathan, S. V. N. 2009. Hash Kernels for Structured Data. Journal of\nMachine Learning Research, 2615\u20132637.\nShiryayev, Albert N. 1984. Probability. Springer.\nShor, Naum Z. 1985. Minimization Methods for Non-Differentiable Functions. Springer.\nShotton, Jamie,"
  },
  {
    "vector_id": 1665,
    "chunk_id": "p409_c5",
    "page_number": 409,
    "text": "Journal of\nMachine Learning Research, 2615\u20132637.\nShiryayev, Albert N. 1984. Probability. Springer.\nShor, Naum Z. 1985. Minimization Methods for Non-Differentiable Functions. Springer.\nShotton, Jamie, Winn, John, Rother, Carsten, and Criminisi, Antonio. 2006. Texton-\nBoost: Joint Appearance, Shape and Context Modeling for Multi-Class Object Recog-\nnition and Segmentation. In: Proceedings of the European Conference on Computer\nVision.\nSmith, Adrian F. M., and Spiegelhalter, David. 1980. Bayes Factors and Choice Criteria\nfor Linear Models. Journal of the Royal Statistical Society B, 42(2), 213\u2013220.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1666,
    "chunk_id": "p409_c6",
    "page_number": 409,
    "text": "0.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1667,
    "chunk_id": "p410_c0",
    "page_number": 410,
    "text": "404 References\nSnoek, Jasper, Larochelle, Hugo, and Adams, Ryan P. 2012. Practical Bayesian Op-\ntimization of Machine Learning Algorithms. In: Advances in Neural Information\nProcessing Systems.\nSpearman, Charles. 1904. \u201cGeneral Intelligence,\u201d Objectively Determined and Mea-\nsured. American Journal of Psychology, 15(2), 201\u2013292.\nSriperumbudur, Bharath K., Gretton, Arthur, Fukumizu, Kenji, Sch \u00a8olkopf, Bernhard,\nand Lanckriet, Gert R. G. 2010. Hilbert Space Embeddings and Metrics on Proba-\nbility Measures. Journal of Machine Learning Research, 11, 1517\u20131561.\nSteinwart, Ingo. 2007. How to Compare Different Loss Functions and Their Risks.\nConstructive Approximation, 26, 225\u2013287.\nSteinwart, Ingo, and Christmann, Andreas. 2008. Support Vector Machines. Springer.\nStoer, Josef, and Burlirsch, Rola"
  },
  {
    "vector_id": 1668,
    "chunk_id": "p410_c1",
    "page_number": 410,
    "text": "Different Loss Functions and Their Risks.\nConstructive Approximation, 26, 225\u2013287.\nSteinwart, Ingo, and Christmann, Andreas. 2008. Support Vector Machines. Springer.\nStoer, Josef, and Burlirsch, Roland. 2002.Introduction to Numerical Analysis. Springer.\nStrang, Gilbert. 1993. The Fundamental Theorem of Linear Algebra. The American\nMathematical Monthly, 100(9), 848\u2013855.\nStrang, Gilbert. 2003. Introduction to Linear Algebra. Wellesley-Cambridge Press.\nStray , Jonathan. 2016.The Curious Journalist\u2019s Guide to Data . Tow Center for Digital\nJournalism at Columbia\u2019s Graduate School of Journalism.\nStrogatz, Steven. 2014. Writing about Math for the Perplexed and the Traumatized.\nNotices of the American Mathematical Society, 61(3), 286\u2013291.\nSucar, Luis E., and Gillies, Duncan F. 1994. Probabilistic"
  },
  {
    "vector_id": 1669,
    "chunk_id": "p410_c2",
    "page_number": 410,
    "text": "rogatz, Steven. 2014. Writing about Math for the Perplexed and the Traumatized.\nNotices of the American Mathematical Society, 61(3), 286\u2013291.\nSucar, Luis E., and Gillies, Duncan F. 1994. Probabilistic Reasoning in High-Level\nVision. Image and Vision Computing, 12(1), 42\u201360.\nSzeliski, Richard, Zabih, Ramin, and Scharstein, Daniel, et al. 2008. A Compar-\native Study of Energy Minimization Methods for Markov Random Fields with\nSmoothness-Based Priors. IEEE Transactions on Pattern Analysis and Machine In-\ntelligence, 30(6), 1068\u20131080.\nTandra, Haryono. 2014. The Relationship between the Change of Variable Theorem\nand the Fundamental Theorem of Calculus for the Lebesgue Integral. Teaching of\nMathematics, 17(2), 76\u201383.\nTenenbaum, Joshua B., De Silva, Vin, and Langford, John C. 2000. A Global Geom"
  },
  {
    "vector_id": 1670,
    "chunk_id": "p410_c3",
    "page_number": 410,
    "text": "ariable Theorem\nand the Fundamental Theorem of Calculus for the Lebesgue Integral. Teaching of\nMathematics, 17(2), 76\u201383.\nTenenbaum, Joshua B., De Silva, Vin, and Langford, John C. 2000. A Global Geometric\nFramework for Nonlinear Dimensionality Reduction. Science, 290(5500), 2319\u2013\n2323.\nTibshirani, Robert. 1996. Regression Selection and Shrinkage via the Lasso. Journal\nof the Royal Statistical Society B, 58(1), 267\u2013288.\nTipping, Michael E., and Bishop, Christopher M. 1999. Probabilistic Principal Compo-\nnent Analysis. Journal of the Royal Statistical Society: Series B, 61(3), 611\u2013622.\nTitsias, Michalis K., and Lawrence, Neil D. 2010. Bayesian Gaussian Process Latent\nVariable Model. In: Proceedings of the International Conference on Artificial Intelli-\ngence and Statistics.\nToussaint, Marc."
  },
  {
    "vector_id": 1671,
    "chunk_id": "p410_c4",
    "page_number": 410,
    "text": "Michalis K., and Lawrence, Neil D. 2010. Bayesian Gaussian Process Latent\nVariable Model. In: Proceedings of the International Conference on Artificial Intelli-\ngence and Statistics.\nToussaint, Marc. 2012. Some Notes on Gradient Descent . https://ipvs.informatik.uni-\nstuttgart.de/mlr/marc/notes/gradientDescent.pdf.\nTrefethen, Lloyd N., and Bau III, David. 1997. Numerical Linear Algebra. SIAM.\nTucker, Ledyard R. 1966. Some Mathematical Notes on Three-Mode Factor Analysis.\nPsychometrika, 31(3), 279\u2013311.\nVapnik, Vladimir N. 1998. Statistical Learning Theory. Wiley .\nVapnik, Vladimir N. 1999. An Overview of Statistical Learning Theory . IEEE Transac-\ntions on Neural Networks, 10(5), 988\u2013999.\nVapnik, Vladimir N. 2000. The Nature of Statistical Learning Theory. Springer.\nVishwanathan, S. V. N.,"
  },
  {
    "vector_id": 1672,
    "chunk_id": "p410_c5",
    "page_number": 410,
    "text": "Overview of Statistical Learning Theory . IEEE Transac-\ntions on Neural Networks, 10(5), 988\u2013999.\nVapnik, Vladimir N. 2000. The Nature of Statistical Learning Theory. Springer.\nVishwanathan, S. V. N., Schraudolph, Nicol N., Kondor, Risi, and Borgwardt,\nKarsten M. 2010. Graph Kernels. Journal of Machine Learning Research, 11, 1201\u2013\n1242.\nvon Luxburg, Ulrike, and Sch \u00a8olkopf, Bernhard. 2011. Statistical Learning Theory:\nModels, Concepts, and Results. Pages 651\u2013706 of: D. M. Gabbay , S. Hartmann,\nJ. Woods (ed), Handbook of the History of Logic, vol. 10. Elsevier.\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1673,
    "chunk_id": "p410_c6",
    "page_number": 410,
    "text": "s for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1674,
    "chunk_id": "p411_c0",
    "page_number": 411,
    "text": "References 405\nWahba, Grace. 1990. Spline Models for Observational Data. Society for Industrial and\nApplied Mathematics.\nWalpole, Ronald E., Myers, Raymond H., Myers, Sharon L., and Ye, Keying. 2011.\nProbability and Statistics for Engineers and Scientists. Prentice Hall.\nWasserman, Larry . 2004.All of Statistics. Springer.\nWasserman, Larry . 2007.All of Nonparametric Statistics. Springer.\nWhittle, Peter. 2000. Probability via Expectation. Springer.\nWickham, Hadley . 2014. Tidy Data.Journal of Statistical Software, 59, 1\u201323.\nWilliams, Christopher K. I. 1997. Computing with Infinite Networks. In: Advances in\nNeural Information Processing Systems.\nYu, Yaoliang, Cheng, Hao, Schuurmans, Dale, and Szepesv \u00b4ari, Csaba. 2013. Charac-\nterizing the Representer Theorem. In: Proceedings of the Interna"
  },
  {
    "vector_id": 1675,
    "chunk_id": "p411_c1",
    "page_number": 411,
    "text": ": Advances in\nNeural Information Processing Systems.\nYu, Yaoliang, Cheng, Hao, Schuurmans, Dale, and Szepesv \u00b4ari, Csaba. 2013. Charac-\nterizing the Representer Theorem. In: Proceedings of the International Conference on\nMachine Learning.\nZadrozny , Bianca, and Elkan, Charles. 2001. Obtaining Calibrated Probability Esti-\nmates from Decision Trees and Naive Bayesian Classifiers. In: Proceedings of the\nInternational Conference on Machine Learning.\nZhang, Haizhang, Xu, Yuesheng, and Zhang, Jun. 2009. Reproducing Kernel Banach\nSpaces for Machine Learning.Journal of Machine Learning Research, 10, 2741\u20132775.\nZia, Royce K. P., Redish, Edward F., and McKay , Susan R. 2009. Making Sense of the\nLegendre Transform. American Journal of Physics, 77(614), 614\u2013622.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C"
  },
  {
    "vector_id": 1676,
    "chunk_id": "p411_c2",
    "page_number": 411,
    "text": "741\u20132775.\nZia, Royce K. P., Redish, Edward F., and McKay , Susan R. 2009. Making Sense of the\nLegendre Transform. American Journal of Physics, 77(614), 614\u2013622.\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1677,
    "chunk_id": "p413_c0",
    "page_number": 413,
    "text": "Index\n1-of-K representation, 364\n\u21131 norm, 71\n\u21132 norm, 72\nabduction, 258\nAbel-Ruffini theorem, 334\nAbelian group, 36\nabsolutely homogeneous, 71\nactivation function, 315\naffine mapping, 63\naffine subspace, 61\nAkaike information criterion, 288\nalgebra, 17\nalgebraic multiplicity , 106\nanalytic, 143\nancestral sampling, 340, 364\nangle, 76\nassociativity , 24, 26, 36\nattribute, 253\naugmented matrix, 29\nauto-encoder, 343\nautomatic differentiation, 161\nautomorphism, 49\nbackpropagation, 159\nbasic variable, 30\nbasis, 44\nbasis vector, 45\nBayes factor, 287\nBayes\u2019 law, 185\nBayes\u2019 rule, 185\nBayes\u2019 theorem, 185\nBayesian GP-LVM, 347\nBayesian inference, 274\nBayesian information criterion, 288\nBayesian linear regression, 303\nBayesian model selection, 286\nBayesian network, 278, 283\nBayesian PCA, 346\nBernoulli"
  },
  {
    "vector_id": 1678,
    "chunk_id": "p413_c1",
    "page_number": 413,
    "text": "5\nBayesian GP-LVM, 347\nBayesian inference, 274\nBayesian information criterion, 288\nBayesian linear regression, 303\nBayesian model selection, 286\nBayesian network, 278, 283\nBayesian PCA, 346\nBernoulli distribution, 205\nBeta distribution, 206\nbilinear mapping, 72\nbijective, 48\nbinary classification, 370\nBinomial distribution, 206\nblind-source separation, 346\nBorel \u03c3-algebra, 180\ncanonical basis, 45\ncanonical feature map, 389\ncanonical link function, 315\ncategorical variable, 180\nCauchy-Schwarz inequality , 75\nchange-of-variable technique, 219\ncharacteristic polynomial, 104\nCholesky decomposition, 114\nCholesky factor, 114\nCholesky factorization, 114\nclass, 370\nclassification, 315\nclosure, 36\ncode, 343\ncodirected, 105\ncodomain, 58, 139\ncollinear, 105\ncolumn, 22\ncolumn space, 59\ncolumn vector,"
  },
  {
    "vector_id": 1679,
    "chunk_id": "p413_c2",
    "page_number": 413,
    "text": ", 114\nCholesky factor, 114\nCholesky factorization, 114\nclass, 370\nclassification, 315\nclosure, 36\ncode, 343\ncodirected, 105\ncodomain, 58, 139\ncollinear, 105\ncolumn, 22\ncolumn space, 59\ncolumn vector, 22, 38\ncompleting the squares, 307\nconcave function, 236\ncondition number, 230\nconditional probability , 179\nconditionally independent, 195\nconjugate, 208\nconjugate prior, 208\nconvex conjugate, 242\nconvex function, 236\nconvex hull, 386\nconvex optimization problem, 236, 239\nconvex set, 236\ncoordinate, 50\ncoordinate representation, 50\ncoordinate vector, 50\ncorrelation, 191\ncovariance, 190\ncovariance matrix, 190, 198\ncovariate, 253\nCP decomposition, 136\ncross-covariance, 191\ncross-validation, 258, 263\ncumulative distribution function, 178,\n181\nd-separation, 281\n407\nThis material is published by C"
  },
  {
    "vector_id": 1680,
    "chunk_id": "p413_c3",
    "page_number": 413,
    "text": "matrix, 190, 198\ncovariate, 253\nCP decomposition, 136\ncross-covariance, 191\ncross-validation, 258, 263\ncumulative distribution function, 178,\n181\nd-separation, 281\n407\nThis material is published by Cambridge University Press asMathematics for Machine Learning by\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong (2020). This version is free to view\nand download for personal use only . Not for re-distribution, re-sale, or use in derivative works.\n\u00a9by M. P. Deisenroth, A. A. Faisal, and C. S. Ong, 2024.https://mml-book.com."
  },
  {
    "vector_id": 1681,
    "chunk_id": "p414_c0",
    "page_number": 414,
    "text": "408 Index\ndata covariance matrix, 318\ndata point, 253\ndata-fit term, 302\ndecoder, 343\ndeep auto-encoder, 347\ndefective, 111\ndenominator layout, 151\nderivative, 141\ndesign matrix, 294, 296\ndeterminant, 99\ndiagonal matrix, 115\ndiagonalizable, 116\ndiagonalization, 116\ndifference quotient, 141\ndimension, 45\ndimensionality reduction, 317\ndirected graphical model, 278, 283\ndirection, 61\ndirection space, 61\ndistance, 75\ndistribution, 177\ndistributivity , 24, 26\ndomain, 58, 139\ndot product, 72\ndual SVM, 385\nEckart-Young theorem, 131, 334\neigendecomposition, 116\neigenspace, 106\neigenspectrum, 106\neigenvalue, 105\neigenvalue equation, 105\neigenvector, 105\nelementary transformations, 28\nEM algorithm, 360\nembarrassingly parallel, 264\nempirical covariance, 192\nempirical mean, 192\nempirical risk, 260\nemp"
  },
  {
    "vector_id": 1682,
    "chunk_id": "p414_c1",
    "page_number": 414,
    "text": "value, 105\neigenvalue equation, 105\neigenvector, 105\nelementary transformations, 28\nEM algorithm, 360\nembarrassingly parallel, 264\nempirical covariance, 192\nempirical mean, 192\nempirical risk, 260\nempirical risk minimization, 257, 260\nencoder, 343\nendomorphism, 49\nepigraph, 236\nequivalent, 56\nerror function, 294\nerror term, 382\nEuclidean distance, 72, 75\nEuclidean norm, 72\nEuclidean vector space, 73\nevent space, 175\nevidence, 186, 285, 306\nexample, 253\nexpected risk, 261\nexpected value, 187\nexponential family , 205, 211\nextended Kalman filter, 170\nfactor analysis, 346\nfactor graph, 283\nfeature, 253\nfeature map, 254\nfeature matrix, 296\nfeature vector, 295\nFisher discriminant analysis, 136\nFisher-Neyman theorem, 210\nforward mode, 161\nfree variable, 30\nfull rank, 47\nfull SVD, 128\nfundamental"
  },
  {
    "vector_id": 1683,
    "chunk_id": "p414_c2",
    "page_number": 414,
    "text": ", 253\nfeature map, 254\nfeature matrix, 296\nfeature vector, 295\nFisher discriminant analysis, 136\nFisher-Neyman theorem, 210\nforward mode, 161\nfree variable, 30\nfull rank, 47\nfull SVD, 128\nfundamental theorem of linear\nmappings, 60\nGaussian elimination, 31\nGaussian mixture model, 349\nGaussian process, 316\nGaussian process latent-variable model,\n347\ngeneral linear group, 37\ngeneral solution, 28, 30\ngeneralized linear model, 272, 315\ngenerating set, 44\ngenerative process, 272, 286\ngenerator, 344\ngeometric multiplicity , 108\nGivens rotation, 94\nglobal minimum, 225\nGP-LVM, 347\ngradient, 146\nGram matrix, 389\nGram-Schmidt orthogonalization, 89\ngraphical model, 278\ngroup, 36\nHadamard product, 23\nhard margin SVM, 377\nHessian, 164\nHessian eigenmaps, 136\nHessian matrix, 165\nhinge loss, 381\nhistogram,"
  },
  {
    "vector_id": 1684,
    "chunk_id": "p414_c3",
    "page_number": 414,
    "text": "trix, 389\nGram-Schmidt orthogonalization, 89\ngraphical model, 278\ngroup, 36\nHadamard product, 23\nhard margin SVM, 377\nHessian, 164\nHessian eigenmaps, 136\nHessian matrix, 165\nhinge loss, 381\nhistogram, 369\nhyperparameter, 258\nhyperplane, 61, 62\nhyperprior, 281\ni.i.d., 195\nICA, 346\nidentity automorphism, 49\nidentity mapping, 49\nidentity matrix, 23\nimage, 58, 139\nindependent and identically distributed,\n195, 260, 266\nindependent component analysis, 346\ninference network, 344\ninjective, 48\ninner product, 73\ninner product space, 73\nintermediate variables, 162\ninverse, 24\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1685,
    "chunk_id": "p414_c4",
    "page_number": 414,
    "text": "ematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1686,
    "chunk_id": "p415_c0",
    "page_number": 415,
    "text": "Index 409\ninverse element, 36\ninvertible, 24\nIsomap, 136\nisomorphism, 49\nJacobian, 146, 150\nJacobian determinant, 152\nJeffreys-Lindley paradox, 287\nJensen\u2019s inequality , 239\njoint probability , 178\nKarhunen-Lo`eve transform, 318\nkernel, 33, 47, 58, 254, 388\nkernel density estimation, 369\nkernel matrix, 389\nkernel PCA, 347\nkernel trick, 316, 347, 389\nlabel, 253\nLagrange multiplier, 234\nLagrangian, 234\nLagrangian dual problem, 234\nLaplace approximation, 170\nLaplace expansion, 102\nLaplacian eigenmaps, 136\nLASSO, 303, 316\nlatent variable, 275\nlaw, 177, 181\nlaw of total variance, 203\nleading coefficient, 30\nleast-squares loss, 154\nleast-squares problem, 261\nleast-squares solution, 88\nleft-singular vectors, 119\nLegendre transform, 242\nLegendre-Fenchel transform, 242\nlength, 71\nlikelihood, 185, 2"
  },
  {
    "vector_id": 1687,
    "chunk_id": "p415_c1",
    "page_number": 415,
    "text": "cient, 30\nleast-squares loss, 154\nleast-squares problem, 261\nleast-squares solution, 88\nleft-singular vectors, 119\nLegendre transform, 242\nLegendre-Fenchel transform, 242\nlength, 71\nlikelihood, 185, 265, 269, 291\nline, 61, 82\nlinear combination, 40\nlinear manifold, 61\nlinear mapping, 48\nlinear program, 239\nlinear subspace, 39\nlinear transformation, 48\nlinearly dependent, 40\nlinearly independent, 40\nlink function, 272\nloading, 322\nlocal minimum, 225\nlog-partition function, 211\nlogistic regression, 315\nlogistic sigmoid, 315\nloss function, 260, 381\nloss term, 382\nlower-triangular matrix, 101\nMaclaurin series, 143\nManhattan norm, 71\nMAP, 300\nMAP estimation, 269\nmargin, 374\nmarginal, 190\nmarginal likelihood, 186, 286, 306\nmarginal probability , 179\nmarginalization property , 184\nMarkov random f"
  },
  {
    "vector_id": 1688,
    "chunk_id": "p415_c2",
    "page_number": 415,
    "text": "aurin series, 143\nManhattan norm, 71\nMAP, 300\nMAP estimation, 269\nmargin, 374\nmarginal, 190\nmarginal likelihood, 186, 286, 306\nmarginal probability , 179\nmarginalization property , 184\nMarkov random field, 283\nmatrix, 22\nmatrix factorization, 98\nmaximum a posteriori, 300\nmaximum a posteriori estimation, 269\nmaximum likelihood, 257\nmaximum likelihood estimate, 296\nmaximum likelihood estimation, 265,\n293\nmean, 187\nmean function, 309\nmean vector, 198\nmeasure, 180\nmedian, 188\nmetric, 76\nminimal, 44\nminimax inequality , 234\nmisfit term, 302\nmixture model, 349\nmixture weight, 349\nmode, 188\nmodel, 251\nmodel evidence, 286\nmodel selection, 258\nMoore-Penrose pseudo-inverse, 35\nmultidimensional scaling, 136\nmultiplication by scalars, 37\nmultivariate, 178\nmultivariate Gaussian distribution, 198\nmultiv"
  },
  {
    "vector_id": 1689,
    "chunk_id": "p415_c3",
    "page_number": 415,
    "text": "1\nmodel evidence, 286\nmodel selection, 258\nMoore-Penrose pseudo-inverse, 35\nmultidimensional scaling, 136\nmultiplication by scalars, 37\nmultivariate, 178\nmultivariate Gaussian distribution, 198\nmultivariate Taylor series, 166\nnatural parameters, 212\nnegative log-likelihood, 265\nnested cross-validation, 258, 284\nneutral element, 36\nnoninvertible, 24\nnonsingular, 24\nnorm, 71\nnormal distribution, 197\nnormal equation, 86\nnormal vector, 80\nnull space, 33, 47, 58\nnumerator layout, 150\nOccam\u2019s razor, 285\nONB, 79\none-hot encoding, 364\nordered basis, 50\northogonal, 77\northogonal basis, 79\northogonal complement, 79\northogonal matrix, 78\northonormal, 77\northonormal basis, 79\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1690,
    "chunk_id": "p415_c4",
    "page_number": 415,
    "text": "mplement, 79\northogonal matrix, 78\northonormal, 77\northonormal basis, 79\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  },
  {
    "vector_id": 1691,
    "chunk_id": "p416_c0",
    "page_number": 416,
    "text": "410 Index\nouter product, 38\noverfitting, 262, 271, 299\nPageRank, 114\nparameters, 61\nparametric equation, 61\npartial derivative, 146\nparticular solution, 27, 30\nPCA, 317\npdf, 181\npenalty term, 263\npivot, 30\nplane, 62\nplate, 281\npopulation mean and covariance, 191\npositive definite, 71, 73, 74, 76\nposterior, 185, 269\nposterior odds, 287\npower iteration, 334\npower series representation, 145\nPPCA, 340\npreconditioner, 230\npredictor, 12, 255\nprimal problem, 234\nprincipal component, 322\nprincipal component analysis, 136, 317\nprincipal subspace, 327\nprior, 185, 269\nprior odds, 287\nprobabilistic inverse, 186\nprobabilistic PCA, 340\nprobabilistic programming, 278\nprobability , 175\nprobability density function, 181\nprobability distribution, 172\nprobability integral transform, 217\nprobability mass func"
  },
  {
    "vector_id": 1692,
    "chunk_id": "p416_c1",
    "page_number": 416,
    "text": "e, 186\nprobabilistic PCA, 340\nprobabilistic programming, 278\nprobability , 175\nprobability density function, 181\nprobability distribution, 172\nprobability integral transform, 217\nprobability mass function, 178\nproduct rule, 184\nprojection, 82\nprojection error, 88\nprojection matrix, 82\npseudo-inverse, 86\nrandom variable, 172, 175\nrange, 58\nrank, 47\nrank deficient, 47\nrank-k approximation, 130\nrank-nullity theorem, 60\nraw-score formula for variance, 193\nrecognition network, 344\nreconstruction error, 88, 327\nreduced hull, 388\nreduced row-echelon form, 31\nreduced SVD, 129\nREF, 30\nregression, 289\nregular, 24\nregularization, 262, 302, 382\nregularization parameter, 263, 302, 380\nregularized least squares, 302\nregularizer, 263, 302, 380, 382\nrepresenter theorem, 384\nresponsibility , 352\nreverse mo"
  },
  {
    "vector_id": 1693,
    "chunk_id": "p416_c2",
    "page_number": 416,
    "text": "egular, 24\nregularization, 262, 302, 382\nregularization parameter, 263, 302, 380\nregularized least squares, 302\nregularizer, 263, 302, 380, 382\nrepresenter theorem, 384\nresponsibility , 352\nreverse mode, 161\nright-singular vectors, 119\nRMSE, 298\nroot mean square error, 298\nrotation, 91\nrotation matrix, 92\nrow, 22\nrow vector, 22, 38\nrow-echelon form, 30\nsample mean, 192\nsample space, 175\nscalar, 37\nscalar product, 72\nsigmoid, 213\nsimilar, 56\nsingular, 24\nsingular value decomposition, 119\nsingular value equation, 124\nsingular value matrix, 119\nsingular values, 119\nslack variable, 379\nsoft margin SVM, 379, 380\nsolution, 20\nspan, 44\nspecial solution, 27\nspectral clustering, 136\nspectral norm, 131\nspectral theorem, 111\nspectrum, 106\nsquare matrix, 25\nstandard basis, 45\nstandard deviation, 190\ns"
  },
  {
    "vector_id": 1694,
    "chunk_id": "p416_c3",
    "page_number": 416,
    "text": "SVM, 379, 380\nsolution, 20\nspan, 44\nspecial solution, 27\nspectral clustering, 136\nspectral norm, 131\nspectral theorem, 111\nspectrum, 106\nsquare matrix, 25\nstandard basis, 45\nstandard deviation, 190\nstandard normal distribution, 198\nstandardization, 336\nstatistical independence, 194\nstatistical learning theory , 265\nstochastic gradient descent, 231\nstrong duality , 236\nsufficient statistics, 210\nsum rule, 184\nsupport point, 61\nsupport vector, 384\nsupporting hyperplane, 242\nsurjective, 48\nSVD, 119\nSVD theorem, 119\nsymmetric, 73, 76\nsymmetric matrix, 25\nsymmetric, positive definite, 74\nsymmetric, positive semidefinite, 74\nsystem of linear equations, 20\ntarget space, 175\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1695,
    "chunk_id": "p416_c4",
    "page_number": 416,
    "text": ", positive semidefinite, 74\nsystem of linear equations, 20\ntarget space, 175\nDraft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback:https://mml-book.com."
  },
  {
    "vector_id": 1696,
    "chunk_id": "p417_c0",
    "page_number": 417,
    "text": "Index 411\nTaylor polynomial, 142, 166\nTaylor series, 142\ntest error, 300\ntest set, 262, 284\nTikhonov regularization, 265\ntrace, 103\ntraining, 12\ntraining error, 300\ntraining set, 260, 292\ntransfer function, 315\ntransformation matrix, 51\ntranslation vector, 63\ntranspose, 25, 38\ntriangle inequality , 71, 76\ntruncated SVD, 129\nTucker decomposition, 136\nunderfitting, 271\nundirected graphical model, 283\nuniform distribution, 182\nunivariate, 178\nunscented transform, 170\nupper-triangular matrix, 101\nvalidation set, 263, 284\nvariable selection, 316\nvariance, 190\nvector, 37\nvector addition, 37\nvector space, 37\nvector space homomorphism, 48\nvector space with inner product, 73\nvector subspace, 39\nweak duality , 235\nzero-one loss, 381\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambr"
  },
  {
    "vector_id": 1697,
    "chunk_id": "p417_c1",
    "page_number": 417,
    "text": "pace, 37\nvector space homomorphism, 48\nvector space with inner product, 73\nvector subspace, 39\nweak duality , 235\nzero-one loss, 381\n\u00a92024 M. P. Deisenroth, A. A. Faisal, C. S. Ong. Published by Cambridge University Press (2020)."
  }
]